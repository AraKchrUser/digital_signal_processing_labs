{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9199231",
   "metadata": {},
   "source": [
    "**Лабораторный практикум по курсу «Распознавание диктора», Университет ИТМО, 2021**\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dd7db",
   "metadata": {},
   "source": [
    "**Лабораторная работа №3. Построение дикторских моделей и их сравнение**\n",
    "\n",
    "**Цель работы:** изучение процедуры построения дикторских моделей с использованием глубоких нейросетевых архитектур.\n",
    "\n",
    "**Краткое описание:** в рамках настоящей лабораторной работы предлагается изучить и реализовать схему построения дикторских моделей с использованием глубокой нейросетевой архитектуры, построенной на основе ResNet-блоков. Процедуры обучения и тестирования предлагается рассмотреть по отношению к задаче идентификации на закрытом множестве, то есть для ситуации, когда дикторские классы являются строго заданными. Тестирование полученной системы предполагает использование доли правильных ответов (accuracy) в качестве целевой метрики оценки качества.\n",
    "\n",
    "**Данные:** в качестве данных для выполнения лабораторной работы предлагается использовать базу [VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html).\n",
    "\n",
    "**Содержание лабораторной работы**\n",
    "\n",
    "1. Подготовка данных для обучения и тестирования блока построения дикторских моделей.\t\t\t\t\t\t\t\n",
    "\n",
    "2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных.\n",
    "\n",
    "3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных.\n",
    "\n",
    "4. Тестированное блока построения дикторских моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f862cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython extension to reload modules before executing user code\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import of modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import torch.profiler\n",
    "\n",
    "from common import download_dataset, concatenate, extract_dataset, part_extract, download_protocol, split_musan\n",
    "from exercises_blank import train_dataset_loader, test_dataset_loader, ResNet, MainModel, train_network, test_network\n",
    "from ResNetBlocks import BasicBlock\n",
    "from LossFunction import AAMSoftmaxLoss\n",
    "from Optimizer import SGDOptimizer\n",
    "from Scheduler import OneCycleLRScheduler\n",
    "from load_save_pth import saveParameters, loadParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3baf71",
   "metadata": {},
   "source": [
    "**1. Подготовка данных для обучения и тестирования детектора речевой активности**\n",
    "\n",
    "В ходе выполнения лабораторной работы необходимы данные для выполнения процедуры обучения и процедуры тестирования нейросетевого блока генерации дикторских моделей. Возьмём в качестве этих данных звукозаписи, сохраненные в формат *wav*, из корпуса [VoxCeleb1 dev set](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html). Данный корпус содержит 148,642 звукозаписи (частота дискретизации равна 16кГц) для 1,211 дикторов женского и мужского пола, разговаривающих преимущественно на английском языке.\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить загрузку и распаковку звуковых wav-файлов из корпуса VoxCeleb1 dev set.\n",
    "\n",
    "![Рисунок 1](https://analyticsindiamag.com/wp-content/uploads/2020/12/image.png \"VoxCeleb. Крупномасштабная аудиовизуальная база данных человеческой речи.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7ecdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download VoxCeleb1 (test set)\n",
    "# with open('../data/lists/datasets.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# download_dataset(lines, user='voxceleb1902', password='nx0bl2v2', save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fd3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Concatenate archives for VoxCeleb1 dev set\n",
    "# with open('../data/lists/concat_arch.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# concatenate(lines, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed3a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Extract VoxCeleb1 dev set\n",
    "# extract_dataset(save_path='../data/voxceleb1_dev', fname='../data/vox1_dev_wav.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875ec4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Download VoxCeleb1 identification protocol\n",
    "# with open('../data/lists/protocols.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# download_protocol(lines, save_path='../data/voxceleb1_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6aee",
   "metadata": {},
   "source": [
    "**2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных**\n",
    "\n",
    "Построение современных дикторских моделей, как правило, выполняется с использованием нейросетевых архитектур, многие из которых позаимствованы из области обработки цифровых изображений. Одними из наиболее распространенных нейросетевых архитектур, используемыми для построения дикторских моделей, являются [ResNet-подобные архитектуры](https://arxiv.org/pdf/1512.03385.pdf). В рамках настоящего пункта предлагается выполнить адаптацию нейросетевой архитектуры ResNet34 для решения задачи генерации дикторских моделей (дикторских эмбеддингов). *Дикторский эмбеддинг* – это высокоуровневый вектор-признаков, состоящий, например, из 128, 256 и т.п. значений, содержащий особенности голоса конкретного человека. При решении задачи распознавания диктора можно выделить эталонные и тестовые дикторские эмбеддинги. *Эталонные эмбеддинги* формируются на этапе регистрации дикторской модели определённого человека и находятся в некотором хранилище данных. *Тестовые эмбеддинги* формируются на этапе непосредственного использования системы голосовой биометрии на практике, когда некоторый пользователь пытается получить доступ к соответствующим ресурсам. Система голосовой биометрии сравнивает по определённой метрике эталонные и тестовые эмбеддинги, формируя оценку сравнения, которая, после её обработки блоком принятия решения, позволяет сделать вывод о том, эмбеддинги одинаковых или разных дикторов сравниваются между собой.\n",
    "\n",
    "Адаптация различных нейросетевых архитектур из обработки изображений к решению задачи построения дикторских моделей является непростой задачей. Возьмём за основу готовое решение, предложенной в рамках [следующей публикации](https://arxiv.org/pdf/2002.06033.pdf) и адаптируем его применительно к выполнению настоящей лабораторной работы.\n",
    "\n",
    "Необходимо отметить, что построение дикторских моделей, как правило, требует наличия *акустических признаков*, вычисленных для звукозаписей тренировочной, валидационной и тестовой баз данных. В качестве примера подобных признаков в рамках настоящей лабораторной работы воспользуемся *логарифмами энергий на выходе мел-банка фильтров*. Важно отметить, что акустические признаки подвергаются некоторым процедурам предобработки перед их непосредственной передачей в блок построения дикторских моделей. В качестве этих процедур можно выделить: нормализация и масштабирование признаков, сохранение только речевых фреймов на основе разметки детектора речевой активности и т.п.\n",
    "\n",
    "После того, как акустические признаки подготовлены, они могут быть переданы на блок построения дикторских моделей. Как правило, структура современных дикторских моделей соответствует структуре [x-векторных архитектур](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf). Эти архитектуры состоят из четырёх ключевых элементов: \n",
    "\n",
    "1. **Фреймовый уровень.** Предназначен для формирования локальных представлений голоса конкретного человека. На этом уровне как раз и применяются нейросетевые архитектуры на базе свёрточных нейронных сетей, например, ResNet, позволяющих с использованием каскадной схемы из множества фильтров с локальной маской захватить некоторый локальный контекст шаблона голоса человека. Выходом фреймового уровня является набор высокоуровневых представлений (карт-признаков), содержащих локальные особенности голоса человека.\n",
    "\n",
    "2. **Уровень статистического пулинга** позволяет сформировать промежуточный вектор-признаков, фиксированной длины, которая является одинаковой для звукозаписи любой длительности. В ходе работы блока статистического пулинга происходит удаление временной размерности, присутствующей в картах-признаков. Это достигается путём выполнения процедуры усреднения карт-признаков вдоль оси времени. Выходом уровня статистического пулинга являются вектор среднего и вектор среднеквадратического отклонения, вычисленные на основе карт-признаков. Эти вектора конкатенируются и передаются для дальнейшей обработки на сегментом уровне.\n",
    "\n",
    "3. **Сегментный уровень.** Предназначен для трансформации промежуточного вектора, как правило, высокой размерности, в компактный вектор-признаков, представляющий собой дикторский эмбеддинг. Необходимо отметить, что на сегментном уровне расположены один или несколько полносвязных нейросетевых слоёв, а обработка данных выполняется по отношению ко всей звукозаписи, а не только к некоторому её локальному контексту, как на фреймовом уровне.\n",
    "\n",
    "4. **Уровень выходного слоя.** Представляет полносвязный слой с softmax-функциями активации. Количество активаций равно числу дикторов в тренирочной выборке. На вход выходноя слоя подаётся дикторский эмбеддинг, а на выходе – формируется набор апостериорных вероятностей, определяющих принадлежность эмбеддинга к одному из дикторских классов в тренировочной выборке. Необходимо отметить, что, как правило, в современных нейросетевых системах построения дикторских моделей выходной используется только на этапе обучения параметров и на этапе тестирования не используется (на этапе тестирования используются только три первых уровня архитектуры).\n",
    "\n",
    "Обучение модели генерации дикторских эмбеддингов выполняется путём решения задачи *классификации* или, выражаясь терминами из области биометрии, *идентификации на закрытом множестве* (количество дикторских меток является строго фиксированным). В качестве используемой стоимостной функции выступает *категориальная кросс-энтропия*. Обучение выполняется с помощью мини-батчей, содержащих короткие фрагменты карт акустических признаков (длительностью несколько секунд) различных дикторов из тренировочной базы данных. Обучение на коротких фрагментов позволяет избежать сильного переобучения нейросетевой модели. При выполнении процедуры обучения требуется подобрать набор гиперпараметров, выбрать обучения и метод численной оптимизации.\n",
    "\n",
    "Для успешного выполнения настоящего пункта необходимо сделать следующее:\n",
    "\n",
    "1. Сгенерировать списки тренировочных, валидационных и тестовых данных на основе идентификационного протокола базы VoxCeleb1, содержащегося в файле **../data/voxceleb1_test/iden_split.txt**. При генерации списков требуется исключить из них звукозаписи дикторов, которые входят в базу [VoxCeleb1 test set](https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip). Это позволит выполнить тестирования обученных блоков генерации дикторских моделей на протоколе [VoxCeleb1-O cleaned](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt), который составлен по отношению к данным из VoxCeleb1 test set, в лабораторной работе №4.\n",
    "\n",
    "2. Инициализировать обучаемую дикторскую модель, выбрав любой возможный вариант её архитектуры, предлагаемый в рамках лабораторной работы. При реализации блока статистического пулинга предлагается выбрать либо его классический вариант, предложенный в [следующей работе](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), либо его более продвинутую версию основанную на использовании [механизмов внимания](https://arxiv.org/pdf/1803.10963.pdf). Использование последней версии статистического пулинга позволяет реализовать детектор речевой активности прямо внутри блока построения дикторских моделей.\n",
    "\n",
    "3. Инициализировать загрузчики тренировочной и валидационной выборки.\n",
    "\n",
    "4. Инициализировать оптимизатор и планировщик для выполнения процедуры обучения.\n",
    "\n",
    "5. Описать процедуру валидации/тестирования блока построения дикторских моделей.\n",
    "\n",
    "6. Описать процедуру обучения и запустить её, контролируя значения стоимостной функции и доли правильных ответов на тренировочном множестве, а также долю правильных ответов на валидационном множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3380f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "\n",
    "# Acoustic features\n",
    "n_mels            = 40                                   # number of mel filters in bank filters\n",
    "log_input         = True                                 # logarithm of features by level\n",
    "\n",
    "# Neural network architecture\n",
    "layers            = [3, 4, 6, 3]                         # number of ResNet blocks in different level of frame level\n",
    "activation        = nn.LeakyReLU                         # activation function used in ResNet blocks\n",
    "num_filters       = [32, 64, 128, 256]                   # number of filters of ResNet blocks in different level of frame level\n",
    "encoder_type      = 'SP'                                 # type of statistic pooling layer ('SP'  – classical statistic pooling \n",
    "                                                         # layer and 'ASP' – attentive statistic pooling)\n",
    "nOut              = 512                                  # embedding size\n",
    "\n",
    "# Loss function for angular losses\n",
    "margin            = 0.35                                 # margin parameter\n",
    "scale             = 32.0                                 # scale parameter\n",
    "\n",
    "# Train dataloader\n",
    "max_frames_train  = 200                                  # number of frame to train\n",
    "train_path        = '../data/voxceleb1_dev/wav'          # path to train wav files\n",
    "batch_size_train  = 16                                  # batch size to train\n",
    "pin_memory        = False                                # pin memory\n",
    "num_workers_train = 4                                    # number of workers to train\n",
    "shuffle           = True                                 # shuffling of training examples\n",
    "\n",
    "# Validation dataloader\n",
    "max_frames_val    = 1000                                 # number of frame to validate\n",
    "val_path          = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_val    = 16                                  # batch size to validate\n",
    "num_workers_val   = 4                                    # number of workers to validate\n",
    "\n",
    "# Test dataloader\n",
    "max_frames_test   = 1000                                 # number of frame to test\n",
    "test_path         = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_test   = 16                                  # batch size to test\n",
    "num_workers_test  = 4                                    # number of workers to test\n",
    "\n",
    "# Optimizer\n",
    "lr                = 2.5                                  # learning rate value\n",
    "weight_decay      = 0                                    # weight decay value\n",
    "\n",
    "# Scheduler\n",
    "val_interval      = 1                                    # frequency of validation step\n",
    "max_epoch         = 40                                   # number of epochs\n",
    "\n",
    "# Augmentation\n",
    "musan_path        = '../data/musan_split'                # path to splitted SLR17 dataset\n",
    "rir_path          = '../data/RIRS_NOISES/simulated_rirs' # path to SLR28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data lists\n",
    "train_list = []\n",
    "val_list   = []\n",
    "test_list  = []\n",
    "\n",
    "with open('../data/voxceleb1_test/iden_split.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "black_list = os.listdir('../data/voxceleb1_test/wav')   # exclude speaker IDs from VoxCeleb1 test set\n",
    "num_train_spk = []                                      # number of train speakers\n",
    "\n",
    "for line in lines:\n",
    "    line   = line.strip().split(' ')\n",
    "    spk_id = line[1].split('/')[0]\n",
    "    \n",
    "    if not (spk_id in black_list):\n",
    "        num_train_spk.append(spk_id)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Train list\n",
    "    if (line[0] == '1'):\n",
    "        train_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Validation list\n",
    "    elif (line[0] == '2'):\n",
    "        val_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Test list\n",
    "    elif (line[0] == '3'):\n",
    "        test_list.append(' '.join([spk_id, line[1]]))\n",
    "        \n",
    "num_train_spk = len(set(num_train_spk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e497662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder SP.\n",
      "Initialised AAM softmax margin 0.350 scale 32.000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model = MainModel(model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, max_frames=max_frames_train, train_path=train_path)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=shuffle) \n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5358d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised SGD optimizer.\n",
      "Initialised OneCycle LR scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.30, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=20, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef333a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_save = './runs/ResNet_without_augmentation'\n",
    "writer = SummaryWriter(path_to_save)\n",
    "writer.add_graph(main_model, next(iter(train_loader))[0].cuda())\n",
    "\n",
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "   \n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "\n",
    "    writer.add_scalar(\"Top1/train\", train_top1, num_epoch)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, num_epoch)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        writer.add_scalar(\"Top1/test\", val_top1, num_epoch/val_interval)\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9d796",
   "metadata": {},
   "source": [
    "**3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных**\n",
    "\n",
    "Известно, что рроцедуры формирования и передачи речевого сигнала могут сопровождаться воздействием шумов и помех, приводящих к искажению сигнала. В качестве примеров искажающих факторов, влияющих на ухудшение качестве речевого сигнала можно привести: импульсный отклик помещения (реверберация), фоновый шум голосов группы нецелевых дикторов, звук телевизора или радиоприёмника и т.п. Разработка конвейера системы голосовой биометрии требует учёта воздействия искажающих факторов на качество её работы. Поскольку процедура построения современных дикторских моделей основана на обучении глубоких нейронных сетей, требующих большие объёмы данных для обучения их параметров, возможным вариантом увеличения тренировочной выборки может являться использование методов аугментации статистических данных. *Аугментация* – методика создания дополнительных обучающих примеров из имеющихся данных путём внесения в них искажений, которые могут потенциально возникнуть на этапе итогового тестирования системы.\n",
    "\n",
    "Как правило, при решении задачи аугментации данных в речевой обработке используются дополнительные базы шумов и помех. В качестве примеров можно привести базы [SLR17](https://openslr.org/17/) (корпус музыкальных, речевых и шумовых звукозаписей) и [SLR28](https://openslr.org/28/) (база данных реальных и симулированных импульсных откликов комнат, а также изотропных и точечных шумов). Важно отметить, что перед применением с использованием методов аугментации подобных баз к имеющимся данным, требуется убедиться, что частоты дискретизации искажающих баз и оригинальных данных являются одинаковыми. Применительно к рассматриваемому лабораторному практикуму частоты дискретизации всех используемых звукозаписей должны быть равными 16кГц.\n",
    "\n",
    "Как известно, можно выделить два режима аугментации данных: *онлайн* (применяется в ходе процедуры обучения) и *оффлайн* (применяется до процедуры обучения) аугментацию. В рамках настоящей лабораторной работы предлагается использовать онлайн аугментацию в силу не очень большого набора тренировочных данных и большей гибкости экспериментов, чем вс случае онлайн аугментации. Необходимо отметить, что применение онлайн аугментации на практике замедляет процедуру обучения, по сравнению с оффлайн аугментацией, так как наложение искажений, извлечение акустических признаков и их возможная предобработка требует определённого машинного времени.\n",
    "\n",
    "В рамках настоящего пункта предлагается сделать следующее:\n",
    "\n",
    "1. Загрузить и извлечь данные из базы SLR17 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию. Поскольку звукозаписи рассматриваемой базы являются достаточно длинными, рекомендуется предварительно разбить эту базу на более маленькие фрагменты (например, длительностью 5 секунд с шагом 3 секунды), сохранив их на диск. \n",
    "\n",
    "2. Загрузить и извлечь данные из базы SLR28 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию.\n",
    "\n",
    "3. Модернизировать загрузчик тренировочных данных под возможность случайного наложения (искажаем исходные звукозаписи) и не наложения (не искажаем исходные звукозаписи) одного из четырёх типов искажений (реверберация, музыкальный шум, фоновый шум голосов нескольких дикторов, неструктурированный шум), описанных внутри класса **AugmentWAV** следующего программного кода: **../common/DatasetLoader.py**.\n",
    "\n",
    "4. Используя процедуру обучения из предыдущего пункта с идентичными настройками выполнить тренировку параметров блока генерации дикторских моделей на исходных данных при наличии их аугментирвоанных копий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557d1ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Download SLR17 (MUSAN) and SLR28 (RIR noises) datasets\n",
    "# with open('../data/lists/augment_datasets.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# download_dataset(lines, user=None, password=None, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f83ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract SLR17 (MUSAN)\n",
    "# extract_dataset(save_path='../data', fname='../data/musan.tar.gz')\n",
    "\n",
    "# # Extract SLR28 (RIR noises)\n",
    "# part_extract(save_path='../data', fname='../data/rirs_noises.zip', target=['RIRS_NOISES/simulated_rirs/mediumroom', 'RIRS_NOISES/simulated_rirs/smallroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a403f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split MUSAN (SLR17) dataset for faster random access\n",
    "# split_musan(save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c002fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder SP.\n",
      "Initialised AAM softmax margin 0.350 scale 32.000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model = MainModel(model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2730a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, \n",
    "                                     max_frames=max_frames_train, \n",
    "                                     train_path=train_path, \n",
    "                                     augment=True, \n",
    "                                     musan_path=musan_path, \n",
    "                                     rir_path=rir_path)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=shuffle)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e990b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised SGD optimizer.\n",
      "Initialised OneCycle LR scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.30, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=20, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a8beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 494, LR 0.211973 Loss 13.726121, Accuracy 31.536%\n",
      "Epoch 1, Batch 495, LR 0.212085 Loss 13.724342, Accuracy 31.553%\n",
      "Epoch 1, Batch 496, LR 0.212196 Loss 13.723010, Accuracy 31.571%\n",
      "Epoch 1, Batch 497, LR 0.212308 Loss 13.721512, Accuracy 31.572%\n",
      "Epoch 1, Batch 498, LR 0.212420 Loss 13.720065, Accuracy 31.590%\n",
      "Epoch 1, Batch 499, LR 0.212532 Loss 13.718787, Accuracy 31.599%\n",
      "Epoch 1, Batch 500, LR 0.212643 Loss 13.716822, Accuracy 31.603%\n",
      "Epoch 1, Batch 501, LR 0.212755 Loss 13.715209, Accuracy 31.618%\n",
      "Epoch 1, Batch 502, LR 0.212868 Loss 13.714168, Accuracy 31.616%\n",
      "Epoch 1, Batch 503, LR 0.212980 Loss 13.713108, Accuracy 31.618%\n",
      "Epoch 1, Batch 504, LR 0.213092 Loss 13.711141, Accuracy 31.641%\n",
      "Epoch 1, Batch 505, LR 0.213204 Loss 13.710305, Accuracy 31.643%\n",
      "Epoch 1, Batch 506, LR 0.213317 Loss 13.709068, Accuracy 31.658%\n",
      "Epoch 1, Batch 507, LR 0.213429 Loss 13.706287, Accuracy 31.683%\n",
      "Epoch 1, Batch 508, LR 0.213541 Loss 13.704394, Accuracy 31.702%\n",
      "Epoch 1, Batch 509, LR 0.213654 Loss 13.703562, Accuracy 31.709%\n",
      "Epoch 1, Batch 510, LR 0.213767 Loss 13.701106, Accuracy 31.736%\n",
      "Epoch 1, Batch 511, LR 0.213879 Loss 13.700388, Accuracy 31.750%\n",
      "Epoch 1, Batch 512, LR 0.213992 Loss 13.699252, Accuracy 31.760%\n",
      "Epoch 1, Batch 513, LR 0.214105 Loss 13.698031, Accuracy 31.772%\n",
      "Epoch 1, Batch 514, LR 0.214218 Loss 13.696894, Accuracy 31.791%\n",
      "Epoch 1, Batch 515, LR 0.214331 Loss 13.696250, Accuracy 31.804%\n",
      "Epoch 1, Batch 516, LR 0.214444 Loss 13.693205, Accuracy 31.827%\n",
      "Epoch 1, Batch 517, LR 0.214557 Loss 13.690745, Accuracy 31.853%\n",
      "Epoch 1, Batch 518, LR 0.214670 Loss 13.689852, Accuracy 31.865%\n",
      "Epoch 1, Batch 519, LR 0.214783 Loss 13.688582, Accuracy 31.876%\n",
      "Epoch 1, Batch 520, LR 0.214897 Loss 13.687283, Accuracy 31.890%\n",
      "Epoch 1, Batch 521, LR 0.215010 Loss 13.686202, Accuracy 31.890%\n",
      "Epoch 1, Batch 522, LR 0.215123 Loss 13.684524, Accuracy 31.906%\n",
      "Epoch 1, Batch 523, LR 0.215237 Loss 13.684231, Accuracy 31.903%\n",
      "Epoch 1, Batch 524, LR 0.215350 Loss 13.682271, Accuracy 31.912%\n",
      "Epoch 1, Batch 525, LR 0.215464 Loss 13.681013, Accuracy 31.921%\n",
      "Epoch 1, Batch 526, LR 0.215578 Loss 13.678155, Accuracy 31.954%\n",
      "Epoch 1, Batch 527, LR 0.215692 Loss 13.676247, Accuracy 31.969%\n",
      "Epoch 1, Batch 528, LR 0.215805 Loss 13.674895, Accuracy 31.977%\n",
      "Epoch 1, Batch 529, LR 0.215919 Loss 13.674258, Accuracy 31.980%\n",
      "Epoch 1, Batch 530, LR 0.216033 Loss 13.673284, Accuracy 31.989%\n",
      "Epoch 1, Batch 531, LR 0.216147 Loss 13.672126, Accuracy 31.992%\n",
      "Epoch 1, Batch 532, LR 0.216262 Loss 13.671415, Accuracy 32.008%\n",
      "Epoch 1, Batch 533, LR 0.216376 Loss 13.670680, Accuracy 32.015%\n",
      "Epoch 1, Batch 534, LR 0.216490 Loss 13.669026, Accuracy 32.031%\n",
      "Epoch 1, Batch 535, LR 0.216604 Loss 13.666970, Accuracy 32.049%\n",
      "Epoch 1, Batch 536, LR 0.216719 Loss 13.664696, Accuracy 32.068%\n",
      "Epoch 1, Batch 537, LR 0.216833 Loss 13.663377, Accuracy 32.087%\n",
      "Epoch 1, Batch 538, LR 0.216948 Loss 13.660904, Accuracy 32.113%\n",
      "Epoch 1, Batch 539, LR 0.217062 Loss 13.660121, Accuracy 32.112%\n",
      "Epoch 1, Batch 540, LR 0.217177 Loss 13.658962, Accuracy 32.117%\n",
      "Epoch 1, Batch 541, LR 0.217292 Loss 13.657384, Accuracy 32.125%\n",
      "Epoch 1, Batch 542, LR 0.217407 Loss 13.657253, Accuracy 32.116%\n",
      "Epoch 1, Batch 543, LR 0.217521 Loss 13.656676, Accuracy 32.116%\n",
      "Epoch 1, Batch 544, LR 0.217636 Loss 13.655704, Accuracy 32.126%\n",
      "Epoch 1, Batch 545, LR 0.217751 Loss 13.655228, Accuracy 32.112%\n",
      "Epoch 1, Batch 546, LR 0.217867 Loss 13.654314, Accuracy 32.121%\n",
      "Epoch 1, Batch 547, LR 0.217982 Loss 13.653640, Accuracy 32.128%\n",
      "Epoch 1, Batch 548, LR 0.218097 Loss 13.652761, Accuracy 32.134%\n",
      "Epoch 1, Batch 549, LR 0.218212 Loss 13.651272, Accuracy 32.155%\n",
      "Epoch 1, Batch 550, LR 0.218328 Loss 13.649993, Accuracy 32.166%\n",
      "Epoch 1, Batch 551, LR 0.218443 Loss 13.649798, Accuracy 32.172%\n",
      "Epoch 1, Batch 552, LR 0.218559 Loss 13.647814, Accuracy 32.201%\n",
      "Epoch 1, Batch 553, LR 0.218674 Loss 13.646616, Accuracy 32.212%\n",
      "Epoch 1, Batch 554, LR 0.218790 Loss 13.645317, Accuracy 32.223%\n",
      "Epoch 1, Batch 555, LR 0.218905 Loss 13.643630, Accuracy 32.238%\n",
      "Epoch 1, Batch 556, LR 0.219021 Loss 13.641833, Accuracy 32.256%\n",
      "Epoch 1, Batch 557, LR 0.219137 Loss 13.640803, Accuracy 32.268%\n",
      "Epoch 1, Batch 558, LR 0.219253 Loss 13.639996, Accuracy 32.276%\n",
      "Epoch 1, Batch 559, LR 0.219369 Loss 13.637642, Accuracy 32.305%\n",
      "Epoch 1, Batch 560, LR 0.219485 Loss 13.636894, Accuracy 32.305%\n",
      "Epoch 1, Batch 561, LR 0.219601 Loss 13.635606, Accuracy 32.318%\n",
      "Epoch 1, Batch 562, LR 0.219717 Loss 13.634754, Accuracy 32.333%\n",
      "Epoch 1, Batch 563, LR 0.219833 Loss 13.634159, Accuracy 32.341%\n",
      "Epoch 1, Batch 564, LR 0.219950 Loss 13.633084, Accuracy 32.346%\n",
      "Epoch 1, Batch 565, LR 0.220066 Loss 13.631472, Accuracy 32.363%\n",
      "Epoch 1, Batch 566, LR 0.220183 Loss 13.630069, Accuracy 32.368%\n",
      "Epoch 1, Batch 567, LR 0.220299 Loss 13.628477, Accuracy 32.387%\n",
      "Epoch 1, Batch 568, LR 0.220416 Loss 13.627778, Accuracy 32.392%\n",
      "Epoch 1, Batch 569, LR 0.220532 Loss 13.626879, Accuracy 32.398%\n",
      "Epoch 1, Batch 570, LR 0.220649 Loss 13.624308, Accuracy 32.416%\n",
      "Epoch 1, Batch 571, LR 0.220766 Loss 13.623803, Accuracy 32.423%\n",
      "Epoch 1, Batch 572, LR 0.220883 Loss 13.623026, Accuracy 32.427%\n",
      "Epoch 1, Batch 573, LR 0.221000 Loss 13.621190, Accuracy 32.444%\n",
      "Epoch 1, Batch 574, LR 0.221117 Loss 13.619784, Accuracy 32.461%\n",
      "Epoch 1, Batch 575, LR 0.221234 Loss 13.618101, Accuracy 32.473%\n",
      "Epoch 1, Batch 576, LR 0.221351 Loss 13.619034, Accuracy 32.469%\n",
      "Epoch 1, Batch 577, LR 0.221468 Loss 13.616367, Accuracy 32.493%\n",
      "Epoch 1, Batch 578, LR 0.221585 Loss 13.613823, Accuracy 32.512%\n",
      "Epoch 1, Batch 579, LR 0.221703 Loss 13.613048, Accuracy 32.520%\n",
      "Epoch 1, Batch 580, LR 0.221820 Loss 13.611158, Accuracy 32.530%\n",
      "Epoch 1, Batch 581, LR 0.221938 Loss 13.609936, Accuracy 32.535%\n",
      "Epoch 1, Batch 582, LR 0.222055 Loss 13.608969, Accuracy 32.553%\n",
      "Epoch 1, Batch 583, LR 0.222173 Loss 13.607464, Accuracy 32.577%\n",
      "Epoch 1, Batch 584, LR 0.222291 Loss 13.605502, Accuracy 32.593%\n",
      "Epoch 1, Batch 585, LR 0.222408 Loss 13.604179, Accuracy 32.606%\n",
      "Epoch 1, Batch 586, LR 0.222526 Loss 13.603116, Accuracy 32.611%\n",
      "Epoch 1, Batch 587, LR 0.222644 Loss 13.601248, Accuracy 32.626%\n",
      "Epoch 1, Batch 588, LR 0.222762 Loss 13.599502, Accuracy 32.636%\n",
      "Epoch 1, Batch 589, LR 0.222880 Loss 13.597941, Accuracy 32.647%\n",
      "Epoch 1, Batch 590, LR 0.222998 Loss 13.596820, Accuracy 32.659%\n",
      "Epoch 1, Batch 591, LR 0.223116 Loss 13.596306, Accuracy 32.655%\n",
      "Epoch 1, Batch 592, LR 0.223234 Loss 13.596183, Accuracy 32.655%\n",
      "Epoch 1, Batch 593, LR 0.223353 Loss 13.596126, Accuracy 32.648%\n",
      "Epoch 1, Batch 594, LR 0.223471 Loss 13.594744, Accuracy 32.663%\n",
      "Epoch 1, Batch 595, LR 0.223590 Loss 13.592637, Accuracy 32.689%\n",
      "Epoch 1, Batch 596, LR 0.223708 Loss 13.590989, Accuracy 32.704%\n",
      "Epoch 1, Batch 597, LR 0.223827 Loss 13.589443, Accuracy 32.720%\n",
      "Epoch 1, Batch 598, LR 0.223945 Loss 13.587820, Accuracy 32.735%\n",
      "Epoch 1, Batch 599, LR 0.224064 Loss 13.587503, Accuracy 32.737%\n",
      "Epoch 1, Batch 600, LR 0.224183 Loss 13.585313, Accuracy 32.754%\n",
      "Epoch 1, Batch 601, LR 0.224302 Loss 13.584106, Accuracy 32.775%\n",
      "Epoch 1, Batch 602, LR 0.224420 Loss 13.581937, Accuracy 32.790%\n",
      "Epoch 1, Batch 603, LR 0.224539 Loss 13.580669, Accuracy 32.798%\n",
      "Epoch 1, Batch 604, LR 0.224658 Loss 13.580832, Accuracy 32.797%\n",
      "Epoch 1, Batch 605, LR 0.224778 Loss 13.578539, Accuracy 32.816%\n",
      "Epoch 1, Batch 606, LR 0.224897 Loss 13.576968, Accuracy 32.824%\n",
      "Epoch 1, Batch 607, LR 0.225016 Loss 13.576491, Accuracy 32.823%\n",
      "Epoch 1, Batch 608, LR 0.225135 Loss 13.575569, Accuracy 32.830%\n",
      "Epoch 1, Batch 609, LR 0.225255 Loss 13.574503, Accuracy 32.833%\n",
      "Epoch 1, Batch 610, LR 0.225374 Loss 13.573531, Accuracy 32.843%\n",
      "Epoch 1, Batch 611, LR 0.225494 Loss 13.572443, Accuracy 32.844%\n",
      "Epoch 1, Batch 612, LR 0.225613 Loss 13.569887, Accuracy 32.871%\n",
      "Epoch 1, Batch 613, LR 0.225733 Loss 13.568113, Accuracy 32.889%\n",
      "Epoch 1, Batch 614, LR 0.225853 Loss 13.566857, Accuracy 32.907%\n",
      "Epoch 1, Batch 615, LR 0.225972 Loss 13.565403, Accuracy 32.914%\n",
      "Epoch 1, Batch 616, LR 0.226092 Loss 13.564326, Accuracy 32.920%\n",
      "Epoch 1, Batch 617, LR 0.226212 Loss 13.564071, Accuracy 32.920%\n",
      "Epoch 1, Batch 618, LR 0.226332 Loss 13.562014, Accuracy 32.936%\n",
      "Epoch 1, Batch 619, LR 0.226452 Loss 13.559691, Accuracy 32.954%\n",
      "Epoch 1, Batch 620, LR 0.226572 Loss 13.558966, Accuracy 32.959%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 621, LR 0.226693 Loss 13.556568, Accuracy 32.979%\n",
      "Epoch 1, Batch 622, LR 0.226813 Loss 13.555038, Accuracy 32.992%\n",
      "Epoch 1, Batch 623, LR 0.226933 Loss 13.553958, Accuracy 33.003%\n",
      "Epoch 1, Batch 624, LR 0.227054 Loss 13.553338, Accuracy 33.010%\n",
      "Epoch 1, Batch 625, LR 0.227174 Loss 13.551256, Accuracy 33.029%\n",
      "Epoch 1, Batch 626, LR 0.227295 Loss 13.551372, Accuracy 33.035%\n",
      "Epoch 1, Batch 627, LR 0.227415 Loss 13.549684, Accuracy 33.046%\n",
      "Epoch 1, Batch 628, LR 0.227536 Loss 13.548665, Accuracy 33.068%\n",
      "Epoch 1, Batch 629, LR 0.227657 Loss 13.546736, Accuracy 33.092%\n",
      "Epoch 1, Batch 630, LR 0.227778 Loss 13.545203, Accuracy 33.108%\n",
      "Epoch 1, Batch 631, LR 0.227898 Loss 13.543513, Accuracy 33.117%\n",
      "Epoch 1, Batch 632, LR 0.228019 Loss 13.541363, Accuracy 33.134%\n",
      "Epoch 1, Batch 633, LR 0.228140 Loss 13.540273, Accuracy 33.142%\n",
      "Epoch 1, Batch 634, LR 0.228261 Loss 13.539788, Accuracy 33.153%\n",
      "Epoch 1, Batch 635, LR 0.228383 Loss 13.538838, Accuracy 33.164%\n",
      "Epoch 1, Batch 636, LR 0.228504 Loss 13.538253, Accuracy 33.170%\n",
      "Epoch 1, Batch 637, LR 0.228625 Loss 13.536593, Accuracy 33.184%\n",
      "Epoch 1, Batch 638, LR 0.228746 Loss 13.535956, Accuracy 33.192%\n",
      "Epoch 1, Batch 639, LR 0.228868 Loss 13.533825, Accuracy 33.205%\n",
      "Epoch 1, Batch 640, LR 0.228989 Loss 13.532755, Accuracy 33.214%\n",
      "Epoch 1, Batch 641, LR 0.229111 Loss 13.532077, Accuracy 33.218%\n",
      "Epoch 1, Batch 642, LR 0.229233 Loss 13.530902, Accuracy 33.221%\n",
      "Epoch 1, Batch 643, LR 0.229354 Loss 13.529770, Accuracy 33.232%\n",
      "Epoch 1, Batch 644, LR 0.229476 Loss 13.528853, Accuracy 33.242%\n",
      "Epoch 1, Batch 645, LR 0.229598 Loss 13.528551, Accuracy 33.246%\n",
      "Epoch 1, Batch 646, LR 0.229720 Loss 13.526682, Accuracy 33.264%\n",
      "Epoch 1, Batch 647, LR 0.229842 Loss 13.525627, Accuracy 33.276%\n",
      "Epoch 1, Batch 648, LR 0.229964 Loss 13.524356, Accuracy 33.288%\n",
      "Epoch 1, Batch 649, LR 0.230086 Loss 13.523294, Accuracy 33.292%\n",
      "Epoch 1, Batch 650, LR 0.230208 Loss 13.522443, Accuracy 33.299%\n",
      "Epoch 1, Batch 651, LR 0.230330 Loss 13.520815, Accuracy 33.306%\n",
      "Epoch 1, Batch 652, LR 0.230453 Loss 13.519356, Accuracy 33.316%\n",
      "Epoch 1, Batch 653, LR 0.230575 Loss 13.518388, Accuracy 33.319%\n",
      "Epoch 1, Batch 654, LR 0.230697 Loss 13.517843, Accuracy 33.326%\n",
      "Epoch 1, Batch 655, LR 0.230820 Loss 13.515907, Accuracy 33.346%\n",
      "Epoch 1, Batch 656, LR 0.230942 Loss 13.513551, Accuracy 33.362%\n",
      "Epoch 1, Batch 657, LR 0.231065 Loss 13.512327, Accuracy 33.373%\n",
      "Epoch 1, Batch 658, LR 0.231188 Loss 13.511924, Accuracy 33.373%\n",
      "Epoch 1, Batch 659, LR 0.231311 Loss 13.510527, Accuracy 33.384%\n",
      "Epoch 1, Batch 660, LR 0.231433 Loss 13.509816, Accuracy 33.394%\n",
      "Epoch 1, Batch 661, LR 0.231556 Loss 13.508816, Accuracy 33.400%\n",
      "Epoch 1, Batch 662, LR 0.231679 Loss 13.508260, Accuracy 33.408%\n",
      "Epoch 1, Batch 663, LR 0.231802 Loss 13.506065, Accuracy 33.429%\n",
      "Epoch 1, Batch 664, LR 0.231925 Loss 13.505283, Accuracy 33.436%\n",
      "Epoch 1, Batch 665, LR 0.232049 Loss 13.503377, Accuracy 33.454%\n",
      "Epoch 1, Batch 666, LR 0.232172 Loss 13.501182, Accuracy 33.469%\n",
      "Epoch 1, Batch 667, LR 0.232295 Loss 13.499719, Accuracy 33.480%\n",
      "Epoch 1, Batch 668, LR 0.232419 Loss 13.498566, Accuracy 33.487%\n",
      "Epoch 1, Batch 669, LR 0.232542 Loss 13.497556, Accuracy 33.497%\n",
      "Epoch 1, Batch 670, LR 0.232666 Loss 13.495474, Accuracy 33.507%\n",
      "Epoch 1, Batch 671, LR 0.232789 Loss 13.494136, Accuracy 33.511%\n",
      "Epoch 1, Batch 672, LR 0.232913 Loss 13.492816, Accuracy 33.522%\n",
      "Epoch 1, Batch 673, LR 0.233037 Loss 13.491342, Accuracy 33.532%\n",
      "Epoch 1, Batch 674, LR 0.233160 Loss 13.489279, Accuracy 33.552%\n",
      "Epoch 1, Batch 675, LR 0.233284 Loss 13.488704, Accuracy 33.554%\n",
      "Epoch 1, Batch 676, LR 0.233408 Loss 13.487386, Accuracy 33.565%\n",
      "Epoch 1, Batch 677, LR 0.233532 Loss 13.486469, Accuracy 33.571%\n",
      "Epoch 1, Batch 678, LR 0.233656 Loss 13.486653, Accuracy 33.574%\n",
      "Epoch 1, Batch 679, LR 0.233780 Loss 13.484553, Accuracy 33.582%\n",
      "Epoch 1, Batch 680, LR 0.233904 Loss 13.484363, Accuracy 33.582%\n",
      "Epoch 1, Batch 681, LR 0.234029 Loss 13.483325, Accuracy 33.591%\n",
      "Epoch 1, Batch 682, LR 0.234153 Loss 13.482173, Accuracy 33.602%\n",
      "Epoch 1, Batch 683, LR 0.234277 Loss 13.481263, Accuracy 33.615%\n",
      "Epoch 1, Batch 684, LR 0.234402 Loss 13.480927, Accuracy 33.617%\n",
      "Epoch 1, Batch 685, LR 0.234526 Loss 13.479348, Accuracy 33.635%\n",
      "Epoch 1, Batch 686, LR 0.234651 Loss 13.477725, Accuracy 33.648%\n",
      "Epoch 1, Batch 687, LR 0.234776 Loss 13.475945, Accuracy 33.665%\n",
      "Epoch 1, Batch 688, LR 0.234900 Loss 13.473244, Accuracy 33.690%\n",
      "Epoch 1, Batch 689, LR 0.235025 Loss 13.472902, Accuracy 33.688%\n",
      "Epoch 1, Batch 690, LR 0.235150 Loss 13.471726, Accuracy 33.699%\n",
      "Epoch 1, Batch 691, LR 0.235275 Loss 13.469164, Accuracy 33.728%\n",
      "Epoch 1, Batch 692, LR 0.235400 Loss 13.466821, Accuracy 33.737%\n",
      "Epoch 1, Batch 693, LR 0.235525 Loss 13.464955, Accuracy 33.754%\n",
      "Epoch 1, Batch 694, LR 0.235650 Loss 13.463857, Accuracy 33.768%\n",
      "Epoch 1, Batch 695, LR 0.235775 Loss 13.461939, Accuracy 33.778%\n",
      "Epoch 1, Batch 696, LR 0.235901 Loss 13.460494, Accuracy 33.790%\n",
      "Epoch 1, Batch 697, LR 0.236026 Loss 13.458971, Accuracy 33.804%\n",
      "Epoch 1, Batch 698, LR 0.236151 Loss 13.458024, Accuracy 33.803%\n",
      "Epoch 1, Batch 699, LR 0.236277 Loss 13.457144, Accuracy 33.807%\n",
      "Epoch 1, Batch 700, LR 0.236402 Loss 13.456089, Accuracy 33.819%\n",
      "Epoch 1, Batch 701, LR 0.236528 Loss 13.454773, Accuracy 33.828%\n",
      "Epoch 1, Batch 702, LR 0.236654 Loss 13.452653, Accuracy 33.849%\n",
      "Epoch 1, Batch 703, LR 0.236780 Loss 13.451918, Accuracy 33.855%\n",
      "Epoch 1, Batch 704, LR 0.236905 Loss 13.450054, Accuracy 33.871%\n",
      "Epoch 1, Batch 705, LR 0.237031 Loss 13.448057, Accuracy 33.893%\n",
      "Epoch 1, Batch 706, LR 0.237157 Loss 13.447010, Accuracy 33.899%\n",
      "Epoch 1, Batch 707, LR 0.237283 Loss 13.445322, Accuracy 33.904%\n",
      "Epoch 1, Batch 708, LR 0.237409 Loss 13.444211, Accuracy 33.918%\n",
      "Epoch 1, Batch 709, LR 0.237535 Loss 13.442175, Accuracy 33.933%\n",
      "Epoch 1, Batch 710, LR 0.237662 Loss 13.439932, Accuracy 33.954%\n",
      "Epoch 1, Batch 711, LR 0.237788 Loss 13.439030, Accuracy 33.962%\n",
      "Epoch 1, Batch 712, LR 0.237914 Loss 13.437661, Accuracy 33.976%\n",
      "Epoch 1, Batch 713, LR 0.238041 Loss 13.436048, Accuracy 33.990%\n",
      "Epoch 1, Batch 714, LR 0.238167 Loss 13.434094, Accuracy 34.000%\n",
      "Epoch 1, Batch 715, LR 0.238294 Loss 13.433360, Accuracy 33.999%\n",
      "Epoch 1, Batch 716, LR 0.238420 Loss 13.432476, Accuracy 34.005%\n",
      "Epoch 1, Batch 717, LR 0.238547 Loss 13.432098, Accuracy 34.010%\n",
      "Epoch 1, Batch 718, LR 0.238674 Loss 13.430323, Accuracy 34.024%\n",
      "Epoch 1, Batch 719, LR 0.238801 Loss 13.429392, Accuracy 34.033%\n",
      "Epoch 1, Batch 720, LR 0.238927 Loss 13.427753, Accuracy 34.045%\n",
      "Epoch 1, Batch 721, LR 0.239054 Loss 13.424996, Accuracy 34.077%\n",
      "Epoch 1, Batch 722, LR 0.239181 Loss 13.423486, Accuracy 34.087%\n",
      "Epoch 1, Batch 723, LR 0.239308 Loss 13.422187, Accuracy 34.098%\n",
      "Epoch 1, Batch 724, LR 0.239436 Loss 13.420578, Accuracy 34.117%\n",
      "Epoch 1, Batch 725, LR 0.239563 Loss 13.418924, Accuracy 34.138%\n",
      "Epoch 1, Batch 726, LR 0.239690 Loss 13.417994, Accuracy 34.138%\n",
      "Epoch 1, Batch 727, LR 0.239818 Loss 13.416094, Accuracy 34.156%\n",
      "Epoch 1, Batch 728, LR 0.239945 Loss 13.414209, Accuracy 34.172%\n",
      "Epoch 1, Batch 729, LR 0.240072 Loss 13.413279, Accuracy 34.178%\n",
      "Epoch 1, Batch 730, LR 0.240200 Loss 13.411346, Accuracy 34.192%\n",
      "Epoch 1, Batch 731, LR 0.240328 Loss 13.409820, Accuracy 34.206%\n",
      "Epoch 1, Batch 732, LR 0.240455 Loss 13.409026, Accuracy 34.211%\n",
      "Epoch 1, Batch 733, LR 0.240583 Loss 13.408078, Accuracy 34.215%\n",
      "Epoch 1, Batch 734, LR 0.240711 Loss 13.407855, Accuracy 34.220%\n",
      "Epoch 1, Batch 735, LR 0.240839 Loss 13.407344, Accuracy 34.228%\n",
      "Epoch 1, Batch 736, LR 0.240967 Loss 13.405952, Accuracy 34.240%\n",
      "Epoch 1, Batch 737, LR 0.241095 Loss 13.404747, Accuracy 34.251%\n",
      "Epoch 1, Batch 738, LR 0.241223 Loss 13.403211, Accuracy 34.255%\n",
      "Epoch 1, Batch 739, LR 0.241351 Loss 13.402054, Accuracy 34.268%\n",
      "Epoch 1, Batch 740, LR 0.241479 Loss 13.400848, Accuracy 34.276%\n",
      "Epoch 1, Batch 741, LR 0.241608 Loss 13.399982, Accuracy 34.278%\n",
      "Epoch 1, Batch 742, LR 0.241736 Loss 13.398556, Accuracy 34.284%\n",
      "Epoch 1, Batch 743, LR 0.241864 Loss 13.397377, Accuracy 34.294%\n",
      "Epoch 1, Batch 744, LR 0.241993 Loss 13.396474, Accuracy 34.299%\n",
      "Epoch 1, Batch 745, LR 0.242121 Loss 13.395345, Accuracy 34.302%\n",
      "Epoch 1, Batch 746, LR 0.242250 Loss 13.393604, Accuracy 34.311%\n",
      "Epoch 1, Batch 747, LR 0.242379 Loss 13.393364, Accuracy 34.309%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 748, LR 0.242507 Loss 13.392659, Accuracy 34.313%\n",
      "Epoch 1, Batch 749, LR 0.242636 Loss 13.391425, Accuracy 34.319%\n",
      "Epoch 1, Batch 750, LR 0.242765 Loss 13.390797, Accuracy 34.325%\n",
      "Epoch 1, Batch 751, LR 0.242894 Loss 13.389527, Accuracy 34.338%\n",
      "Epoch 1, Batch 752, LR 0.243023 Loss 13.387710, Accuracy 34.356%\n",
      "Epoch 1, Batch 753, LR 0.243152 Loss 13.385397, Accuracy 34.376%\n",
      "Epoch 1, Batch 754, LR 0.243281 Loss 13.383986, Accuracy 34.392%\n",
      "Epoch 1, Batch 755, LR 0.243411 Loss 13.382758, Accuracy 34.399%\n",
      "Epoch 1, Batch 756, LR 0.243540 Loss 13.381623, Accuracy 34.409%\n",
      "Epoch 1, Batch 757, LR 0.243669 Loss 13.380702, Accuracy 34.412%\n",
      "Epoch 1, Batch 758, LR 0.243799 Loss 13.380523, Accuracy 34.413%\n",
      "Epoch 1, Batch 759, LR 0.243928 Loss 13.379232, Accuracy 34.423%\n",
      "Epoch 1, Batch 760, LR 0.244058 Loss 13.377881, Accuracy 34.434%\n",
      "Epoch 1, Batch 761, LR 0.244187 Loss 13.376573, Accuracy 34.441%\n",
      "Epoch 1, Batch 762, LR 0.244317 Loss 13.375109, Accuracy 34.455%\n",
      "Epoch 1, Batch 763, LR 0.244447 Loss 13.373931, Accuracy 34.463%\n",
      "Epoch 1, Batch 764, LR 0.244577 Loss 13.372651, Accuracy 34.469%\n",
      "Epoch 1, Batch 765, LR 0.244707 Loss 13.372202, Accuracy 34.472%\n",
      "Epoch 1, Batch 766, LR 0.244837 Loss 13.370372, Accuracy 34.489%\n",
      "Epoch 1, Batch 767, LR 0.244967 Loss 13.369522, Accuracy 34.496%\n",
      "Epoch 1, Batch 768, LR 0.245097 Loss 13.368485, Accuracy 34.500%\n",
      "Epoch 1, Batch 769, LR 0.245227 Loss 13.367666, Accuracy 34.509%\n",
      "Epoch 1, Batch 770, LR 0.245357 Loss 13.366780, Accuracy 34.513%\n",
      "Epoch 1, Batch 771, LR 0.245487 Loss 13.365619, Accuracy 34.521%\n",
      "Epoch 1, Batch 772, LR 0.245618 Loss 13.363963, Accuracy 34.531%\n",
      "Epoch 1, Batch 773, LR 0.245748 Loss 13.363366, Accuracy 34.533%\n",
      "Epoch 1, Batch 774, LR 0.245879 Loss 13.362725, Accuracy 34.538%\n",
      "Epoch 1, Batch 775, LR 0.246009 Loss 13.361320, Accuracy 34.556%\n",
      "Epoch 1, Batch 776, LR 0.246140 Loss 13.359614, Accuracy 34.572%\n",
      "Epoch 1, Batch 777, LR 0.246271 Loss 13.358444, Accuracy 34.580%\n",
      "Epoch 1, Batch 778, LR 0.246401 Loss 13.357408, Accuracy 34.595%\n",
      "Epoch 1, Batch 779, LR 0.246532 Loss 13.356123, Accuracy 34.604%\n",
      "Epoch 1, Batch 780, LR 0.246663 Loss 13.354567, Accuracy 34.621%\n",
      "Epoch 1, Batch 781, LR 0.246794 Loss 13.353660, Accuracy 34.632%\n",
      "Epoch 1, Batch 782, LR 0.246925 Loss 13.352629, Accuracy 34.641%\n",
      "Epoch 1, Batch 783, LR 0.247056 Loss 13.351332, Accuracy 34.657%\n",
      "Epoch 1, Batch 784, LR 0.247187 Loss 13.349716, Accuracy 34.674%\n",
      "Epoch 1, Batch 785, LR 0.247319 Loss 13.348068, Accuracy 34.686%\n",
      "Epoch 1, Batch 786, LR 0.247450 Loss 13.346640, Accuracy 34.698%\n",
      "Epoch 1, Batch 787, LR 0.247581 Loss 13.345947, Accuracy 34.707%\n",
      "Epoch 1, Batch 788, LR 0.247713 Loss 13.344315, Accuracy 34.720%\n",
      "Epoch 1, Batch 789, LR 0.247844 Loss 13.342821, Accuracy 34.728%\n",
      "Epoch 1, Batch 790, LR 0.247976 Loss 13.341982, Accuracy 34.734%\n",
      "Epoch 1, Batch 791, LR 0.248107 Loss 13.340676, Accuracy 34.744%\n",
      "Epoch 1, Batch 792, LR 0.248239 Loss 13.339047, Accuracy 34.755%\n",
      "Epoch 1, Batch 793, LR 0.248371 Loss 13.337365, Accuracy 34.765%\n",
      "Epoch 1, Batch 794, LR 0.248503 Loss 13.336635, Accuracy 34.773%\n",
      "Epoch 1, Batch 795, LR 0.248635 Loss 13.336493, Accuracy 34.775%\n",
      "Epoch 1, Batch 796, LR 0.248767 Loss 13.334935, Accuracy 34.786%\n",
      "Epoch 1, Batch 797, LR 0.248899 Loss 13.333376, Accuracy 34.800%\n",
      "Epoch 1, Batch 798, LR 0.249031 Loss 13.332299, Accuracy 34.804%\n",
      "Epoch 1, Batch 799, LR 0.249163 Loss 13.331253, Accuracy 34.812%\n",
      "Epoch 1, Batch 800, LR 0.249295 Loss 13.330933, Accuracy 34.814%\n",
      "Epoch 1, Batch 801, LR 0.249427 Loss 13.329216, Accuracy 34.828%\n",
      "Epoch 1, Batch 802, LR 0.249560 Loss 13.327839, Accuracy 34.832%\n",
      "Epoch 1, Batch 803, LR 0.249692 Loss 13.326527, Accuracy 34.840%\n",
      "Epoch 1, Batch 804, LR 0.249825 Loss 13.325216, Accuracy 34.850%\n",
      "Epoch 1, Batch 805, LR 0.249957 Loss 13.324510, Accuracy 34.857%\n",
      "Epoch 1, Batch 806, LR 0.250090 Loss 13.323597, Accuracy 34.862%\n",
      "Epoch 1, Batch 807, LR 0.250223 Loss 13.322010, Accuracy 34.872%\n",
      "Epoch 1, Batch 808, LR 0.250355 Loss 13.320674, Accuracy 34.887%\n",
      "Epoch 1, Batch 809, LR 0.250488 Loss 13.320407, Accuracy 34.889%\n",
      "Epoch 1, Batch 810, LR 0.250621 Loss 13.319125, Accuracy 34.900%\n",
      "Epoch 1, Batch 811, LR 0.250754 Loss 13.318676, Accuracy 34.901%\n",
      "Epoch 1, Batch 812, LR 0.250887 Loss 13.317607, Accuracy 34.913%\n",
      "Epoch 1, Batch 813, LR 0.251020 Loss 13.317118, Accuracy 34.915%\n",
      "Epoch 1, Batch 814, LR 0.251153 Loss 13.316909, Accuracy 34.912%\n",
      "Epoch 1, Batch 815, LR 0.251287 Loss 13.316204, Accuracy 34.918%\n",
      "Epoch 1, Batch 816, LR 0.251420 Loss 13.314619, Accuracy 34.929%\n",
      "Epoch 1, Batch 817, LR 0.251553 Loss 13.314090, Accuracy 34.930%\n",
      "Epoch 1, Batch 818, LR 0.251687 Loss 13.313530, Accuracy 34.935%\n",
      "Epoch 1, Batch 819, LR 0.251820 Loss 13.312670, Accuracy 34.939%\n",
      "Epoch 1, Batch 820, LR 0.251954 Loss 13.311044, Accuracy 34.957%\n",
      "Epoch 1, Batch 821, LR 0.252087 Loss 13.309586, Accuracy 34.973%\n",
      "Epoch 1, Batch 822, LR 0.252221 Loss 13.308617, Accuracy 34.983%\n",
      "Epoch 1, Batch 823, LR 0.252355 Loss 13.307417, Accuracy 34.993%\n",
      "Epoch 1, Batch 824, LR 0.252489 Loss 13.306332, Accuracy 35.003%\n",
      "Epoch 1, Batch 825, LR 0.252622 Loss 13.305301, Accuracy 35.013%\n",
      "Epoch 1, Batch 826, LR 0.252756 Loss 13.304000, Accuracy 35.023%\n",
      "Epoch 1, Batch 827, LR 0.252890 Loss 13.302874, Accuracy 35.032%\n",
      "Epoch 1, Batch 828, LR 0.253025 Loss 13.301810, Accuracy 35.039%\n",
      "Epoch 1, Batch 829, LR 0.253159 Loss 13.300183, Accuracy 35.053%\n",
      "Epoch 1, Batch 830, LR 0.253293 Loss 13.298346, Accuracy 35.073%\n",
      "Epoch 1, Batch 831, LR 0.253427 Loss 13.296903, Accuracy 35.079%\n",
      "Epoch 1, Batch 832, LR 0.253562 Loss 13.296476, Accuracy 35.082%\n",
      "Epoch 1, Batch 833, LR 0.253696 Loss 13.295070, Accuracy 35.095%\n",
      "Epoch 1, Batch 834, LR 0.253830 Loss 13.292517, Accuracy 35.116%\n",
      "Epoch 1, Batch 835, LR 0.253965 Loss 13.291726, Accuracy 35.126%\n",
      "Epoch 1, Batch 836, LR 0.254100 Loss 13.290907, Accuracy 35.126%\n",
      "Epoch 1, Batch 837, LR 0.254234 Loss 13.289884, Accuracy 35.142%\n",
      "Epoch 1, Batch 838, LR 0.254369 Loss 13.289668, Accuracy 35.139%\n",
      "Epoch 1, Batch 839, LR 0.254504 Loss 13.288737, Accuracy 35.151%\n",
      "Epoch 1, Batch 840, LR 0.254639 Loss 13.286788, Accuracy 35.167%\n",
      "Epoch 1, Batch 841, LR 0.254774 Loss 13.285987, Accuracy 35.174%\n",
      "Epoch 1, Batch 842, LR 0.254909 Loss 13.283937, Accuracy 35.194%\n",
      "Epoch 1, Batch 843, LR 0.255044 Loss 13.282538, Accuracy 35.209%\n",
      "Epoch 1, Batch 844, LR 0.255179 Loss 13.281642, Accuracy 35.219%\n",
      "Epoch 1, Batch 845, LR 0.255314 Loss 13.279887, Accuracy 35.235%\n",
      "Epoch 1, Batch 846, LR 0.255449 Loss 13.279026, Accuracy 35.239%\n",
      "Epoch 1, Batch 847, LR 0.255585 Loss 13.277624, Accuracy 35.253%\n",
      "Epoch 1, Batch 848, LR 0.255720 Loss 13.276696, Accuracy 35.263%\n",
      "Epoch 1, Batch 849, LR 0.255856 Loss 13.276068, Accuracy 35.271%\n",
      "Epoch 1, Batch 850, LR 0.255991 Loss 13.274710, Accuracy 35.284%\n",
      "Epoch 1, Batch 851, LR 0.256127 Loss 13.273378, Accuracy 35.299%\n",
      "Epoch 1, Batch 852, LR 0.256263 Loss 13.273032, Accuracy 35.305%\n",
      "Epoch 1, Batch 853, LR 0.256398 Loss 13.271878, Accuracy 35.317%\n",
      "Epoch 1, Batch 854, LR 0.256534 Loss 13.270715, Accuracy 35.322%\n",
      "Epoch 1, Batch 855, LR 0.256670 Loss 13.268800, Accuracy 35.337%\n",
      "Epoch 1, Batch 856, LR 0.256806 Loss 13.267472, Accuracy 35.348%\n",
      "Epoch 1, Batch 857, LR 0.256942 Loss 13.265657, Accuracy 35.360%\n",
      "Epoch 1, Batch 858, LR 0.257078 Loss 13.264049, Accuracy 35.373%\n",
      "Epoch 1, Batch 859, LR 0.257214 Loss 13.263359, Accuracy 35.374%\n",
      "Epoch 1, Batch 860, LR 0.257350 Loss 13.262180, Accuracy 35.382%\n",
      "Epoch 1, Batch 861, LR 0.257487 Loss 13.260957, Accuracy 35.391%\n",
      "Epoch 1, Batch 862, LR 0.257623 Loss 13.260135, Accuracy 35.393%\n",
      "Epoch 1, Batch 863, LR 0.257759 Loss 13.258791, Accuracy 35.406%\n",
      "Epoch 1, Batch 864, LR 0.257896 Loss 13.257908, Accuracy 35.412%\n",
      "Epoch 1, Batch 865, LR 0.258032 Loss 13.256986, Accuracy 35.415%\n",
      "Epoch 1, Batch 866, LR 0.258169 Loss 13.255570, Accuracy 35.424%\n",
      "Epoch 1, Batch 867, LR 0.258306 Loss 13.254698, Accuracy 35.427%\n",
      "Epoch 1, Batch 868, LR 0.258442 Loss 13.253888, Accuracy 35.429%\n",
      "Epoch 1, Batch 869, LR 0.258579 Loss 13.252614, Accuracy 35.438%\n",
      "Epoch 1, Batch 870, LR 0.258716 Loss 13.251667, Accuracy 35.446%\n",
      "Epoch 1, Batch 871, LR 0.258853 Loss 13.250445, Accuracy 35.457%\n",
      "Epoch 1, Batch 872, LR 0.258990 Loss 13.249223, Accuracy 35.464%\n",
      "Epoch 1, Batch 873, LR 0.259127 Loss 13.248172, Accuracy 35.475%\n",
      "Epoch 1, Batch 874, LR 0.259264 Loss 13.246909, Accuracy 35.487%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 875, LR 0.259401 Loss 13.245443, Accuracy 35.500%\n",
      "Epoch 1, Batch 876, LR 0.259539 Loss 13.244156, Accuracy 35.511%\n",
      "Epoch 1, Batch 877, LR 0.259676 Loss 13.242965, Accuracy 35.527%\n",
      "Epoch 1, Batch 878, LR 0.259813 Loss 13.241449, Accuracy 35.534%\n",
      "Epoch 1, Batch 879, LR 0.259951 Loss 13.240739, Accuracy 35.535%\n",
      "Epoch 1, Batch 880, LR 0.260088 Loss 13.239198, Accuracy 35.542%\n",
      "Epoch 1, Batch 881, LR 0.260226 Loss 13.237894, Accuracy 35.561%\n",
      "Epoch 1, Batch 882, LR 0.260364 Loss 13.236905, Accuracy 35.567%\n",
      "Epoch 1, Batch 883, LR 0.260501 Loss 13.235790, Accuracy 35.576%\n",
      "Epoch 1, Batch 884, LR 0.260639 Loss 13.234194, Accuracy 35.585%\n",
      "Epoch 1, Batch 885, LR 0.260777 Loss 13.233113, Accuracy 35.591%\n",
      "Epoch 1, Batch 886, LR 0.260915 Loss 13.232331, Accuracy 35.597%\n",
      "Epoch 1, Batch 887, LR 0.261053 Loss 13.230817, Accuracy 35.605%\n",
      "Epoch 1, Batch 888, LR 0.261191 Loss 13.229997, Accuracy 35.615%\n",
      "Epoch 1, Batch 889, LR 0.261329 Loss 13.230373, Accuracy 35.616%\n",
      "Epoch 1, Batch 890, LR 0.261467 Loss 13.229033, Accuracy 35.622%\n",
      "Epoch 1, Batch 891, LR 0.261605 Loss 13.227919, Accuracy 35.632%\n",
      "Epoch 1, Batch 892, LR 0.261744 Loss 13.226446, Accuracy 35.640%\n",
      "Epoch 1, Batch 893, LR 0.261882 Loss 13.224936, Accuracy 35.653%\n",
      "Epoch 1, Batch 894, LR 0.262021 Loss 13.224521, Accuracy 35.660%\n",
      "Epoch 1, Batch 895, LR 0.262159 Loss 13.223086, Accuracy 35.669%\n",
      "Epoch 1, Batch 896, LR 0.262298 Loss 13.222647, Accuracy 35.678%\n",
      "Epoch 1, Batch 897, LR 0.262436 Loss 13.221596, Accuracy 35.688%\n",
      "Epoch 1, Batch 898, LR 0.262575 Loss 13.221055, Accuracy 35.692%\n",
      "Epoch 1, Batch 899, LR 0.262714 Loss 13.219487, Accuracy 35.707%\n",
      "Epoch 1, Batch 900, LR 0.262853 Loss 13.218607, Accuracy 35.710%\n",
      "Epoch 1, Batch 901, LR 0.262991 Loss 13.217093, Accuracy 35.726%\n",
      "Epoch 1, Batch 902, LR 0.263130 Loss 13.215846, Accuracy 35.737%\n",
      "Epoch 1, Batch 903, LR 0.263269 Loss 13.214417, Accuracy 35.748%\n",
      "Epoch 1, Batch 904, LR 0.263409 Loss 13.213532, Accuracy 35.753%\n",
      "Epoch 1, Batch 905, LR 0.263548 Loss 13.211951, Accuracy 35.767%\n",
      "Epoch 1, Batch 906, LR 0.263687 Loss 13.210175, Accuracy 35.779%\n",
      "Epoch 1, Batch 907, LR 0.263826 Loss 13.208569, Accuracy 35.784%\n",
      "Epoch 1, Batch 908, LR 0.263966 Loss 13.207824, Accuracy 35.788%\n",
      "Epoch 1, Batch 909, LR 0.264105 Loss 13.206957, Accuracy 35.795%\n",
      "Epoch 1, Batch 910, LR 0.264245 Loss 13.204692, Accuracy 35.816%\n",
      "Epoch 1, Batch 911, LR 0.264384 Loss 13.204392, Accuracy 35.820%\n",
      "Epoch 1, Batch 912, LR 0.264524 Loss 13.202319, Accuracy 35.839%\n",
      "Epoch 1, Batch 913, LR 0.264663 Loss 13.200414, Accuracy 35.851%\n",
      "Epoch 1, Batch 914, LR 0.264803 Loss 13.199223, Accuracy 35.859%\n",
      "Epoch 1, Batch 915, LR 0.264943 Loss 13.197753, Accuracy 35.872%\n",
      "Epoch 1, Batch 916, LR 0.265083 Loss 13.197223, Accuracy 35.872%\n",
      "Epoch 1, Batch 917, LR 0.265223 Loss 13.195982, Accuracy 35.884%\n",
      "Epoch 1, Batch 918, LR 0.265363 Loss 13.194585, Accuracy 35.897%\n",
      "Epoch 1, Batch 919, LR 0.265503 Loss 13.193755, Accuracy 35.903%\n",
      "Epoch 1, Batch 920, LR 0.265643 Loss 13.191431, Accuracy 35.925%\n",
      "Epoch 1, Batch 921, LR 0.265783 Loss 13.190501, Accuracy 35.938%\n",
      "Epoch 1, Batch 922, LR 0.265924 Loss 13.189327, Accuracy 35.949%\n",
      "Epoch 1, Batch 923, LR 0.266064 Loss 13.187613, Accuracy 35.965%\n",
      "Epoch 1, Batch 924, LR 0.266204 Loss 13.186315, Accuracy 35.970%\n",
      "Epoch 1, Batch 925, LR 0.266345 Loss 13.184410, Accuracy 35.985%\n",
      "Epoch 1, Batch 926, LR 0.266485 Loss 13.182653, Accuracy 35.998%\n",
      "Epoch 1, Batch 927, LR 0.266626 Loss 13.181691, Accuracy 36.005%\n",
      "Epoch 1, Batch 928, LR 0.266767 Loss 13.180491, Accuracy 36.015%\n",
      "Epoch 1, Batch 929, LR 0.266907 Loss 13.178640, Accuracy 36.026%\n",
      "Epoch 1, Batch 930, LR 0.267048 Loss 13.177533, Accuracy 36.036%\n",
      "Epoch 1, Batch 931, LR 0.267189 Loss 13.176068, Accuracy 36.050%\n",
      "Epoch 1, Batch 932, LR 0.267330 Loss 13.174815, Accuracy 36.060%\n",
      "Epoch 1, Batch 933, LR 0.267471 Loss 13.172517, Accuracy 36.077%\n",
      "Epoch 1, Batch 934, LR 0.267612 Loss 13.171733, Accuracy 36.081%\n",
      "Epoch 1, Batch 935, LR 0.267753 Loss 13.171077, Accuracy 36.088%\n",
      "Epoch 1, Batch 936, LR 0.267894 Loss 13.170385, Accuracy 36.093%\n",
      "Epoch 1, Batch 937, LR 0.268036 Loss 13.169360, Accuracy 36.095%\n",
      "Epoch 1, Batch 938, LR 0.268177 Loss 13.168569, Accuracy 36.102%\n",
      "Epoch 1, Batch 939, LR 0.268318 Loss 13.166769, Accuracy 36.116%\n",
      "Epoch 1, Batch 940, LR 0.268460 Loss 13.165079, Accuracy 36.127%\n",
      "Epoch 1, Batch 941, LR 0.268601 Loss 13.163867, Accuracy 36.138%\n",
      "Epoch 1, Batch 942, LR 0.268743 Loss 13.162769, Accuracy 36.148%\n",
      "Epoch 1, Batch 943, LR 0.268885 Loss 13.160910, Accuracy 36.160%\n",
      "Epoch 1, Batch 944, LR 0.269026 Loss 13.159429, Accuracy 36.173%\n",
      "Epoch 1, Batch 945, LR 0.269168 Loss 13.158224, Accuracy 36.181%\n",
      "Epoch 1, Batch 946, LR 0.269310 Loss 13.157094, Accuracy 36.189%\n",
      "Epoch 1, Batch 947, LR 0.269452 Loss 13.156196, Accuracy 36.195%\n",
      "Epoch 1, Batch 948, LR 0.269594 Loss 13.154916, Accuracy 36.203%\n",
      "Epoch 1, Batch 949, LR 0.269736 Loss 13.154011, Accuracy 36.212%\n",
      "Epoch 1, Batch 950, LR 0.269878 Loss 13.153002, Accuracy 36.220%\n",
      "Epoch 1, Batch 951, LR 0.270020 Loss 13.152058, Accuracy 36.228%\n",
      "Epoch 1, Batch 952, LR 0.270162 Loss 13.150631, Accuracy 36.233%\n",
      "Epoch 1, Batch 953, LR 0.270305 Loss 13.149297, Accuracy 36.240%\n",
      "Epoch 1, Batch 954, LR 0.270447 Loss 13.148103, Accuracy 36.251%\n",
      "Epoch 1, Batch 955, LR 0.270590 Loss 13.147436, Accuracy 36.261%\n",
      "Epoch 1, Batch 956, LR 0.270732 Loss 13.145701, Accuracy 36.276%\n",
      "Epoch 1, Batch 957, LR 0.270875 Loss 13.144923, Accuracy 36.283%\n",
      "Epoch 1, Batch 958, LR 0.271017 Loss 13.144081, Accuracy 36.290%\n",
      "Epoch 1, Batch 959, LR 0.271160 Loss 13.143533, Accuracy 36.294%\n",
      "Epoch 1, Batch 960, LR 0.271303 Loss 13.142227, Accuracy 36.304%\n",
      "Epoch 1, Batch 961, LR 0.271446 Loss 13.140573, Accuracy 36.319%\n",
      "Epoch 1, Batch 962, LR 0.271588 Loss 13.139166, Accuracy 36.332%\n",
      "Epoch 1, Batch 963, LR 0.271731 Loss 13.138862, Accuracy 36.332%\n",
      "Epoch 1, Batch 964, LR 0.271874 Loss 13.137698, Accuracy 36.344%\n",
      "Epoch 1, Batch 965, LR 0.272017 Loss 13.137051, Accuracy 36.348%\n",
      "Epoch 1, Batch 966, LR 0.272161 Loss 13.136018, Accuracy 36.361%\n",
      "Epoch 1, Batch 967, LR 0.272304 Loss 13.136023, Accuracy 36.357%\n",
      "Epoch 1, Batch 968, LR 0.272447 Loss 13.135403, Accuracy 36.363%\n",
      "Epoch 1, Batch 969, LR 0.272590 Loss 13.134181, Accuracy 36.374%\n",
      "Epoch 1, Batch 970, LR 0.272734 Loss 13.132723, Accuracy 36.381%\n",
      "Epoch 1, Batch 971, LR 0.272877 Loss 13.131782, Accuracy 36.384%\n",
      "Epoch 1, Batch 972, LR 0.273021 Loss 13.130291, Accuracy 36.398%\n",
      "Epoch 1, Batch 973, LR 0.273164 Loss 13.128796, Accuracy 36.416%\n",
      "Epoch 1, Batch 974, LR 0.273308 Loss 13.127587, Accuracy 36.424%\n",
      "Epoch 1, Batch 975, LR 0.273452 Loss 13.126205, Accuracy 36.433%\n",
      "Epoch 1, Batch 976, LR 0.273596 Loss 13.125288, Accuracy 36.437%\n",
      "Epoch 1, Batch 977, LR 0.273740 Loss 13.124650, Accuracy 36.441%\n",
      "Epoch 1, Batch 978, LR 0.273883 Loss 13.123934, Accuracy 36.452%\n",
      "Epoch 1, Batch 979, LR 0.274027 Loss 13.122669, Accuracy 36.462%\n",
      "Epoch 1, Batch 980, LR 0.274172 Loss 13.121721, Accuracy 36.468%\n",
      "Epoch 1, Batch 981, LR 0.274316 Loss 13.121104, Accuracy 36.471%\n",
      "Epoch 1, Batch 982, LR 0.274460 Loss 13.119966, Accuracy 36.486%\n",
      "Epoch 1, Batch 983, LR 0.274604 Loss 13.119779, Accuracy 36.488%\n",
      "Epoch 1, Batch 984, LR 0.274748 Loss 13.118704, Accuracy 36.496%\n",
      "Epoch 1, Batch 985, LR 0.274893 Loss 13.117396, Accuracy 36.503%\n",
      "Epoch 1, Batch 986, LR 0.275037 Loss 13.116840, Accuracy 36.504%\n",
      "Epoch 1, Batch 987, LR 0.275182 Loss 13.115573, Accuracy 36.516%\n",
      "Epoch 1, Batch 988, LR 0.275326 Loss 13.114152, Accuracy 36.533%\n",
      "Epoch 1, Batch 989, LR 0.275471 Loss 13.113252, Accuracy 36.539%\n",
      "Epoch 1, Batch 990, LR 0.275616 Loss 13.111975, Accuracy 36.547%\n",
      "Epoch 1, Batch 991, LR 0.275761 Loss 13.111077, Accuracy 36.553%\n",
      "Epoch 1, Batch 992, LR 0.275905 Loss 13.110321, Accuracy 36.556%\n",
      "Epoch 1, Batch 993, LR 0.276050 Loss 13.109875, Accuracy 36.561%\n",
      "Epoch 1, Batch 994, LR 0.276195 Loss 13.108767, Accuracy 36.574%\n",
      "Epoch 1, Batch 995, LR 0.276340 Loss 13.107754, Accuracy 36.578%\n",
      "Epoch 1, Batch 996, LR 0.276485 Loss 13.105999, Accuracy 36.595%\n",
      "Epoch 1, Batch 997, LR 0.276631 Loss 13.105330, Accuracy 36.597%\n",
      "Epoch 1, Batch 998, LR 0.276776 Loss 13.104379, Accuracy 36.606%\n",
      "Epoch 1, Batch 999, LR 0.276921 Loss 13.103531, Accuracy 36.614%\n",
      "Epoch 1, Batch 1000, LR 0.277066 Loss 13.102469, Accuracy 36.618%\n",
      "Epoch 1, Batch 1001, LR 0.277212 Loss 13.101647, Accuracy 36.625%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1002, LR 0.277357 Loss 13.100090, Accuracy 36.635%\n",
      "Epoch 1, Batch 1003, LR 0.277503 Loss 13.098490, Accuracy 36.648%\n",
      "Epoch 1, Batch 1004, LR 0.277648 Loss 13.097828, Accuracy 36.651%\n",
      "Epoch 1, Batch 1005, LR 0.277794 Loss 13.096333, Accuracy 36.668%\n",
      "Epoch 1, Batch 1006, LR 0.277940 Loss 13.095334, Accuracy 36.675%\n",
      "Epoch 1, Batch 1007, LR 0.278086 Loss 13.094057, Accuracy 36.687%\n",
      "Epoch 1, Batch 1008, LR 0.278232 Loss 13.093432, Accuracy 36.689%\n",
      "Epoch 1, Batch 1009, LR 0.278378 Loss 13.092375, Accuracy 36.698%\n",
      "Epoch 1, Batch 1010, LR 0.278524 Loss 13.091235, Accuracy 36.705%\n",
      "Epoch 1, Batch 1011, LR 0.278670 Loss 13.090507, Accuracy 36.709%\n",
      "Epoch 1, Batch 1012, LR 0.278816 Loss 13.089758, Accuracy 36.718%\n",
      "Epoch 1, Batch 1013, LR 0.278962 Loss 13.088164, Accuracy 36.731%\n",
      "Epoch 1, Batch 1014, LR 0.279108 Loss 13.087593, Accuracy 36.736%\n",
      "Epoch 1, Batch 1015, LR 0.279255 Loss 13.086723, Accuracy 36.743%\n",
      "Epoch 1, Batch 1016, LR 0.279401 Loss 13.085790, Accuracy 36.747%\n",
      "Epoch 1, Batch 1017, LR 0.279547 Loss 13.084296, Accuracy 36.762%\n",
      "Epoch 1, Batch 1018, LR 0.279694 Loss 13.083002, Accuracy 36.773%\n",
      "Epoch 1, Batch 1019, LR 0.279841 Loss 13.082580, Accuracy 36.771%\n",
      "Epoch 1, Batch 1020, LR 0.279987 Loss 13.080877, Accuracy 36.785%\n",
      "Epoch 1, Batch 1021, LR 0.280134 Loss 13.080201, Accuracy 36.793%\n",
      "Epoch 1, Batch 1022, LR 0.280281 Loss 13.078992, Accuracy 36.797%\n",
      "Epoch 1, Batch 1023, LR 0.280427 Loss 13.077986, Accuracy 36.807%\n",
      "Epoch 1, Batch 1024, LR 0.280574 Loss 13.076166, Accuracy 36.821%\n",
      "Epoch 1, Batch 1025, LR 0.280721 Loss 13.076042, Accuracy 36.822%\n",
      "Epoch 1, Batch 1026, LR 0.280868 Loss 13.075876, Accuracy 36.822%\n",
      "Epoch 1, Batch 1027, LR 0.281016 Loss 13.074141, Accuracy 36.837%\n",
      "Epoch 1, Batch 1028, LR 0.281163 Loss 13.073125, Accuracy 36.841%\n",
      "Epoch 1, Batch 1029, LR 0.281310 Loss 13.072046, Accuracy 36.852%\n",
      "Epoch 1, Batch 1030, LR 0.281457 Loss 13.070713, Accuracy 36.866%\n",
      "Epoch 1, Batch 1031, LR 0.281605 Loss 13.069908, Accuracy 36.872%\n",
      "Epoch 1, Batch 1032, LR 0.281752 Loss 13.069824, Accuracy 36.872%\n",
      "Epoch 1, Batch 1033, LR 0.281899 Loss 13.068761, Accuracy 36.884%\n",
      "Epoch 1, Batch 1034, LR 0.282047 Loss 13.068360, Accuracy 36.886%\n",
      "Epoch 1, Batch 1035, LR 0.282195 Loss 13.066952, Accuracy 36.897%\n",
      "Epoch 1, Batch 1036, LR 0.282342 Loss 13.065016, Accuracy 36.916%\n",
      "Epoch 1, Batch 1037, LR 0.282490 Loss 13.064431, Accuracy 36.922%\n",
      "Epoch 1, Batch 1038, LR 0.282638 Loss 13.063452, Accuracy 36.932%\n",
      "Epoch 1, Batch 1039, LR 0.282786 Loss 13.063256, Accuracy 36.929%\n",
      "Epoch 1, Batch 1040, LR 0.282934 Loss 13.062209, Accuracy 36.941%\n",
      "Epoch 1, Batch 1041, LR 0.283082 Loss 13.060977, Accuracy 36.950%\n",
      "Epoch 1, Batch 1042, LR 0.283230 Loss 13.059264, Accuracy 36.959%\n",
      "Epoch 1, Batch 1043, LR 0.283378 Loss 13.058417, Accuracy 36.963%\n",
      "Epoch 1, Batch 1044, LR 0.283526 Loss 13.057844, Accuracy 36.969%\n",
      "Epoch 1, Batch 1045, LR 0.283674 Loss 13.056233, Accuracy 36.985%\n",
      "Epoch 1, Batch 1046, LR 0.283823 Loss 13.054845, Accuracy 36.997%\n",
      "Epoch 1, Batch 1047, LR 0.283971 Loss 13.054117, Accuracy 36.999%\n",
      "Epoch 1, Loss (train set) 13.054117, Accuracy (train set) 36.999%\n",
      "Epoch 2, Batch 1, LR 0.284120 Loss 11.337594, Accuracy 52.344%\n",
      "Epoch 2, Batch 2, LR 0.284268 Loss 11.830251, Accuracy 48.047%\n",
      "Epoch 2, Batch 3, LR 0.284417 Loss 11.837751, Accuracy 47.135%\n",
      "Epoch 2, Batch 4, LR 0.284565 Loss 11.780095, Accuracy 47.461%\n",
      "Epoch 2, Batch 5, LR 0.284714 Loss 11.692384, Accuracy 47.344%\n",
      "Epoch 2, Batch 6, LR 0.284863 Loss 11.785906, Accuracy 47.266%\n",
      "Epoch 2, Batch 7, LR 0.285012 Loss 11.749486, Accuracy 47.321%\n",
      "Epoch 2, Batch 8, LR 0.285161 Loss 11.818622, Accuracy 47.070%\n",
      "Epoch 2, Batch 9, LR 0.285310 Loss 11.833157, Accuracy 46.962%\n",
      "Epoch 2, Batch 10, LR 0.285459 Loss 11.893631, Accuracy 46.484%\n",
      "Epoch 2, Batch 11, LR 0.285608 Loss 11.913116, Accuracy 46.662%\n",
      "Epoch 2, Batch 12, LR 0.285757 Loss 11.917216, Accuracy 46.354%\n",
      "Epoch 2, Batch 13, LR 0.285906 Loss 11.855094, Accuracy 46.995%\n",
      "Epoch 2, Batch 14, LR 0.286055 Loss 11.825072, Accuracy 47.210%\n",
      "Epoch 2, Batch 15, LR 0.286205 Loss 11.810112, Accuracy 47.240%\n",
      "Epoch 2, Batch 16, LR 0.286354 Loss 11.805843, Accuracy 47.266%\n",
      "Epoch 2, Batch 17, LR 0.286504 Loss 11.801851, Accuracy 47.289%\n",
      "Epoch 2, Batch 18, LR 0.286653 Loss 11.845572, Accuracy 47.005%\n",
      "Epoch 2, Batch 19, LR 0.286803 Loss 11.825386, Accuracy 47.368%\n",
      "Epoch 2, Batch 20, LR 0.286952 Loss 11.795586, Accuracy 47.695%\n",
      "Epoch 2, Batch 21, LR 0.287102 Loss 11.773201, Accuracy 47.917%\n",
      "Epoch 2, Batch 22, LR 0.287252 Loss 11.799767, Accuracy 47.727%\n",
      "Epoch 2, Batch 23, LR 0.287402 Loss 11.805515, Accuracy 47.554%\n",
      "Epoch 2, Batch 24, LR 0.287552 Loss 11.787087, Accuracy 47.689%\n",
      "Epoch 2, Batch 25, LR 0.287702 Loss 11.772538, Accuracy 47.875%\n",
      "Epoch 2, Batch 26, LR 0.287852 Loss 11.764838, Accuracy 47.806%\n",
      "Epoch 2, Batch 27, LR 0.288002 Loss 11.772018, Accuracy 47.714%\n",
      "Epoch 2, Batch 28, LR 0.288152 Loss 11.766764, Accuracy 47.740%\n",
      "Epoch 2, Batch 29, LR 0.288302 Loss 11.759669, Accuracy 47.737%\n",
      "Epoch 2, Batch 30, LR 0.288453 Loss 11.739116, Accuracy 47.760%\n",
      "Epoch 2, Batch 31, LR 0.288603 Loss 11.741119, Accuracy 47.581%\n",
      "Epoch 2, Batch 32, LR 0.288754 Loss 11.737146, Accuracy 47.681%\n",
      "Epoch 2, Batch 33, LR 0.288904 Loss 11.735049, Accuracy 47.680%\n",
      "Epoch 2, Batch 34, LR 0.289055 Loss 11.710546, Accuracy 47.978%\n",
      "Epoch 2, Batch 35, LR 0.289205 Loss 11.714316, Accuracy 48.036%\n",
      "Epoch 2, Batch 36, LR 0.289356 Loss 11.708226, Accuracy 48.025%\n",
      "Epoch 2, Batch 37, LR 0.289507 Loss 11.699051, Accuracy 48.269%\n",
      "Epoch 2, Batch 38, LR 0.289658 Loss 11.708121, Accuracy 48.314%\n",
      "Epoch 2, Batch 39, LR 0.289808 Loss 11.711364, Accuracy 48.197%\n",
      "Epoch 2, Batch 40, LR 0.289959 Loss 11.714238, Accuracy 48.145%\n",
      "Epoch 2, Batch 41, LR 0.290110 Loss 11.703698, Accuracy 48.152%\n",
      "Epoch 2, Batch 42, LR 0.290262 Loss 11.698907, Accuracy 48.214%\n",
      "Epoch 2, Batch 43, LR 0.290413 Loss 11.695034, Accuracy 48.219%\n",
      "Epoch 2, Batch 44, LR 0.290564 Loss 11.698854, Accuracy 48.384%\n",
      "Epoch 2, Batch 45, LR 0.290715 Loss 11.706434, Accuracy 48.299%\n",
      "Epoch 2, Batch 46, LR 0.290867 Loss 11.709264, Accuracy 48.302%\n",
      "Epoch 2, Batch 47, LR 0.291018 Loss 11.709956, Accuracy 48.271%\n",
      "Epoch 2, Batch 48, LR 0.291169 Loss 11.707522, Accuracy 48.242%\n",
      "Epoch 2, Batch 49, LR 0.291321 Loss 11.718872, Accuracy 48.151%\n",
      "Epoch 2, Batch 50, LR 0.291473 Loss 11.725118, Accuracy 48.125%\n",
      "Epoch 2, Batch 51, LR 0.291624 Loss 11.705766, Accuracy 48.284%\n",
      "Epoch 2, Batch 52, LR 0.291776 Loss 11.706654, Accuracy 48.212%\n",
      "Epoch 2, Batch 53, LR 0.291928 Loss 11.709493, Accuracy 48.261%\n",
      "Epoch 2, Batch 54, LR 0.292080 Loss 11.715929, Accuracy 48.032%\n",
      "Epoch 2, Batch 55, LR 0.292231 Loss 11.717689, Accuracy 48.054%\n",
      "Epoch 2, Batch 56, LR 0.292383 Loss 11.713182, Accuracy 48.117%\n",
      "Epoch 2, Batch 57, LR 0.292535 Loss 11.724363, Accuracy 48.026%\n",
      "Epoch 2, Batch 58, LR 0.292688 Loss 11.726026, Accuracy 47.966%\n",
      "Epoch 2, Batch 59, LR 0.292840 Loss 11.730299, Accuracy 47.881%\n",
      "Epoch 2, Batch 60, LR 0.292992 Loss 11.723428, Accuracy 47.930%\n",
      "Epoch 2, Batch 61, LR 0.293144 Loss 11.726224, Accuracy 47.861%\n",
      "Epoch 2, Batch 62, LR 0.293297 Loss 11.732287, Accuracy 47.770%\n",
      "Epoch 2, Batch 63, LR 0.293449 Loss 11.734606, Accuracy 47.731%\n",
      "Epoch 2, Batch 64, LR 0.293602 Loss 11.738901, Accuracy 47.705%\n",
      "Epoch 2, Batch 65, LR 0.293754 Loss 11.733593, Accuracy 47.752%\n",
      "Epoch 2, Batch 66, LR 0.293907 Loss 11.739240, Accuracy 47.656%\n",
      "Epoch 2, Batch 67, LR 0.294059 Loss 11.744282, Accuracy 47.610%\n",
      "Epoch 2, Batch 68, LR 0.294212 Loss 11.748560, Accuracy 47.564%\n",
      "Epoch 2, Batch 69, LR 0.294365 Loss 11.754898, Accuracy 47.509%\n",
      "Epoch 2, Batch 70, LR 0.294518 Loss 11.759111, Accuracy 47.533%\n",
      "Epoch 2, Batch 71, LR 0.294671 Loss 11.751616, Accuracy 47.634%\n",
      "Epoch 2, Batch 72, LR 0.294824 Loss 11.753110, Accuracy 47.537%\n",
      "Epoch 2, Batch 73, LR 0.294977 Loss 11.761223, Accuracy 47.421%\n",
      "Epoch 2, Batch 74, LR 0.295130 Loss 11.750587, Accuracy 47.519%\n",
      "Epoch 2, Batch 75, LR 0.295283 Loss 11.742670, Accuracy 47.646%\n",
      "Epoch 2, Batch 76, LR 0.295436 Loss 11.738605, Accuracy 47.687%\n",
      "Epoch 2, Batch 77, LR 0.295590 Loss 11.730217, Accuracy 47.788%\n",
      "Epoch 2, Batch 78, LR 0.295743 Loss 11.724834, Accuracy 47.867%\n",
      "Epoch 2, Batch 79, LR 0.295897 Loss 11.724054, Accuracy 47.844%\n",
      "Epoch 2, Batch 80, LR 0.296050 Loss 11.726667, Accuracy 47.871%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 81, LR 0.296204 Loss 11.730036, Accuracy 47.840%\n",
      "Epoch 2, Batch 82, LR 0.296357 Loss 11.732121, Accuracy 47.799%\n",
      "Epoch 2, Batch 83, LR 0.296511 Loss 11.734713, Accuracy 47.797%\n",
      "Epoch 2, Batch 84, LR 0.296665 Loss 11.733888, Accuracy 47.777%\n",
      "Epoch 2, Batch 85, LR 0.296819 Loss 11.736364, Accuracy 47.739%\n",
      "Epoch 2, Batch 86, LR 0.296972 Loss 11.738282, Accuracy 47.702%\n",
      "Epoch 2, Batch 87, LR 0.297126 Loss 11.740370, Accuracy 47.692%\n",
      "Epoch 2, Batch 88, LR 0.297280 Loss 11.742149, Accuracy 47.638%\n",
      "Epoch 2, Batch 89, LR 0.297434 Loss 11.740331, Accuracy 47.656%\n",
      "Epoch 2, Batch 90, LR 0.297589 Loss 11.740658, Accuracy 47.613%\n",
      "Epoch 2, Batch 91, LR 0.297743 Loss 11.741292, Accuracy 47.630%\n",
      "Epoch 2, Batch 92, LR 0.297897 Loss 11.742734, Accuracy 47.614%\n",
      "Epoch 2, Batch 93, LR 0.298051 Loss 11.738418, Accuracy 47.639%\n",
      "Epoch 2, Batch 94, LR 0.298206 Loss 11.738544, Accuracy 47.698%\n",
      "Epoch 2, Batch 95, LR 0.298360 Loss 11.745143, Accuracy 47.664%\n",
      "Epoch 2, Batch 96, LR 0.298515 Loss 11.740280, Accuracy 47.705%\n",
      "Epoch 2, Batch 97, LR 0.298669 Loss 11.742733, Accuracy 47.664%\n",
      "Epoch 2, Batch 98, LR 0.298824 Loss 11.741958, Accuracy 47.648%\n",
      "Epoch 2, Batch 99, LR 0.298979 Loss 11.741914, Accuracy 47.625%\n",
      "Epoch 2, Batch 100, LR 0.299134 Loss 11.739838, Accuracy 47.633%\n",
      "Epoch 2, Batch 101, LR 0.299288 Loss 11.741321, Accuracy 47.594%\n",
      "Epoch 2, Batch 102, LR 0.299443 Loss 11.735310, Accuracy 47.641%\n",
      "Epoch 2, Batch 103, LR 0.299598 Loss 11.732777, Accuracy 47.709%\n",
      "Epoch 2, Batch 104, LR 0.299753 Loss 11.733310, Accuracy 47.671%\n",
      "Epoch 2, Batch 105, LR 0.299908 Loss 11.727695, Accuracy 47.716%\n",
      "Epoch 2, Batch 106, LR 0.300064 Loss 11.722320, Accuracy 47.767%\n",
      "Epoch 2, Batch 107, LR 0.300219 Loss 11.720961, Accuracy 47.758%\n",
      "Epoch 2, Batch 108, LR 0.300374 Loss 11.713556, Accuracy 47.779%\n",
      "Epoch 2, Batch 109, LR 0.300529 Loss 11.712432, Accuracy 47.814%\n",
      "Epoch 2, Batch 110, LR 0.300685 Loss 11.715758, Accuracy 47.756%\n",
      "Epoch 2, Batch 111, LR 0.300840 Loss 11.713726, Accuracy 47.776%\n",
      "Epoch 2, Batch 112, LR 0.300996 Loss 11.708214, Accuracy 47.817%\n",
      "Epoch 2, Batch 113, LR 0.301151 Loss 11.708196, Accuracy 47.822%\n",
      "Epoch 2, Batch 114, LR 0.301307 Loss 11.703639, Accuracy 47.862%\n",
      "Epoch 2, Batch 115, LR 0.301463 Loss 11.699270, Accuracy 47.908%\n",
      "Epoch 2, Batch 116, LR 0.301619 Loss 11.695634, Accuracy 47.905%\n",
      "Epoch 2, Batch 117, LR 0.301774 Loss 11.694437, Accuracy 47.937%\n",
      "Epoch 2, Batch 118, LR 0.301930 Loss 11.695613, Accuracy 47.914%\n",
      "Epoch 2, Batch 119, LR 0.302086 Loss 11.695613, Accuracy 47.906%\n",
      "Epoch 2, Batch 120, LR 0.302242 Loss 11.695080, Accuracy 47.943%\n",
      "Epoch 2, Batch 121, LR 0.302398 Loss 11.696446, Accuracy 47.889%\n",
      "Epoch 2, Batch 122, LR 0.302555 Loss 11.695985, Accuracy 47.880%\n",
      "Epoch 2, Batch 123, LR 0.302711 Loss 11.692704, Accuracy 47.929%\n",
      "Epoch 2, Batch 124, LR 0.302867 Loss 11.688401, Accuracy 47.959%\n",
      "Epoch 2, Batch 125, LR 0.303024 Loss 11.685902, Accuracy 48.019%\n",
      "Epoch 2, Batch 126, LR 0.303180 Loss 11.683362, Accuracy 48.016%\n",
      "Epoch 2, Batch 127, LR 0.303336 Loss 11.681181, Accuracy 48.068%\n",
      "Epoch 2, Batch 128, LR 0.303493 Loss 11.678808, Accuracy 48.053%\n",
      "Epoch 2, Batch 129, LR 0.303650 Loss 11.677400, Accuracy 48.062%\n",
      "Epoch 2, Batch 130, LR 0.303806 Loss 11.682807, Accuracy 48.005%\n",
      "Epoch 2, Batch 131, LR 0.303963 Loss 11.682576, Accuracy 47.984%\n",
      "Epoch 2, Batch 132, LR 0.304120 Loss 11.681783, Accuracy 48.005%\n",
      "Epoch 2, Batch 133, LR 0.304277 Loss 11.677367, Accuracy 48.003%\n",
      "Epoch 2, Batch 134, LR 0.304434 Loss 11.675114, Accuracy 48.029%\n",
      "Epoch 2, Batch 135, LR 0.304591 Loss 11.678397, Accuracy 48.003%\n",
      "Epoch 2, Batch 136, LR 0.304748 Loss 11.678722, Accuracy 48.024%\n",
      "Epoch 2, Batch 137, LR 0.304905 Loss 11.683119, Accuracy 48.010%\n",
      "Epoch 2, Batch 138, LR 0.305062 Loss 11.683788, Accuracy 48.007%\n",
      "Epoch 2, Batch 139, LR 0.305219 Loss 11.685622, Accuracy 47.943%\n",
      "Epoch 2, Batch 140, LR 0.305376 Loss 11.683331, Accuracy 47.985%\n",
      "Epoch 2, Batch 141, LR 0.305534 Loss 11.686506, Accuracy 47.983%\n",
      "Epoch 2, Batch 142, LR 0.305691 Loss 11.684250, Accuracy 48.008%\n",
      "Epoch 2, Batch 143, LR 0.305849 Loss 11.684780, Accuracy 48.033%\n",
      "Epoch 2, Batch 144, LR 0.306006 Loss 11.684406, Accuracy 48.047%\n",
      "Epoch 2, Batch 145, LR 0.306164 Loss 11.684436, Accuracy 48.017%\n",
      "Epoch 2, Batch 146, LR 0.306322 Loss 11.682508, Accuracy 48.063%\n",
      "Epoch 2, Batch 147, LR 0.306479 Loss 11.688200, Accuracy 48.028%\n",
      "Epoch 2, Batch 148, LR 0.306637 Loss 11.686901, Accuracy 48.084%\n",
      "Epoch 2, Batch 149, LR 0.306795 Loss 11.688466, Accuracy 48.039%\n",
      "Epoch 2, Batch 150, LR 0.306953 Loss 11.685450, Accuracy 48.031%\n",
      "Epoch 2, Batch 151, LR 0.307111 Loss 11.683581, Accuracy 48.086%\n",
      "Epoch 2, Batch 152, LR 0.307269 Loss 11.680536, Accuracy 48.083%\n",
      "Epoch 2, Batch 153, LR 0.307427 Loss 11.679337, Accuracy 48.065%\n",
      "Epoch 2, Batch 154, LR 0.307585 Loss 11.679189, Accuracy 48.072%\n",
      "Epoch 2, Batch 155, LR 0.307744 Loss 11.684984, Accuracy 48.034%\n",
      "Epoch 2, Batch 156, LR 0.307902 Loss 11.682380, Accuracy 48.082%\n",
      "Epoch 2, Batch 157, LR 0.308060 Loss 11.681407, Accuracy 48.104%\n",
      "Epoch 2, Batch 158, LR 0.308219 Loss 11.677147, Accuracy 48.136%\n",
      "Epoch 2, Batch 159, LR 0.308377 Loss 11.683630, Accuracy 48.089%\n",
      "Epoch 2, Batch 160, LR 0.308536 Loss 11.679709, Accuracy 48.105%\n",
      "Epoch 2, Batch 161, LR 0.308694 Loss 11.678166, Accuracy 48.103%\n",
      "Epoch 2, Batch 162, LR 0.308853 Loss 11.676721, Accuracy 48.129%\n",
      "Epoch 2, Batch 163, LR 0.309012 Loss 11.673736, Accuracy 48.150%\n",
      "Epoch 2, Batch 164, LR 0.309171 Loss 11.670342, Accuracy 48.195%\n",
      "Epoch 2, Batch 165, LR 0.309329 Loss 11.671743, Accuracy 48.187%\n",
      "Epoch 2, Batch 166, LR 0.309488 Loss 11.671083, Accuracy 48.235%\n",
      "Epoch 2, Batch 167, LR 0.309647 Loss 11.671114, Accuracy 48.246%\n",
      "Epoch 2, Batch 168, LR 0.309806 Loss 11.669876, Accuracy 48.233%\n",
      "Epoch 2, Batch 169, LR 0.309966 Loss 11.671840, Accuracy 48.211%\n",
      "Epoch 2, Batch 170, LR 0.310125 Loss 11.670057, Accuracy 48.217%\n",
      "Epoch 2, Batch 171, LR 0.310284 Loss 11.669230, Accuracy 48.214%\n",
      "Epoch 2, Batch 172, LR 0.310443 Loss 11.667643, Accuracy 48.229%\n",
      "Epoch 2, Batch 173, LR 0.310603 Loss 11.665046, Accuracy 48.234%\n",
      "Epoch 2, Batch 174, LR 0.310762 Loss 11.661764, Accuracy 48.280%\n",
      "Epoch 2, Batch 175, LR 0.310922 Loss 11.662078, Accuracy 48.259%\n",
      "Epoch 2, Batch 176, LR 0.311081 Loss 11.659368, Accuracy 48.291%\n",
      "Epoch 2, Batch 177, LR 0.311241 Loss 11.658450, Accuracy 48.274%\n",
      "Epoch 2, Batch 178, LR 0.311401 Loss 11.658940, Accuracy 48.262%\n",
      "Epoch 2, Batch 179, LR 0.311560 Loss 11.658136, Accuracy 48.254%\n",
      "Epoch 2, Batch 180, LR 0.311720 Loss 11.658044, Accuracy 48.264%\n",
      "Epoch 2, Batch 181, LR 0.311880 Loss 11.659334, Accuracy 48.217%\n",
      "Epoch 2, Batch 182, LR 0.312040 Loss 11.663097, Accuracy 48.184%\n",
      "Epoch 2, Batch 183, LR 0.312200 Loss 11.663437, Accuracy 48.177%\n",
      "Epoch 2, Batch 184, LR 0.312360 Loss 11.662904, Accuracy 48.191%\n",
      "Epoch 2, Batch 185, LR 0.312520 Loss 11.665884, Accuracy 48.155%\n",
      "Epoch 2, Batch 186, LR 0.312680 Loss 11.666305, Accuracy 48.156%\n",
      "Epoch 2, Batch 187, LR 0.312841 Loss 11.666383, Accuracy 48.162%\n",
      "Epoch 2, Batch 188, LR 0.313001 Loss 11.663841, Accuracy 48.176%\n",
      "Epoch 2, Batch 189, LR 0.313161 Loss 11.662142, Accuracy 48.177%\n",
      "Epoch 2, Batch 190, LR 0.313322 Loss 11.660686, Accuracy 48.183%\n",
      "Epoch 2, Batch 191, LR 0.313482 Loss 11.657834, Accuracy 48.221%\n",
      "Epoch 2, Batch 192, LR 0.313643 Loss 11.659857, Accuracy 48.210%\n",
      "Epoch 2, Batch 193, LR 0.313803 Loss 11.661409, Accuracy 48.199%\n",
      "Epoch 2, Batch 194, LR 0.313964 Loss 11.663069, Accuracy 48.168%\n",
      "Epoch 2, Batch 195, LR 0.314125 Loss 11.663259, Accuracy 48.177%\n",
      "Epoch 2, Batch 196, LR 0.314286 Loss 11.663635, Accuracy 48.166%\n",
      "Epoch 2, Batch 197, LR 0.314447 Loss 11.662677, Accuracy 48.156%\n",
      "Epoch 2, Batch 198, LR 0.314608 Loss 11.660058, Accuracy 48.189%\n",
      "Epoch 2, Batch 199, LR 0.314769 Loss 11.657721, Accuracy 48.229%\n",
      "Epoch 2, Batch 200, LR 0.314930 Loss 11.660337, Accuracy 48.211%\n",
      "Epoch 2, Batch 201, LR 0.315091 Loss 11.657634, Accuracy 48.220%\n",
      "Epoch 2, Batch 202, LR 0.315252 Loss 11.657126, Accuracy 48.213%\n",
      "Epoch 2, Batch 203, LR 0.315413 Loss 11.656468, Accuracy 48.241%\n",
      "Epoch 2, Batch 204, LR 0.315574 Loss 11.658001, Accuracy 48.231%\n",
      "Epoch 2, Batch 205, LR 0.315736 Loss 11.658327, Accuracy 48.239%\n",
      "Epoch 2, Batch 206, LR 0.315897 Loss 11.660714, Accuracy 48.240%\n",
      "Epoch 2, Batch 207, LR 0.316059 Loss 11.658241, Accuracy 48.249%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 208, LR 0.316220 Loss 11.660578, Accuracy 48.216%\n",
      "Epoch 2, Batch 209, LR 0.316382 Loss 11.661876, Accuracy 48.198%\n",
      "Epoch 2, Batch 210, LR 0.316544 Loss 11.658521, Accuracy 48.211%\n",
      "Epoch 2, Batch 211, LR 0.316705 Loss 11.652137, Accuracy 48.271%\n",
      "Epoch 2, Batch 212, LR 0.316867 Loss 11.652423, Accuracy 48.294%\n",
      "Epoch 2, Batch 213, LR 0.317029 Loss 11.651851, Accuracy 48.313%\n",
      "Epoch 2, Batch 214, LR 0.317191 Loss 11.652390, Accuracy 48.302%\n",
      "Epoch 2, Batch 215, LR 0.317353 Loss 11.649787, Accuracy 48.318%\n",
      "Epoch 2, Batch 216, LR 0.317515 Loss 11.648994, Accuracy 48.318%\n",
      "Epoch 2, Batch 217, LR 0.317677 Loss 11.649564, Accuracy 48.304%\n",
      "Epoch 2, Batch 218, LR 0.317839 Loss 11.647812, Accuracy 48.312%\n",
      "Epoch 2, Batch 219, LR 0.318002 Loss 11.649104, Accuracy 48.313%\n",
      "Epoch 2, Batch 220, LR 0.318164 Loss 11.646682, Accuracy 48.335%\n",
      "Epoch 2, Batch 221, LR 0.318326 Loss 11.647498, Accuracy 48.310%\n",
      "Epoch 2, Batch 222, LR 0.318489 Loss 11.646546, Accuracy 48.318%\n",
      "Epoch 2, Batch 223, LR 0.318651 Loss 11.646043, Accuracy 48.311%\n",
      "Epoch 2, Batch 224, LR 0.318814 Loss 11.643476, Accuracy 48.322%\n",
      "Epoch 2, Batch 225, LR 0.318976 Loss 11.639914, Accuracy 48.340%\n",
      "Epoch 2, Batch 226, LR 0.319139 Loss 11.639067, Accuracy 48.375%\n",
      "Epoch 2, Batch 227, LR 0.319302 Loss 11.636591, Accuracy 48.376%\n",
      "Epoch 2, Batch 228, LR 0.319465 Loss 11.634054, Accuracy 48.383%\n",
      "Epoch 2, Batch 229, LR 0.319628 Loss 11.634106, Accuracy 48.362%\n",
      "Epoch 2, Batch 230, LR 0.319790 Loss 11.637372, Accuracy 48.332%\n",
      "Epoch 2, Batch 231, LR 0.319953 Loss 11.637230, Accuracy 48.333%\n",
      "Epoch 2, Batch 232, LR 0.320117 Loss 11.637661, Accuracy 48.306%\n",
      "Epoch 2, Batch 233, LR 0.320280 Loss 11.636121, Accuracy 48.340%\n",
      "Epoch 2, Batch 234, LR 0.320443 Loss 11.636144, Accuracy 48.334%\n",
      "Epoch 2, Batch 235, LR 0.320606 Loss 11.636790, Accuracy 48.334%\n",
      "Epoch 2, Batch 236, LR 0.320769 Loss 11.636065, Accuracy 48.345%\n",
      "Epoch 2, Batch 237, LR 0.320933 Loss 11.635011, Accuracy 48.365%\n",
      "Epoch 2, Batch 238, LR 0.321096 Loss 11.633353, Accuracy 48.385%\n",
      "Epoch 2, Batch 239, LR 0.321260 Loss 11.635129, Accuracy 48.379%\n",
      "Epoch 2, Batch 240, LR 0.321423 Loss 11.635034, Accuracy 48.382%\n",
      "Epoch 2, Batch 241, LR 0.321587 Loss 11.634397, Accuracy 48.415%\n",
      "Epoch 2, Batch 242, LR 0.321751 Loss 11.635033, Accuracy 48.405%\n",
      "Epoch 2, Batch 243, LR 0.321914 Loss 11.634180, Accuracy 48.415%\n",
      "Epoch 2, Batch 244, LR 0.322078 Loss 11.634573, Accuracy 48.412%\n",
      "Epoch 2, Batch 245, LR 0.322242 Loss 11.635678, Accuracy 48.425%\n",
      "Epoch 2, Batch 246, LR 0.322406 Loss 11.635911, Accuracy 48.415%\n",
      "Epoch 2, Batch 247, LR 0.322570 Loss 11.634761, Accuracy 48.431%\n",
      "Epoch 2, Batch 248, LR 0.322734 Loss 11.633308, Accuracy 48.438%\n",
      "Epoch 2, Batch 249, LR 0.322898 Loss 11.634549, Accuracy 48.406%\n",
      "Epoch 2, Batch 250, LR 0.323062 Loss 11.636081, Accuracy 48.375%\n",
      "Epoch 2, Batch 251, LR 0.323226 Loss 11.637116, Accuracy 48.350%\n",
      "Epoch 2, Batch 252, LR 0.323391 Loss 11.635671, Accuracy 48.363%\n",
      "Epoch 2, Batch 253, LR 0.323555 Loss 11.634155, Accuracy 48.391%\n",
      "Epoch 2, Batch 254, LR 0.323719 Loss 11.635278, Accuracy 48.367%\n",
      "Epoch 2, Batch 255, LR 0.323884 Loss 11.635093, Accuracy 48.361%\n",
      "Epoch 2, Batch 256, LR 0.324048 Loss 11.634325, Accuracy 48.380%\n",
      "Epoch 2, Batch 257, LR 0.324213 Loss 11.636305, Accuracy 48.358%\n",
      "Epoch 2, Batch 258, LR 0.324378 Loss 11.636086, Accuracy 48.350%\n",
      "Epoch 2, Batch 259, LR 0.324542 Loss 11.634216, Accuracy 48.353%\n",
      "Epoch 2, Batch 260, LR 0.324707 Loss 11.632913, Accuracy 48.356%\n",
      "Epoch 2, Batch 261, LR 0.324872 Loss 11.632297, Accuracy 48.375%\n",
      "Epoch 2, Batch 262, LR 0.325037 Loss 11.631147, Accuracy 48.384%\n",
      "Epoch 2, Batch 263, LR 0.325202 Loss 11.626712, Accuracy 48.408%\n",
      "Epoch 2, Batch 264, LR 0.325367 Loss 11.625908, Accuracy 48.411%\n",
      "Epoch 2, Batch 265, LR 0.325532 Loss 11.625386, Accuracy 48.420%\n",
      "Epoch 2, Batch 266, LR 0.325697 Loss 11.624578, Accuracy 48.423%\n",
      "Epoch 2, Batch 267, LR 0.325863 Loss 11.622983, Accuracy 48.438%\n",
      "Epoch 2, Batch 268, LR 0.326028 Loss 11.623085, Accuracy 48.435%\n",
      "Epoch 2, Batch 269, LR 0.326193 Loss 11.622703, Accuracy 48.438%\n",
      "Epoch 2, Batch 270, LR 0.326359 Loss 11.622805, Accuracy 48.423%\n",
      "Epoch 2, Batch 271, LR 0.326524 Loss 11.622047, Accuracy 48.423%\n",
      "Epoch 2, Batch 272, LR 0.326690 Loss 11.619705, Accuracy 48.417%\n",
      "Epoch 2, Batch 273, LR 0.326855 Loss 11.617693, Accuracy 48.443%\n",
      "Epoch 2, Batch 274, LR 0.327021 Loss 11.619995, Accuracy 48.429%\n",
      "Epoch 2, Batch 275, LR 0.327187 Loss 11.619447, Accuracy 48.432%\n",
      "Epoch 2, Batch 276, LR 0.327352 Loss 11.617940, Accuracy 48.454%\n",
      "Epoch 2, Batch 277, LR 0.327518 Loss 11.615967, Accuracy 48.460%\n",
      "Epoch 2, Batch 278, LR 0.327684 Loss 11.615841, Accuracy 48.468%\n",
      "Epoch 2, Batch 279, LR 0.327850 Loss 11.614034, Accuracy 48.482%\n",
      "Epoch 2, Batch 280, LR 0.328016 Loss 11.614397, Accuracy 48.477%\n",
      "Epoch 2, Batch 281, LR 0.328182 Loss 11.614372, Accuracy 48.482%\n",
      "Epoch 2, Batch 282, LR 0.328348 Loss 11.615394, Accuracy 48.471%\n",
      "Epoch 2, Batch 283, LR 0.328514 Loss 11.618054, Accuracy 48.462%\n",
      "Epoch 2, Batch 284, LR 0.328681 Loss 11.618022, Accuracy 48.462%\n",
      "Epoch 2, Batch 285, LR 0.328847 Loss 11.616760, Accuracy 48.470%\n",
      "Epoch 2, Batch 286, LR 0.329013 Loss 11.620063, Accuracy 48.421%\n",
      "Epoch 2, Batch 287, LR 0.329180 Loss 11.617580, Accuracy 48.424%\n",
      "Epoch 2, Batch 288, LR 0.329346 Loss 11.617892, Accuracy 48.419%\n",
      "Epoch 2, Batch 289, LR 0.329513 Loss 11.616431, Accuracy 48.424%\n",
      "Epoch 2, Batch 290, LR 0.329680 Loss 11.615285, Accuracy 48.429%\n",
      "Epoch 2, Batch 291, LR 0.329846 Loss 11.616152, Accuracy 48.416%\n",
      "Epoch 2, Batch 292, LR 0.330013 Loss 11.617076, Accuracy 48.424%\n",
      "Epoch 2, Batch 293, LR 0.330180 Loss 11.619354, Accuracy 48.398%\n",
      "Epoch 2, Batch 294, LR 0.330347 Loss 11.616809, Accuracy 48.408%\n",
      "Epoch 2, Batch 295, LR 0.330514 Loss 11.619223, Accuracy 48.398%\n",
      "Epoch 2, Batch 296, LR 0.330681 Loss 11.619963, Accuracy 48.403%\n",
      "Epoch 2, Batch 297, LR 0.330848 Loss 11.619413, Accuracy 48.411%\n",
      "Epoch 2, Batch 298, LR 0.331015 Loss 11.617476, Accuracy 48.430%\n",
      "Epoch 2, Batch 299, LR 0.331182 Loss 11.618904, Accuracy 48.401%\n",
      "Epoch 2, Batch 300, LR 0.331349 Loss 11.616728, Accuracy 48.432%\n",
      "Epoch 2, Batch 301, LR 0.331517 Loss 11.616690, Accuracy 48.425%\n",
      "Epoch 2, Batch 302, LR 0.331684 Loss 11.614326, Accuracy 48.440%\n",
      "Epoch 2, Batch 303, LR 0.331852 Loss 11.616782, Accuracy 48.425%\n",
      "Epoch 2, Batch 304, LR 0.332019 Loss 11.619263, Accuracy 48.384%\n",
      "Epoch 2, Batch 305, LR 0.332187 Loss 11.618617, Accuracy 48.391%\n",
      "Epoch 2, Batch 306, LR 0.332354 Loss 11.617069, Accuracy 48.402%\n",
      "Epoch 2, Batch 307, LR 0.332522 Loss 11.617156, Accuracy 48.415%\n",
      "Epoch 2, Batch 308, LR 0.332690 Loss 11.615017, Accuracy 48.422%\n",
      "Epoch 2, Batch 309, LR 0.332857 Loss 11.613721, Accuracy 48.430%\n",
      "Epoch 2, Batch 310, LR 0.333025 Loss 11.611530, Accuracy 48.458%\n",
      "Epoch 2, Batch 311, LR 0.333193 Loss 11.608786, Accuracy 48.470%\n",
      "Epoch 2, Batch 312, LR 0.333361 Loss 11.608627, Accuracy 48.465%\n",
      "Epoch 2, Batch 313, LR 0.333529 Loss 11.612387, Accuracy 48.428%\n",
      "Epoch 2, Batch 314, LR 0.333697 Loss 11.611038, Accuracy 48.423%\n",
      "Epoch 2, Batch 315, LR 0.333866 Loss 11.610978, Accuracy 48.415%\n",
      "Epoch 2, Batch 316, LR 0.334034 Loss 11.607425, Accuracy 48.452%\n",
      "Epoch 2, Batch 317, LR 0.334202 Loss 11.607741, Accuracy 48.450%\n",
      "Epoch 2, Batch 318, LR 0.334370 Loss 11.608044, Accuracy 48.447%\n",
      "Epoch 2, Batch 319, LR 0.334539 Loss 11.605988, Accuracy 48.482%\n",
      "Epoch 2, Batch 320, LR 0.334707 Loss 11.605102, Accuracy 48.499%\n",
      "Epoch 2, Batch 321, LR 0.334876 Loss 11.604092, Accuracy 48.496%\n",
      "Epoch 2, Batch 322, LR 0.335044 Loss 11.603724, Accuracy 48.493%\n",
      "Epoch 2, Batch 323, LR 0.335213 Loss 11.604015, Accuracy 48.479%\n",
      "Epoch 2, Batch 324, LR 0.335382 Loss 11.603934, Accuracy 48.478%\n",
      "Epoch 2, Batch 325, LR 0.335551 Loss 11.602431, Accuracy 48.495%\n",
      "Epoch 2, Batch 326, LR 0.335719 Loss 11.603181, Accuracy 48.476%\n",
      "Epoch 2, Batch 327, LR 0.335888 Loss 11.604681, Accuracy 48.461%\n",
      "Epoch 2, Batch 328, LR 0.336057 Loss 11.602409, Accuracy 48.466%\n",
      "Epoch 2, Batch 329, LR 0.336226 Loss 11.599856, Accuracy 48.483%\n",
      "Epoch 2, Batch 330, LR 0.336395 Loss 11.601715, Accuracy 48.478%\n",
      "Epoch 2, Batch 331, LR 0.336565 Loss 11.600445, Accuracy 48.489%\n",
      "Epoch 2, Batch 332, LR 0.336734 Loss 11.599989, Accuracy 48.489%\n",
      "Epoch 2, Batch 333, LR 0.336903 Loss 11.598514, Accuracy 48.506%\n",
      "Epoch 2, Batch 334, LR 0.337072 Loss 11.598510, Accuracy 48.505%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 335, LR 0.337242 Loss 11.597484, Accuracy 48.514%\n",
      "Epoch 2, Batch 336, LR 0.337411 Loss 11.598849, Accuracy 48.505%\n",
      "Epoch 2, Batch 337, LR 0.337581 Loss 11.599568, Accuracy 48.495%\n",
      "Epoch 2, Batch 338, LR 0.337750 Loss 11.598675, Accuracy 48.507%\n",
      "Epoch 2, Batch 339, LR 0.337920 Loss 11.597091, Accuracy 48.507%\n",
      "Epoch 2, Batch 340, LR 0.338090 Loss 11.598356, Accuracy 48.497%\n",
      "Epoch 2, Batch 341, LR 0.338259 Loss 11.598127, Accuracy 48.504%\n",
      "Epoch 2, Batch 342, LR 0.338429 Loss 11.596820, Accuracy 48.515%\n",
      "Epoch 2, Batch 343, LR 0.338599 Loss 11.596597, Accuracy 48.508%\n",
      "Epoch 2, Batch 344, LR 0.338769 Loss 11.595925, Accuracy 48.501%\n",
      "Epoch 2, Batch 345, LR 0.338939 Loss 11.595497, Accuracy 48.512%\n",
      "Epoch 2, Batch 346, LR 0.339109 Loss 11.593314, Accuracy 48.532%\n",
      "Epoch 2, Batch 347, LR 0.339279 Loss 11.593892, Accuracy 48.532%\n",
      "Epoch 2, Batch 348, LR 0.339450 Loss 11.595499, Accuracy 48.516%\n",
      "Epoch 2, Batch 349, LR 0.339620 Loss 11.595764, Accuracy 48.514%\n",
      "Epoch 2, Batch 350, LR 0.339790 Loss 11.593972, Accuracy 48.509%\n",
      "Epoch 2, Batch 351, LR 0.339960 Loss 11.592580, Accuracy 48.529%\n",
      "Epoch 2, Batch 352, LR 0.340131 Loss 11.592541, Accuracy 48.520%\n",
      "Epoch 2, Batch 353, LR 0.340301 Loss 11.592408, Accuracy 48.522%\n",
      "Epoch 2, Batch 354, LR 0.340472 Loss 11.592419, Accuracy 48.528%\n",
      "Epoch 2, Batch 355, LR 0.340643 Loss 11.592395, Accuracy 48.523%\n",
      "Epoch 2, Batch 356, LR 0.340813 Loss 11.590284, Accuracy 48.534%\n",
      "Epoch 2, Batch 357, LR 0.340984 Loss 11.589721, Accuracy 48.549%\n",
      "Epoch 2, Batch 358, LR 0.341155 Loss 11.589987, Accuracy 48.527%\n",
      "Epoch 2, Batch 359, LR 0.341326 Loss 11.591084, Accuracy 48.511%\n",
      "Epoch 2, Batch 360, LR 0.341496 Loss 11.591186, Accuracy 48.505%\n",
      "Epoch 2, Batch 361, LR 0.341667 Loss 11.592442, Accuracy 48.500%\n",
      "Epoch 2, Batch 362, LR 0.341839 Loss 11.592816, Accuracy 48.509%\n",
      "Epoch 2, Batch 363, LR 0.342010 Loss 11.592477, Accuracy 48.517%\n",
      "Epoch 2, Batch 364, LR 0.342181 Loss 11.590438, Accuracy 48.530%\n",
      "Epoch 2, Batch 365, LR 0.342352 Loss 11.590965, Accuracy 48.519%\n",
      "Epoch 2, Batch 366, LR 0.342523 Loss 11.589423, Accuracy 48.529%\n",
      "Epoch 2, Batch 367, LR 0.342695 Loss 11.588414, Accuracy 48.544%\n",
      "Epoch 2, Batch 368, LR 0.342866 Loss 11.586996, Accuracy 48.559%\n",
      "Epoch 2, Batch 369, LR 0.343037 Loss 11.586368, Accuracy 48.556%\n",
      "Epoch 2, Batch 370, LR 0.343209 Loss 11.582151, Accuracy 48.587%\n",
      "Epoch 2, Batch 371, LR 0.343381 Loss 11.582044, Accuracy 48.591%\n",
      "Epoch 2, Batch 372, LR 0.343552 Loss 11.583137, Accuracy 48.585%\n",
      "Epoch 2, Batch 373, LR 0.343724 Loss 11.583367, Accuracy 48.584%\n",
      "Epoch 2, Batch 374, LR 0.343896 Loss 11.581559, Accuracy 48.607%\n",
      "Epoch 2, Batch 375, LR 0.344067 Loss 11.581259, Accuracy 48.600%\n",
      "Epoch 2, Batch 376, LR 0.344239 Loss 11.579727, Accuracy 48.618%\n",
      "Epoch 2, Batch 377, LR 0.344411 Loss 11.578239, Accuracy 48.636%\n",
      "Epoch 2, Batch 378, LR 0.344583 Loss 11.576340, Accuracy 48.657%\n",
      "Epoch 2, Batch 379, LR 0.344755 Loss 11.576956, Accuracy 48.654%\n",
      "Epoch 2, Batch 380, LR 0.344928 Loss 11.576504, Accuracy 48.653%\n",
      "Epoch 2, Batch 381, LR 0.345100 Loss 11.576524, Accuracy 48.661%\n",
      "Epoch 2, Batch 382, LR 0.345272 Loss 11.575658, Accuracy 48.667%\n",
      "Epoch 2, Batch 383, LR 0.345444 Loss 11.574246, Accuracy 48.672%\n",
      "Epoch 2, Batch 384, LR 0.345617 Loss 11.574489, Accuracy 48.661%\n",
      "Epoch 2, Batch 385, LR 0.345789 Loss 11.575438, Accuracy 48.647%\n",
      "Epoch 2, Batch 386, LR 0.345962 Loss 11.575312, Accuracy 48.638%\n",
      "Epoch 2, Batch 387, LR 0.346134 Loss 11.576168, Accuracy 48.627%\n",
      "Epoch 2, Batch 388, LR 0.346307 Loss 11.575122, Accuracy 48.635%\n",
      "Epoch 2, Batch 389, LR 0.346479 Loss 11.573916, Accuracy 48.642%\n",
      "Epoch 2, Batch 390, LR 0.346652 Loss 11.573224, Accuracy 48.650%\n",
      "Epoch 2, Batch 391, LR 0.346825 Loss 11.573041, Accuracy 48.653%\n",
      "Epoch 2, Batch 392, LR 0.346998 Loss 11.572266, Accuracy 48.657%\n",
      "Epoch 2, Batch 393, LR 0.347171 Loss 11.571471, Accuracy 48.660%\n",
      "Epoch 2, Batch 394, LR 0.347344 Loss 11.570908, Accuracy 48.658%\n",
      "Epoch 2, Batch 395, LR 0.347517 Loss 11.570018, Accuracy 48.661%\n",
      "Epoch 2, Batch 396, LR 0.347690 Loss 11.568724, Accuracy 48.658%\n",
      "Epoch 2, Batch 397, LR 0.347863 Loss 11.568433, Accuracy 48.656%\n",
      "Epoch 2, Batch 398, LR 0.348036 Loss 11.565500, Accuracy 48.677%\n",
      "Epoch 2, Batch 399, LR 0.348209 Loss 11.566274, Accuracy 48.671%\n",
      "Epoch 2, Batch 400, LR 0.348383 Loss 11.565876, Accuracy 48.680%\n",
      "Epoch 2, Batch 401, LR 0.348556 Loss 11.565923, Accuracy 48.675%\n",
      "Epoch 2, Batch 402, LR 0.348730 Loss 11.564740, Accuracy 48.686%\n",
      "Epoch 2, Batch 403, LR 0.348903 Loss 11.565421, Accuracy 48.670%\n",
      "Epoch 2, Batch 404, LR 0.349077 Loss 11.564239, Accuracy 48.670%\n",
      "Epoch 2, Batch 405, LR 0.349250 Loss 11.563725, Accuracy 48.665%\n",
      "Epoch 2, Batch 406, LR 0.349424 Loss 11.564575, Accuracy 48.649%\n",
      "Epoch 2, Batch 407, LR 0.349598 Loss 11.564623, Accuracy 48.643%\n",
      "Epoch 2, Batch 408, LR 0.349772 Loss 11.565524, Accuracy 48.635%\n",
      "Epoch 2, Batch 409, LR 0.349946 Loss 11.566249, Accuracy 48.621%\n",
      "Epoch 2, Batch 410, LR 0.350119 Loss 11.565925, Accuracy 48.619%\n",
      "Epoch 2, Batch 411, LR 0.350293 Loss 11.565081, Accuracy 48.631%\n",
      "Epoch 2, Batch 412, LR 0.350468 Loss 11.563699, Accuracy 48.644%\n",
      "Epoch 2, Batch 413, LR 0.350642 Loss 11.563085, Accuracy 48.653%\n",
      "Epoch 2, Batch 414, LR 0.350816 Loss 11.562601, Accuracy 48.673%\n",
      "Epoch 2, Batch 415, LR 0.350990 Loss 11.562718, Accuracy 48.675%\n",
      "Epoch 2, Batch 416, LR 0.351164 Loss 11.562342, Accuracy 48.668%\n",
      "Epoch 2, Batch 417, LR 0.351339 Loss 11.561684, Accuracy 48.672%\n",
      "Epoch 2, Batch 418, LR 0.351513 Loss 11.560106, Accuracy 48.692%\n",
      "Epoch 2, Batch 419, LR 0.351688 Loss 11.558972, Accuracy 48.702%\n",
      "Epoch 2, Batch 420, LR 0.351862 Loss 11.560127, Accuracy 48.702%\n",
      "Epoch 2, Batch 421, LR 0.352037 Loss 11.560245, Accuracy 48.697%\n",
      "Epoch 2, Batch 422, LR 0.352211 Loss 11.558931, Accuracy 48.717%\n",
      "Epoch 2, Batch 423, LR 0.352386 Loss 11.558120, Accuracy 48.739%\n",
      "Epoch 2, Batch 424, LR 0.352561 Loss 11.557953, Accuracy 48.747%\n",
      "Epoch 2, Batch 425, LR 0.352736 Loss 11.556982, Accuracy 48.748%\n",
      "Epoch 2, Batch 426, LR 0.352911 Loss 11.556117, Accuracy 48.753%\n",
      "Epoch 2, Batch 427, LR 0.353086 Loss 11.556554, Accuracy 48.750%\n",
      "Epoch 2, Batch 428, LR 0.353261 Loss 11.556312, Accuracy 48.766%\n",
      "Epoch 2, Batch 429, LR 0.353436 Loss 11.555487, Accuracy 48.789%\n",
      "Epoch 2, Batch 430, LR 0.353611 Loss 11.554393, Accuracy 48.790%\n",
      "Epoch 2, Batch 431, LR 0.353786 Loss 11.553133, Accuracy 48.793%\n",
      "Epoch 2, Batch 432, LR 0.353961 Loss 11.551854, Accuracy 48.799%\n",
      "Epoch 2, Batch 433, LR 0.354137 Loss 11.551954, Accuracy 48.793%\n",
      "Epoch 2, Batch 434, LR 0.354312 Loss 11.551481, Accuracy 48.805%\n",
      "Epoch 2, Batch 435, LR 0.354487 Loss 11.551411, Accuracy 48.806%\n",
      "Epoch 2, Batch 436, LR 0.354663 Loss 11.552008, Accuracy 48.801%\n",
      "Epoch 2, Batch 437, LR 0.354838 Loss 11.551122, Accuracy 48.811%\n",
      "Epoch 2, Batch 438, LR 0.355014 Loss 11.550027, Accuracy 48.809%\n",
      "Epoch 2, Batch 439, LR 0.355190 Loss 11.547957, Accuracy 48.831%\n",
      "Epoch 2, Batch 440, LR 0.355366 Loss 11.547341, Accuracy 48.841%\n",
      "Epoch 2, Batch 441, LR 0.355541 Loss 11.547372, Accuracy 48.836%\n",
      "Epoch 2, Batch 442, LR 0.355717 Loss 11.546982, Accuracy 48.835%\n",
      "Epoch 2, Batch 443, LR 0.355893 Loss 11.548359, Accuracy 48.829%\n",
      "Epoch 2, Batch 444, LR 0.356069 Loss 11.548365, Accuracy 48.828%\n",
      "Epoch 2, Batch 445, LR 0.356245 Loss 11.549031, Accuracy 48.810%\n",
      "Epoch 2, Batch 446, LR 0.356421 Loss 11.547935, Accuracy 48.828%\n",
      "Epoch 2, Batch 447, LR 0.356597 Loss 11.547171, Accuracy 48.839%\n",
      "Epoch 2, Batch 448, LR 0.356774 Loss 11.546353, Accuracy 48.847%\n",
      "Epoch 2, Batch 449, LR 0.356950 Loss 11.546036, Accuracy 48.839%\n",
      "Epoch 2, Batch 450, LR 0.357126 Loss 11.543989, Accuracy 48.849%\n",
      "Epoch 2, Batch 451, LR 0.357303 Loss 11.544259, Accuracy 48.834%\n",
      "Epoch 2, Batch 452, LR 0.357479 Loss 11.544093, Accuracy 48.832%\n",
      "Epoch 2, Batch 453, LR 0.357656 Loss 11.543467, Accuracy 48.836%\n",
      "Epoch 2, Batch 454, LR 0.357832 Loss 11.541609, Accuracy 48.856%\n",
      "Epoch 2, Batch 455, LR 0.358009 Loss 11.538702, Accuracy 48.874%\n",
      "Epoch 2, Batch 456, LR 0.358185 Loss 11.538781, Accuracy 48.857%\n",
      "Epoch 2, Batch 457, LR 0.358362 Loss 11.537128, Accuracy 48.863%\n",
      "Epoch 2, Batch 458, LR 0.358539 Loss 11.537348, Accuracy 48.855%\n",
      "Epoch 2, Batch 459, LR 0.358716 Loss 11.535847, Accuracy 48.858%\n",
      "Epoch 2, Batch 460, LR 0.358893 Loss 11.535525, Accuracy 48.867%\n",
      "Epoch 2, Batch 461, LR 0.359070 Loss 11.533784, Accuracy 48.875%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 462, LR 0.359247 Loss 11.533522, Accuracy 48.867%\n",
      "Epoch 2, Batch 463, LR 0.359424 Loss 11.533255, Accuracy 48.871%\n",
      "Epoch 2, Batch 464, LR 0.359601 Loss 11.533055, Accuracy 48.875%\n",
      "Epoch 2, Batch 465, LR 0.359778 Loss 11.533516, Accuracy 48.873%\n",
      "Epoch 2, Batch 466, LR 0.359956 Loss 11.532719, Accuracy 48.885%\n",
      "Epoch 2, Batch 467, LR 0.360133 Loss 11.530675, Accuracy 48.893%\n",
      "Epoch 2, Batch 468, LR 0.360310 Loss 11.529338, Accuracy 48.907%\n",
      "Epoch 2, Batch 469, LR 0.360488 Loss 11.528451, Accuracy 48.917%\n",
      "Epoch 2, Batch 470, LR 0.360665 Loss 11.527334, Accuracy 48.926%\n",
      "Epoch 2, Batch 471, LR 0.360843 Loss 11.526981, Accuracy 48.932%\n",
      "Epoch 2, Batch 472, LR 0.361021 Loss 11.525710, Accuracy 48.946%\n",
      "Epoch 2, Batch 473, LR 0.361198 Loss 11.525949, Accuracy 48.945%\n",
      "Epoch 2, Batch 474, LR 0.361376 Loss 11.523054, Accuracy 48.973%\n",
      "Epoch 2, Batch 475, LR 0.361554 Loss 11.522229, Accuracy 48.985%\n",
      "Epoch 2, Batch 476, LR 0.361732 Loss 11.521050, Accuracy 49.002%\n",
      "Epoch 2, Batch 477, LR 0.361910 Loss 11.520198, Accuracy 49.006%\n",
      "Epoch 2, Batch 478, LR 0.362088 Loss 11.520299, Accuracy 49.005%\n",
      "Epoch 2, Batch 479, LR 0.362266 Loss 11.520530, Accuracy 49.000%\n",
      "Epoch 2, Batch 480, LR 0.362444 Loss 11.519268, Accuracy 49.007%\n",
      "Epoch 2, Batch 481, LR 0.362622 Loss 11.519114, Accuracy 49.014%\n",
      "Epoch 2, Batch 482, LR 0.362800 Loss 11.517375, Accuracy 49.034%\n",
      "Epoch 2, Batch 483, LR 0.362979 Loss 11.515799, Accuracy 49.047%\n",
      "Epoch 2, Batch 484, LR 0.363157 Loss 11.514556, Accuracy 49.064%\n",
      "Epoch 2, Batch 485, LR 0.363336 Loss 11.514477, Accuracy 49.064%\n",
      "Epoch 2, Batch 486, LR 0.363514 Loss 11.513408, Accuracy 49.058%\n",
      "Epoch 2, Batch 487, LR 0.363693 Loss 11.513209, Accuracy 49.060%\n",
      "Epoch 2, Batch 488, LR 0.363871 Loss 11.513241, Accuracy 49.055%\n",
      "Epoch 2, Batch 489, LR 0.364050 Loss 11.513384, Accuracy 49.051%\n",
      "Epoch 2, Batch 490, LR 0.364229 Loss 11.513521, Accuracy 49.047%\n",
      "Epoch 2, Batch 491, LR 0.364407 Loss 11.513425, Accuracy 49.044%\n",
      "Epoch 2, Batch 492, LR 0.364586 Loss 11.512900, Accuracy 49.054%\n",
      "Epoch 2, Batch 493, LR 0.364765 Loss 11.512388, Accuracy 49.062%\n",
      "Epoch 2, Batch 494, LR 0.364944 Loss 11.511725, Accuracy 49.069%\n",
      "Epoch 2, Batch 495, LR 0.365123 Loss 11.510985, Accuracy 49.069%\n",
      "Epoch 2, Batch 496, LR 0.365302 Loss 11.510687, Accuracy 49.064%\n",
      "Epoch 2, Batch 497, LR 0.365481 Loss 11.509755, Accuracy 49.068%\n",
      "Epoch 2, Batch 498, LR 0.365660 Loss 11.509159, Accuracy 49.076%\n",
      "Epoch 2, Batch 499, LR 0.365840 Loss 11.509034, Accuracy 49.079%\n",
      "Epoch 2, Batch 500, LR 0.366019 Loss 11.509224, Accuracy 49.073%\n",
      "Epoch 2, Batch 501, LR 0.366198 Loss 11.509061, Accuracy 49.085%\n",
      "Epoch 2, Batch 502, LR 0.366378 Loss 11.508681, Accuracy 49.082%\n",
      "Epoch 2, Batch 503, LR 0.366557 Loss 11.508297, Accuracy 49.093%\n",
      "Epoch 2, Batch 504, LR 0.366737 Loss 11.508391, Accuracy 49.089%\n",
      "Epoch 2, Batch 505, LR 0.366916 Loss 11.507059, Accuracy 49.107%\n",
      "Epoch 2, Batch 506, LR 0.367096 Loss 11.506151, Accuracy 49.114%\n",
      "Epoch 2, Batch 507, LR 0.367276 Loss 11.506118, Accuracy 49.123%\n",
      "Epoch 2, Batch 508, LR 0.367456 Loss 11.506930, Accuracy 49.123%\n",
      "Epoch 2, Batch 509, LR 0.367635 Loss 11.504981, Accuracy 49.142%\n",
      "Epoch 2, Batch 510, LR 0.367815 Loss 11.505254, Accuracy 49.141%\n",
      "Epoch 2, Batch 511, LR 0.367995 Loss 11.503738, Accuracy 49.145%\n",
      "Epoch 2, Batch 512, LR 0.368175 Loss 11.503549, Accuracy 49.144%\n",
      "Epoch 2, Batch 513, LR 0.368355 Loss 11.502594, Accuracy 49.144%\n",
      "Epoch 2, Batch 514, LR 0.368536 Loss 11.502180, Accuracy 49.158%\n",
      "Epoch 2, Batch 515, LR 0.368716 Loss 11.501053, Accuracy 49.167%\n",
      "Epoch 2, Batch 516, LR 0.368896 Loss 11.500670, Accuracy 49.170%\n",
      "Epoch 2, Batch 517, LR 0.369076 Loss 11.499504, Accuracy 49.182%\n",
      "Epoch 2, Batch 518, LR 0.369257 Loss 11.498894, Accuracy 49.187%\n",
      "Epoch 2, Batch 519, LR 0.369437 Loss 11.498497, Accuracy 49.198%\n",
      "Epoch 2, Batch 520, LR 0.369618 Loss 11.498210, Accuracy 49.201%\n",
      "Epoch 2, Batch 521, LR 0.369798 Loss 11.496572, Accuracy 49.217%\n",
      "Epoch 2, Batch 522, LR 0.369979 Loss 11.496657, Accuracy 49.216%\n",
      "Epoch 2, Batch 523, LR 0.370159 Loss 11.497087, Accuracy 49.222%\n",
      "Epoch 2, Batch 524, LR 0.370340 Loss 11.497632, Accuracy 49.222%\n",
      "Epoch 2, Batch 525, LR 0.370521 Loss 11.497549, Accuracy 49.223%\n",
      "Epoch 2, Batch 526, LR 0.370702 Loss 11.496262, Accuracy 49.238%\n",
      "Epoch 2, Batch 527, LR 0.370883 Loss 11.494519, Accuracy 49.254%\n",
      "Epoch 2, Batch 528, LR 0.371064 Loss 11.494681, Accuracy 49.254%\n",
      "Epoch 2, Batch 529, LR 0.371245 Loss 11.492741, Accuracy 49.265%\n",
      "Epoch 2, Batch 530, LR 0.371426 Loss 11.493181, Accuracy 49.263%\n",
      "Epoch 2, Batch 531, LR 0.371607 Loss 11.493082, Accuracy 49.263%\n",
      "Epoch 2, Batch 532, LR 0.371788 Loss 11.492818, Accuracy 49.263%\n",
      "Epoch 2, Batch 533, LR 0.371969 Loss 11.494537, Accuracy 49.255%\n",
      "Epoch 2, Batch 534, LR 0.372151 Loss 11.493909, Accuracy 49.260%\n",
      "Epoch 2, Batch 535, LR 0.372332 Loss 11.493179, Accuracy 49.261%\n",
      "Epoch 2, Batch 536, LR 0.372514 Loss 11.492133, Accuracy 49.273%\n",
      "Epoch 2, Batch 537, LR 0.372695 Loss 11.492290, Accuracy 49.265%\n",
      "Epoch 2, Batch 538, LR 0.372877 Loss 11.492446, Accuracy 49.264%\n",
      "Epoch 2, Batch 539, LR 0.373058 Loss 11.491573, Accuracy 49.272%\n",
      "Epoch 2, Batch 540, LR 0.373240 Loss 11.491614, Accuracy 49.272%\n",
      "Epoch 2, Batch 541, LR 0.373422 Loss 11.490975, Accuracy 49.279%\n",
      "Epoch 2, Batch 542, LR 0.373603 Loss 11.490120, Accuracy 49.289%\n",
      "Epoch 2, Batch 543, LR 0.373785 Loss 11.490699, Accuracy 49.291%\n",
      "Epoch 2, Batch 544, LR 0.373967 Loss 11.491933, Accuracy 49.272%\n",
      "Epoch 2, Batch 545, LR 0.374149 Loss 11.491476, Accuracy 49.282%\n",
      "Epoch 2, Batch 546, LR 0.374331 Loss 11.490958, Accuracy 49.289%\n",
      "Epoch 2, Batch 547, LR 0.374513 Loss 11.490989, Accuracy 49.286%\n",
      "Epoch 2, Batch 548, LR 0.374695 Loss 11.490648, Accuracy 49.293%\n",
      "Epoch 2, Batch 549, LR 0.374878 Loss 11.489191, Accuracy 49.304%\n",
      "Epoch 2, Batch 550, LR 0.375060 Loss 11.488490, Accuracy 49.310%\n",
      "Epoch 2, Batch 551, LR 0.375242 Loss 11.487282, Accuracy 49.324%\n",
      "Epoch 2, Batch 552, LR 0.375424 Loss 11.487140, Accuracy 49.329%\n",
      "Epoch 2, Batch 553, LR 0.375607 Loss 11.488221, Accuracy 49.318%\n",
      "Epoch 2, Batch 554, LR 0.375789 Loss 11.487178, Accuracy 49.317%\n",
      "Epoch 2, Batch 555, LR 0.375972 Loss 11.485577, Accuracy 49.333%\n",
      "Epoch 2, Batch 556, LR 0.376155 Loss 11.483778, Accuracy 49.351%\n",
      "Epoch 2, Batch 557, LR 0.376337 Loss 11.482189, Accuracy 49.360%\n",
      "Epoch 2, Batch 558, LR 0.376520 Loss 11.481026, Accuracy 49.371%\n",
      "Epoch 2, Batch 559, LR 0.376703 Loss 11.479123, Accuracy 49.389%\n",
      "Epoch 2, Batch 560, LR 0.376886 Loss 11.479628, Accuracy 49.386%\n",
      "Epoch 2, Batch 561, LR 0.377068 Loss 11.479337, Accuracy 49.384%\n",
      "Epoch 2, Batch 562, LR 0.377251 Loss 11.478103, Accuracy 49.398%\n",
      "Epoch 2, Batch 563, LR 0.377434 Loss 11.476889, Accuracy 49.407%\n",
      "Epoch 2, Batch 564, LR 0.377617 Loss 11.477248, Accuracy 49.406%\n",
      "Epoch 2, Batch 565, LR 0.377801 Loss 11.477518, Accuracy 49.403%\n",
      "Epoch 2, Batch 566, LR 0.377984 Loss 11.478046, Accuracy 49.398%\n",
      "Epoch 2, Batch 567, LR 0.378167 Loss 11.477293, Accuracy 49.403%\n",
      "Epoch 2, Batch 568, LR 0.378350 Loss 11.477630, Accuracy 49.389%\n",
      "Epoch 2, Batch 569, LR 0.378534 Loss 11.476686, Accuracy 49.397%\n",
      "Epoch 2, Batch 570, LR 0.378717 Loss 11.475386, Accuracy 49.413%\n",
      "Epoch 2, Batch 571, LR 0.378901 Loss 11.475044, Accuracy 49.421%\n",
      "Epoch 2, Batch 572, LR 0.379084 Loss 11.474547, Accuracy 49.420%\n",
      "Epoch 2, Batch 573, LR 0.379268 Loss 11.474793, Accuracy 49.418%\n",
      "Epoch 2, Batch 574, LR 0.379451 Loss 11.473917, Accuracy 49.426%\n",
      "Epoch 2, Batch 575, LR 0.379635 Loss 11.472821, Accuracy 49.428%\n",
      "Epoch 2, Batch 576, LR 0.379819 Loss 11.472044, Accuracy 49.428%\n",
      "Epoch 2, Batch 577, LR 0.380003 Loss 11.471598, Accuracy 49.445%\n",
      "Epoch 2, Batch 578, LR 0.380187 Loss 11.470880, Accuracy 49.451%\n",
      "Epoch 2, Batch 579, LR 0.380371 Loss 11.470257, Accuracy 49.454%\n",
      "Epoch 2, Batch 580, LR 0.380555 Loss 11.470224, Accuracy 49.453%\n",
      "Epoch 2, Batch 581, LR 0.380739 Loss 11.470039, Accuracy 49.451%\n",
      "Epoch 2, Batch 582, LR 0.380923 Loss 11.470208, Accuracy 49.451%\n",
      "Epoch 2, Batch 583, LR 0.381107 Loss 11.469210, Accuracy 49.451%\n",
      "Epoch 2, Batch 584, LR 0.381291 Loss 11.468308, Accuracy 49.461%\n",
      "Epoch 2, Batch 585, LR 0.381476 Loss 11.468946, Accuracy 49.458%\n",
      "Epoch 2, Batch 586, LR 0.381660 Loss 11.467219, Accuracy 49.481%\n",
      "Epoch 2, Batch 587, LR 0.381844 Loss 11.466641, Accuracy 49.488%\n",
      "Epoch 2, Batch 588, LR 0.382029 Loss 11.466403, Accuracy 49.487%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 589, LR 0.382213 Loss 11.466615, Accuracy 49.488%\n",
      "Epoch 2, Batch 590, LR 0.382398 Loss 11.466061, Accuracy 49.488%\n",
      "Epoch 2, Batch 591, LR 0.382583 Loss 11.464479, Accuracy 49.502%\n",
      "Epoch 2, Batch 592, LR 0.382767 Loss 11.464045, Accuracy 49.508%\n",
      "Epoch 2, Batch 593, LR 0.382952 Loss 11.463345, Accuracy 49.509%\n",
      "Epoch 2, Batch 594, LR 0.383137 Loss 11.462659, Accuracy 49.516%\n",
      "Epoch 2, Batch 595, LR 0.383322 Loss 11.462030, Accuracy 49.526%\n",
      "Epoch 2, Batch 596, LR 0.383507 Loss 11.460812, Accuracy 49.528%\n",
      "Epoch 2, Batch 597, LR 0.383692 Loss 11.460069, Accuracy 49.541%\n",
      "Epoch 2, Batch 598, LR 0.383877 Loss 11.458827, Accuracy 49.549%\n",
      "Epoch 2, Batch 599, LR 0.384062 Loss 11.457867, Accuracy 49.566%\n",
      "Epoch 2, Batch 600, LR 0.384247 Loss 11.457183, Accuracy 49.568%\n",
      "Epoch 2, Batch 601, LR 0.384432 Loss 11.457228, Accuracy 49.565%\n",
      "Epoch 2, Batch 602, LR 0.384618 Loss 11.457216, Accuracy 49.557%\n",
      "Epoch 2, Batch 603, LR 0.384803 Loss 11.457460, Accuracy 49.556%\n",
      "Epoch 2, Batch 604, LR 0.384988 Loss 11.457072, Accuracy 49.556%\n",
      "Epoch 2, Batch 605, LR 0.385174 Loss 11.455828, Accuracy 49.566%\n",
      "Epoch 2, Batch 606, LR 0.385359 Loss 11.456018, Accuracy 49.569%\n",
      "Epoch 2, Batch 607, LR 0.385545 Loss 11.455080, Accuracy 49.578%\n",
      "Epoch 2, Batch 608, LR 0.385731 Loss 11.455074, Accuracy 49.582%\n",
      "Epoch 2, Batch 609, LR 0.385916 Loss 11.453877, Accuracy 49.592%\n",
      "Epoch 2, Batch 610, LR 0.386102 Loss 11.453234, Accuracy 49.593%\n",
      "Epoch 2, Batch 611, LR 0.386288 Loss 11.451514, Accuracy 49.604%\n",
      "Epoch 2, Batch 612, LR 0.386474 Loss 11.450228, Accuracy 49.616%\n",
      "Epoch 2, Batch 613, LR 0.386660 Loss 11.449861, Accuracy 49.619%\n",
      "Epoch 2, Batch 614, LR 0.386846 Loss 11.451698, Accuracy 49.599%\n",
      "Epoch 2, Batch 615, LR 0.387032 Loss 11.450452, Accuracy 49.615%\n",
      "Epoch 2, Batch 616, LR 0.387218 Loss 11.450002, Accuracy 49.618%\n",
      "Epoch 2, Batch 617, LR 0.387404 Loss 11.450336, Accuracy 49.621%\n",
      "Epoch 2, Batch 618, LR 0.387590 Loss 11.450694, Accuracy 49.616%\n",
      "Epoch 2, Batch 619, LR 0.387776 Loss 11.450555, Accuracy 49.613%\n",
      "Epoch 2, Batch 620, LR 0.387963 Loss 11.450400, Accuracy 49.612%\n",
      "Epoch 2, Batch 621, LR 0.388149 Loss 11.449023, Accuracy 49.625%\n",
      "Epoch 2, Batch 622, LR 0.388336 Loss 11.448386, Accuracy 49.628%\n",
      "Epoch 2, Batch 623, LR 0.388522 Loss 11.449749, Accuracy 49.618%\n",
      "Epoch 2, Batch 624, LR 0.388709 Loss 11.449144, Accuracy 49.619%\n",
      "Epoch 2, Batch 625, LR 0.388895 Loss 11.448467, Accuracy 49.612%\n",
      "Epoch 2, Batch 626, LR 0.389082 Loss 11.447490, Accuracy 49.618%\n",
      "Epoch 2, Batch 627, LR 0.389269 Loss 11.446963, Accuracy 49.615%\n",
      "Epoch 2, Batch 628, LR 0.389455 Loss 11.446549, Accuracy 49.618%\n",
      "Epoch 2, Batch 629, LR 0.389642 Loss 11.446907, Accuracy 49.616%\n",
      "Epoch 2, Batch 630, LR 0.389829 Loss 11.445996, Accuracy 49.623%\n",
      "Epoch 2, Batch 631, LR 0.390016 Loss 11.446353, Accuracy 49.617%\n",
      "Epoch 2, Batch 632, LR 0.390203 Loss 11.445134, Accuracy 49.639%\n",
      "Epoch 2, Batch 633, LR 0.390390 Loss 11.445183, Accuracy 49.645%\n",
      "Epoch 2, Batch 634, LR 0.390577 Loss 11.444898, Accuracy 49.643%\n",
      "Epoch 2, Batch 635, LR 0.390765 Loss 11.444953, Accuracy 49.636%\n",
      "Epoch 2, Batch 636, LR 0.390952 Loss 11.445126, Accuracy 49.628%\n",
      "Epoch 2, Batch 637, LR 0.391139 Loss 11.445211, Accuracy 49.630%\n",
      "Epoch 2, Batch 638, LR 0.391327 Loss 11.443463, Accuracy 49.642%\n",
      "Epoch 2, Batch 639, LR 0.391514 Loss 11.443371, Accuracy 49.641%\n",
      "Epoch 2, Batch 640, LR 0.391701 Loss 11.442562, Accuracy 49.655%\n",
      "Epoch 2, Batch 641, LR 0.391889 Loss 11.441933, Accuracy 49.661%\n",
      "Epoch 2, Batch 642, LR 0.392077 Loss 11.441515, Accuracy 49.659%\n",
      "Epoch 2, Batch 643, LR 0.392264 Loss 11.440933, Accuracy 49.663%\n",
      "Epoch 2, Batch 644, LR 0.392452 Loss 11.439601, Accuracy 49.669%\n",
      "Epoch 2, Batch 645, LR 0.392640 Loss 11.438516, Accuracy 49.675%\n",
      "Epoch 2, Batch 646, LR 0.392828 Loss 11.437883, Accuracy 49.682%\n",
      "Epoch 2, Batch 647, LR 0.393015 Loss 11.437762, Accuracy 49.676%\n",
      "Epoch 2, Batch 648, LR 0.393203 Loss 11.436930, Accuracy 49.678%\n",
      "Epoch 2, Batch 649, LR 0.393391 Loss 11.436985, Accuracy 49.676%\n",
      "Epoch 2, Batch 650, LR 0.393579 Loss 11.435889, Accuracy 49.681%\n",
      "Epoch 2, Batch 651, LR 0.393768 Loss 11.434355, Accuracy 49.698%\n",
      "Epoch 2, Batch 652, LR 0.393956 Loss 11.433411, Accuracy 49.705%\n",
      "Epoch 2, Batch 653, LR 0.394144 Loss 11.433496, Accuracy 49.703%\n",
      "Epoch 2, Batch 654, LR 0.394332 Loss 11.433158, Accuracy 49.706%\n",
      "Epoch 2, Batch 655, LR 0.394521 Loss 11.433038, Accuracy 49.708%\n",
      "Epoch 2, Batch 656, LR 0.394709 Loss 11.431825, Accuracy 49.717%\n",
      "Epoch 2, Batch 657, LR 0.394898 Loss 11.431988, Accuracy 49.715%\n",
      "Epoch 2, Batch 658, LR 0.395086 Loss 11.430794, Accuracy 49.726%\n",
      "Epoch 2, Batch 659, LR 0.395275 Loss 11.429998, Accuracy 49.733%\n",
      "Epoch 2, Batch 660, LR 0.395463 Loss 11.430433, Accuracy 49.725%\n",
      "Epoch 2, Batch 661, LR 0.395652 Loss 11.430069, Accuracy 49.732%\n",
      "Epoch 2, Batch 662, LR 0.395841 Loss 11.429899, Accuracy 49.733%\n",
      "Epoch 2, Batch 663, LR 0.396030 Loss 11.430180, Accuracy 49.735%\n",
      "Epoch 2, Batch 664, LR 0.396218 Loss 11.429863, Accuracy 49.747%\n",
      "Epoch 2, Batch 665, LR 0.396407 Loss 11.429412, Accuracy 49.754%\n",
      "Epoch 2, Batch 666, LR 0.396596 Loss 11.430178, Accuracy 49.737%\n",
      "Epoch 2, Batch 667, LR 0.396785 Loss 11.429875, Accuracy 49.740%\n",
      "Epoch 2, Batch 668, LR 0.396974 Loss 11.429453, Accuracy 49.739%\n",
      "Epoch 2, Batch 669, LR 0.397164 Loss 11.429201, Accuracy 49.742%\n",
      "Epoch 2, Batch 670, LR 0.397353 Loss 11.428637, Accuracy 49.747%\n",
      "Epoch 2, Batch 671, LR 0.397542 Loss 11.428387, Accuracy 49.750%\n",
      "Epoch 2, Batch 672, LR 0.397731 Loss 11.427731, Accuracy 49.751%\n",
      "Epoch 2, Batch 673, LR 0.397921 Loss 11.428145, Accuracy 49.748%\n",
      "Epoch 2, Batch 674, LR 0.398110 Loss 11.426710, Accuracy 49.761%\n",
      "Epoch 2, Batch 675, LR 0.398300 Loss 11.426097, Accuracy 49.765%\n",
      "Epoch 2, Batch 676, LR 0.398489 Loss 11.424814, Accuracy 49.773%\n",
      "Epoch 2, Batch 677, LR 0.398679 Loss 11.424906, Accuracy 49.773%\n",
      "Epoch 2, Batch 678, LR 0.398868 Loss 11.423965, Accuracy 49.780%\n",
      "Epoch 2, Batch 679, LR 0.399058 Loss 11.422528, Accuracy 49.791%\n",
      "Epoch 2, Batch 680, LR 0.399248 Loss 11.422646, Accuracy 49.789%\n",
      "Epoch 2, Batch 681, LR 0.399438 Loss 11.421104, Accuracy 49.799%\n",
      "Epoch 2, Batch 682, LR 0.399628 Loss 11.420997, Accuracy 49.798%\n",
      "Epoch 2, Batch 683, LR 0.399818 Loss 11.420657, Accuracy 49.798%\n",
      "Epoch 2, Batch 684, LR 0.400008 Loss 11.420727, Accuracy 49.798%\n",
      "Epoch 2, Batch 685, LR 0.400198 Loss 11.420052, Accuracy 49.806%\n",
      "Epoch 2, Batch 686, LR 0.400388 Loss 11.419246, Accuracy 49.811%\n",
      "Epoch 2, Batch 687, LR 0.400578 Loss 11.419811, Accuracy 49.810%\n",
      "Epoch 2, Batch 688, LR 0.400768 Loss 11.419894, Accuracy 49.813%\n",
      "Epoch 2, Batch 689, LR 0.400959 Loss 11.419866, Accuracy 49.814%\n",
      "Epoch 2, Batch 690, LR 0.401149 Loss 11.418994, Accuracy 49.813%\n",
      "Epoch 2, Batch 691, LR 0.401339 Loss 11.418312, Accuracy 49.820%\n",
      "Epoch 2, Batch 692, LR 0.401530 Loss 11.417523, Accuracy 49.827%\n",
      "Epoch 2, Batch 693, LR 0.401720 Loss 11.417022, Accuracy 49.837%\n",
      "Epoch 2, Batch 694, LR 0.401911 Loss 11.415774, Accuracy 49.844%\n",
      "Epoch 2, Batch 695, LR 0.402102 Loss 11.415121, Accuracy 49.846%\n",
      "Epoch 2, Batch 696, LR 0.402292 Loss 11.414640, Accuracy 49.850%\n",
      "Epoch 2, Batch 697, LR 0.402483 Loss 11.414587, Accuracy 49.852%\n",
      "Epoch 2, Batch 698, LR 0.402674 Loss 11.413994, Accuracy 49.857%\n",
      "Epoch 2, Batch 699, LR 0.402865 Loss 11.413465, Accuracy 49.863%\n",
      "Epoch 2, Batch 700, LR 0.403056 Loss 11.411848, Accuracy 49.881%\n",
      "Epoch 2, Batch 701, LR 0.403247 Loss 11.410989, Accuracy 49.885%\n",
      "Epoch 2, Batch 702, LR 0.403438 Loss 11.410720, Accuracy 49.881%\n",
      "Epoch 2, Batch 703, LR 0.403629 Loss 11.410370, Accuracy 49.883%\n",
      "Epoch 2, Batch 704, LR 0.403820 Loss 11.409154, Accuracy 49.895%\n",
      "Epoch 2, Batch 705, LR 0.404011 Loss 11.408491, Accuracy 49.902%\n",
      "Epoch 2, Batch 706, LR 0.404202 Loss 11.407320, Accuracy 49.910%\n",
      "Epoch 2, Batch 707, LR 0.404394 Loss 11.406135, Accuracy 49.920%\n",
      "Epoch 2, Batch 708, LR 0.404585 Loss 11.405936, Accuracy 49.921%\n",
      "Epoch 2, Batch 709, LR 0.404776 Loss 11.405014, Accuracy 49.926%\n",
      "Epoch 2, Batch 710, LR 0.404968 Loss 11.405594, Accuracy 49.921%\n",
      "Epoch 2, Batch 711, LR 0.405159 Loss 11.404554, Accuracy 49.933%\n",
      "Epoch 2, Batch 712, LR 0.405351 Loss 11.404296, Accuracy 49.934%\n",
      "Epoch 2, Batch 713, LR 0.405543 Loss 11.402961, Accuracy 49.944%\n",
      "Epoch 2, Batch 714, LR 0.405734 Loss 11.401536, Accuracy 49.958%\n",
      "Epoch 2, Batch 715, LR 0.405926 Loss 11.400905, Accuracy 49.961%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 716, LR 0.406118 Loss 11.400514, Accuracy 49.963%\n",
      "Epoch 2, Batch 717, LR 0.406310 Loss 11.400052, Accuracy 49.971%\n",
      "Epoch 2, Batch 718, LR 0.406502 Loss 11.400298, Accuracy 49.972%\n",
      "Epoch 2, Batch 719, LR 0.406694 Loss 11.399243, Accuracy 49.984%\n",
      "Epoch 2, Batch 720, LR 0.406886 Loss 11.398389, Accuracy 49.997%\n",
      "Epoch 2, Batch 721, LR 0.407078 Loss 11.398324, Accuracy 49.995%\n",
      "Epoch 2, Batch 722, LR 0.407270 Loss 11.397214, Accuracy 50.001%\n",
      "Epoch 2, Batch 723, LR 0.407462 Loss 11.397072, Accuracy 49.998%\n",
      "Epoch 2, Batch 724, LR 0.407655 Loss 11.397132, Accuracy 49.992%\n",
      "Epoch 2, Batch 725, LR 0.407847 Loss 11.396648, Accuracy 49.989%\n",
      "Epoch 2, Batch 726, LR 0.408039 Loss 11.395991, Accuracy 49.992%\n",
      "Epoch 2, Batch 727, LR 0.408232 Loss 11.395292, Accuracy 50.000%\n",
      "Epoch 2, Batch 728, LR 0.408424 Loss 11.394973, Accuracy 50.001%\n",
      "Epoch 2, Batch 729, LR 0.408617 Loss 11.393340, Accuracy 50.016%\n",
      "Epoch 2, Batch 730, LR 0.408810 Loss 11.393278, Accuracy 50.015%\n",
      "Epoch 2, Batch 731, LR 0.409002 Loss 11.392496, Accuracy 50.019%\n",
      "Epoch 2, Batch 732, LR 0.409195 Loss 11.393119, Accuracy 50.014%\n",
      "Epoch 2, Batch 733, LR 0.409388 Loss 11.392143, Accuracy 50.018%\n",
      "Epoch 2, Batch 734, LR 0.409581 Loss 11.391658, Accuracy 50.027%\n",
      "Epoch 2, Batch 735, LR 0.409774 Loss 11.391801, Accuracy 50.031%\n",
      "Epoch 2, Batch 736, LR 0.409966 Loss 11.390637, Accuracy 50.046%\n",
      "Epoch 2, Batch 737, LR 0.410159 Loss 11.390478, Accuracy 50.047%\n",
      "Epoch 2, Batch 738, LR 0.410353 Loss 11.391025, Accuracy 50.052%\n",
      "Epoch 2, Batch 739, LR 0.410546 Loss 11.390305, Accuracy 50.057%\n",
      "Epoch 2, Batch 740, LR 0.410739 Loss 11.389327, Accuracy 50.064%\n",
      "Epoch 2, Batch 741, LR 0.410932 Loss 11.389362, Accuracy 50.063%\n",
      "Epoch 2, Batch 742, LR 0.411125 Loss 11.389554, Accuracy 50.060%\n",
      "Epoch 2, Batch 743, LR 0.411319 Loss 11.388690, Accuracy 50.066%\n",
      "Epoch 2, Batch 744, LR 0.411512 Loss 11.388797, Accuracy 50.065%\n",
      "Epoch 2, Batch 745, LR 0.411706 Loss 11.388196, Accuracy 50.071%\n",
      "Epoch 2, Batch 746, LR 0.411899 Loss 11.387250, Accuracy 50.079%\n",
      "Epoch 2, Batch 747, LR 0.412093 Loss 11.387601, Accuracy 50.073%\n",
      "Epoch 2, Batch 748, LR 0.412286 Loss 11.386728, Accuracy 50.077%\n",
      "Epoch 2, Batch 749, LR 0.412480 Loss 11.386227, Accuracy 50.084%\n",
      "Epoch 2, Batch 750, LR 0.412674 Loss 11.384896, Accuracy 50.098%\n",
      "Epoch 2, Batch 751, LR 0.412868 Loss 11.384850, Accuracy 50.100%\n",
      "Epoch 2, Batch 752, LR 0.413061 Loss 11.384384, Accuracy 50.100%\n",
      "Epoch 2, Batch 753, LR 0.413255 Loss 11.384804, Accuracy 50.090%\n",
      "Epoch 2, Batch 754, LR 0.413449 Loss 11.384392, Accuracy 50.093%\n",
      "Epoch 2, Batch 755, LR 0.413643 Loss 11.383195, Accuracy 50.102%\n",
      "Epoch 2, Batch 756, LR 0.413837 Loss 11.382757, Accuracy 50.105%\n",
      "Epoch 2, Batch 757, LR 0.414032 Loss 11.382852, Accuracy 50.099%\n",
      "Epoch 2, Batch 758, LR 0.414226 Loss 11.382048, Accuracy 50.104%\n",
      "Epoch 2, Batch 759, LR 0.414420 Loss 11.381093, Accuracy 50.116%\n",
      "Epoch 2, Batch 760, LR 0.414614 Loss 11.380839, Accuracy 50.117%\n",
      "Epoch 2, Batch 761, LR 0.414809 Loss 11.379922, Accuracy 50.132%\n",
      "Epoch 2, Batch 762, LR 0.415003 Loss 11.380184, Accuracy 50.135%\n",
      "Epoch 2, Batch 763, LR 0.415198 Loss 11.379384, Accuracy 50.142%\n",
      "Epoch 2, Batch 764, LR 0.415392 Loss 11.378610, Accuracy 50.150%\n",
      "Epoch 2, Batch 765, LR 0.415587 Loss 11.378198, Accuracy 50.154%\n",
      "Epoch 2, Batch 766, LR 0.415781 Loss 11.376987, Accuracy 50.160%\n",
      "Epoch 2, Batch 767, LR 0.415976 Loss 11.376841, Accuracy 50.159%\n",
      "Epoch 2, Batch 768, LR 0.416171 Loss 11.376068, Accuracy 50.166%\n",
      "Epoch 2, Batch 769, LR 0.416366 Loss 11.375558, Accuracy 50.171%\n",
      "Epoch 2, Batch 770, LR 0.416561 Loss 11.375086, Accuracy 50.171%\n",
      "Epoch 2, Batch 771, LR 0.416756 Loss 11.374326, Accuracy 50.174%\n",
      "Epoch 2, Batch 772, LR 0.416951 Loss 11.373635, Accuracy 50.179%\n",
      "Epoch 2, Batch 773, LR 0.417146 Loss 11.372754, Accuracy 50.179%\n",
      "Epoch 2, Batch 774, LR 0.417341 Loss 11.372047, Accuracy 50.188%\n",
      "Epoch 2, Batch 775, LR 0.417536 Loss 11.371464, Accuracy 50.195%\n",
      "Epoch 2, Batch 776, LR 0.417731 Loss 11.371146, Accuracy 50.191%\n",
      "Epoch 2, Batch 777, LR 0.417926 Loss 11.371135, Accuracy 50.188%\n",
      "Epoch 2, Batch 778, LR 0.418122 Loss 11.370861, Accuracy 50.193%\n",
      "Epoch 2, Batch 779, LR 0.418317 Loss 11.370513, Accuracy 50.190%\n",
      "Epoch 2, Batch 780, LR 0.418512 Loss 11.369480, Accuracy 50.201%\n",
      "Epoch 2, Batch 781, LR 0.418708 Loss 11.368520, Accuracy 50.210%\n",
      "Epoch 2, Batch 782, LR 0.418903 Loss 11.368901, Accuracy 50.205%\n",
      "Epoch 2, Batch 783, LR 0.419099 Loss 11.369372, Accuracy 50.202%\n",
      "Epoch 2, Batch 784, LR 0.419295 Loss 11.368629, Accuracy 50.207%\n",
      "Epoch 2, Batch 785, LR 0.419490 Loss 11.368777, Accuracy 50.204%\n",
      "Epoch 2, Batch 786, LR 0.419686 Loss 11.368355, Accuracy 50.207%\n",
      "Epoch 2, Batch 787, LR 0.419882 Loss 11.368759, Accuracy 50.203%\n",
      "Epoch 2, Batch 788, LR 0.420078 Loss 11.369242, Accuracy 50.199%\n",
      "Epoch 2, Batch 789, LR 0.420274 Loss 11.368356, Accuracy 50.211%\n",
      "Epoch 2, Batch 790, LR 0.420470 Loss 11.367881, Accuracy 50.212%\n",
      "Epoch 2, Batch 791, LR 0.420666 Loss 11.367670, Accuracy 50.208%\n",
      "Epoch 2, Batch 792, LR 0.420862 Loss 11.366748, Accuracy 50.211%\n",
      "Epoch 2, Batch 793, LR 0.421058 Loss 11.366796, Accuracy 50.209%\n",
      "Epoch 2, Batch 794, LR 0.421254 Loss 11.366796, Accuracy 50.207%\n",
      "Epoch 2, Batch 795, LR 0.421451 Loss 11.366007, Accuracy 50.212%\n",
      "Epoch 2, Batch 796, LR 0.421647 Loss 11.365260, Accuracy 50.213%\n",
      "Epoch 2, Batch 797, LR 0.421843 Loss 11.364221, Accuracy 50.217%\n",
      "Epoch 2, Batch 798, LR 0.422040 Loss 11.364319, Accuracy 50.212%\n",
      "Epoch 2, Batch 799, LR 0.422236 Loss 11.364021, Accuracy 50.213%\n",
      "Epoch 2, Batch 800, LR 0.422433 Loss 11.362979, Accuracy 50.221%\n",
      "Epoch 2, Batch 801, LR 0.422629 Loss 11.362372, Accuracy 50.220%\n",
      "Epoch 2, Batch 802, LR 0.422826 Loss 11.362252, Accuracy 50.221%\n",
      "Epoch 2, Batch 803, LR 0.423023 Loss 11.362136, Accuracy 50.219%\n",
      "Epoch 2, Batch 804, LR 0.423219 Loss 11.361520, Accuracy 50.224%\n",
      "Epoch 2, Batch 805, LR 0.423416 Loss 11.361219, Accuracy 50.231%\n",
      "Epoch 2, Batch 806, LR 0.423613 Loss 11.360550, Accuracy 50.236%\n",
      "Epoch 2, Batch 807, LR 0.423810 Loss 11.360845, Accuracy 50.236%\n",
      "Epoch 2, Batch 808, LR 0.424007 Loss 11.361182, Accuracy 50.237%\n",
      "Epoch 2, Batch 809, LR 0.424204 Loss 11.359906, Accuracy 50.246%\n",
      "Epoch 2, Batch 810, LR 0.424401 Loss 11.359985, Accuracy 50.239%\n",
      "Epoch 2, Batch 811, LR 0.424598 Loss 11.359790, Accuracy 50.244%\n",
      "Epoch 2, Batch 812, LR 0.424796 Loss 11.359491, Accuracy 50.244%\n",
      "Epoch 2, Batch 813, LR 0.424993 Loss 11.359264, Accuracy 50.247%\n",
      "Epoch 2, Batch 814, LR 0.425190 Loss 11.359075, Accuracy 50.250%\n",
      "Epoch 2, Batch 815, LR 0.425388 Loss 11.358043, Accuracy 50.262%\n",
      "Epoch 2, Batch 816, LR 0.425585 Loss 11.358746, Accuracy 50.254%\n",
      "Epoch 2, Batch 817, LR 0.425783 Loss 11.358827, Accuracy 50.253%\n",
      "Epoch 2, Batch 818, LR 0.425980 Loss 11.358354, Accuracy 50.256%\n",
      "Epoch 2, Batch 819, LR 0.426178 Loss 11.358257, Accuracy 50.256%\n",
      "Epoch 2, Batch 820, LR 0.426375 Loss 11.357519, Accuracy 50.260%\n",
      "Epoch 2, Batch 821, LR 0.426573 Loss 11.357616, Accuracy 50.260%\n",
      "Epoch 2, Batch 822, LR 0.426771 Loss 11.356119, Accuracy 50.271%\n",
      "Epoch 2, Batch 823, LR 0.426969 Loss 11.355191, Accuracy 50.276%\n",
      "Epoch 2, Batch 824, LR 0.427167 Loss 11.354903, Accuracy 50.281%\n",
      "Epoch 2, Batch 825, LR 0.427364 Loss 11.354558, Accuracy 50.277%\n",
      "Epoch 2, Batch 826, LR 0.427562 Loss 11.353975, Accuracy 50.285%\n",
      "Epoch 2, Batch 827, LR 0.427761 Loss 11.353689, Accuracy 50.286%\n",
      "Epoch 2, Batch 828, LR 0.427959 Loss 11.353792, Accuracy 50.283%\n",
      "Epoch 2, Batch 829, LR 0.428157 Loss 11.354226, Accuracy 50.282%\n",
      "Epoch 2, Batch 830, LR 0.428355 Loss 11.353713, Accuracy 50.289%\n",
      "Epoch 2, Batch 831, LR 0.428553 Loss 11.353183, Accuracy 50.288%\n",
      "Epoch 2, Batch 832, LR 0.428752 Loss 11.352166, Accuracy 50.296%\n",
      "Epoch 2, Batch 833, LR 0.428950 Loss 11.351417, Accuracy 50.300%\n",
      "Epoch 2, Batch 834, LR 0.429148 Loss 11.351350, Accuracy 50.302%\n",
      "Epoch 2, Batch 835, LR 0.429347 Loss 11.350987, Accuracy 50.302%\n",
      "Epoch 2, Batch 836, LR 0.429545 Loss 11.351433, Accuracy 50.297%\n",
      "Epoch 2, Batch 837, LR 0.429744 Loss 11.350791, Accuracy 50.305%\n",
      "Epoch 2, Batch 838, LR 0.429943 Loss 11.350189, Accuracy 50.312%\n",
      "Epoch 2, Batch 839, LR 0.430141 Loss 11.349024, Accuracy 50.320%\n",
      "Epoch 2, Batch 840, LR 0.430340 Loss 11.349628, Accuracy 50.318%\n",
      "Epoch 2, Batch 841, LR 0.430539 Loss 11.348499, Accuracy 50.326%\n",
      "Epoch 2, Batch 842, LR 0.430738 Loss 11.349409, Accuracy 50.314%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 843, LR 0.430937 Loss 11.349016, Accuracy 50.318%\n",
      "Epoch 2, Batch 844, LR 0.431136 Loss 11.349621, Accuracy 50.309%\n",
      "Epoch 2, Batch 845, LR 0.431335 Loss 11.349284, Accuracy 50.312%\n",
      "Epoch 2, Batch 846, LR 0.431534 Loss 11.349277, Accuracy 50.311%\n",
      "Epoch 2, Batch 847, LR 0.431733 Loss 11.348921, Accuracy 50.315%\n",
      "Epoch 2, Batch 848, LR 0.431932 Loss 11.348766, Accuracy 50.320%\n",
      "Epoch 2, Batch 849, LR 0.432131 Loss 11.348186, Accuracy 50.317%\n",
      "Epoch 2, Batch 850, LR 0.432331 Loss 11.347695, Accuracy 50.315%\n",
      "Epoch 2, Batch 851, LR 0.432530 Loss 11.347547, Accuracy 50.313%\n",
      "Epoch 2, Batch 852, LR 0.432730 Loss 11.347445, Accuracy 50.307%\n",
      "Epoch 2, Batch 853, LR 0.432929 Loss 11.346402, Accuracy 50.314%\n",
      "Epoch 2, Batch 854, LR 0.433129 Loss 11.346387, Accuracy 50.317%\n",
      "Epoch 2, Batch 855, LR 0.433328 Loss 11.346792, Accuracy 50.317%\n",
      "Epoch 2, Batch 856, LR 0.433528 Loss 11.346875, Accuracy 50.317%\n",
      "Epoch 2, Batch 857, LR 0.433728 Loss 11.347146, Accuracy 50.314%\n",
      "Epoch 2, Batch 858, LR 0.433927 Loss 11.347556, Accuracy 50.310%\n",
      "Epoch 2, Batch 859, LR 0.434127 Loss 11.347511, Accuracy 50.307%\n",
      "Epoch 2, Batch 860, LR 0.434327 Loss 11.347269, Accuracy 50.306%\n",
      "Epoch 2, Batch 861, LR 0.434527 Loss 11.346959, Accuracy 50.310%\n",
      "Epoch 2, Batch 862, LR 0.434727 Loss 11.347078, Accuracy 50.309%\n",
      "Epoch 2, Batch 863, LR 0.434927 Loss 11.346245, Accuracy 50.318%\n",
      "Epoch 2, Batch 864, LR 0.435127 Loss 11.346112, Accuracy 50.316%\n",
      "Epoch 2, Batch 865, LR 0.435327 Loss 11.345136, Accuracy 50.326%\n",
      "Epoch 2, Batch 866, LR 0.435527 Loss 11.345027, Accuracy 50.327%\n",
      "Epoch 2, Batch 867, LR 0.435728 Loss 11.344363, Accuracy 50.337%\n",
      "Epoch 2, Batch 868, LR 0.435928 Loss 11.344298, Accuracy 50.337%\n",
      "Epoch 2, Batch 869, LR 0.436128 Loss 11.343557, Accuracy 50.342%\n",
      "Epoch 2, Batch 870, LR 0.436329 Loss 11.342911, Accuracy 50.343%\n",
      "Epoch 2, Batch 871, LR 0.436529 Loss 11.342684, Accuracy 50.344%\n",
      "Epoch 2, Batch 872, LR 0.436730 Loss 11.342893, Accuracy 50.343%\n",
      "Epoch 2, Batch 873, LR 0.436930 Loss 11.342598, Accuracy 50.346%\n",
      "Epoch 2, Batch 874, LR 0.437131 Loss 11.342035, Accuracy 50.351%\n",
      "Epoch 2, Batch 875, LR 0.437332 Loss 11.341828, Accuracy 50.357%\n",
      "Epoch 2, Batch 876, LR 0.437532 Loss 11.341072, Accuracy 50.367%\n",
      "Epoch 2, Batch 877, LR 0.437733 Loss 11.340704, Accuracy 50.371%\n",
      "Epoch 2, Batch 878, LR 0.437934 Loss 11.339949, Accuracy 50.378%\n",
      "Epoch 2, Batch 879, LR 0.438135 Loss 11.339741, Accuracy 50.373%\n",
      "Epoch 2, Batch 880, LR 0.438336 Loss 11.339146, Accuracy 50.376%\n",
      "Epoch 2, Batch 881, LR 0.438537 Loss 11.338373, Accuracy 50.378%\n",
      "Epoch 2, Batch 882, LR 0.438738 Loss 11.337866, Accuracy 50.385%\n",
      "Epoch 2, Batch 883, LR 0.438939 Loss 11.337483, Accuracy 50.391%\n",
      "Epoch 2, Batch 884, LR 0.439140 Loss 11.337175, Accuracy 50.394%\n",
      "Epoch 2, Batch 885, LR 0.439341 Loss 11.336007, Accuracy 50.403%\n",
      "Epoch 2, Batch 886, LR 0.439543 Loss 11.335638, Accuracy 50.408%\n",
      "Epoch 2, Batch 887, LR 0.439744 Loss 11.335325, Accuracy 50.414%\n",
      "Epoch 2, Batch 888, LR 0.439945 Loss 11.334946, Accuracy 50.413%\n",
      "Epoch 2, Batch 889, LR 0.440147 Loss 11.334974, Accuracy 50.413%\n",
      "Epoch 2, Batch 890, LR 0.440348 Loss 11.334821, Accuracy 50.412%\n",
      "Epoch 2, Batch 891, LR 0.440550 Loss 11.334310, Accuracy 50.411%\n",
      "Epoch 2, Batch 892, LR 0.440752 Loss 11.333836, Accuracy 50.415%\n",
      "Epoch 2, Batch 893, LR 0.440953 Loss 11.333448, Accuracy 50.419%\n",
      "Epoch 2, Batch 894, LR 0.441155 Loss 11.333704, Accuracy 50.412%\n",
      "Epoch 2, Batch 895, LR 0.441357 Loss 11.334139, Accuracy 50.409%\n",
      "Epoch 2, Batch 896, LR 0.441559 Loss 11.333312, Accuracy 50.417%\n",
      "Epoch 2, Batch 897, LR 0.441760 Loss 11.332029, Accuracy 50.426%\n",
      "Epoch 2, Batch 898, LR 0.441962 Loss 11.330867, Accuracy 50.435%\n",
      "Epoch 2, Batch 899, LR 0.442164 Loss 11.330538, Accuracy 50.435%\n",
      "Epoch 2, Batch 900, LR 0.442366 Loss 11.330670, Accuracy 50.439%\n",
      "Epoch 2, Batch 901, LR 0.442569 Loss 11.329504, Accuracy 50.449%\n",
      "Epoch 2, Batch 902, LR 0.442771 Loss 11.328912, Accuracy 50.455%\n",
      "Epoch 2, Batch 903, LR 0.442973 Loss 11.327992, Accuracy 50.465%\n",
      "Epoch 2, Batch 904, LR 0.443175 Loss 11.327319, Accuracy 50.469%\n",
      "Epoch 2, Batch 905, LR 0.443378 Loss 11.326617, Accuracy 50.474%\n",
      "Epoch 2, Batch 906, LR 0.443580 Loss 11.326641, Accuracy 50.475%\n",
      "Epoch 2, Batch 907, LR 0.443782 Loss 11.326421, Accuracy 50.473%\n",
      "Epoch 2, Batch 908, LR 0.443985 Loss 11.326083, Accuracy 50.472%\n",
      "Epoch 2, Batch 909, LR 0.444187 Loss 11.325035, Accuracy 50.484%\n",
      "Epoch 2, Batch 910, LR 0.444390 Loss 11.324512, Accuracy 50.488%\n",
      "Epoch 2, Batch 911, LR 0.444593 Loss 11.324213, Accuracy 50.491%\n",
      "Epoch 2, Batch 912, LR 0.444795 Loss 11.323813, Accuracy 50.493%\n",
      "Epoch 2, Batch 913, LR 0.444998 Loss 11.323170, Accuracy 50.495%\n",
      "Epoch 2, Batch 914, LR 0.445201 Loss 11.321692, Accuracy 50.506%\n",
      "Epoch 2, Batch 915, LR 0.445404 Loss 11.321126, Accuracy 50.509%\n",
      "Epoch 2, Batch 916, LR 0.445607 Loss 11.319925, Accuracy 50.519%\n",
      "Epoch 2, Batch 917, LR 0.445810 Loss 11.319332, Accuracy 50.522%\n",
      "Epoch 2, Batch 918, LR 0.446013 Loss 11.319233, Accuracy 50.523%\n",
      "Epoch 2, Batch 919, LR 0.446216 Loss 11.319224, Accuracy 50.528%\n",
      "Epoch 2, Batch 920, LR 0.446419 Loss 11.318246, Accuracy 50.541%\n",
      "Epoch 2, Batch 921, LR 0.446622 Loss 11.317762, Accuracy 50.548%\n",
      "Epoch 2, Batch 922, LR 0.446825 Loss 11.316721, Accuracy 50.557%\n",
      "Epoch 2, Batch 923, LR 0.447029 Loss 11.315745, Accuracy 50.565%\n",
      "Epoch 2, Batch 924, LR 0.447232 Loss 11.315384, Accuracy 50.567%\n",
      "Epoch 2, Batch 925, LR 0.447435 Loss 11.315336, Accuracy 50.565%\n",
      "Epoch 2, Batch 926, LR 0.447639 Loss 11.314878, Accuracy 50.570%\n",
      "Epoch 2, Batch 927, LR 0.447842 Loss 11.314642, Accuracy 50.576%\n",
      "Epoch 2, Batch 928, LR 0.448046 Loss 11.313980, Accuracy 50.578%\n",
      "Epoch 2, Batch 929, LR 0.448250 Loss 11.313034, Accuracy 50.584%\n",
      "Epoch 2, Batch 930, LR 0.448453 Loss 11.312112, Accuracy 50.589%\n",
      "Epoch 2, Batch 931, LR 0.448657 Loss 11.312232, Accuracy 50.593%\n",
      "Epoch 2, Batch 932, LR 0.448861 Loss 11.311409, Accuracy 50.594%\n",
      "Epoch 2, Batch 933, LR 0.449065 Loss 11.311663, Accuracy 50.594%\n",
      "Epoch 2, Batch 934, LR 0.449269 Loss 11.311404, Accuracy 50.600%\n",
      "Epoch 2, Batch 935, LR 0.449472 Loss 11.311184, Accuracy 50.605%\n",
      "Epoch 2, Batch 936, LR 0.449676 Loss 11.310637, Accuracy 50.614%\n",
      "Epoch 2, Batch 937, LR 0.449881 Loss 11.309215, Accuracy 50.627%\n",
      "Epoch 2, Batch 938, LR 0.450085 Loss 11.308878, Accuracy 50.633%\n",
      "Epoch 2, Batch 939, LR 0.450289 Loss 11.307715, Accuracy 50.640%\n",
      "Epoch 2, Batch 940, LR 0.450493 Loss 11.306279, Accuracy 50.651%\n",
      "Epoch 2, Batch 941, LR 0.450697 Loss 11.305566, Accuracy 50.655%\n",
      "Epoch 2, Batch 942, LR 0.450902 Loss 11.305537, Accuracy 50.653%\n",
      "Epoch 2, Batch 943, LR 0.451106 Loss 11.305239, Accuracy 50.653%\n",
      "Epoch 2, Batch 944, LR 0.451310 Loss 11.305318, Accuracy 50.653%\n",
      "Epoch 2, Batch 945, LR 0.451515 Loss 11.305029, Accuracy 50.656%\n",
      "Epoch 2, Batch 946, LR 0.451719 Loss 11.304288, Accuracy 50.659%\n",
      "Epoch 2, Batch 947, LR 0.451924 Loss 11.303731, Accuracy 50.662%\n",
      "Epoch 2, Batch 948, LR 0.452129 Loss 11.303642, Accuracy 50.661%\n",
      "Epoch 2, Batch 949, LR 0.452333 Loss 11.303525, Accuracy 50.661%\n",
      "Epoch 2, Batch 950, LR 0.452538 Loss 11.303428, Accuracy 50.658%\n",
      "Epoch 2, Batch 951, LR 0.452743 Loss 11.302996, Accuracy 50.662%\n",
      "Epoch 2, Batch 952, LR 0.452948 Loss 11.302739, Accuracy 50.661%\n",
      "Epoch 2, Batch 953, LR 0.453153 Loss 11.301759, Accuracy 50.673%\n",
      "Epoch 2, Batch 954, LR 0.453358 Loss 11.301342, Accuracy 50.679%\n",
      "Epoch 2, Batch 955, LR 0.453563 Loss 11.300855, Accuracy 50.681%\n",
      "Epoch 2, Batch 956, LR 0.453768 Loss 11.300236, Accuracy 50.682%\n",
      "Epoch 2, Batch 957, LR 0.453973 Loss 11.300071, Accuracy 50.682%\n",
      "Epoch 2, Batch 958, LR 0.454178 Loss 11.299681, Accuracy 50.688%\n",
      "Epoch 2, Batch 959, LR 0.454383 Loss 11.299172, Accuracy 50.689%\n",
      "Epoch 2, Batch 960, LR 0.454589 Loss 11.299375, Accuracy 50.688%\n",
      "Epoch 2, Batch 961, LR 0.454794 Loss 11.299025, Accuracy 50.696%\n",
      "Epoch 2, Batch 962, LR 0.454999 Loss 11.298251, Accuracy 50.702%\n",
      "Epoch 2, Batch 963, LR 0.455205 Loss 11.298080, Accuracy 50.699%\n",
      "Epoch 2, Batch 964, LR 0.455410 Loss 11.296951, Accuracy 50.707%\n",
      "Epoch 2, Batch 965, LR 0.455616 Loss 11.295980, Accuracy 50.719%\n",
      "Epoch 2, Batch 966, LR 0.455822 Loss 11.294892, Accuracy 50.735%\n",
      "Epoch 2, Batch 967, LR 0.456027 Loss 11.294005, Accuracy 50.741%\n",
      "Epoch 2, Batch 968, LR 0.456233 Loss 11.293215, Accuracy 50.750%\n",
      "Epoch 2, Batch 969, LR 0.456439 Loss 11.292328, Accuracy 50.762%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 970, LR 0.456645 Loss 11.291728, Accuracy 50.764%\n",
      "Epoch 2, Batch 971, LR 0.456850 Loss 11.291036, Accuracy 50.772%\n",
      "Epoch 2, Batch 972, LR 0.457056 Loss 11.290413, Accuracy 50.780%\n",
      "Epoch 2, Batch 973, LR 0.457262 Loss 11.290013, Accuracy 50.785%\n",
      "Epoch 2, Batch 974, LR 0.457468 Loss 11.289625, Accuracy 50.780%\n",
      "Epoch 2, Batch 975, LR 0.457675 Loss 11.289026, Accuracy 50.786%\n",
      "Epoch 2, Batch 976, LR 0.457881 Loss 11.288592, Accuracy 50.784%\n",
      "Epoch 2, Batch 977, LR 0.458087 Loss 11.288236, Accuracy 50.789%\n",
      "Epoch 2, Batch 978, LR 0.458293 Loss 11.288178, Accuracy 50.785%\n",
      "Epoch 2, Batch 979, LR 0.458499 Loss 11.287186, Accuracy 50.788%\n",
      "Epoch 2, Batch 980, LR 0.458706 Loss 11.286657, Accuracy 50.792%\n",
      "Epoch 2, Batch 981, LR 0.458912 Loss 11.286539, Accuracy 50.795%\n",
      "Epoch 2, Batch 982, LR 0.459119 Loss 11.286281, Accuracy 50.795%\n",
      "Epoch 2, Batch 983, LR 0.459325 Loss 11.285711, Accuracy 50.800%\n",
      "Epoch 2, Batch 984, LR 0.459532 Loss 11.284813, Accuracy 50.804%\n",
      "Epoch 2, Batch 985, LR 0.459738 Loss 11.284478, Accuracy 50.811%\n",
      "Epoch 2, Batch 986, LR 0.459945 Loss 11.283717, Accuracy 50.814%\n",
      "Epoch 2, Batch 987, LR 0.460152 Loss 11.283292, Accuracy 50.815%\n",
      "Epoch 2, Batch 988, LR 0.460359 Loss 11.283095, Accuracy 50.818%\n",
      "Epoch 2, Batch 989, LR 0.460566 Loss 11.282355, Accuracy 50.816%\n",
      "Epoch 2, Batch 990, LR 0.460772 Loss 11.281845, Accuracy 50.825%\n",
      "Epoch 2, Batch 991, LR 0.460979 Loss 11.281386, Accuracy 50.832%\n",
      "Epoch 2, Batch 992, LR 0.461186 Loss 11.280558, Accuracy 50.843%\n",
      "Epoch 2, Batch 993, LR 0.461393 Loss 11.280163, Accuracy 50.843%\n",
      "Epoch 2, Batch 994, LR 0.461601 Loss 11.280062, Accuracy 50.847%\n",
      "Epoch 2, Batch 995, LR 0.461808 Loss 11.279623, Accuracy 50.846%\n",
      "Epoch 2, Batch 996, LR 0.462015 Loss 11.278967, Accuracy 50.853%\n",
      "Epoch 2, Batch 997, LR 0.462222 Loss 11.278003, Accuracy 50.855%\n",
      "Epoch 2, Batch 998, LR 0.462430 Loss 11.277679, Accuracy 50.855%\n",
      "Epoch 2, Batch 999, LR 0.462637 Loss 11.277218, Accuracy 50.856%\n",
      "Epoch 2, Batch 1000, LR 0.462844 Loss 11.276609, Accuracy 50.857%\n",
      "Epoch 2, Batch 1001, LR 0.463052 Loss 11.276522, Accuracy 50.856%\n",
      "Epoch 2, Batch 1002, LR 0.463259 Loss 11.275603, Accuracy 50.862%\n",
      "Epoch 2, Batch 1003, LR 0.463467 Loss 11.275667, Accuracy 50.861%\n",
      "Epoch 2, Batch 1004, LR 0.463675 Loss 11.274998, Accuracy 50.867%\n",
      "Epoch 2, Batch 1005, LR 0.463882 Loss 11.274544, Accuracy 50.872%\n",
      "Epoch 2, Batch 1006, LR 0.464090 Loss 11.274399, Accuracy 50.878%\n",
      "Epoch 2, Batch 1007, LR 0.464298 Loss 11.274487, Accuracy 50.877%\n",
      "Epoch 2, Batch 1008, LR 0.464506 Loss 11.274148, Accuracy 50.880%\n",
      "Epoch 2, Batch 1009, LR 0.464714 Loss 11.273658, Accuracy 50.886%\n",
      "Epoch 2, Batch 1010, LR 0.464922 Loss 11.273489, Accuracy 50.884%\n",
      "Epoch 2, Batch 1011, LR 0.465130 Loss 11.273229, Accuracy 50.881%\n",
      "Epoch 2, Batch 1012, LR 0.465338 Loss 11.272975, Accuracy 50.885%\n",
      "Epoch 2, Batch 1013, LR 0.465546 Loss 11.272935, Accuracy 50.881%\n",
      "Epoch 2, Batch 1014, LR 0.465754 Loss 11.272618, Accuracy 50.879%\n",
      "Epoch 2, Batch 1015, LR 0.465962 Loss 11.272462, Accuracy 50.881%\n",
      "Epoch 2, Batch 1016, LR 0.466170 Loss 11.271850, Accuracy 50.885%\n",
      "Epoch 2, Batch 1017, LR 0.466379 Loss 11.270439, Accuracy 50.896%\n",
      "Epoch 2, Batch 1018, LR 0.466587 Loss 11.269699, Accuracy 50.897%\n",
      "Epoch 2, Batch 1019, LR 0.466796 Loss 11.269529, Accuracy 50.893%\n",
      "Epoch 2, Batch 1020, LR 0.467004 Loss 11.268597, Accuracy 50.898%\n",
      "Epoch 2, Batch 1021, LR 0.467213 Loss 11.268341, Accuracy 50.900%\n",
      "Epoch 2, Batch 1022, LR 0.467421 Loss 11.268474, Accuracy 50.891%\n",
      "Epoch 2, Batch 1023, LR 0.467630 Loss 11.267721, Accuracy 50.896%\n",
      "Epoch 2, Batch 1024, LR 0.467839 Loss 11.267996, Accuracy 50.893%\n",
      "Epoch 2, Batch 1025, LR 0.468047 Loss 11.266843, Accuracy 50.903%\n",
      "Epoch 2, Batch 1026, LR 0.468256 Loss 11.267210, Accuracy 50.898%\n",
      "Epoch 2, Batch 1027, LR 0.468465 Loss 11.266890, Accuracy 50.904%\n",
      "Epoch 2, Batch 1028, LR 0.468674 Loss 11.266710, Accuracy 50.907%\n",
      "Epoch 2, Batch 1029, LR 0.468883 Loss 11.266115, Accuracy 50.910%\n",
      "Epoch 2, Batch 1030, LR 0.469092 Loss 11.265919, Accuracy 50.913%\n",
      "Epoch 2, Batch 1031, LR 0.469301 Loss 11.265880, Accuracy 50.915%\n",
      "Epoch 2, Batch 1032, LR 0.469510 Loss 11.265978, Accuracy 50.911%\n",
      "Epoch 2, Batch 1033, LR 0.469719 Loss 11.265213, Accuracy 50.922%\n",
      "Epoch 2, Batch 1034, LR 0.469928 Loss 11.264671, Accuracy 50.924%\n",
      "Epoch 2, Batch 1035, LR 0.470138 Loss 11.264629, Accuracy 50.924%\n",
      "Epoch 2, Batch 1036, LR 0.470347 Loss 11.263787, Accuracy 50.933%\n",
      "Epoch 2, Batch 1037, LR 0.470556 Loss 11.263361, Accuracy 50.939%\n",
      "Epoch 2, Batch 1038, LR 0.470766 Loss 11.262262, Accuracy 50.954%\n",
      "Epoch 2, Batch 1039, LR 0.470975 Loss 11.261864, Accuracy 50.956%\n",
      "Epoch 2, Batch 1040, LR 0.471185 Loss 11.260981, Accuracy 50.963%\n",
      "Epoch 2, Batch 1041, LR 0.471395 Loss 11.260608, Accuracy 50.967%\n",
      "Epoch 2, Batch 1042, LR 0.471604 Loss 11.260130, Accuracy 50.969%\n",
      "Epoch 2, Batch 1043, LR 0.471814 Loss 11.260212, Accuracy 50.969%\n",
      "Epoch 2, Batch 1044, LR 0.472024 Loss 11.260197, Accuracy 50.966%\n",
      "Epoch 2, Batch 1045, LR 0.472233 Loss 11.259724, Accuracy 50.976%\n",
      "Epoch 2, Batch 1046, LR 0.472443 Loss 11.259138, Accuracy 50.981%\n",
      "Epoch 2, Batch 1047, LR 0.472653 Loss 11.258618, Accuracy 50.987%\n",
      "Epoch 2, Loss (train set) 11.258618, Accuracy (train set) 50.987%\n",
      "Epoch 3, Batch 1, LR 0.472863 Loss 10.540416, Accuracy 56.250%\n",
      "Epoch 3, Batch 2, LR 0.473073 Loss 10.264050, Accuracy 58.594%\n",
      "Epoch 3, Batch 3, LR 0.473283 Loss 10.351644, Accuracy 58.073%\n",
      "Epoch 3, Batch 4, LR 0.473493 Loss 10.344928, Accuracy 58.203%\n",
      "Epoch 3, Batch 5, LR 0.473704 Loss 10.316536, Accuracy 58.594%\n",
      "Epoch 3, Batch 6, LR 0.473914 Loss 10.419465, Accuracy 58.724%\n",
      "Epoch 3, Batch 7, LR 0.474124 Loss 10.464600, Accuracy 58.259%\n",
      "Epoch 3, Batch 8, LR 0.474334 Loss 10.467893, Accuracy 58.105%\n",
      "Epoch 3, Batch 9, LR 0.474545 Loss 10.431102, Accuracy 58.594%\n",
      "Epoch 3, Batch 10, LR 0.474755 Loss 10.498987, Accuracy 58.125%\n",
      "Epoch 3, Batch 11, LR 0.474966 Loss 10.642301, Accuracy 56.747%\n",
      "Epoch 3, Batch 12, LR 0.475176 Loss 10.644846, Accuracy 56.641%\n",
      "Epoch 3, Batch 13, LR 0.475387 Loss 10.611920, Accuracy 56.731%\n",
      "Epoch 3, Batch 14, LR 0.475598 Loss 10.630994, Accuracy 57.031%\n",
      "Epoch 3, Batch 15, LR 0.475808 Loss 10.601601, Accuracy 57.083%\n",
      "Epoch 3, Batch 16, LR 0.476019 Loss 10.557899, Accuracy 57.373%\n",
      "Epoch 3, Batch 17, LR 0.476230 Loss 10.537269, Accuracy 57.445%\n",
      "Epoch 3, Batch 18, LR 0.476441 Loss 10.543532, Accuracy 57.335%\n",
      "Epoch 3, Batch 19, LR 0.476652 Loss 10.555747, Accuracy 57.196%\n",
      "Epoch 3, Batch 20, LR 0.476862 Loss 10.541782, Accuracy 57.383%\n",
      "Epoch 3, Batch 21, LR 0.477074 Loss 10.537193, Accuracy 57.515%\n",
      "Epoch 3, Batch 22, LR 0.477285 Loss 10.556164, Accuracy 57.351%\n",
      "Epoch 3, Batch 23, LR 0.477496 Loss 10.555050, Accuracy 57.371%\n",
      "Epoch 3, Batch 24, LR 0.477707 Loss 10.584296, Accuracy 57.031%\n",
      "Epoch 3, Batch 25, LR 0.477918 Loss 10.616109, Accuracy 56.812%\n",
      "Epoch 3, Batch 26, LR 0.478129 Loss 10.615799, Accuracy 56.881%\n",
      "Epoch 3, Batch 27, LR 0.478341 Loss 10.606257, Accuracy 56.973%\n",
      "Epoch 3, Batch 28, LR 0.478552 Loss 10.614469, Accuracy 56.892%\n",
      "Epoch 3, Batch 29, LR 0.478763 Loss 10.615217, Accuracy 56.816%\n",
      "Epoch 3, Batch 30, LR 0.478975 Loss 10.597767, Accuracy 57.031%\n",
      "Epoch 3, Batch 31, LR 0.479187 Loss 10.590826, Accuracy 57.308%\n",
      "Epoch 3, Batch 32, LR 0.479398 Loss 10.566659, Accuracy 57.593%\n",
      "Epoch 3, Batch 33, LR 0.479610 Loss 10.559132, Accuracy 57.647%\n",
      "Epoch 3, Batch 34, LR 0.479821 Loss 10.557774, Accuracy 57.721%\n",
      "Epoch 3, Batch 35, LR 0.480033 Loss 10.579015, Accuracy 57.433%\n",
      "Epoch 3, Batch 36, LR 0.480245 Loss 10.593795, Accuracy 57.205%\n",
      "Epoch 3, Batch 37, LR 0.480457 Loss 10.594535, Accuracy 57.242%\n",
      "Epoch 3, Batch 38, LR 0.480669 Loss 10.599707, Accuracy 57.237%\n",
      "Epoch 3, Batch 39, LR 0.480881 Loss 10.590479, Accuracy 57.392%\n",
      "Epoch 3, Batch 40, LR 0.481093 Loss 10.577305, Accuracy 57.559%\n",
      "Epoch 3, Batch 41, LR 0.481305 Loss 10.566032, Accuracy 57.527%\n",
      "Epoch 3, Batch 42, LR 0.481517 Loss 10.576115, Accuracy 57.403%\n",
      "Epoch 3, Batch 43, LR 0.481729 Loss 10.586639, Accuracy 57.231%\n",
      "Epoch 3, Batch 44, LR 0.481941 Loss 10.589713, Accuracy 57.085%\n",
      "Epoch 3, Batch 45, LR 0.482153 Loss 10.595382, Accuracy 57.066%\n",
      "Epoch 3, Batch 46, LR 0.482366 Loss 10.598072, Accuracy 57.133%\n",
      "Epoch 3, Batch 47, LR 0.482578 Loss 10.600007, Accuracy 57.181%\n",
      "Epoch 3, Batch 48, LR 0.482791 Loss 10.585535, Accuracy 57.292%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 49, LR 0.483003 Loss 10.578773, Accuracy 57.270%\n",
      "Epoch 3, Batch 50, LR 0.483216 Loss 10.568064, Accuracy 57.375%\n",
      "Epoch 3, Batch 51, LR 0.483428 Loss 10.576276, Accuracy 57.322%\n",
      "Epoch 3, Batch 52, LR 0.483641 Loss 10.574393, Accuracy 57.317%\n",
      "Epoch 3, Batch 53, LR 0.483853 Loss 10.563354, Accuracy 57.400%\n",
      "Epoch 3, Batch 54, LR 0.484066 Loss 10.559006, Accuracy 57.321%\n",
      "Epoch 3, Batch 55, LR 0.484279 Loss 10.571265, Accuracy 57.287%\n",
      "Epoch 3, Batch 56, LR 0.484492 Loss 10.578690, Accuracy 57.227%\n",
      "Epoch 3, Batch 57, LR 0.484705 Loss 10.587568, Accuracy 57.127%\n",
      "Epoch 3, Batch 58, LR 0.484918 Loss 10.586441, Accuracy 57.152%\n",
      "Epoch 3, Batch 59, LR 0.485131 Loss 10.575797, Accuracy 57.256%\n",
      "Epoch 3, Batch 60, LR 0.485344 Loss 10.572835, Accuracy 57.305%\n",
      "Epoch 3, Batch 61, LR 0.485557 Loss 10.569108, Accuracy 57.377%\n",
      "Epoch 3, Batch 62, LR 0.485770 Loss 10.562709, Accuracy 57.472%\n",
      "Epoch 3, Batch 63, LR 0.485983 Loss 10.573072, Accuracy 57.316%\n",
      "Epoch 3, Batch 64, LR 0.486196 Loss 10.577693, Accuracy 57.202%\n",
      "Epoch 3, Batch 65, LR 0.486410 Loss 10.578825, Accuracy 57.284%\n",
      "Epoch 3, Batch 66, LR 0.486623 Loss 10.573380, Accuracy 57.327%\n",
      "Epoch 3, Batch 67, LR 0.486836 Loss 10.573208, Accuracy 57.358%\n",
      "Epoch 3, Batch 68, LR 0.487050 Loss 10.577942, Accuracy 57.307%\n",
      "Epoch 3, Batch 69, LR 0.487263 Loss 10.583485, Accuracy 57.246%\n",
      "Epoch 3, Batch 70, LR 0.487477 Loss 10.583769, Accuracy 57.254%\n",
      "Epoch 3, Batch 71, LR 0.487691 Loss 10.581922, Accuracy 57.262%\n",
      "Epoch 3, Batch 72, LR 0.487904 Loss 10.583772, Accuracy 57.216%\n",
      "Epoch 3, Batch 73, LR 0.488118 Loss 10.590302, Accuracy 57.149%\n",
      "Epoch 3, Batch 74, LR 0.488332 Loss 10.588095, Accuracy 57.116%\n",
      "Epoch 3, Batch 75, LR 0.488545 Loss 10.593179, Accuracy 57.083%\n",
      "Epoch 3, Batch 76, LR 0.488759 Loss 10.606479, Accuracy 56.959%\n",
      "Epoch 3, Batch 77, LR 0.488973 Loss 10.608542, Accuracy 56.940%\n",
      "Epoch 3, Batch 78, LR 0.489187 Loss 10.614469, Accuracy 56.871%\n",
      "Epoch 3, Batch 79, LR 0.489401 Loss 10.618652, Accuracy 56.794%\n",
      "Epoch 3, Batch 80, LR 0.489615 Loss 10.617271, Accuracy 56.826%\n",
      "Epoch 3, Batch 81, LR 0.489829 Loss 10.613350, Accuracy 56.829%\n",
      "Epoch 3, Batch 82, LR 0.490044 Loss 10.619098, Accuracy 56.764%\n",
      "Epoch 3, Batch 83, LR 0.490258 Loss 10.613892, Accuracy 56.824%\n",
      "Epoch 3, Batch 84, LR 0.490472 Loss 10.615353, Accuracy 56.799%\n",
      "Epoch 3, Batch 85, LR 0.490686 Loss 10.611999, Accuracy 56.811%\n",
      "Epoch 3, Batch 86, LR 0.490901 Loss 10.616020, Accuracy 56.786%\n",
      "Epoch 3, Batch 87, LR 0.491115 Loss 10.610779, Accuracy 56.843%\n",
      "Epoch 3, Batch 88, LR 0.491330 Loss 10.620745, Accuracy 56.747%\n",
      "Epoch 3, Batch 89, LR 0.491544 Loss 10.616799, Accuracy 56.750%\n",
      "Epoch 3, Batch 90, LR 0.491759 Loss 10.616230, Accuracy 56.771%\n",
      "Epoch 3, Batch 91, LR 0.491973 Loss 10.621001, Accuracy 56.722%\n",
      "Epoch 3, Batch 92, LR 0.492188 Loss 10.617640, Accuracy 56.751%\n",
      "Epoch 3, Batch 93, LR 0.492403 Loss 10.622960, Accuracy 56.695%\n",
      "Epoch 3, Batch 94, LR 0.492618 Loss 10.625705, Accuracy 56.657%\n",
      "Epoch 3, Batch 95, LR 0.492833 Loss 10.627211, Accuracy 56.628%\n",
      "Epoch 3, Batch 96, LR 0.493047 Loss 10.624795, Accuracy 56.641%\n",
      "Epoch 3, Batch 97, LR 0.493262 Loss 10.627385, Accuracy 56.629%\n",
      "Epoch 3, Batch 98, LR 0.493477 Loss 10.628178, Accuracy 56.601%\n",
      "Epoch 3, Batch 99, LR 0.493692 Loss 10.636008, Accuracy 56.581%\n",
      "Epoch 3, Batch 100, LR 0.493908 Loss 10.630513, Accuracy 56.672%\n",
      "Epoch 3, Batch 101, LR 0.494123 Loss 10.633964, Accuracy 56.637%\n",
      "Epoch 3, Batch 102, LR 0.494338 Loss 10.637676, Accuracy 56.587%\n",
      "Epoch 3, Batch 103, LR 0.494553 Loss 10.639495, Accuracy 56.561%\n",
      "Epoch 3, Batch 104, LR 0.494768 Loss 10.643465, Accuracy 56.475%\n",
      "Epoch 3, Batch 105, LR 0.494984 Loss 10.641847, Accuracy 56.406%\n",
      "Epoch 3, Batch 106, LR 0.495199 Loss 10.647288, Accuracy 56.331%\n",
      "Epoch 3, Batch 107, LR 0.495415 Loss 10.646171, Accuracy 56.352%\n",
      "Epoch 3, Batch 108, LR 0.495630 Loss 10.646642, Accuracy 56.308%\n",
      "Epoch 3, Batch 109, LR 0.495846 Loss 10.649202, Accuracy 56.279%\n",
      "Epoch 3, Batch 110, LR 0.496061 Loss 10.643238, Accuracy 56.335%\n",
      "Epoch 3, Batch 111, LR 0.496277 Loss 10.639476, Accuracy 56.370%\n",
      "Epoch 3, Batch 112, LR 0.496493 Loss 10.635589, Accuracy 56.431%\n",
      "Epoch 3, Batch 113, LR 0.496708 Loss 10.635683, Accuracy 56.450%\n",
      "Epoch 3, Batch 114, LR 0.496924 Loss 10.635604, Accuracy 56.456%\n",
      "Epoch 3, Batch 115, LR 0.497140 Loss 10.638534, Accuracy 56.413%\n",
      "Epoch 3, Batch 116, LR 0.497356 Loss 10.638653, Accuracy 56.378%\n",
      "Epoch 3, Batch 117, LR 0.497572 Loss 10.637493, Accuracy 56.404%\n",
      "Epoch 3, Batch 118, LR 0.497788 Loss 10.633602, Accuracy 56.462%\n",
      "Epoch 3, Batch 119, LR 0.498004 Loss 10.631792, Accuracy 56.467%\n",
      "Epoch 3, Batch 120, LR 0.498220 Loss 10.635056, Accuracy 56.439%\n",
      "Epoch 3, Batch 121, LR 0.498436 Loss 10.634154, Accuracy 56.463%\n",
      "Epoch 3, Batch 122, LR 0.498653 Loss 10.633587, Accuracy 56.455%\n",
      "Epoch 3, Batch 123, LR 0.498869 Loss 10.631356, Accuracy 56.441%\n",
      "Epoch 3, Batch 124, LR 0.499085 Loss 10.632028, Accuracy 56.439%\n",
      "Epoch 3, Batch 125, LR 0.499302 Loss 10.633401, Accuracy 56.413%\n",
      "Epoch 3, Batch 126, LR 0.499518 Loss 10.633824, Accuracy 56.448%\n",
      "Epoch 3, Batch 127, LR 0.499735 Loss 10.635408, Accuracy 56.385%\n",
      "Epoch 3, Batch 128, LR 0.499951 Loss 10.631744, Accuracy 56.421%\n",
      "Epoch 3, Batch 129, LR 0.500168 Loss 10.635306, Accuracy 56.395%\n",
      "Epoch 3, Batch 130, LR 0.500384 Loss 10.639926, Accuracy 56.358%\n",
      "Epoch 3, Batch 131, LR 0.500601 Loss 10.642688, Accuracy 56.339%\n",
      "Epoch 3, Batch 132, LR 0.500818 Loss 10.641673, Accuracy 56.357%\n",
      "Epoch 3, Batch 133, LR 0.501034 Loss 10.638945, Accuracy 56.414%\n",
      "Epoch 3, Batch 134, LR 0.501251 Loss 10.640950, Accuracy 56.402%\n",
      "Epoch 3, Batch 135, LR 0.501468 Loss 10.642053, Accuracy 56.458%\n",
      "Epoch 3, Batch 136, LR 0.501685 Loss 10.640157, Accuracy 56.474%\n",
      "Epoch 3, Batch 137, LR 0.501902 Loss 10.641400, Accuracy 56.455%\n",
      "Epoch 3, Batch 138, LR 0.502119 Loss 10.641378, Accuracy 56.454%\n",
      "Epoch 3, Batch 139, LR 0.502336 Loss 10.646483, Accuracy 56.385%\n",
      "Epoch 3, Batch 140, LR 0.502553 Loss 10.647035, Accuracy 56.378%\n",
      "Epoch 3, Batch 141, LR 0.502771 Loss 10.645864, Accuracy 56.366%\n",
      "Epoch 3, Batch 142, LR 0.502988 Loss 10.647375, Accuracy 56.360%\n",
      "Epoch 3, Batch 143, LR 0.503205 Loss 10.648265, Accuracy 56.387%\n",
      "Epoch 3, Batch 144, LR 0.503422 Loss 10.649641, Accuracy 56.396%\n",
      "Epoch 3, Batch 145, LR 0.503640 Loss 10.646487, Accuracy 56.428%\n",
      "Epoch 3, Batch 146, LR 0.503857 Loss 10.638715, Accuracy 56.491%\n",
      "Epoch 3, Batch 147, LR 0.504075 Loss 10.636925, Accuracy 56.494%\n",
      "Epoch 3, Batch 148, LR 0.504292 Loss 10.638072, Accuracy 56.461%\n",
      "Epoch 3, Batch 149, LR 0.504510 Loss 10.639181, Accuracy 56.465%\n",
      "Epoch 3, Batch 150, LR 0.504727 Loss 10.643476, Accuracy 56.453%\n",
      "Epoch 3, Batch 151, LR 0.504945 Loss 10.640661, Accuracy 56.493%\n",
      "Epoch 3, Batch 152, LR 0.505163 Loss 10.638712, Accuracy 56.507%\n",
      "Epoch 3, Batch 153, LR 0.505381 Loss 10.637957, Accuracy 56.500%\n",
      "Epoch 3, Batch 154, LR 0.505599 Loss 10.637055, Accuracy 56.519%\n",
      "Epoch 3, Batch 155, LR 0.505816 Loss 10.637606, Accuracy 56.492%\n",
      "Epoch 3, Batch 156, LR 0.506034 Loss 10.637465, Accuracy 56.490%\n",
      "Epoch 3, Batch 157, LR 0.506252 Loss 10.642136, Accuracy 56.454%\n",
      "Epoch 3, Batch 158, LR 0.506470 Loss 10.645536, Accuracy 56.403%\n",
      "Epoch 3, Batch 159, LR 0.506689 Loss 10.648171, Accuracy 56.368%\n",
      "Epoch 3, Batch 160, LR 0.506907 Loss 10.647871, Accuracy 56.357%\n",
      "Epoch 3, Batch 161, LR 0.507125 Loss 10.644782, Accuracy 56.415%\n",
      "Epoch 3, Batch 162, LR 0.507343 Loss 10.642703, Accuracy 56.428%\n",
      "Epoch 3, Batch 163, LR 0.507561 Loss 10.640962, Accuracy 56.418%\n",
      "Epoch 3, Batch 164, LR 0.507780 Loss 10.642588, Accuracy 56.398%\n",
      "Epoch 3, Batch 165, LR 0.507998 Loss 10.641270, Accuracy 56.397%\n",
      "Epoch 3, Batch 166, LR 0.508217 Loss 10.636254, Accuracy 56.438%\n",
      "Epoch 3, Batch 167, LR 0.508435 Loss 10.637066, Accuracy 56.414%\n",
      "Epoch 3, Batch 168, LR 0.508654 Loss 10.638317, Accuracy 56.413%\n",
      "Epoch 3, Batch 169, LR 0.508872 Loss 10.638762, Accuracy 56.416%\n",
      "Epoch 3, Batch 170, LR 0.509091 Loss 10.635825, Accuracy 56.438%\n",
      "Epoch 3, Batch 171, LR 0.509310 Loss 10.632383, Accuracy 56.474%\n",
      "Epoch 3, Batch 172, LR 0.509528 Loss 10.635582, Accuracy 56.445%\n",
      "Epoch 3, Batch 173, LR 0.509747 Loss 10.638086, Accuracy 56.444%\n",
      "Epoch 3, Batch 174, LR 0.509966 Loss 10.636915, Accuracy 56.452%\n",
      "Epoch 3, Batch 175, LR 0.510185 Loss 10.638847, Accuracy 56.442%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 176, LR 0.510404 Loss 10.639272, Accuracy 56.436%\n",
      "Epoch 3, Batch 177, LR 0.510623 Loss 10.637271, Accuracy 56.480%\n",
      "Epoch 3, Batch 178, LR 0.510842 Loss 10.634431, Accuracy 56.513%\n",
      "Epoch 3, Batch 179, LR 0.511061 Loss 10.633853, Accuracy 56.503%\n",
      "Epoch 3, Batch 180, LR 0.511280 Loss 10.633072, Accuracy 56.515%\n",
      "Epoch 3, Batch 181, LR 0.511499 Loss 10.634133, Accuracy 56.466%\n",
      "Epoch 3, Batch 182, LR 0.511719 Loss 10.634691, Accuracy 56.447%\n",
      "Epoch 3, Batch 183, LR 0.511938 Loss 10.637490, Accuracy 56.438%\n",
      "Epoch 3, Batch 184, LR 0.512157 Loss 10.632121, Accuracy 56.492%\n",
      "Epoch 3, Batch 185, LR 0.512377 Loss 10.631247, Accuracy 56.486%\n",
      "Epoch 3, Batch 186, LR 0.512596 Loss 10.630635, Accuracy 56.510%\n",
      "Epoch 3, Batch 187, LR 0.512816 Loss 10.626650, Accuracy 56.534%\n",
      "Epoch 3, Batch 188, LR 0.513035 Loss 10.624758, Accuracy 56.562%\n",
      "Epoch 3, Batch 189, LR 0.513255 Loss 10.625136, Accuracy 56.577%\n",
      "Epoch 3, Batch 190, LR 0.513474 Loss 10.624512, Accuracy 56.587%\n",
      "Epoch 3, Batch 191, LR 0.513694 Loss 10.627105, Accuracy 56.573%\n",
      "Epoch 3, Batch 192, LR 0.513914 Loss 10.623403, Accuracy 56.592%\n",
      "Epoch 3, Batch 193, LR 0.514134 Loss 10.620738, Accuracy 56.610%\n",
      "Epoch 3, Batch 194, LR 0.514354 Loss 10.619888, Accuracy 56.608%\n",
      "Epoch 3, Batch 195, LR 0.514573 Loss 10.620359, Accuracy 56.587%\n",
      "Epoch 3, Batch 196, LR 0.514793 Loss 10.621450, Accuracy 56.597%\n",
      "Epoch 3, Batch 197, LR 0.515013 Loss 10.618595, Accuracy 56.603%\n",
      "Epoch 3, Batch 198, LR 0.515233 Loss 10.619246, Accuracy 56.593%\n",
      "Epoch 3, Batch 199, LR 0.515454 Loss 10.620285, Accuracy 56.572%\n",
      "Epoch 3, Batch 200, LR 0.515674 Loss 10.621408, Accuracy 56.547%\n",
      "Epoch 3, Batch 201, LR 0.515894 Loss 10.618727, Accuracy 56.569%\n",
      "Epoch 3, Batch 202, LR 0.516114 Loss 10.620130, Accuracy 56.559%\n",
      "Epoch 3, Batch 203, LR 0.516334 Loss 10.621128, Accuracy 56.523%\n",
      "Epoch 3, Batch 204, LR 0.516555 Loss 10.619155, Accuracy 56.537%\n",
      "Epoch 3, Batch 205, LR 0.516775 Loss 10.623607, Accuracy 56.521%\n",
      "Epoch 3, Batch 206, LR 0.516996 Loss 10.620417, Accuracy 56.550%\n",
      "Epoch 3, Batch 207, LR 0.517216 Loss 10.619508, Accuracy 56.563%\n",
      "Epoch 3, Batch 208, LR 0.517437 Loss 10.620319, Accuracy 56.558%\n",
      "Epoch 3, Batch 209, LR 0.517657 Loss 10.620300, Accuracy 56.560%\n",
      "Epoch 3, Batch 210, LR 0.517878 Loss 10.620980, Accuracy 56.540%\n",
      "Epoch 3, Batch 211, LR 0.518099 Loss 10.622083, Accuracy 56.528%\n",
      "Epoch 3, Batch 212, LR 0.518319 Loss 10.621789, Accuracy 56.537%\n",
      "Epoch 3, Batch 213, LR 0.518540 Loss 10.622716, Accuracy 56.540%\n",
      "Epoch 3, Batch 214, LR 0.518761 Loss 10.624580, Accuracy 56.527%\n",
      "Epoch 3, Batch 215, LR 0.518982 Loss 10.623535, Accuracy 56.537%\n",
      "Epoch 3, Batch 216, LR 0.519203 Loss 10.621452, Accuracy 56.568%\n",
      "Epoch 3, Batch 217, LR 0.519424 Loss 10.621580, Accuracy 56.570%\n",
      "Epoch 3, Batch 218, LR 0.519645 Loss 10.619476, Accuracy 56.562%\n",
      "Epoch 3, Batch 219, LR 0.519866 Loss 10.619644, Accuracy 56.557%\n",
      "Epoch 3, Batch 220, LR 0.520087 Loss 10.619730, Accuracy 56.552%\n",
      "Epoch 3, Batch 221, LR 0.520308 Loss 10.615186, Accuracy 56.600%\n",
      "Epoch 3, Batch 222, LR 0.520530 Loss 10.613118, Accuracy 56.627%\n",
      "Epoch 3, Batch 223, LR 0.520751 Loss 10.611753, Accuracy 56.653%\n",
      "Epoch 3, Batch 224, LR 0.520972 Loss 10.610701, Accuracy 56.641%\n",
      "Epoch 3, Batch 225, LR 0.521194 Loss 10.614463, Accuracy 56.604%\n",
      "Epoch 3, Batch 226, LR 0.521415 Loss 10.612694, Accuracy 56.610%\n",
      "Epoch 3, Batch 227, LR 0.521637 Loss 10.613067, Accuracy 56.611%\n",
      "Epoch 3, Batch 228, LR 0.521858 Loss 10.613272, Accuracy 56.623%\n",
      "Epoch 3, Batch 229, LR 0.522080 Loss 10.609820, Accuracy 56.639%\n",
      "Epoch 3, Batch 230, LR 0.522301 Loss 10.609824, Accuracy 56.630%\n",
      "Epoch 3, Batch 231, LR 0.522523 Loss 10.607345, Accuracy 56.642%\n",
      "Epoch 3, Batch 232, LR 0.522745 Loss 10.608108, Accuracy 56.651%\n",
      "Epoch 3, Batch 233, LR 0.522967 Loss 10.603762, Accuracy 56.686%\n",
      "Epoch 3, Batch 234, LR 0.523188 Loss 10.602458, Accuracy 56.691%\n",
      "Epoch 3, Batch 235, LR 0.523410 Loss 10.602302, Accuracy 56.702%\n",
      "Epoch 3, Batch 236, LR 0.523632 Loss 10.602627, Accuracy 56.713%\n",
      "Epoch 3, Batch 237, LR 0.523854 Loss 10.600912, Accuracy 56.711%\n",
      "Epoch 3, Batch 238, LR 0.524076 Loss 10.601933, Accuracy 56.706%\n",
      "Epoch 3, Batch 239, LR 0.524298 Loss 10.605546, Accuracy 56.672%\n",
      "Epoch 3, Batch 240, LR 0.524520 Loss 10.606463, Accuracy 56.667%\n",
      "Epoch 3, Batch 241, LR 0.524743 Loss 10.606984, Accuracy 56.684%\n",
      "Epoch 3, Batch 242, LR 0.524965 Loss 10.605027, Accuracy 56.705%\n",
      "Epoch 3, Batch 243, LR 0.525187 Loss 10.604067, Accuracy 56.707%\n",
      "Epoch 3, Batch 244, LR 0.525410 Loss 10.602003, Accuracy 56.727%\n",
      "Epoch 3, Batch 245, LR 0.525632 Loss 10.601817, Accuracy 56.728%\n",
      "Epoch 3, Batch 246, LR 0.525854 Loss 10.600674, Accuracy 56.730%\n",
      "Epoch 3, Batch 247, LR 0.526077 Loss 10.598870, Accuracy 56.737%\n",
      "Epoch 3, Batch 248, LR 0.526299 Loss 10.600410, Accuracy 56.710%\n",
      "Epoch 3, Batch 249, LR 0.526522 Loss 10.602156, Accuracy 56.711%\n",
      "Epoch 3, Batch 250, LR 0.526745 Loss 10.602434, Accuracy 56.706%\n",
      "Epoch 3, Batch 251, LR 0.526967 Loss 10.601547, Accuracy 56.720%\n",
      "Epoch 3, Batch 252, LR 0.527190 Loss 10.601350, Accuracy 56.706%\n",
      "Epoch 3, Batch 253, LR 0.527413 Loss 10.601892, Accuracy 56.685%\n",
      "Epoch 3, Batch 254, LR 0.527636 Loss 10.601400, Accuracy 56.696%\n",
      "Epoch 3, Batch 255, LR 0.527858 Loss 10.603077, Accuracy 56.673%\n",
      "Epoch 3, Batch 256, LR 0.528081 Loss 10.601638, Accuracy 56.696%\n",
      "Epoch 3, Batch 257, LR 0.528304 Loss 10.600255, Accuracy 56.727%\n",
      "Epoch 3, Batch 258, LR 0.528527 Loss 10.600633, Accuracy 56.701%\n",
      "Epoch 3, Batch 259, LR 0.528750 Loss 10.601852, Accuracy 56.681%\n",
      "Epoch 3, Batch 260, LR 0.528973 Loss 10.602191, Accuracy 56.692%\n",
      "Epoch 3, Batch 261, LR 0.529197 Loss 10.601268, Accuracy 56.696%\n",
      "Epoch 3, Batch 262, LR 0.529420 Loss 10.600025, Accuracy 56.712%\n",
      "Epoch 3, Batch 263, LR 0.529643 Loss 10.598907, Accuracy 56.731%\n",
      "Epoch 3, Batch 264, LR 0.529866 Loss 10.596599, Accuracy 56.762%\n",
      "Epoch 3, Batch 265, LR 0.530090 Loss 10.592906, Accuracy 56.795%\n",
      "Epoch 3, Batch 266, LR 0.530313 Loss 10.591616, Accuracy 56.811%\n",
      "Epoch 3, Batch 267, LR 0.530537 Loss 10.590961, Accuracy 56.803%\n",
      "Epoch 3, Batch 268, LR 0.530760 Loss 10.593201, Accuracy 56.786%\n",
      "Epoch 3, Batch 269, LR 0.530984 Loss 10.592712, Accuracy 56.779%\n",
      "Epoch 3, Batch 270, LR 0.531207 Loss 10.593252, Accuracy 56.777%\n",
      "Epoch 3, Batch 271, LR 0.531431 Loss 10.590263, Accuracy 56.792%\n",
      "Epoch 3, Batch 272, LR 0.531655 Loss 10.591766, Accuracy 56.787%\n",
      "Epoch 3, Batch 273, LR 0.531878 Loss 10.591783, Accuracy 56.802%\n",
      "Epoch 3, Batch 274, LR 0.532102 Loss 10.588470, Accuracy 56.823%\n",
      "Epoch 3, Batch 275, LR 0.532326 Loss 10.586307, Accuracy 56.832%\n",
      "Epoch 3, Batch 276, LR 0.532550 Loss 10.586197, Accuracy 56.827%\n",
      "Epoch 3, Batch 277, LR 0.532774 Loss 10.586837, Accuracy 56.823%\n",
      "Epoch 3, Batch 278, LR 0.532998 Loss 10.585360, Accuracy 56.820%\n",
      "Epoch 3, Batch 279, LR 0.533222 Loss 10.585805, Accuracy 56.804%\n",
      "Epoch 3, Batch 280, LR 0.533446 Loss 10.585489, Accuracy 56.802%\n",
      "Epoch 3, Batch 281, LR 0.533670 Loss 10.583604, Accuracy 56.806%\n",
      "Epoch 3, Batch 282, LR 0.533894 Loss 10.583976, Accuracy 56.799%\n",
      "Epoch 3, Batch 283, LR 0.534119 Loss 10.583137, Accuracy 56.802%\n",
      "Epoch 3, Batch 284, LR 0.534343 Loss 10.582451, Accuracy 56.819%\n",
      "Epoch 3, Batch 285, LR 0.534567 Loss 10.582486, Accuracy 56.837%\n",
      "Epoch 3, Batch 286, LR 0.534792 Loss 10.582090, Accuracy 56.829%\n",
      "Epoch 3, Batch 287, LR 0.535016 Loss 10.582997, Accuracy 56.819%\n",
      "Epoch 3, Batch 288, LR 0.535241 Loss 10.584231, Accuracy 56.798%\n",
      "Epoch 3, Batch 289, LR 0.535465 Loss 10.582624, Accuracy 56.839%\n",
      "Epoch 3, Batch 290, LR 0.535690 Loss 10.578227, Accuracy 56.878%\n",
      "Epoch 3, Batch 291, LR 0.535914 Loss 10.580385, Accuracy 56.870%\n",
      "Epoch 3, Batch 292, LR 0.536139 Loss 10.580118, Accuracy 56.868%\n",
      "Epoch 3, Batch 293, LR 0.536364 Loss 10.578880, Accuracy 56.861%\n",
      "Epoch 3, Batch 294, LR 0.536589 Loss 10.577732, Accuracy 56.864%\n",
      "Epoch 3, Batch 295, LR 0.536813 Loss 10.579963, Accuracy 56.854%\n",
      "Epoch 3, Batch 296, LR 0.537038 Loss 10.579538, Accuracy 56.862%\n",
      "Epoch 3, Batch 297, LR 0.537263 Loss 10.579106, Accuracy 56.852%\n",
      "Epoch 3, Batch 298, LR 0.537488 Loss 10.581482, Accuracy 56.835%\n",
      "Epoch 3, Batch 299, LR 0.537713 Loss 10.579904, Accuracy 56.854%\n",
      "Epoch 3, Batch 300, LR 0.537938 Loss 10.581686, Accuracy 56.823%\n",
      "Epoch 3, Batch 301, LR 0.538163 Loss 10.578870, Accuracy 56.847%\n",
      "Epoch 3, Batch 302, LR 0.538388 Loss 10.578203, Accuracy 56.855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 303, LR 0.538614 Loss 10.578254, Accuracy 56.869%\n",
      "Epoch 3, Batch 304, LR 0.538839 Loss 10.577899, Accuracy 56.872%\n",
      "Epoch 3, Batch 305, LR 0.539064 Loss 10.580159, Accuracy 56.834%\n",
      "Epoch 3, Batch 306, LR 0.539290 Loss 10.579986, Accuracy 56.824%\n",
      "Epoch 3, Batch 307, LR 0.539515 Loss 10.580866, Accuracy 56.810%\n",
      "Epoch 3, Batch 308, LR 0.539740 Loss 10.581426, Accuracy 56.805%\n",
      "Epoch 3, Batch 309, LR 0.539966 Loss 10.580623, Accuracy 56.816%\n",
      "Epoch 3, Batch 310, LR 0.540191 Loss 10.581254, Accuracy 56.809%\n",
      "Epoch 3, Batch 311, LR 0.540417 Loss 10.582222, Accuracy 56.808%\n",
      "Epoch 3, Batch 312, LR 0.540643 Loss 10.580414, Accuracy 56.823%\n",
      "Epoch 3, Batch 313, LR 0.540868 Loss 10.581009, Accuracy 56.822%\n",
      "Epoch 3, Batch 314, LR 0.541094 Loss 10.582217, Accuracy 56.810%\n",
      "Epoch 3, Batch 315, LR 0.541320 Loss 10.582721, Accuracy 56.813%\n",
      "Epoch 3, Batch 316, LR 0.541546 Loss 10.581885, Accuracy 56.831%\n",
      "Epoch 3, Batch 317, LR 0.541772 Loss 10.580897, Accuracy 56.844%\n",
      "Epoch 3, Batch 318, LR 0.541998 Loss 10.580174, Accuracy 56.857%\n",
      "Epoch 3, Batch 319, LR 0.542224 Loss 10.578438, Accuracy 56.870%\n",
      "Epoch 3, Batch 320, LR 0.542450 Loss 10.577221, Accuracy 56.882%\n",
      "Epoch 3, Batch 321, LR 0.542676 Loss 10.576605, Accuracy 56.885%\n",
      "Epoch 3, Batch 322, LR 0.542902 Loss 10.575468, Accuracy 56.903%\n",
      "Epoch 3, Batch 323, LR 0.543128 Loss 10.577533, Accuracy 56.874%\n",
      "Epoch 3, Batch 324, LR 0.543354 Loss 10.577447, Accuracy 56.884%\n",
      "Epoch 3, Batch 325, LR 0.543581 Loss 10.576360, Accuracy 56.880%\n",
      "Epoch 3, Batch 326, LR 0.543807 Loss 10.577882, Accuracy 56.871%\n",
      "Epoch 3, Batch 327, LR 0.544033 Loss 10.576552, Accuracy 56.881%\n",
      "Epoch 3, Batch 328, LR 0.544260 Loss 10.578120, Accuracy 56.867%\n",
      "Epoch 3, Batch 329, LR 0.544486 Loss 10.579713, Accuracy 56.844%\n",
      "Epoch 3, Batch 330, LR 0.544713 Loss 10.580870, Accuracy 56.835%\n",
      "Epoch 3, Batch 331, LR 0.544939 Loss 10.581183, Accuracy 56.831%\n",
      "Epoch 3, Batch 332, LR 0.545166 Loss 10.583845, Accuracy 56.805%\n",
      "Epoch 3, Batch 333, LR 0.545392 Loss 10.583513, Accuracy 56.808%\n",
      "Epoch 3, Batch 334, LR 0.545619 Loss 10.584269, Accuracy 56.790%\n",
      "Epoch 3, Batch 335, LR 0.545846 Loss 10.586072, Accuracy 56.784%\n",
      "Epoch 3, Batch 336, LR 0.546073 Loss 10.584739, Accuracy 56.799%\n",
      "Epoch 3, Batch 337, LR 0.546300 Loss 10.584212, Accuracy 56.802%\n",
      "Epoch 3, Batch 338, LR 0.546526 Loss 10.583610, Accuracy 56.807%\n",
      "Epoch 3, Batch 339, LR 0.546753 Loss 10.582983, Accuracy 56.810%\n",
      "Epoch 3, Batch 340, LR 0.546980 Loss 10.584225, Accuracy 56.804%\n",
      "Epoch 3, Batch 341, LR 0.547207 Loss 10.587961, Accuracy 56.768%\n",
      "Epoch 3, Batch 342, LR 0.547434 Loss 10.587904, Accuracy 56.764%\n",
      "Epoch 3, Batch 343, LR 0.547662 Loss 10.589137, Accuracy 56.756%\n",
      "Epoch 3, Batch 344, LR 0.547889 Loss 10.591460, Accuracy 56.729%\n",
      "Epoch 3, Batch 345, LR 0.548116 Loss 10.591233, Accuracy 56.723%\n",
      "Epoch 3, Batch 346, LR 0.548343 Loss 10.590322, Accuracy 56.735%\n",
      "Epoch 3, Batch 347, LR 0.548571 Loss 10.590738, Accuracy 56.739%\n",
      "Epoch 3, Batch 348, LR 0.548798 Loss 10.592128, Accuracy 56.724%\n",
      "Epoch 3, Batch 349, LR 0.549025 Loss 10.591006, Accuracy 56.729%\n",
      "Epoch 3, Batch 350, LR 0.549253 Loss 10.591802, Accuracy 56.710%\n",
      "Epoch 3, Batch 351, LR 0.549480 Loss 10.592273, Accuracy 56.704%\n",
      "Epoch 3, Batch 352, LR 0.549708 Loss 10.592630, Accuracy 56.694%\n",
      "Epoch 3, Batch 353, LR 0.549936 Loss 10.591616, Accuracy 56.701%\n",
      "Epoch 3, Batch 354, LR 0.550163 Loss 10.591626, Accuracy 56.707%\n",
      "Epoch 3, Batch 355, LR 0.550391 Loss 10.594060, Accuracy 56.692%\n",
      "Epoch 3, Batch 356, LR 0.550619 Loss 10.594852, Accuracy 56.695%\n",
      "Epoch 3, Batch 357, LR 0.550847 Loss 10.594442, Accuracy 56.696%\n",
      "Epoch 3, Batch 358, LR 0.551074 Loss 10.595530, Accuracy 56.686%\n",
      "Epoch 3, Batch 359, LR 0.551302 Loss 10.596193, Accuracy 56.677%\n",
      "Epoch 3, Batch 360, LR 0.551530 Loss 10.597396, Accuracy 56.656%\n",
      "Epoch 3, Batch 361, LR 0.551758 Loss 10.597745, Accuracy 56.653%\n",
      "Epoch 3, Batch 362, LR 0.551986 Loss 10.597214, Accuracy 56.667%\n",
      "Epoch 3, Batch 363, LR 0.552214 Loss 10.595722, Accuracy 56.693%\n",
      "Epoch 3, Batch 364, LR 0.552442 Loss 10.595091, Accuracy 56.703%\n",
      "Epoch 3, Batch 365, LR 0.552671 Loss 10.595070, Accuracy 56.689%\n",
      "Epoch 3, Batch 366, LR 0.552899 Loss 10.594908, Accuracy 56.694%\n",
      "Epoch 3, Batch 367, LR 0.553127 Loss 10.593595, Accuracy 56.710%\n",
      "Epoch 3, Batch 368, LR 0.553356 Loss 10.594132, Accuracy 56.711%\n",
      "Epoch 3, Batch 369, LR 0.553584 Loss 10.593196, Accuracy 56.716%\n",
      "Epoch 3, Batch 370, LR 0.553812 Loss 10.594324, Accuracy 56.696%\n",
      "Epoch 3, Batch 371, LR 0.554041 Loss 10.592914, Accuracy 56.703%\n",
      "Epoch 3, Batch 372, LR 0.554269 Loss 10.591722, Accuracy 56.716%\n",
      "Epoch 3, Batch 373, LR 0.554498 Loss 10.591740, Accuracy 56.705%\n",
      "Epoch 3, Batch 374, LR 0.554726 Loss 10.590826, Accuracy 56.710%\n",
      "Epoch 3, Batch 375, LR 0.554955 Loss 10.590895, Accuracy 56.704%\n",
      "Epoch 3, Batch 376, LR 0.555184 Loss 10.590065, Accuracy 56.705%\n",
      "Epoch 3, Batch 377, LR 0.555413 Loss 10.591877, Accuracy 56.698%\n",
      "Epoch 3, Batch 378, LR 0.555641 Loss 10.592231, Accuracy 56.707%\n",
      "Epoch 3, Batch 379, LR 0.555870 Loss 10.592643, Accuracy 56.699%\n",
      "Epoch 3, Batch 380, LR 0.556099 Loss 10.593349, Accuracy 56.698%\n",
      "Epoch 3, Batch 381, LR 0.556328 Loss 10.594401, Accuracy 56.693%\n",
      "Epoch 3, Batch 382, LR 0.556557 Loss 10.592628, Accuracy 56.702%\n",
      "Epoch 3, Batch 383, LR 0.556786 Loss 10.592814, Accuracy 56.691%\n",
      "Epoch 3, Batch 384, LR 0.557015 Loss 10.591284, Accuracy 56.712%\n",
      "Epoch 3, Batch 385, LR 0.557244 Loss 10.589501, Accuracy 56.731%\n",
      "Epoch 3, Batch 386, LR 0.557473 Loss 10.589212, Accuracy 56.742%\n",
      "Epoch 3, Batch 387, LR 0.557703 Loss 10.589415, Accuracy 56.745%\n",
      "Epoch 3, Batch 388, LR 0.557932 Loss 10.590518, Accuracy 56.731%\n",
      "Epoch 3, Batch 389, LR 0.558161 Loss 10.588767, Accuracy 56.746%\n",
      "Epoch 3, Batch 390, LR 0.558391 Loss 10.587591, Accuracy 56.759%\n",
      "Epoch 3, Batch 391, LR 0.558620 Loss 10.586143, Accuracy 56.773%\n",
      "Epoch 3, Batch 392, LR 0.558850 Loss 10.585765, Accuracy 56.774%\n",
      "Epoch 3, Batch 393, LR 0.559079 Loss 10.586680, Accuracy 56.765%\n",
      "Epoch 3, Batch 394, LR 0.559309 Loss 10.585147, Accuracy 56.764%\n",
      "Epoch 3, Batch 395, LR 0.559538 Loss 10.584920, Accuracy 56.766%\n",
      "Epoch 3, Batch 396, LR 0.559768 Loss 10.582581, Accuracy 56.789%\n",
      "Epoch 3, Batch 397, LR 0.559998 Loss 10.582056, Accuracy 56.803%\n",
      "Epoch 3, Batch 398, LR 0.560227 Loss 10.582059, Accuracy 56.802%\n",
      "Epoch 3, Batch 399, LR 0.560457 Loss 10.580959, Accuracy 56.808%\n",
      "Epoch 3, Batch 400, LR 0.560687 Loss 10.580563, Accuracy 56.830%\n",
      "Epoch 3, Batch 401, LR 0.560917 Loss 10.578301, Accuracy 56.848%\n",
      "Epoch 3, Batch 402, LR 0.561147 Loss 10.577779, Accuracy 56.860%\n",
      "Epoch 3, Batch 403, LR 0.561377 Loss 10.578517, Accuracy 56.851%\n",
      "Epoch 3, Batch 404, LR 0.561607 Loss 10.577634, Accuracy 56.857%\n",
      "Epoch 3, Batch 405, LR 0.561837 Loss 10.576563, Accuracy 56.867%\n",
      "Epoch 3, Batch 406, LR 0.562067 Loss 10.577844, Accuracy 56.870%\n",
      "Epoch 3, Batch 407, LR 0.562297 Loss 10.577089, Accuracy 56.876%\n",
      "Epoch 3, Batch 408, LR 0.562527 Loss 10.576321, Accuracy 56.884%\n",
      "Epoch 3, Batch 409, LR 0.562758 Loss 10.575260, Accuracy 56.898%\n",
      "Epoch 3, Batch 410, LR 0.562988 Loss 10.573353, Accuracy 56.911%\n",
      "Epoch 3, Batch 411, LR 0.563218 Loss 10.572868, Accuracy 56.911%\n",
      "Epoch 3, Batch 412, LR 0.563449 Loss 10.571572, Accuracy 56.914%\n",
      "Epoch 3, Batch 413, LR 0.563679 Loss 10.572967, Accuracy 56.908%\n",
      "Epoch 3, Batch 414, LR 0.563910 Loss 10.572684, Accuracy 56.912%\n",
      "Epoch 3, Batch 415, LR 0.564140 Loss 10.571800, Accuracy 56.924%\n",
      "Epoch 3, Batch 416, LR 0.564371 Loss 10.571172, Accuracy 56.915%\n",
      "Epoch 3, Batch 417, LR 0.564601 Loss 10.571966, Accuracy 56.909%\n",
      "Epoch 3, Batch 418, LR 0.564832 Loss 10.571639, Accuracy 56.914%\n",
      "Epoch 3, Batch 419, LR 0.565063 Loss 10.571473, Accuracy 56.921%\n",
      "Epoch 3, Batch 420, LR 0.565294 Loss 10.570006, Accuracy 56.946%\n",
      "Epoch 3, Batch 421, LR 0.565524 Loss 10.567649, Accuracy 56.972%\n",
      "Epoch 3, Batch 422, LR 0.565755 Loss 10.568211, Accuracy 56.955%\n",
      "Epoch 3, Batch 423, LR 0.565986 Loss 10.567686, Accuracy 56.959%\n",
      "Epoch 3, Batch 424, LR 0.566217 Loss 10.570631, Accuracy 56.939%\n",
      "Epoch 3, Batch 425, LR 0.566448 Loss 10.571216, Accuracy 56.936%\n",
      "Epoch 3, Batch 426, LR 0.566679 Loss 10.570918, Accuracy 56.947%\n",
      "Epoch 3, Batch 427, LR 0.566910 Loss 10.571431, Accuracy 56.945%\n",
      "Epoch 3, Batch 428, LR 0.567141 Loss 10.572832, Accuracy 56.927%\n",
      "Epoch 3, Batch 429, LR 0.567373 Loss 10.572376, Accuracy 56.926%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 430, LR 0.567604 Loss 10.572414, Accuracy 56.917%\n",
      "Epoch 3, Batch 431, LR 0.567835 Loss 10.571467, Accuracy 56.919%\n",
      "Epoch 3, Batch 432, LR 0.568067 Loss 10.571858, Accuracy 56.928%\n",
      "Epoch 3, Batch 433, LR 0.568298 Loss 10.572540, Accuracy 56.903%\n",
      "Epoch 3, Batch 434, LR 0.568529 Loss 10.573676, Accuracy 56.902%\n",
      "Epoch 3, Batch 435, LR 0.568761 Loss 10.573150, Accuracy 56.906%\n",
      "Epoch 3, Batch 436, LR 0.568992 Loss 10.573024, Accuracy 56.911%\n",
      "Epoch 3, Batch 437, LR 0.569224 Loss 10.573335, Accuracy 56.915%\n",
      "Epoch 3, Batch 438, LR 0.569456 Loss 10.574011, Accuracy 56.914%\n",
      "Epoch 3, Batch 439, LR 0.569687 Loss 10.574262, Accuracy 56.910%\n",
      "Epoch 3, Batch 440, LR 0.569919 Loss 10.572839, Accuracy 56.926%\n",
      "Epoch 3, Batch 441, LR 0.570151 Loss 10.570983, Accuracy 56.930%\n",
      "Epoch 3, Batch 442, LR 0.570382 Loss 10.570916, Accuracy 56.936%\n",
      "Epoch 3, Batch 443, LR 0.570614 Loss 10.569487, Accuracy 56.950%\n",
      "Epoch 3, Batch 444, LR 0.570846 Loss 10.568314, Accuracy 56.970%\n",
      "Epoch 3, Batch 445, LR 0.571078 Loss 10.567733, Accuracy 56.979%\n",
      "Epoch 3, Batch 446, LR 0.571310 Loss 10.567090, Accuracy 56.991%\n",
      "Epoch 3, Batch 447, LR 0.571542 Loss 10.566514, Accuracy 56.995%\n",
      "Epoch 3, Batch 448, LR 0.571774 Loss 10.565118, Accuracy 57.003%\n",
      "Epoch 3, Batch 449, LR 0.572006 Loss 10.565348, Accuracy 56.995%\n",
      "Epoch 3, Batch 450, LR 0.572239 Loss 10.563962, Accuracy 56.997%\n",
      "Epoch 3, Batch 451, LR 0.572471 Loss 10.563170, Accuracy 56.998%\n",
      "Epoch 3, Batch 452, LR 0.572703 Loss 10.562964, Accuracy 57.009%\n",
      "Epoch 3, Batch 453, LR 0.572935 Loss 10.564557, Accuracy 57.000%\n",
      "Epoch 3, Batch 454, LR 0.573168 Loss 10.563530, Accuracy 57.004%\n",
      "Epoch 3, Batch 455, LR 0.573400 Loss 10.563866, Accuracy 57.004%\n",
      "Epoch 3, Batch 456, LR 0.573633 Loss 10.563988, Accuracy 56.997%\n",
      "Epoch 3, Batch 457, LR 0.573865 Loss 10.564443, Accuracy 56.992%\n",
      "Epoch 3, Batch 458, LR 0.574098 Loss 10.562443, Accuracy 57.006%\n",
      "Epoch 3, Batch 459, LR 0.574330 Loss 10.561866, Accuracy 57.007%\n",
      "Epoch 3, Batch 460, LR 0.574563 Loss 10.562102, Accuracy 57.013%\n",
      "Epoch 3, Batch 461, LR 0.574795 Loss 10.561344, Accuracy 57.016%\n",
      "Epoch 3, Batch 462, LR 0.575028 Loss 10.562471, Accuracy 57.004%\n",
      "Epoch 3, Batch 463, LR 0.575261 Loss 10.563660, Accuracy 56.991%\n",
      "Epoch 3, Batch 464, LR 0.575494 Loss 10.563412, Accuracy 56.987%\n",
      "Epoch 3, Batch 465, LR 0.575727 Loss 10.562900, Accuracy 56.998%\n",
      "Epoch 3, Batch 466, LR 0.575960 Loss 10.561678, Accuracy 57.014%\n",
      "Epoch 3, Batch 467, LR 0.576193 Loss 10.561390, Accuracy 57.015%\n",
      "Epoch 3, Batch 468, LR 0.576426 Loss 10.561599, Accuracy 57.011%\n",
      "Epoch 3, Batch 469, LR 0.576659 Loss 10.560554, Accuracy 57.021%\n",
      "Epoch 3, Batch 470, LR 0.576892 Loss 10.560881, Accuracy 57.015%\n",
      "Epoch 3, Batch 471, LR 0.577125 Loss 10.561555, Accuracy 57.010%\n",
      "Epoch 3, Batch 472, LR 0.577358 Loss 10.562209, Accuracy 57.001%\n",
      "Epoch 3, Batch 473, LR 0.577591 Loss 10.560536, Accuracy 57.020%\n",
      "Epoch 3, Batch 474, LR 0.577825 Loss 10.559065, Accuracy 57.033%\n",
      "Epoch 3, Batch 475, LR 0.578058 Loss 10.558400, Accuracy 57.053%\n",
      "Epoch 3, Batch 476, LR 0.578291 Loss 10.558238, Accuracy 57.051%\n",
      "Epoch 3, Batch 477, LR 0.578525 Loss 10.558136, Accuracy 57.062%\n",
      "Epoch 3, Batch 478, LR 0.578758 Loss 10.556027, Accuracy 57.090%\n",
      "Epoch 3, Batch 479, LR 0.578992 Loss 10.555877, Accuracy 57.092%\n",
      "Epoch 3, Batch 480, LR 0.579225 Loss 10.555592, Accuracy 57.087%\n",
      "Epoch 3, Batch 481, LR 0.579459 Loss 10.555147, Accuracy 57.083%\n",
      "Epoch 3, Batch 482, LR 0.579692 Loss 10.556574, Accuracy 57.075%\n",
      "Epoch 3, Batch 483, LR 0.579926 Loss 10.556564, Accuracy 57.081%\n",
      "Epoch 3, Batch 484, LR 0.580160 Loss 10.555514, Accuracy 57.093%\n",
      "Epoch 3, Batch 485, LR 0.580394 Loss 10.554447, Accuracy 57.105%\n",
      "Epoch 3, Batch 486, LR 0.580627 Loss 10.553115, Accuracy 57.113%\n",
      "Epoch 3, Batch 487, LR 0.580861 Loss 10.553006, Accuracy 57.113%\n",
      "Epoch 3, Batch 488, LR 0.581095 Loss 10.552380, Accuracy 57.110%\n",
      "Epoch 3, Batch 489, LR 0.581329 Loss 10.551502, Accuracy 57.110%\n",
      "Epoch 3, Batch 490, LR 0.581563 Loss 10.551369, Accuracy 57.116%\n",
      "Epoch 3, Batch 491, LR 0.581797 Loss 10.550395, Accuracy 57.128%\n",
      "Epoch 3, Batch 492, LR 0.582031 Loss 10.550051, Accuracy 57.128%\n",
      "Epoch 3, Batch 493, LR 0.582266 Loss 10.549211, Accuracy 57.142%\n",
      "Epoch 3, Batch 494, LR 0.582500 Loss 10.549961, Accuracy 57.131%\n",
      "Epoch 3, Batch 495, LR 0.582734 Loss 10.550136, Accuracy 57.128%\n",
      "Epoch 3, Batch 496, LR 0.582968 Loss 10.550047, Accuracy 57.130%\n",
      "Epoch 3, Batch 497, LR 0.583203 Loss 10.549495, Accuracy 57.133%\n",
      "Epoch 3, Batch 498, LR 0.583437 Loss 10.549091, Accuracy 57.136%\n",
      "Epoch 3, Batch 499, LR 0.583671 Loss 10.548957, Accuracy 57.131%\n",
      "Epoch 3, Batch 500, LR 0.583906 Loss 10.549052, Accuracy 57.127%\n",
      "Epoch 3, Batch 501, LR 0.584140 Loss 10.549657, Accuracy 57.112%\n",
      "Epoch 3, Batch 502, LR 0.584375 Loss 10.550195, Accuracy 57.097%\n",
      "Epoch 3, Batch 503, LR 0.584610 Loss 10.550671, Accuracy 57.101%\n",
      "Epoch 3, Batch 504, LR 0.584844 Loss 10.550479, Accuracy 57.093%\n",
      "Epoch 3, Batch 505, LR 0.585079 Loss 10.549405, Accuracy 57.098%\n",
      "Epoch 3, Batch 506, LR 0.585314 Loss 10.550173, Accuracy 57.088%\n",
      "Epoch 3, Batch 507, LR 0.585548 Loss 10.549519, Accuracy 57.093%\n",
      "Epoch 3, Batch 508, LR 0.585783 Loss 10.550128, Accuracy 57.073%\n",
      "Epoch 3, Batch 509, LR 0.586018 Loss 10.549593, Accuracy 57.083%\n",
      "Epoch 3, Batch 510, LR 0.586253 Loss 10.547388, Accuracy 57.099%\n",
      "Epoch 3, Batch 511, LR 0.586488 Loss 10.547684, Accuracy 57.097%\n",
      "Epoch 3, Batch 512, LR 0.586723 Loss 10.546562, Accuracy 57.101%\n",
      "Epoch 3, Batch 513, LR 0.586958 Loss 10.547270, Accuracy 57.095%\n",
      "Epoch 3, Batch 514, LR 0.587193 Loss 10.547007, Accuracy 57.084%\n",
      "Epoch 3, Batch 515, LR 0.587428 Loss 10.546745, Accuracy 57.084%\n",
      "Epoch 3, Batch 516, LR 0.587664 Loss 10.544899, Accuracy 57.101%\n",
      "Epoch 3, Batch 517, LR 0.587899 Loss 10.545177, Accuracy 57.105%\n",
      "Epoch 3, Batch 518, LR 0.588134 Loss 10.545111, Accuracy 57.105%\n",
      "Epoch 3, Batch 519, LR 0.588369 Loss 10.545857, Accuracy 57.102%\n",
      "Epoch 3, Batch 520, LR 0.588605 Loss 10.544564, Accuracy 57.111%\n",
      "Epoch 3, Batch 521, LR 0.588840 Loss 10.543755, Accuracy 57.106%\n",
      "Epoch 3, Batch 522, LR 0.589076 Loss 10.543648, Accuracy 57.093%\n",
      "Epoch 3, Batch 523, LR 0.589311 Loss 10.543098, Accuracy 57.090%\n",
      "Epoch 3, Batch 524, LR 0.589547 Loss 10.542258, Accuracy 57.092%\n",
      "Epoch 3, Batch 525, LR 0.589782 Loss 10.541198, Accuracy 57.107%\n",
      "Epoch 3, Batch 526, LR 0.590018 Loss 10.540981, Accuracy 57.103%\n",
      "Epoch 3, Batch 527, LR 0.590254 Loss 10.540394, Accuracy 57.096%\n",
      "Epoch 3, Batch 528, LR 0.590489 Loss 10.540741, Accuracy 57.090%\n",
      "Epoch 3, Batch 529, LR 0.590725 Loss 10.540122, Accuracy 57.083%\n",
      "Epoch 3, Batch 530, LR 0.590961 Loss 10.539727, Accuracy 57.087%\n",
      "Epoch 3, Batch 531, LR 0.591197 Loss 10.539801, Accuracy 57.087%\n",
      "Epoch 3, Batch 532, LR 0.591433 Loss 10.540645, Accuracy 57.080%\n",
      "Epoch 3, Batch 533, LR 0.591669 Loss 10.541062, Accuracy 57.080%\n",
      "Epoch 3, Batch 534, LR 0.591905 Loss 10.541102, Accuracy 57.088%\n",
      "Epoch 3, Batch 535, LR 0.592141 Loss 10.540648, Accuracy 57.087%\n",
      "Epoch 3, Batch 536, LR 0.592377 Loss 10.541191, Accuracy 57.076%\n",
      "Epoch 3, Batch 537, LR 0.592613 Loss 10.540826, Accuracy 57.078%\n",
      "Epoch 3, Batch 538, LR 0.592849 Loss 10.540012, Accuracy 57.078%\n",
      "Epoch 3, Batch 539, LR 0.593086 Loss 10.540529, Accuracy 57.078%\n",
      "Epoch 3, Batch 540, LR 0.593322 Loss 10.540477, Accuracy 57.075%\n",
      "Epoch 3, Batch 541, LR 0.593558 Loss 10.539615, Accuracy 57.077%\n",
      "Epoch 3, Batch 542, LR 0.593794 Loss 10.539381, Accuracy 57.074%\n",
      "Epoch 3, Batch 543, LR 0.594031 Loss 10.538271, Accuracy 57.082%\n",
      "Epoch 3, Batch 544, LR 0.594267 Loss 10.538183, Accuracy 57.077%\n",
      "Epoch 3, Batch 545, LR 0.594504 Loss 10.536336, Accuracy 57.101%\n",
      "Epoch 3, Batch 546, LR 0.594740 Loss 10.536941, Accuracy 57.100%\n",
      "Epoch 3, Batch 547, LR 0.594977 Loss 10.537934, Accuracy 57.106%\n",
      "Epoch 3, Batch 548, LR 0.595214 Loss 10.539022, Accuracy 57.100%\n",
      "Epoch 3, Batch 549, LR 0.595450 Loss 10.539272, Accuracy 57.108%\n",
      "Epoch 3, Batch 550, LR 0.595687 Loss 10.538166, Accuracy 57.118%\n",
      "Epoch 3, Batch 551, LR 0.595924 Loss 10.539092, Accuracy 57.108%\n",
      "Epoch 3, Batch 552, LR 0.596161 Loss 10.538509, Accuracy 57.111%\n",
      "Epoch 3, Batch 553, LR 0.596398 Loss 10.537744, Accuracy 57.119%\n",
      "Epoch 3, Batch 554, LR 0.596634 Loss 10.538187, Accuracy 57.119%\n",
      "Epoch 3, Batch 555, LR 0.596871 Loss 10.538664, Accuracy 57.110%\n",
      "Epoch 3, Batch 556, LR 0.597108 Loss 10.538308, Accuracy 57.106%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 557, LR 0.597345 Loss 10.537834, Accuracy 57.106%\n",
      "Epoch 3, Batch 558, LR 0.597582 Loss 10.537396, Accuracy 57.100%\n",
      "Epoch 3, Batch 559, LR 0.597820 Loss 10.537998, Accuracy 57.089%\n",
      "Epoch 3, Batch 560, LR 0.598057 Loss 10.537374, Accuracy 57.091%\n",
      "Epoch 3, Batch 561, LR 0.598294 Loss 10.536577, Accuracy 57.109%\n",
      "Epoch 3, Batch 562, LR 0.598531 Loss 10.536583, Accuracy 57.112%\n",
      "Epoch 3, Batch 563, LR 0.598769 Loss 10.536172, Accuracy 57.126%\n",
      "Epoch 3, Batch 564, LR 0.599006 Loss 10.535684, Accuracy 57.128%\n",
      "Epoch 3, Batch 565, LR 0.599243 Loss 10.534678, Accuracy 57.135%\n",
      "Epoch 3, Batch 566, LR 0.599481 Loss 10.533983, Accuracy 57.144%\n",
      "Epoch 3, Batch 567, LR 0.599718 Loss 10.533905, Accuracy 57.146%\n",
      "Epoch 3, Batch 568, LR 0.599956 Loss 10.534860, Accuracy 57.141%\n",
      "Epoch 3, Batch 569, LR 0.600193 Loss 10.534840, Accuracy 57.136%\n",
      "Epoch 3, Batch 570, LR 0.600431 Loss 10.534952, Accuracy 57.135%\n",
      "Epoch 3, Batch 571, LR 0.600669 Loss 10.535293, Accuracy 57.135%\n",
      "Epoch 3, Batch 572, LR 0.600906 Loss 10.534992, Accuracy 57.134%\n",
      "Epoch 3, Batch 573, LR 0.601144 Loss 10.534163, Accuracy 57.134%\n",
      "Epoch 3, Batch 574, LR 0.601382 Loss 10.532682, Accuracy 57.141%\n",
      "Epoch 3, Batch 575, LR 0.601620 Loss 10.533571, Accuracy 57.132%\n",
      "Epoch 3, Batch 576, LR 0.601858 Loss 10.533735, Accuracy 57.130%\n",
      "Epoch 3, Batch 577, LR 0.602096 Loss 10.534174, Accuracy 57.126%\n",
      "Epoch 3, Batch 578, LR 0.602334 Loss 10.534079, Accuracy 57.131%\n",
      "Epoch 3, Batch 579, LR 0.602572 Loss 10.533419, Accuracy 57.142%\n",
      "Epoch 3, Batch 580, LR 0.602810 Loss 10.532999, Accuracy 57.142%\n",
      "Epoch 3, Batch 581, LR 0.603048 Loss 10.531863, Accuracy 57.154%\n",
      "Epoch 3, Batch 582, LR 0.603286 Loss 10.530637, Accuracy 57.174%\n",
      "Epoch 3, Batch 583, LR 0.603524 Loss 10.530599, Accuracy 57.165%\n",
      "Epoch 3, Batch 584, LR 0.603762 Loss 10.530437, Accuracy 57.164%\n",
      "Epoch 3, Batch 585, LR 0.604001 Loss 10.530447, Accuracy 57.167%\n",
      "Epoch 3, Batch 586, LR 0.604239 Loss 10.529866, Accuracy 57.170%\n",
      "Epoch 3, Batch 587, LR 0.604477 Loss 10.528762, Accuracy 57.175%\n",
      "Epoch 3, Batch 588, LR 0.604716 Loss 10.528656, Accuracy 57.173%\n",
      "Epoch 3, Batch 589, LR 0.604954 Loss 10.526928, Accuracy 57.188%\n",
      "Epoch 3, Batch 590, LR 0.605193 Loss 10.525994, Accuracy 57.189%\n",
      "Epoch 3, Batch 591, LR 0.605431 Loss 10.526020, Accuracy 57.177%\n",
      "Epoch 3, Batch 592, LR 0.605670 Loss 10.525722, Accuracy 57.182%\n",
      "Epoch 3, Batch 593, LR 0.605909 Loss 10.525999, Accuracy 57.183%\n",
      "Epoch 3, Batch 594, LR 0.606147 Loss 10.525500, Accuracy 57.188%\n",
      "Epoch 3, Batch 595, LR 0.606386 Loss 10.525023, Accuracy 57.195%\n",
      "Epoch 3, Batch 596, LR 0.606625 Loss 10.523599, Accuracy 57.208%\n",
      "Epoch 3, Batch 597, LR 0.606864 Loss 10.523338, Accuracy 57.209%\n",
      "Epoch 3, Batch 598, LR 0.607102 Loss 10.523281, Accuracy 57.208%\n",
      "Epoch 3, Batch 599, LR 0.607341 Loss 10.523792, Accuracy 57.198%\n",
      "Epoch 3, Batch 600, LR 0.607580 Loss 10.523869, Accuracy 57.203%\n",
      "Epoch 3, Batch 601, LR 0.607819 Loss 10.524728, Accuracy 57.200%\n",
      "Epoch 3, Batch 602, LR 0.608058 Loss 10.525711, Accuracy 57.192%\n",
      "Epoch 3, Batch 603, LR 0.608297 Loss 10.524407, Accuracy 57.196%\n",
      "Epoch 3, Batch 604, LR 0.608537 Loss 10.524054, Accuracy 57.196%\n",
      "Epoch 3, Batch 605, LR 0.608776 Loss 10.524145, Accuracy 57.189%\n",
      "Epoch 3, Batch 606, LR 0.609015 Loss 10.523624, Accuracy 57.192%\n",
      "Epoch 3, Batch 607, LR 0.609254 Loss 10.522472, Accuracy 57.201%\n",
      "Epoch 3, Batch 608, LR 0.609494 Loss 10.523059, Accuracy 57.206%\n",
      "Epoch 3, Batch 609, LR 0.609733 Loss 10.522836, Accuracy 57.213%\n",
      "Epoch 3, Batch 610, LR 0.609972 Loss 10.521087, Accuracy 57.230%\n",
      "Epoch 3, Batch 611, LR 0.610212 Loss 10.520265, Accuracy 57.231%\n",
      "Epoch 3, Batch 612, LR 0.610451 Loss 10.521478, Accuracy 57.220%\n",
      "Epoch 3, Batch 613, LR 0.610691 Loss 10.520923, Accuracy 57.226%\n",
      "Epoch 3, Batch 614, LR 0.610930 Loss 10.519942, Accuracy 57.236%\n",
      "Epoch 3, Batch 615, LR 0.611170 Loss 10.518621, Accuracy 57.264%\n",
      "Epoch 3, Batch 616, LR 0.611410 Loss 10.518654, Accuracy 57.268%\n",
      "Epoch 3, Batch 617, LR 0.611649 Loss 10.519246, Accuracy 57.264%\n",
      "Epoch 3, Batch 618, LR 0.611889 Loss 10.518784, Accuracy 57.266%\n",
      "Epoch 3, Batch 619, LR 0.612129 Loss 10.518588, Accuracy 57.272%\n",
      "Epoch 3, Batch 620, LR 0.612369 Loss 10.517565, Accuracy 57.286%\n",
      "Epoch 3, Batch 621, LR 0.612608 Loss 10.517445, Accuracy 57.290%\n",
      "Epoch 3, Batch 622, LR 0.612848 Loss 10.517770, Accuracy 57.291%\n",
      "Epoch 3, Batch 623, LR 0.613088 Loss 10.517468, Accuracy 57.292%\n",
      "Epoch 3, Batch 624, LR 0.613328 Loss 10.517475, Accuracy 57.293%\n",
      "Epoch 3, Batch 625, LR 0.613568 Loss 10.516806, Accuracy 57.304%\n",
      "Epoch 3, Batch 626, LR 0.613808 Loss 10.517043, Accuracy 57.301%\n",
      "Epoch 3, Batch 627, LR 0.614049 Loss 10.516274, Accuracy 57.309%\n",
      "Epoch 3, Batch 628, LR 0.614289 Loss 10.515678, Accuracy 57.321%\n",
      "Epoch 3, Batch 629, LR 0.614529 Loss 10.514827, Accuracy 57.329%\n",
      "Epoch 3, Batch 630, LR 0.614769 Loss 10.513597, Accuracy 57.338%\n",
      "Epoch 3, Batch 631, LR 0.615010 Loss 10.512572, Accuracy 57.342%\n",
      "Epoch 3, Batch 632, LR 0.615250 Loss 10.511622, Accuracy 57.349%\n",
      "Epoch 3, Batch 633, LR 0.615490 Loss 10.511869, Accuracy 57.347%\n",
      "Epoch 3, Batch 634, LR 0.615731 Loss 10.512830, Accuracy 57.342%\n",
      "Epoch 3, Batch 635, LR 0.615971 Loss 10.512243, Accuracy 57.349%\n",
      "Epoch 3, Batch 636, LR 0.616212 Loss 10.511421, Accuracy 57.356%\n",
      "Epoch 3, Batch 637, LR 0.616452 Loss 10.510601, Accuracy 57.362%\n",
      "Epoch 3, Batch 638, LR 0.616693 Loss 10.510543, Accuracy 57.364%\n",
      "Epoch 3, Batch 639, LR 0.616934 Loss 10.509515, Accuracy 57.372%\n",
      "Epoch 3, Batch 640, LR 0.617174 Loss 10.508574, Accuracy 57.373%\n",
      "Epoch 3, Batch 641, LR 0.617415 Loss 10.508005, Accuracy 57.376%\n",
      "Epoch 3, Batch 642, LR 0.617656 Loss 10.508635, Accuracy 57.367%\n",
      "Epoch 3, Batch 643, LR 0.617897 Loss 10.507599, Accuracy 57.373%\n",
      "Epoch 3, Batch 644, LR 0.618138 Loss 10.506789, Accuracy 57.376%\n",
      "Epoch 3, Batch 645, LR 0.618379 Loss 10.506061, Accuracy 57.376%\n",
      "Epoch 3, Batch 646, LR 0.618620 Loss 10.506035, Accuracy 57.378%\n",
      "Epoch 3, Batch 647, LR 0.618861 Loss 10.507132, Accuracy 57.373%\n",
      "Epoch 3, Batch 648, LR 0.619102 Loss 10.506981, Accuracy 57.371%\n",
      "Epoch 3, Batch 649, LR 0.619343 Loss 10.506897, Accuracy 57.372%\n",
      "Epoch 3, Batch 650, LR 0.619584 Loss 10.506764, Accuracy 57.371%\n",
      "Epoch 3, Batch 651, LR 0.619825 Loss 10.506497, Accuracy 57.380%\n",
      "Epoch 3, Batch 652, LR 0.620066 Loss 10.506623, Accuracy 57.384%\n",
      "Epoch 3, Batch 653, LR 0.620307 Loss 10.506233, Accuracy 57.383%\n",
      "Epoch 3, Batch 654, LR 0.620549 Loss 10.505927, Accuracy 57.400%\n",
      "Epoch 3, Batch 655, LR 0.620790 Loss 10.505763, Accuracy 57.402%\n",
      "Epoch 3, Batch 656, LR 0.621032 Loss 10.505733, Accuracy 57.400%\n",
      "Epoch 3, Batch 657, LR 0.621273 Loss 10.505184, Accuracy 57.408%\n",
      "Epoch 3, Batch 658, LR 0.621514 Loss 10.504294, Accuracy 57.411%\n",
      "Epoch 3, Batch 659, LR 0.621756 Loss 10.503964, Accuracy 57.409%\n",
      "Epoch 3, Batch 660, LR 0.621998 Loss 10.502474, Accuracy 57.418%\n",
      "Epoch 3, Batch 661, LR 0.622239 Loss 10.503338, Accuracy 57.408%\n",
      "Epoch 3, Batch 662, LR 0.622481 Loss 10.503391, Accuracy 57.408%\n",
      "Epoch 3, Batch 663, LR 0.622723 Loss 10.503465, Accuracy 57.412%\n",
      "Epoch 3, Batch 664, LR 0.622964 Loss 10.502489, Accuracy 57.428%\n",
      "Epoch 3, Batch 665, LR 0.623206 Loss 10.501764, Accuracy 57.434%\n",
      "Epoch 3, Batch 666, LR 0.623448 Loss 10.501275, Accuracy 57.435%\n",
      "Epoch 3, Batch 667, LR 0.623690 Loss 10.501802, Accuracy 57.429%\n",
      "Epoch 3, Batch 668, LR 0.623932 Loss 10.501105, Accuracy 57.438%\n",
      "Epoch 3, Batch 669, LR 0.624174 Loss 10.501011, Accuracy 57.436%\n",
      "Epoch 3, Batch 670, LR 0.624416 Loss 10.500468, Accuracy 57.445%\n",
      "Epoch 3, Batch 671, LR 0.624658 Loss 10.500325, Accuracy 57.449%\n",
      "Epoch 3, Batch 672, LR 0.624900 Loss 10.498686, Accuracy 57.465%\n",
      "Epoch 3, Batch 673, LR 0.625142 Loss 10.499309, Accuracy 57.467%\n",
      "Epoch 3, Batch 674, LR 0.625384 Loss 10.499337, Accuracy 57.457%\n",
      "Epoch 3, Batch 675, LR 0.625626 Loss 10.498548, Accuracy 57.459%\n",
      "Epoch 3, Batch 676, LR 0.625869 Loss 10.497853, Accuracy 57.468%\n",
      "Epoch 3, Batch 677, LR 0.626111 Loss 10.497291, Accuracy 57.476%\n",
      "Epoch 3, Batch 678, LR 0.626353 Loss 10.496190, Accuracy 57.474%\n",
      "Epoch 3, Batch 679, LR 0.626596 Loss 10.495833, Accuracy 57.479%\n",
      "Epoch 3, Batch 680, LR 0.626838 Loss 10.495790, Accuracy 57.478%\n",
      "Epoch 3, Batch 681, LR 0.627080 Loss 10.494932, Accuracy 57.483%\n",
      "Epoch 3, Batch 682, LR 0.627323 Loss 10.494693, Accuracy 57.488%\n",
      "Epoch 3, Batch 683, LR 0.627566 Loss 10.494161, Accuracy 57.495%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 684, LR 0.627808 Loss 10.493801, Accuracy 57.497%\n",
      "Epoch 3, Batch 685, LR 0.628051 Loss 10.493550, Accuracy 57.500%\n",
      "Epoch 3, Batch 686, LR 0.628293 Loss 10.493118, Accuracy 57.505%\n",
      "Epoch 3, Batch 687, LR 0.628536 Loss 10.492593, Accuracy 57.503%\n",
      "Epoch 3, Batch 688, LR 0.628779 Loss 10.493222, Accuracy 57.497%\n",
      "Epoch 3, Batch 689, LR 0.629022 Loss 10.492754, Accuracy 57.501%\n",
      "Epoch 3, Batch 690, LR 0.629265 Loss 10.491863, Accuracy 57.509%\n",
      "Epoch 3, Batch 691, LR 0.629508 Loss 10.491442, Accuracy 57.512%\n",
      "Epoch 3, Batch 692, LR 0.629751 Loss 10.491385, Accuracy 57.507%\n",
      "Epoch 3, Batch 693, LR 0.629993 Loss 10.491002, Accuracy 57.505%\n",
      "Epoch 3, Batch 694, LR 0.630237 Loss 10.490635, Accuracy 57.505%\n",
      "Epoch 3, Batch 695, LR 0.630480 Loss 10.490106, Accuracy 57.510%\n",
      "Epoch 3, Batch 696, LR 0.630723 Loss 10.490273, Accuracy 57.511%\n",
      "Epoch 3, Batch 697, LR 0.630966 Loss 10.490612, Accuracy 57.501%\n",
      "Epoch 3, Batch 698, LR 0.631209 Loss 10.489499, Accuracy 57.509%\n",
      "Epoch 3, Batch 699, LR 0.631452 Loss 10.490072, Accuracy 57.503%\n",
      "Epoch 3, Batch 700, LR 0.631696 Loss 10.490678, Accuracy 57.500%\n",
      "Epoch 3, Batch 701, LR 0.631939 Loss 10.491340, Accuracy 57.497%\n",
      "Epoch 3, Batch 702, LR 0.632182 Loss 10.491704, Accuracy 57.498%\n",
      "Epoch 3, Batch 703, LR 0.632426 Loss 10.491305, Accuracy 57.497%\n",
      "Epoch 3, Batch 704, LR 0.632669 Loss 10.491042, Accuracy 57.500%\n",
      "Epoch 3, Batch 705, LR 0.632913 Loss 10.491133, Accuracy 57.502%\n",
      "Epoch 3, Batch 706, LR 0.633156 Loss 10.490080, Accuracy 57.515%\n",
      "Epoch 3, Batch 707, LR 0.633400 Loss 10.491110, Accuracy 57.506%\n",
      "Epoch 3, Batch 708, LR 0.633643 Loss 10.490233, Accuracy 57.517%\n",
      "Epoch 3, Batch 709, LR 0.633887 Loss 10.490717, Accuracy 57.508%\n",
      "Epoch 3, Batch 710, LR 0.634131 Loss 10.490608, Accuracy 57.513%\n",
      "Epoch 3, Batch 711, LR 0.634375 Loss 10.489723, Accuracy 57.524%\n",
      "Epoch 3, Batch 712, LR 0.634618 Loss 10.489135, Accuracy 57.531%\n",
      "Epoch 3, Batch 713, LR 0.634862 Loss 10.488803, Accuracy 57.540%\n",
      "Epoch 3, Batch 714, LR 0.635106 Loss 10.488587, Accuracy 57.541%\n",
      "Epoch 3, Batch 715, LR 0.635350 Loss 10.489155, Accuracy 57.536%\n",
      "Epoch 3, Batch 716, LR 0.635594 Loss 10.488087, Accuracy 57.548%\n",
      "Epoch 3, Batch 717, LR 0.635838 Loss 10.488038, Accuracy 57.552%\n",
      "Epoch 3, Batch 718, LR 0.636082 Loss 10.487794, Accuracy 57.551%\n",
      "Epoch 3, Batch 719, LR 0.636326 Loss 10.487770, Accuracy 57.545%\n",
      "Epoch 3, Batch 720, LR 0.636570 Loss 10.487841, Accuracy 57.546%\n",
      "Epoch 3, Batch 721, LR 0.636814 Loss 10.487810, Accuracy 57.542%\n",
      "Epoch 3, Batch 722, LR 0.637059 Loss 10.487395, Accuracy 57.545%\n",
      "Epoch 3, Batch 723, LR 0.637303 Loss 10.487891, Accuracy 57.543%\n",
      "Epoch 3, Batch 724, LR 0.637547 Loss 10.487012, Accuracy 57.545%\n",
      "Epoch 3, Batch 725, LR 0.637792 Loss 10.487233, Accuracy 57.543%\n",
      "Epoch 3, Batch 726, LR 0.638036 Loss 10.487440, Accuracy 57.539%\n",
      "Epoch 3, Batch 727, LR 0.638280 Loss 10.487517, Accuracy 57.536%\n",
      "Epoch 3, Batch 728, LR 0.638525 Loss 10.486718, Accuracy 57.549%\n",
      "Epoch 3, Batch 729, LR 0.638769 Loss 10.487216, Accuracy 57.540%\n",
      "Epoch 3, Batch 730, LR 0.639014 Loss 10.487468, Accuracy 57.535%\n",
      "Epoch 3, Batch 731, LR 0.639258 Loss 10.486465, Accuracy 57.539%\n",
      "Epoch 3, Batch 732, LR 0.639503 Loss 10.485926, Accuracy 57.542%\n",
      "Epoch 3, Batch 733, LR 0.639748 Loss 10.485363, Accuracy 57.546%\n",
      "Epoch 3, Batch 734, LR 0.639993 Loss 10.484058, Accuracy 57.552%\n",
      "Epoch 3, Batch 735, LR 0.640237 Loss 10.484572, Accuracy 57.549%\n",
      "Epoch 3, Batch 736, LR 0.640482 Loss 10.484738, Accuracy 57.555%\n",
      "Epoch 3, Batch 737, LR 0.640727 Loss 10.484422, Accuracy 57.554%\n",
      "Epoch 3, Batch 738, LR 0.640972 Loss 10.484679, Accuracy 57.555%\n",
      "Epoch 3, Batch 739, LR 0.641217 Loss 10.483999, Accuracy 57.559%\n",
      "Epoch 3, Batch 740, LR 0.641462 Loss 10.483699, Accuracy 57.562%\n",
      "Epoch 3, Batch 741, LR 0.641707 Loss 10.483605, Accuracy 57.561%\n",
      "Epoch 3, Batch 742, LR 0.641952 Loss 10.482710, Accuracy 57.563%\n",
      "Epoch 3, Batch 743, LR 0.642197 Loss 10.480969, Accuracy 57.575%\n",
      "Epoch 3, Batch 744, LR 0.642442 Loss 10.480203, Accuracy 57.572%\n",
      "Epoch 3, Batch 745, LR 0.642687 Loss 10.479300, Accuracy 57.580%\n",
      "Epoch 3, Batch 746, LR 0.642932 Loss 10.478580, Accuracy 57.588%\n",
      "Epoch 3, Batch 747, LR 0.643178 Loss 10.478911, Accuracy 57.583%\n",
      "Epoch 3, Batch 748, LR 0.643423 Loss 10.478923, Accuracy 57.584%\n",
      "Epoch 3, Batch 749, LR 0.643668 Loss 10.478178, Accuracy 57.588%\n",
      "Epoch 3, Batch 750, LR 0.643914 Loss 10.478177, Accuracy 57.591%\n",
      "Epoch 3, Batch 751, LR 0.644159 Loss 10.477146, Accuracy 57.597%\n",
      "Epoch 3, Batch 752, LR 0.644405 Loss 10.477013, Accuracy 57.593%\n",
      "Epoch 3, Batch 753, LR 0.644650 Loss 10.477041, Accuracy 57.592%\n",
      "Epoch 3, Batch 754, LR 0.644896 Loss 10.476321, Accuracy 57.592%\n",
      "Epoch 3, Batch 755, LR 0.645141 Loss 10.475378, Accuracy 57.597%\n",
      "Epoch 3, Batch 756, LR 0.645387 Loss 10.475204, Accuracy 57.598%\n",
      "Epoch 3, Batch 757, LR 0.645633 Loss 10.473996, Accuracy 57.610%\n",
      "Epoch 3, Batch 758, LR 0.645878 Loss 10.473930, Accuracy 57.613%\n",
      "Epoch 3, Batch 759, LR 0.646124 Loss 10.473570, Accuracy 57.614%\n",
      "Epoch 3, Batch 760, LR 0.646370 Loss 10.473082, Accuracy 57.618%\n",
      "Epoch 3, Batch 761, LR 0.646616 Loss 10.472892, Accuracy 57.617%\n",
      "Epoch 3, Batch 762, LR 0.646862 Loss 10.473173, Accuracy 57.617%\n",
      "Epoch 3, Batch 763, LR 0.647108 Loss 10.472527, Accuracy 57.624%\n",
      "Epoch 3, Batch 764, LR 0.647354 Loss 10.471796, Accuracy 57.632%\n",
      "Epoch 3, Batch 765, LR 0.647600 Loss 10.471129, Accuracy 57.636%\n",
      "Epoch 3, Batch 766, LR 0.647846 Loss 10.470855, Accuracy 57.644%\n",
      "Epoch 3, Batch 767, LR 0.648092 Loss 10.470897, Accuracy 57.641%\n",
      "Epoch 3, Batch 768, LR 0.648338 Loss 10.470547, Accuracy 57.648%\n",
      "Epoch 3, Batch 769, LR 0.648584 Loss 10.470836, Accuracy 57.647%\n",
      "Epoch 3, Batch 770, LR 0.648830 Loss 10.471127, Accuracy 57.644%\n",
      "Epoch 3, Batch 771, LR 0.649077 Loss 10.470806, Accuracy 57.648%\n",
      "Epoch 3, Batch 772, LR 0.649323 Loss 10.469743, Accuracy 57.657%\n",
      "Epoch 3, Batch 773, LR 0.649569 Loss 10.470257, Accuracy 57.647%\n",
      "Epoch 3, Batch 774, LR 0.649816 Loss 10.469855, Accuracy 57.647%\n",
      "Epoch 3, Batch 775, LR 0.650062 Loss 10.468743, Accuracy 57.660%\n",
      "Epoch 3, Batch 776, LR 0.650309 Loss 10.468381, Accuracy 57.660%\n",
      "Epoch 3, Batch 777, LR 0.650555 Loss 10.468334, Accuracy 57.662%\n",
      "Epoch 3, Batch 778, LR 0.650802 Loss 10.468565, Accuracy 57.666%\n",
      "Epoch 3, Batch 779, LR 0.651048 Loss 10.467310, Accuracy 57.675%\n",
      "Epoch 3, Batch 780, LR 0.651295 Loss 10.466906, Accuracy 57.675%\n",
      "Epoch 3, Batch 781, LR 0.651542 Loss 10.466326, Accuracy 57.680%\n",
      "Epoch 3, Batch 782, LR 0.651788 Loss 10.466715, Accuracy 57.681%\n",
      "Epoch 3, Batch 783, LR 0.652035 Loss 10.466677, Accuracy 57.684%\n",
      "Epoch 3, Batch 784, LR 0.652282 Loss 10.465736, Accuracy 57.693%\n",
      "Epoch 3, Batch 785, LR 0.652529 Loss 10.465487, Accuracy 57.698%\n",
      "Epoch 3, Batch 786, LR 0.652776 Loss 10.464387, Accuracy 57.704%\n",
      "Epoch 3, Batch 787, LR 0.653023 Loss 10.464112, Accuracy 57.708%\n",
      "Epoch 3, Batch 788, LR 0.653270 Loss 10.463810, Accuracy 57.718%\n",
      "Epoch 3, Batch 789, LR 0.653517 Loss 10.463766, Accuracy 57.719%\n",
      "Epoch 3, Batch 790, LR 0.653764 Loss 10.463370, Accuracy 57.724%\n",
      "Epoch 3, Batch 791, LR 0.654011 Loss 10.463654, Accuracy 57.725%\n",
      "Epoch 3, Batch 792, LR 0.654258 Loss 10.464055, Accuracy 57.717%\n",
      "Epoch 3, Batch 793, LR 0.654505 Loss 10.464584, Accuracy 57.710%\n",
      "Epoch 3, Batch 794, LR 0.654752 Loss 10.464694, Accuracy 57.709%\n",
      "Epoch 3, Batch 795, LR 0.655000 Loss 10.464129, Accuracy 57.713%\n",
      "Epoch 3, Batch 796, LR 0.655247 Loss 10.463665, Accuracy 57.712%\n",
      "Epoch 3, Batch 797, LR 0.655494 Loss 10.463320, Accuracy 57.711%\n",
      "Epoch 3, Batch 798, LR 0.655742 Loss 10.462917, Accuracy 57.715%\n",
      "Epoch 3, Batch 799, LR 0.655989 Loss 10.462923, Accuracy 57.716%\n",
      "Epoch 3, Batch 800, LR 0.656237 Loss 10.463796, Accuracy 57.706%\n",
      "Epoch 3, Batch 801, LR 0.656484 Loss 10.464004, Accuracy 57.701%\n",
      "Epoch 3, Batch 802, LR 0.656732 Loss 10.464896, Accuracy 57.693%\n",
      "Epoch 3, Batch 803, LR 0.656979 Loss 10.464139, Accuracy 57.702%\n",
      "Epoch 3, Batch 804, LR 0.657227 Loss 10.463744, Accuracy 57.709%\n",
      "Epoch 3, Batch 805, LR 0.657475 Loss 10.462962, Accuracy 57.718%\n",
      "Epoch 3, Batch 806, LR 0.657722 Loss 10.462661, Accuracy 57.723%\n",
      "Epoch 3, Batch 807, LR 0.657970 Loss 10.462582, Accuracy 57.727%\n",
      "Epoch 3, Batch 808, LR 0.658218 Loss 10.462074, Accuracy 57.733%\n",
      "Epoch 3, Batch 809, LR 0.658466 Loss 10.462538, Accuracy 57.730%\n",
      "Epoch 3, Batch 810, LR 0.658713 Loss 10.463092, Accuracy 57.726%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 811, LR 0.658961 Loss 10.462848, Accuracy 57.726%\n",
      "Epoch 3, Batch 812, LR 0.659209 Loss 10.462188, Accuracy 57.731%\n",
      "Epoch 3, Batch 813, LR 0.659457 Loss 10.461499, Accuracy 57.740%\n",
      "Epoch 3, Batch 814, LR 0.659705 Loss 10.461859, Accuracy 57.731%\n",
      "Epoch 3, Batch 815, LR 0.659953 Loss 10.461550, Accuracy 57.734%\n",
      "Epoch 3, Batch 816, LR 0.660202 Loss 10.461881, Accuracy 57.730%\n",
      "Epoch 3, Batch 817, LR 0.660450 Loss 10.460796, Accuracy 57.735%\n",
      "Epoch 3, Batch 818, LR 0.660698 Loss 10.459862, Accuracy 57.737%\n",
      "Epoch 3, Batch 819, LR 0.660946 Loss 10.459283, Accuracy 57.735%\n",
      "Epoch 3, Batch 820, LR 0.661195 Loss 10.459802, Accuracy 57.728%\n",
      "Epoch 3, Batch 821, LR 0.661443 Loss 10.459212, Accuracy 57.731%\n",
      "Epoch 3, Batch 822, LR 0.661691 Loss 10.459022, Accuracy 57.728%\n",
      "Epoch 3, Batch 823, LR 0.661940 Loss 10.459798, Accuracy 57.718%\n",
      "Epoch 3, Batch 824, LR 0.662188 Loss 10.459710, Accuracy 57.717%\n",
      "Epoch 3, Batch 825, LR 0.662437 Loss 10.460377, Accuracy 57.710%\n",
      "Epoch 3, Batch 826, LR 0.662685 Loss 10.460024, Accuracy 57.712%\n",
      "Epoch 3, Batch 827, LR 0.662934 Loss 10.460467, Accuracy 57.706%\n",
      "Epoch 3, Batch 828, LR 0.663182 Loss 10.460324, Accuracy 57.712%\n",
      "Epoch 3, Batch 829, LR 0.663431 Loss 10.461288, Accuracy 57.705%\n",
      "Epoch 3, Batch 830, LR 0.663680 Loss 10.461454, Accuracy 57.701%\n",
      "Epoch 3, Batch 831, LR 0.663928 Loss 10.461333, Accuracy 57.700%\n",
      "Epoch 3, Batch 832, LR 0.664177 Loss 10.461456, Accuracy 57.700%\n",
      "Epoch 3, Batch 833, LR 0.664426 Loss 10.461862, Accuracy 57.701%\n",
      "Epoch 3, Batch 834, LR 0.664675 Loss 10.461664, Accuracy 57.704%\n",
      "Epoch 3, Batch 835, LR 0.664924 Loss 10.461764, Accuracy 57.701%\n",
      "Epoch 3, Batch 836, LR 0.665173 Loss 10.461692, Accuracy 57.705%\n",
      "Epoch 3, Batch 837, LR 0.665422 Loss 10.461207, Accuracy 57.710%\n",
      "Epoch 3, Batch 838, LR 0.665671 Loss 10.461388, Accuracy 57.711%\n",
      "Epoch 3, Batch 839, LR 0.665920 Loss 10.461346, Accuracy 57.711%\n",
      "Epoch 3, Batch 840, LR 0.666169 Loss 10.460690, Accuracy 57.713%\n",
      "Epoch 3, Batch 841, LR 0.666418 Loss 10.460932, Accuracy 57.710%\n",
      "Epoch 3, Batch 842, LR 0.666667 Loss 10.460535, Accuracy 57.713%\n",
      "Epoch 3, Batch 843, LR 0.666916 Loss 10.459222, Accuracy 57.727%\n",
      "Epoch 3, Batch 844, LR 0.667166 Loss 10.459412, Accuracy 57.727%\n",
      "Epoch 3, Batch 845, LR 0.667415 Loss 10.458690, Accuracy 57.732%\n",
      "Epoch 3, Batch 846, LR 0.667664 Loss 10.458295, Accuracy 57.734%\n",
      "Epoch 3, Batch 847, LR 0.667914 Loss 10.458140, Accuracy 57.738%\n",
      "Epoch 3, Batch 848, LR 0.668163 Loss 10.457343, Accuracy 57.747%\n",
      "Epoch 3, Batch 849, LR 0.668412 Loss 10.456732, Accuracy 57.752%\n",
      "Epoch 3, Batch 850, LR 0.668662 Loss 10.457418, Accuracy 57.749%\n",
      "Epoch 3, Batch 851, LR 0.668911 Loss 10.456827, Accuracy 57.755%\n",
      "Epoch 3, Batch 852, LR 0.669161 Loss 10.457037, Accuracy 57.755%\n",
      "Epoch 3, Batch 853, LR 0.669411 Loss 10.456382, Accuracy 57.765%\n",
      "Epoch 3, Batch 854, LR 0.669660 Loss 10.454972, Accuracy 57.777%\n",
      "Epoch 3, Batch 855, LR 0.669910 Loss 10.454314, Accuracy 57.785%\n",
      "Epoch 3, Batch 856, LR 0.670160 Loss 10.453551, Accuracy 57.794%\n",
      "Epoch 3, Batch 857, LR 0.670410 Loss 10.453008, Accuracy 57.796%\n",
      "Epoch 3, Batch 858, LR 0.670659 Loss 10.452349, Accuracy 57.802%\n",
      "Epoch 3, Batch 859, LR 0.670909 Loss 10.452336, Accuracy 57.802%\n",
      "Epoch 3, Batch 860, LR 0.671159 Loss 10.451319, Accuracy 57.804%\n",
      "Epoch 3, Batch 861, LR 0.671409 Loss 10.451070, Accuracy 57.802%\n",
      "Epoch 3, Batch 862, LR 0.671659 Loss 10.451229, Accuracy 57.801%\n",
      "Epoch 3, Batch 863, LR 0.671909 Loss 10.451608, Accuracy 57.793%\n",
      "Epoch 3, Batch 864, LR 0.672159 Loss 10.451388, Accuracy 57.799%\n",
      "Epoch 3, Batch 865, LR 0.672409 Loss 10.450672, Accuracy 57.807%\n",
      "Epoch 3, Batch 866, LR 0.672659 Loss 10.449920, Accuracy 57.816%\n",
      "Epoch 3, Batch 867, LR 0.672910 Loss 10.450702, Accuracy 57.809%\n",
      "Epoch 3, Batch 868, LR 0.673160 Loss 10.450905, Accuracy 57.807%\n",
      "Epoch 3, Batch 869, LR 0.673410 Loss 10.450973, Accuracy 57.809%\n",
      "Epoch 3, Batch 870, LR 0.673660 Loss 10.452150, Accuracy 57.795%\n",
      "Epoch 3, Batch 871, LR 0.673911 Loss 10.451011, Accuracy 57.802%\n",
      "Epoch 3, Batch 872, LR 0.674161 Loss 10.450916, Accuracy 57.804%\n",
      "Epoch 3, Batch 873, LR 0.674411 Loss 10.450729, Accuracy 57.809%\n",
      "Epoch 3, Batch 874, LR 0.674662 Loss 10.451082, Accuracy 57.804%\n",
      "Epoch 3, Batch 875, LR 0.674912 Loss 10.450449, Accuracy 57.807%\n",
      "Epoch 3, Batch 876, LR 0.675163 Loss 10.449707, Accuracy 57.813%\n",
      "Epoch 3, Batch 877, LR 0.675414 Loss 10.449052, Accuracy 57.822%\n",
      "Epoch 3, Batch 878, LR 0.675664 Loss 10.449452, Accuracy 57.817%\n",
      "Epoch 3, Batch 879, LR 0.675915 Loss 10.449792, Accuracy 57.817%\n",
      "Epoch 3, Batch 880, LR 0.676166 Loss 10.449102, Accuracy 57.821%\n",
      "Epoch 3, Batch 881, LR 0.676416 Loss 10.448273, Accuracy 57.829%\n",
      "Epoch 3, Batch 882, LR 0.676667 Loss 10.448390, Accuracy 57.832%\n",
      "Epoch 3, Batch 883, LR 0.676918 Loss 10.448232, Accuracy 57.836%\n",
      "Epoch 3, Batch 884, LR 0.677169 Loss 10.447668, Accuracy 57.845%\n",
      "Epoch 3, Batch 885, LR 0.677420 Loss 10.447226, Accuracy 57.850%\n",
      "Epoch 3, Batch 886, LR 0.677671 Loss 10.447745, Accuracy 57.842%\n",
      "Epoch 3, Batch 887, LR 0.677922 Loss 10.446929, Accuracy 57.845%\n",
      "Epoch 3, Batch 888, LR 0.678173 Loss 10.446898, Accuracy 57.845%\n",
      "Epoch 3, Batch 889, LR 0.678424 Loss 10.446243, Accuracy 57.848%\n",
      "Epoch 3, Batch 890, LR 0.678675 Loss 10.445873, Accuracy 57.852%\n",
      "Epoch 3, Batch 891, LR 0.678926 Loss 10.445111, Accuracy 57.860%\n",
      "Epoch 3, Batch 892, LR 0.679177 Loss 10.445513, Accuracy 57.857%\n",
      "Epoch 3, Batch 893, LR 0.679428 Loss 10.444521, Accuracy 57.868%\n",
      "Epoch 3, Batch 894, LR 0.679680 Loss 10.444508, Accuracy 57.868%\n",
      "Epoch 3, Batch 895, LR 0.679931 Loss 10.443997, Accuracy 57.874%\n",
      "Epoch 3, Batch 896, LR 0.680182 Loss 10.442862, Accuracy 57.880%\n",
      "Epoch 3, Batch 897, LR 0.680434 Loss 10.442154, Accuracy 57.890%\n",
      "Epoch 3, Batch 898, LR 0.680685 Loss 10.442276, Accuracy 57.887%\n",
      "Epoch 3, Batch 899, LR 0.680936 Loss 10.442240, Accuracy 57.888%\n",
      "Epoch 3, Batch 900, LR 0.681188 Loss 10.442183, Accuracy 57.895%\n",
      "Epoch 3, Batch 901, LR 0.681439 Loss 10.441859, Accuracy 57.901%\n",
      "Epoch 3, Batch 902, LR 0.681691 Loss 10.442366, Accuracy 57.898%\n",
      "Epoch 3, Batch 903, LR 0.681943 Loss 10.441585, Accuracy 57.909%\n",
      "Epoch 3, Batch 904, LR 0.682194 Loss 10.441101, Accuracy 57.917%\n",
      "Epoch 3, Batch 905, LR 0.682446 Loss 10.440748, Accuracy 57.926%\n",
      "Epoch 3, Batch 906, LR 0.682698 Loss 10.440579, Accuracy 57.925%\n",
      "Epoch 3, Batch 907, LR 0.682949 Loss 10.440648, Accuracy 57.927%\n",
      "Epoch 3, Batch 908, LR 0.683201 Loss 10.439585, Accuracy 57.940%\n",
      "Epoch 3, Batch 909, LR 0.683453 Loss 10.439677, Accuracy 57.938%\n",
      "Epoch 3, Batch 910, LR 0.683705 Loss 10.439197, Accuracy 57.940%\n",
      "Epoch 3, Batch 911, LR 0.683957 Loss 10.438412, Accuracy 57.950%\n",
      "Epoch 3, Batch 912, LR 0.684209 Loss 10.439035, Accuracy 57.945%\n",
      "Epoch 3, Batch 913, LR 0.684461 Loss 10.438730, Accuracy 57.946%\n",
      "Epoch 3, Batch 914, LR 0.684713 Loss 10.439135, Accuracy 57.943%\n",
      "Epoch 3, Batch 915, LR 0.684965 Loss 10.439600, Accuracy 57.937%\n",
      "Epoch 3, Batch 916, LR 0.685217 Loss 10.439499, Accuracy 57.942%\n",
      "Epoch 3, Batch 917, LR 0.685469 Loss 10.439563, Accuracy 57.942%\n",
      "Epoch 3, Batch 918, LR 0.685722 Loss 10.438377, Accuracy 57.952%\n",
      "Epoch 3, Batch 919, LR 0.685974 Loss 10.438359, Accuracy 57.951%\n",
      "Epoch 3, Batch 920, LR 0.686226 Loss 10.437492, Accuracy 57.959%\n",
      "Epoch 3, Batch 921, LR 0.686478 Loss 10.436965, Accuracy 57.963%\n",
      "Epoch 3, Batch 922, LR 0.686731 Loss 10.436264, Accuracy 57.972%\n",
      "Epoch 3, Batch 923, LR 0.686983 Loss 10.435931, Accuracy 57.975%\n",
      "Epoch 3, Batch 924, LR 0.687236 Loss 10.435842, Accuracy 57.979%\n",
      "Epoch 3, Batch 925, LR 0.687488 Loss 10.435502, Accuracy 57.985%\n",
      "Epoch 3, Batch 926, LR 0.687741 Loss 10.435207, Accuracy 57.985%\n",
      "Epoch 3, Batch 927, LR 0.687993 Loss 10.434670, Accuracy 57.989%\n",
      "Epoch 3, Batch 928, LR 0.688246 Loss 10.434981, Accuracy 57.988%\n",
      "Epoch 3, Batch 929, LR 0.688498 Loss 10.434784, Accuracy 57.990%\n",
      "Epoch 3, Batch 930, LR 0.688751 Loss 10.435001, Accuracy 57.991%\n",
      "Epoch 3, Batch 931, LR 0.689004 Loss 10.435376, Accuracy 57.990%\n",
      "Epoch 3, Batch 932, LR 0.689256 Loss 10.435430, Accuracy 57.990%\n",
      "Epoch 3, Batch 933, LR 0.689509 Loss 10.435340, Accuracy 57.992%\n",
      "Epoch 3, Batch 934, LR 0.689762 Loss 10.436324, Accuracy 57.986%\n",
      "Epoch 3, Batch 935, LR 0.690015 Loss 10.435934, Accuracy 57.994%\n",
      "Epoch 3, Batch 936, LR 0.690268 Loss 10.436239, Accuracy 57.988%\n",
      "Epoch 3, Batch 937, LR 0.690521 Loss 10.434954, Accuracy 57.997%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 938, LR 0.690774 Loss 10.434656, Accuracy 58.002%\n",
      "Epoch 3, Batch 939, LR 0.691027 Loss 10.434820, Accuracy 58.000%\n",
      "Epoch 3, Batch 940, LR 0.691280 Loss 10.435152, Accuracy 57.997%\n",
      "Epoch 3, Batch 941, LR 0.691533 Loss 10.434211, Accuracy 58.007%\n",
      "Epoch 3, Batch 942, LR 0.691786 Loss 10.434086, Accuracy 58.006%\n",
      "Epoch 3, Batch 943, LR 0.692039 Loss 10.433062, Accuracy 58.015%\n",
      "Epoch 3, Batch 944, LR 0.692293 Loss 10.431987, Accuracy 58.019%\n",
      "Epoch 3, Batch 945, LR 0.692546 Loss 10.432088, Accuracy 58.016%\n",
      "Epoch 3, Batch 946, LR 0.692799 Loss 10.431544, Accuracy 58.019%\n",
      "Epoch 3, Batch 947, LR 0.693052 Loss 10.431753, Accuracy 58.020%\n",
      "Epoch 3, Batch 948, LR 0.693306 Loss 10.431868, Accuracy 58.022%\n",
      "Epoch 3, Batch 949, LR 0.693559 Loss 10.431500, Accuracy 58.028%\n",
      "Epoch 3, Batch 950, LR 0.693813 Loss 10.431198, Accuracy 58.028%\n",
      "Epoch 3, Batch 951, LR 0.694066 Loss 10.431052, Accuracy 58.028%\n",
      "Epoch 3, Batch 952, LR 0.694320 Loss 10.430664, Accuracy 58.029%\n",
      "Epoch 3, Batch 953, LR 0.694573 Loss 10.430936, Accuracy 58.026%\n",
      "Epoch 3, Batch 954, LR 0.694827 Loss 10.430409, Accuracy 58.027%\n",
      "Epoch 3, Batch 955, LR 0.695080 Loss 10.430185, Accuracy 58.028%\n",
      "Epoch 3, Batch 956, LR 0.695334 Loss 10.430495, Accuracy 58.028%\n",
      "Epoch 3, Batch 957, LR 0.695588 Loss 10.430328, Accuracy 58.030%\n",
      "Epoch 3, Batch 958, LR 0.695842 Loss 10.430320, Accuracy 58.034%\n",
      "Epoch 3, Batch 959, LR 0.696095 Loss 10.430397, Accuracy 58.032%\n",
      "Epoch 3, Batch 960, LR 0.696349 Loss 10.430772, Accuracy 58.030%\n",
      "Epoch 3, Batch 961, LR 0.696603 Loss 10.430527, Accuracy 58.034%\n",
      "Epoch 3, Batch 962, LR 0.696857 Loss 10.430934, Accuracy 58.033%\n",
      "Epoch 3, Batch 963, LR 0.697111 Loss 10.431121, Accuracy 58.030%\n",
      "Epoch 3, Batch 964, LR 0.697365 Loss 10.430962, Accuracy 58.039%\n",
      "Epoch 3, Batch 965, LR 0.697619 Loss 10.430632, Accuracy 58.041%\n",
      "Epoch 3, Batch 966, LR 0.697873 Loss 10.429961, Accuracy 58.045%\n",
      "Epoch 3, Batch 967, LR 0.698127 Loss 10.430040, Accuracy 58.048%\n",
      "Epoch 3, Batch 968, LR 0.698381 Loss 10.429660, Accuracy 58.050%\n",
      "Epoch 3, Batch 969, LR 0.698635 Loss 10.428504, Accuracy 58.062%\n",
      "Epoch 3, Batch 970, LR 0.698890 Loss 10.428101, Accuracy 58.065%\n",
      "Epoch 3, Batch 971, LR 0.699144 Loss 10.427941, Accuracy 58.071%\n",
      "Epoch 3, Batch 972, LR 0.699398 Loss 10.427820, Accuracy 58.075%\n",
      "Epoch 3, Batch 973, LR 0.699653 Loss 10.427702, Accuracy 58.079%\n",
      "Epoch 3, Batch 974, LR 0.699907 Loss 10.427562, Accuracy 58.081%\n",
      "Epoch 3, Batch 975, LR 0.700161 Loss 10.427154, Accuracy 58.087%\n",
      "Epoch 3, Batch 976, LR 0.700416 Loss 10.426250, Accuracy 58.094%\n",
      "Epoch 3, Batch 977, LR 0.700670 Loss 10.425650, Accuracy 58.099%\n",
      "Epoch 3, Batch 978, LR 0.700925 Loss 10.425422, Accuracy 58.102%\n",
      "Epoch 3, Batch 979, LR 0.701179 Loss 10.425550, Accuracy 58.098%\n",
      "Epoch 3, Batch 980, LR 0.701434 Loss 10.425507, Accuracy 58.101%\n",
      "Epoch 3, Batch 981, LR 0.701689 Loss 10.424984, Accuracy 58.107%\n",
      "Epoch 3, Batch 982, LR 0.701943 Loss 10.424925, Accuracy 58.112%\n",
      "Epoch 3, Batch 983, LR 0.702198 Loss 10.423904, Accuracy 58.120%\n",
      "Epoch 3, Batch 984, LR 0.702453 Loss 10.423104, Accuracy 58.126%\n",
      "Epoch 3, Batch 985, LR 0.702708 Loss 10.422959, Accuracy 58.131%\n",
      "Epoch 3, Batch 986, LR 0.702962 Loss 10.422730, Accuracy 58.135%\n",
      "Epoch 3, Batch 987, LR 0.703217 Loss 10.422717, Accuracy 58.138%\n",
      "Epoch 3, Batch 988, LR 0.703472 Loss 10.422555, Accuracy 58.141%\n",
      "Epoch 3, Batch 989, LR 0.703727 Loss 10.423137, Accuracy 58.137%\n",
      "Epoch 3, Batch 990, LR 0.703982 Loss 10.422725, Accuracy 58.138%\n",
      "Epoch 3, Batch 991, LR 0.704237 Loss 10.421963, Accuracy 58.148%\n",
      "Epoch 3, Batch 992, LR 0.704492 Loss 10.422200, Accuracy 58.148%\n",
      "Epoch 3, Batch 993, LR 0.704747 Loss 10.422328, Accuracy 58.148%\n",
      "Epoch 3, Batch 994, LR 0.705002 Loss 10.422198, Accuracy 58.150%\n",
      "Epoch 3, Batch 995, LR 0.705258 Loss 10.422355, Accuracy 58.144%\n",
      "Epoch 3, Batch 996, LR 0.705513 Loss 10.422180, Accuracy 58.147%\n",
      "Epoch 3, Batch 997, LR 0.705768 Loss 10.422513, Accuracy 58.146%\n",
      "Epoch 3, Batch 998, LR 0.706023 Loss 10.422525, Accuracy 58.145%\n",
      "Epoch 3, Batch 999, LR 0.706279 Loss 10.422868, Accuracy 58.143%\n",
      "Epoch 3, Batch 1000, LR 0.706534 Loss 10.423030, Accuracy 58.139%\n",
      "Epoch 3, Batch 1001, LR 0.706789 Loss 10.422894, Accuracy 58.138%\n",
      "Epoch 3, Batch 1002, LR 0.707045 Loss 10.422287, Accuracy 58.142%\n",
      "Epoch 3, Batch 1003, LR 0.707300 Loss 10.422197, Accuracy 58.140%\n",
      "Epoch 3, Batch 1004, LR 0.707556 Loss 10.421852, Accuracy 58.142%\n",
      "Epoch 3, Batch 1005, LR 0.707811 Loss 10.421265, Accuracy 58.145%\n",
      "Epoch 3, Batch 1006, LR 0.708067 Loss 10.421938, Accuracy 58.139%\n",
      "Epoch 3, Batch 1007, LR 0.708323 Loss 10.422000, Accuracy 58.138%\n",
      "Epoch 3, Batch 1008, LR 0.708578 Loss 10.421779, Accuracy 58.139%\n",
      "Epoch 3, Batch 1009, LR 0.708834 Loss 10.421816, Accuracy 58.136%\n",
      "Epoch 3, Batch 1010, LR 0.709090 Loss 10.421775, Accuracy 58.137%\n",
      "Epoch 3, Batch 1011, LR 0.709346 Loss 10.421394, Accuracy 58.142%\n",
      "Epoch 3, Batch 1012, LR 0.709601 Loss 10.421289, Accuracy 58.145%\n",
      "Epoch 3, Batch 1013, LR 0.709857 Loss 10.420854, Accuracy 58.149%\n",
      "Epoch 3, Batch 1014, LR 0.710113 Loss 10.420534, Accuracy 58.152%\n",
      "Epoch 3, Batch 1015, LR 0.710369 Loss 10.419537, Accuracy 58.160%\n",
      "Epoch 3, Batch 1016, LR 0.710625 Loss 10.419607, Accuracy 58.161%\n",
      "Epoch 3, Batch 1017, LR 0.710881 Loss 10.419448, Accuracy 58.164%\n",
      "Epoch 3, Batch 1018, LR 0.711137 Loss 10.419534, Accuracy 58.159%\n",
      "Epoch 3, Batch 1019, LR 0.711393 Loss 10.419381, Accuracy 58.164%\n",
      "Epoch 3, Batch 1020, LR 0.711649 Loss 10.419364, Accuracy 58.166%\n",
      "Epoch 3, Batch 1021, LR 0.711905 Loss 10.419220, Accuracy 58.169%\n",
      "Epoch 3, Batch 1022, LR 0.712162 Loss 10.419218, Accuracy 58.167%\n",
      "Epoch 3, Batch 1023, LR 0.712418 Loss 10.418715, Accuracy 58.169%\n",
      "Epoch 3, Batch 1024, LR 0.712674 Loss 10.418171, Accuracy 58.172%\n",
      "Epoch 3, Batch 1025, LR 0.712930 Loss 10.418391, Accuracy 58.175%\n",
      "Epoch 3, Batch 1026, LR 0.713187 Loss 10.417858, Accuracy 58.183%\n",
      "Epoch 3, Batch 1027, LR 0.713443 Loss 10.417658, Accuracy 58.183%\n",
      "Epoch 3, Batch 1028, LR 0.713700 Loss 10.417586, Accuracy 58.189%\n",
      "Epoch 3, Batch 1029, LR 0.713956 Loss 10.417701, Accuracy 58.187%\n",
      "Epoch 3, Batch 1030, LR 0.714212 Loss 10.417957, Accuracy 58.182%\n",
      "Epoch 3, Batch 1031, LR 0.714469 Loss 10.418130, Accuracy 58.182%\n",
      "Epoch 3, Batch 1032, LR 0.714726 Loss 10.418057, Accuracy 58.185%\n",
      "Epoch 3, Batch 1033, LR 0.714982 Loss 10.417549, Accuracy 58.191%\n",
      "Epoch 3, Batch 1034, LR 0.715239 Loss 10.417035, Accuracy 58.198%\n",
      "Epoch 3, Batch 1035, LR 0.715495 Loss 10.416278, Accuracy 58.204%\n",
      "Epoch 3, Batch 1036, LR 0.715752 Loss 10.416333, Accuracy 58.202%\n",
      "Epoch 3, Batch 1037, LR 0.716009 Loss 10.416507, Accuracy 58.204%\n",
      "Epoch 3, Batch 1038, LR 0.716266 Loss 10.415200, Accuracy 58.215%\n",
      "Epoch 3, Batch 1039, LR 0.716523 Loss 10.415444, Accuracy 58.213%\n",
      "Epoch 3, Batch 1040, LR 0.716779 Loss 10.414307, Accuracy 58.223%\n",
      "Epoch 3, Batch 1041, LR 0.717036 Loss 10.413508, Accuracy 58.227%\n",
      "Epoch 3, Batch 1042, LR 0.717293 Loss 10.413712, Accuracy 58.227%\n",
      "Epoch 3, Batch 1043, LR 0.717550 Loss 10.413630, Accuracy 58.227%\n",
      "Epoch 3, Batch 1044, LR 0.717807 Loss 10.413755, Accuracy 58.223%\n",
      "Epoch 3, Batch 1045, LR 0.718064 Loss 10.413607, Accuracy 58.221%\n",
      "Epoch 3, Batch 1046, LR 0.718321 Loss 10.413127, Accuracy 58.231%\n",
      "Epoch 3, Batch 1047, LR 0.718579 Loss 10.412522, Accuracy 58.237%\n",
      "Epoch 3, Loss (train set) 10.412522, Accuracy (train set) 58.237%\n",
      "Epoch 4, Batch 1, LR 0.718836 Loss 10.179808, Accuracy 57.812%\n",
      "Epoch 4, Batch 2, LR 0.719093 Loss 9.834031, Accuracy 60.156%\n",
      "Epoch 4, Batch 3, LR 0.719350 Loss 10.042742, Accuracy 60.938%\n",
      "Epoch 4, Batch 4, LR 0.719607 Loss 9.905193, Accuracy 62.109%\n",
      "Epoch 4, Batch 5, LR 0.719865 Loss 9.783321, Accuracy 62.500%\n",
      "Epoch 4, Batch 6, LR 0.720122 Loss 9.853519, Accuracy 62.370%\n",
      "Epoch 4, Batch 7, LR 0.720379 Loss 9.915401, Accuracy 61.272%\n",
      "Epoch 4, Batch 8, LR 0.720637 Loss 9.875508, Accuracy 61.719%\n",
      "Epoch 4, Batch 9, LR 0.720894 Loss 9.952402, Accuracy 61.545%\n",
      "Epoch 4, Batch 10, LR 0.721152 Loss 9.894450, Accuracy 61.641%\n",
      "Epoch 4, Batch 11, LR 0.721409 Loss 9.889362, Accuracy 61.719%\n",
      "Epoch 4, Batch 12, LR 0.721667 Loss 9.881888, Accuracy 61.914%\n",
      "Epoch 4, Batch 13, LR 0.721925 Loss 9.906608, Accuracy 61.478%\n",
      "Epoch 4, Batch 14, LR 0.722182 Loss 9.933516, Accuracy 61.272%\n",
      "Epoch 4, Batch 15, LR 0.722440 Loss 9.924517, Accuracy 61.667%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 16, LR 0.722698 Loss 9.976015, Accuracy 61.279%\n",
      "Epoch 4, Batch 17, LR 0.722955 Loss 9.954127, Accuracy 61.535%\n",
      "Epoch 4, Batch 18, LR 0.723213 Loss 9.931485, Accuracy 61.502%\n",
      "Epoch 4, Batch 19, LR 0.723471 Loss 9.925383, Accuracy 61.637%\n",
      "Epoch 4, Batch 20, LR 0.723729 Loss 9.948881, Accuracy 61.289%\n",
      "Epoch 4, Batch 21, LR 0.723987 Loss 9.949313, Accuracy 61.272%\n",
      "Epoch 4, Batch 22, LR 0.724245 Loss 9.976487, Accuracy 60.973%\n",
      "Epoch 4, Batch 23, LR 0.724503 Loss 9.966545, Accuracy 61.107%\n",
      "Epoch 4, Batch 24, LR 0.724761 Loss 9.969143, Accuracy 61.230%\n",
      "Epoch 4, Batch 25, LR 0.725019 Loss 9.940147, Accuracy 61.531%\n",
      "Epoch 4, Batch 26, LR 0.725277 Loss 9.944321, Accuracy 61.478%\n",
      "Epoch 4, Batch 27, LR 0.725535 Loss 9.945803, Accuracy 61.516%\n",
      "Epoch 4, Batch 28, LR 0.725793 Loss 9.938521, Accuracy 61.663%\n",
      "Epoch 4, Batch 29, LR 0.726051 Loss 9.925814, Accuracy 61.719%\n",
      "Epoch 4, Batch 30, LR 0.726310 Loss 9.924379, Accuracy 61.771%\n",
      "Epoch 4, Batch 31, LR 0.726568 Loss 9.934537, Accuracy 61.643%\n",
      "Epoch 4, Batch 32, LR 0.726826 Loss 9.949585, Accuracy 61.670%\n",
      "Epoch 4, Batch 33, LR 0.727084 Loss 9.957514, Accuracy 61.695%\n",
      "Epoch 4, Batch 34, LR 0.727343 Loss 9.954625, Accuracy 61.811%\n",
      "Epoch 4, Batch 35, LR 0.727601 Loss 9.949162, Accuracy 61.763%\n",
      "Epoch 4, Batch 36, LR 0.727860 Loss 9.951632, Accuracy 61.871%\n",
      "Epoch 4, Batch 37, LR 0.728118 Loss 9.954839, Accuracy 61.845%\n",
      "Epoch 4, Batch 38, LR 0.728377 Loss 9.959621, Accuracy 61.822%\n",
      "Epoch 4, Batch 39, LR 0.728635 Loss 9.949686, Accuracy 61.919%\n",
      "Epoch 4, Batch 40, LR 0.728894 Loss 9.946787, Accuracy 61.973%\n",
      "Epoch 4, Batch 41, LR 0.729153 Loss 9.931208, Accuracy 62.157%\n",
      "Epoch 4, Batch 42, LR 0.729411 Loss 9.934897, Accuracy 62.072%\n",
      "Epoch 4, Batch 43, LR 0.729670 Loss 9.925233, Accuracy 62.082%\n",
      "Epoch 4, Batch 44, LR 0.729929 Loss 9.916283, Accuracy 62.234%\n",
      "Epoch 4, Batch 45, LR 0.730187 Loss 9.915934, Accuracy 62.222%\n",
      "Epoch 4, Batch 46, LR 0.730446 Loss 9.921286, Accuracy 62.143%\n",
      "Epoch 4, Batch 47, LR 0.730705 Loss 9.932479, Accuracy 62.118%\n",
      "Epoch 4, Batch 48, LR 0.730964 Loss 9.923394, Accuracy 62.240%\n",
      "Epoch 4, Batch 49, LR 0.731223 Loss 9.913857, Accuracy 62.277%\n",
      "Epoch 4, Batch 50, LR 0.731482 Loss 9.913745, Accuracy 62.219%\n",
      "Epoch 4, Batch 51, LR 0.731741 Loss 9.912596, Accuracy 62.270%\n",
      "Epoch 4, Batch 52, LR 0.732000 Loss 9.905292, Accuracy 62.395%\n",
      "Epoch 4, Batch 53, LR 0.732259 Loss 9.896091, Accuracy 62.456%\n",
      "Epoch 4, Batch 54, LR 0.732518 Loss 9.888332, Accuracy 62.486%\n",
      "Epoch 4, Batch 55, LR 0.732777 Loss 9.882468, Accuracy 62.585%\n",
      "Epoch 4, Batch 56, LR 0.733036 Loss 9.891930, Accuracy 62.486%\n",
      "Epoch 4, Batch 57, LR 0.733296 Loss 9.894738, Accuracy 62.473%\n",
      "Epoch 4, Batch 58, LR 0.733555 Loss 9.895735, Accuracy 62.446%\n",
      "Epoch 4, Batch 59, LR 0.733814 Loss 9.889118, Accuracy 62.487%\n",
      "Epoch 4, Batch 60, LR 0.734074 Loss 9.882217, Accuracy 62.578%\n",
      "Epoch 4, Batch 61, LR 0.734333 Loss 9.888831, Accuracy 62.487%\n",
      "Epoch 4, Batch 62, LR 0.734592 Loss 9.894772, Accuracy 62.487%\n",
      "Epoch 4, Batch 63, LR 0.734852 Loss 9.898051, Accuracy 62.351%\n",
      "Epoch 4, Batch 64, LR 0.735111 Loss 9.897224, Accuracy 62.390%\n",
      "Epoch 4, Batch 65, LR 0.735371 Loss 9.900461, Accuracy 62.368%\n",
      "Epoch 4, Batch 66, LR 0.735630 Loss 9.902893, Accuracy 62.334%\n",
      "Epoch 4, Batch 67, LR 0.735890 Loss 9.900365, Accuracy 62.372%\n",
      "Epoch 4, Batch 68, LR 0.736149 Loss 9.904595, Accuracy 62.293%\n",
      "Epoch 4, Batch 69, LR 0.736409 Loss 9.905561, Accuracy 62.240%\n",
      "Epoch 4, Batch 70, LR 0.736669 Loss 9.902561, Accuracy 62.355%\n",
      "Epoch 4, Batch 71, LR 0.736929 Loss 9.903093, Accuracy 62.357%\n",
      "Epoch 4, Batch 72, LR 0.737188 Loss 9.898855, Accuracy 62.391%\n",
      "Epoch 4, Batch 73, LR 0.737448 Loss 9.906226, Accuracy 62.361%\n",
      "Epoch 4, Batch 74, LR 0.737708 Loss 9.905682, Accuracy 62.373%\n",
      "Epoch 4, Batch 75, LR 0.737968 Loss 9.903903, Accuracy 62.375%\n",
      "Epoch 4, Batch 76, LR 0.738228 Loss 9.903467, Accuracy 62.449%\n",
      "Epoch 4, Batch 77, LR 0.738488 Loss 9.909808, Accuracy 62.419%\n",
      "Epoch 4, Batch 78, LR 0.738748 Loss 9.904755, Accuracy 62.480%\n",
      "Epoch 4, Batch 79, LR 0.739008 Loss 9.908735, Accuracy 62.381%\n",
      "Epoch 4, Batch 80, LR 0.739268 Loss 9.919850, Accuracy 62.275%\n",
      "Epoch 4, Batch 81, LR 0.739528 Loss 9.919168, Accuracy 62.211%\n",
      "Epoch 4, Batch 82, LR 0.739788 Loss 9.926106, Accuracy 62.147%\n",
      "Epoch 4, Batch 83, LR 0.740048 Loss 9.924540, Accuracy 62.208%\n",
      "Epoch 4, Batch 84, LR 0.740308 Loss 9.922182, Accuracy 62.305%\n",
      "Epoch 4, Batch 85, LR 0.740568 Loss 9.920033, Accuracy 62.325%\n",
      "Epoch 4, Batch 86, LR 0.740829 Loss 9.925975, Accuracy 62.264%\n",
      "Epoch 4, Batch 87, LR 0.741089 Loss 9.923356, Accuracy 62.267%\n",
      "Epoch 4, Batch 88, LR 0.741349 Loss 9.926155, Accuracy 62.216%\n",
      "Epoch 4, Batch 89, LR 0.741610 Loss 9.927714, Accuracy 62.254%\n",
      "Epoch 4, Batch 90, LR 0.741870 Loss 9.932500, Accuracy 62.196%\n",
      "Epoch 4, Batch 91, LR 0.742131 Loss 9.936571, Accuracy 62.114%\n",
      "Epoch 4, Batch 92, LR 0.742391 Loss 9.938325, Accuracy 62.126%\n",
      "Epoch 4, Batch 93, LR 0.742652 Loss 9.931954, Accuracy 62.147%\n",
      "Epoch 4, Batch 94, LR 0.742912 Loss 9.931890, Accuracy 62.192%\n",
      "Epoch 4, Batch 95, LR 0.743173 Loss 9.929633, Accuracy 62.229%\n",
      "Epoch 4, Batch 96, LR 0.743433 Loss 9.929123, Accuracy 62.272%\n",
      "Epoch 4, Batch 97, LR 0.743694 Loss 9.918946, Accuracy 62.379%\n",
      "Epoch 4, Batch 98, LR 0.743955 Loss 9.914290, Accuracy 62.420%\n",
      "Epoch 4, Batch 99, LR 0.744215 Loss 9.911538, Accuracy 62.461%\n",
      "Epoch 4, Batch 100, LR 0.744476 Loss 9.910520, Accuracy 62.469%\n",
      "Epoch 4, Batch 101, LR 0.744737 Loss 9.910084, Accuracy 62.515%\n",
      "Epoch 4, Batch 102, LR 0.744998 Loss 9.907714, Accuracy 62.577%\n",
      "Epoch 4, Batch 103, LR 0.745259 Loss 9.908422, Accuracy 62.546%\n",
      "Epoch 4, Batch 104, LR 0.745520 Loss 9.906883, Accuracy 62.553%\n",
      "Epoch 4, Batch 105, LR 0.745780 Loss 9.907372, Accuracy 62.582%\n",
      "Epoch 4, Batch 106, LR 0.746041 Loss 9.905164, Accuracy 62.662%\n",
      "Epoch 4, Batch 107, LR 0.746302 Loss 9.900963, Accuracy 62.683%\n",
      "Epoch 4, Batch 108, LR 0.746563 Loss 9.904283, Accuracy 62.688%\n",
      "Epoch 4, Batch 109, LR 0.746825 Loss 9.905295, Accuracy 62.651%\n",
      "Epoch 4, Batch 110, LR 0.747086 Loss 9.897713, Accuracy 62.713%\n",
      "Epoch 4, Batch 111, LR 0.747347 Loss 9.902700, Accuracy 62.655%\n",
      "Epoch 4, Batch 112, LR 0.747608 Loss 9.903825, Accuracy 62.653%\n",
      "Epoch 4, Batch 113, LR 0.747869 Loss 9.901540, Accuracy 62.721%\n",
      "Epoch 4, Batch 114, LR 0.748130 Loss 9.903743, Accuracy 62.637%\n",
      "Epoch 4, Batch 115, LR 0.748392 Loss 9.905522, Accuracy 62.602%\n",
      "Epoch 4, Batch 116, LR 0.748653 Loss 9.905049, Accuracy 62.628%\n",
      "Epoch 4, Batch 117, LR 0.748914 Loss 9.903054, Accuracy 62.680%\n",
      "Epoch 4, Batch 118, LR 0.749176 Loss 9.902349, Accuracy 62.692%\n",
      "Epoch 4, Batch 119, LR 0.749437 Loss 9.906947, Accuracy 62.638%\n",
      "Epoch 4, Batch 120, LR 0.749699 Loss 9.905067, Accuracy 62.682%\n",
      "Epoch 4, Batch 121, LR 0.749960 Loss 9.898595, Accuracy 62.732%\n",
      "Epoch 4, Batch 122, LR 0.750222 Loss 9.895119, Accuracy 62.801%\n",
      "Epoch 4, Batch 123, LR 0.750483 Loss 9.897949, Accuracy 62.760%\n",
      "Epoch 4, Batch 124, LR 0.750745 Loss 9.897978, Accuracy 62.739%\n",
      "Epoch 4, Batch 125, LR 0.751007 Loss 9.894752, Accuracy 62.806%\n",
      "Epoch 4, Batch 126, LR 0.751268 Loss 9.896121, Accuracy 62.754%\n",
      "Epoch 4, Batch 127, LR 0.751530 Loss 9.895132, Accuracy 62.752%\n",
      "Epoch 4, Batch 128, LR 0.751792 Loss 9.898954, Accuracy 62.744%\n",
      "Epoch 4, Batch 129, LR 0.752054 Loss 9.905279, Accuracy 62.682%\n",
      "Epoch 4, Batch 130, LR 0.752315 Loss 9.901072, Accuracy 62.716%\n",
      "Epoch 4, Batch 131, LR 0.752577 Loss 9.900359, Accuracy 62.697%\n",
      "Epoch 4, Batch 132, LR 0.752839 Loss 9.898052, Accuracy 62.689%\n",
      "Epoch 4, Batch 133, LR 0.753101 Loss 9.898458, Accuracy 62.664%\n",
      "Epoch 4, Batch 134, LR 0.753363 Loss 9.899364, Accuracy 62.669%\n",
      "Epoch 4, Batch 135, LR 0.753625 Loss 9.900036, Accuracy 62.674%\n",
      "Epoch 4, Batch 136, LR 0.753887 Loss 9.902170, Accuracy 62.661%\n",
      "Epoch 4, Batch 137, LR 0.754149 Loss 9.900296, Accuracy 62.688%\n",
      "Epoch 4, Batch 138, LR 0.754411 Loss 9.902264, Accuracy 62.664%\n",
      "Epoch 4, Batch 139, LR 0.754673 Loss 9.906904, Accuracy 62.635%\n",
      "Epoch 4, Batch 140, LR 0.754936 Loss 9.908985, Accuracy 62.640%\n",
      "Epoch 4, Batch 141, LR 0.755198 Loss 9.911972, Accuracy 62.661%\n",
      "Epoch 4, Batch 142, LR 0.755460 Loss 9.913313, Accuracy 62.621%\n",
      "Epoch 4, Batch 143, LR 0.755722 Loss 9.911888, Accuracy 62.653%\n",
      "Epoch 4, Batch 144, LR 0.755985 Loss 9.909746, Accuracy 62.646%\n",
      "Epoch 4, Batch 145, LR 0.756247 Loss 9.910778, Accuracy 62.656%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 146, LR 0.756509 Loss 9.912183, Accuracy 62.661%\n",
      "Epoch 4, Batch 147, LR 0.756772 Loss 9.914427, Accuracy 62.675%\n",
      "Epoch 4, Batch 148, LR 0.757034 Loss 9.915663, Accuracy 62.664%\n",
      "Epoch 4, Batch 149, LR 0.757297 Loss 9.915020, Accuracy 62.673%\n",
      "Epoch 4, Batch 150, LR 0.757559 Loss 9.915256, Accuracy 62.661%\n",
      "Epoch 4, Batch 151, LR 0.757822 Loss 9.918541, Accuracy 62.624%\n",
      "Epoch 4, Batch 152, LR 0.758084 Loss 9.912385, Accuracy 62.670%\n",
      "Epoch 4, Batch 153, LR 0.758347 Loss 9.912930, Accuracy 62.643%\n",
      "Epoch 4, Batch 154, LR 0.758610 Loss 9.913753, Accuracy 62.652%\n",
      "Epoch 4, Batch 155, LR 0.758872 Loss 9.914944, Accuracy 62.661%\n",
      "Epoch 4, Batch 156, LR 0.759135 Loss 9.911103, Accuracy 62.695%\n",
      "Epoch 4, Batch 157, LR 0.759398 Loss 9.911062, Accuracy 62.714%\n",
      "Epoch 4, Batch 158, LR 0.759661 Loss 9.914370, Accuracy 62.663%\n",
      "Epoch 4, Batch 159, LR 0.759923 Loss 9.912063, Accuracy 62.662%\n",
      "Epoch 4, Batch 160, LR 0.760186 Loss 9.912144, Accuracy 62.666%\n",
      "Epoch 4, Batch 161, LR 0.760449 Loss 9.912215, Accuracy 62.704%\n",
      "Epoch 4, Batch 162, LR 0.760712 Loss 9.915460, Accuracy 62.678%\n",
      "Epoch 4, Batch 163, LR 0.760975 Loss 9.913337, Accuracy 62.668%\n",
      "Epoch 4, Batch 164, LR 0.761238 Loss 9.916246, Accuracy 62.662%\n",
      "Epoch 4, Batch 165, LR 0.761501 Loss 9.918160, Accuracy 62.614%\n",
      "Epoch 4, Batch 166, LR 0.761764 Loss 9.913895, Accuracy 62.641%\n",
      "Epoch 4, Batch 167, LR 0.762027 Loss 9.913103, Accuracy 62.654%\n",
      "Epoch 4, Batch 168, LR 0.762290 Loss 9.912917, Accuracy 62.653%\n",
      "Epoch 4, Batch 169, LR 0.762554 Loss 9.915501, Accuracy 62.639%\n",
      "Epoch 4, Batch 170, LR 0.762817 Loss 9.918866, Accuracy 62.610%\n",
      "Epoch 4, Batch 171, LR 0.763080 Loss 9.921502, Accuracy 62.578%\n",
      "Epoch 4, Batch 172, LR 0.763343 Loss 9.918528, Accuracy 62.582%\n",
      "Epoch 4, Batch 173, LR 0.763607 Loss 9.920029, Accuracy 62.559%\n",
      "Epoch 4, Batch 174, LR 0.763870 Loss 9.920103, Accuracy 62.554%\n",
      "Epoch 4, Batch 175, LR 0.764133 Loss 9.918792, Accuracy 62.562%\n",
      "Epoch 4, Batch 176, LR 0.764397 Loss 9.916868, Accuracy 62.607%\n",
      "Epoch 4, Batch 177, LR 0.764660 Loss 9.921071, Accuracy 62.562%\n",
      "Epoch 4, Batch 178, LR 0.764924 Loss 9.924699, Accuracy 62.557%\n",
      "Epoch 4, Batch 179, LR 0.765187 Loss 9.919326, Accuracy 62.618%\n",
      "Epoch 4, Batch 180, LR 0.765451 Loss 9.918080, Accuracy 62.622%\n",
      "Epoch 4, Batch 181, LR 0.765714 Loss 9.918035, Accuracy 62.625%\n",
      "Epoch 4, Batch 182, LR 0.765978 Loss 9.917650, Accuracy 62.642%\n",
      "Epoch 4, Batch 183, LR 0.766242 Loss 9.919068, Accuracy 62.637%\n",
      "Epoch 4, Batch 184, LR 0.766505 Loss 9.919685, Accuracy 62.627%\n",
      "Epoch 4, Batch 185, LR 0.766769 Loss 9.922041, Accuracy 62.601%\n",
      "Epoch 4, Batch 186, LR 0.767033 Loss 9.922889, Accuracy 62.571%\n",
      "Epoch 4, Batch 187, LR 0.767297 Loss 9.922519, Accuracy 62.571%\n",
      "Epoch 4, Batch 188, LR 0.767560 Loss 9.923464, Accuracy 62.554%\n",
      "Epoch 4, Batch 189, LR 0.767824 Loss 9.925979, Accuracy 62.545%\n",
      "Epoch 4, Batch 190, LR 0.768088 Loss 9.922498, Accuracy 62.595%\n",
      "Epoch 4, Batch 191, LR 0.768352 Loss 9.923506, Accuracy 62.561%\n",
      "Epoch 4, Batch 192, LR 0.768616 Loss 9.928165, Accuracy 62.561%\n",
      "Epoch 4, Batch 193, LR 0.768880 Loss 9.930267, Accuracy 62.524%\n",
      "Epoch 4, Batch 194, LR 0.769144 Loss 9.930982, Accuracy 62.516%\n",
      "Epoch 4, Batch 195, LR 0.769408 Loss 9.931263, Accuracy 62.516%\n",
      "Epoch 4, Batch 196, LR 0.769672 Loss 9.933775, Accuracy 62.476%\n",
      "Epoch 4, Batch 197, LR 0.769936 Loss 9.932194, Accuracy 62.524%\n",
      "Epoch 4, Batch 198, LR 0.770200 Loss 9.930918, Accuracy 62.543%\n",
      "Epoch 4, Batch 199, LR 0.770465 Loss 9.931242, Accuracy 62.551%\n",
      "Epoch 4, Batch 200, LR 0.770729 Loss 9.934569, Accuracy 62.527%\n",
      "Epoch 4, Batch 201, LR 0.770993 Loss 9.932865, Accuracy 62.547%\n",
      "Epoch 4, Batch 202, LR 0.771257 Loss 9.934025, Accuracy 62.539%\n",
      "Epoch 4, Batch 203, LR 0.771522 Loss 9.935952, Accuracy 62.515%\n",
      "Epoch 4, Batch 204, LR 0.771786 Loss 9.935426, Accuracy 62.515%\n",
      "Epoch 4, Batch 205, LR 0.772051 Loss 9.936653, Accuracy 62.523%\n",
      "Epoch 4, Batch 206, LR 0.772315 Loss 9.935973, Accuracy 62.527%\n",
      "Epoch 4, Batch 207, LR 0.772579 Loss 9.935639, Accuracy 62.515%\n",
      "Epoch 4, Batch 208, LR 0.772844 Loss 9.937800, Accuracy 62.511%\n",
      "Epoch 4, Batch 209, LR 0.773108 Loss 9.936841, Accuracy 62.545%\n",
      "Epoch 4, Batch 210, LR 0.773373 Loss 9.936650, Accuracy 62.545%\n",
      "Epoch 4, Batch 211, LR 0.773638 Loss 9.937780, Accuracy 62.530%\n",
      "Epoch 4, Batch 212, LR 0.773902 Loss 9.934540, Accuracy 62.552%\n",
      "Epoch 4, Batch 213, LR 0.774167 Loss 9.935264, Accuracy 62.537%\n",
      "Epoch 4, Batch 214, LR 0.774432 Loss 9.936141, Accuracy 62.540%\n",
      "Epoch 4, Batch 215, LR 0.774696 Loss 9.935364, Accuracy 62.555%\n",
      "Epoch 4, Batch 216, LR 0.774961 Loss 9.936944, Accuracy 62.536%\n",
      "Epoch 4, Batch 217, LR 0.775226 Loss 9.938208, Accuracy 62.536%\n",
      "Epoch 4, Batch 218, LR 0.775491 Loss 9.940025, Accuracy 62.514%\n",
      "Epoch 4, Batch 219, LR 0.775756 Loss 9.937305, Accuracy 62.521%\n",
      "Epoch 4, Batch 220, LR 0.776021 Loss 9.937957, Accuracy 62.514%\n",
      "Epoch 4, Batch 221, LR 0.776286 Loss 9.939523, Accuracy 62.482%\n",
      "Epoch 4, Batch 222, LR 0.776551 Loss 9.940745, Accuracy 62.458%\n",
      "Epoch 4, Batch 223, LR 0.776816 Loss 9.938704, Accuracy 62.468%\n",
      "Epoch 4, Batch 224, LR 0.777081 Loss 9.938082, Accuracy 62.462%\n",
      "Epoch 4, Batch 225, LR 0.777346 Loss 9.938419, Accuracy 62.448%\n",
      "Epoch 4, Batch 226, LR 0.777611 Loss 9.940723, Accuracy 62.438%\n",
      "Epoch 4, Batch 227, LR 0.777876 Loss 9.939054, Accuracy 62.431%\n",
      "Epoch 4, Batch 228, LR 0.778141 Loss 9.936635, Accuracy 62.455%\n",
      "Epoch 4, Batch 229, LR 0.778406 Loss 9.939916, Accuracy 62.415%\n",
      "Epoch 4, Batch 230, LR 0.778672 Loss 9.940013, Accuracy 62.415%\n",
      "Epoch 4, Batch 231, LR 0.778937 Loss 9.944398, Accuracy 62.378%\n",
      "Epoch 4, Batch 232, LR 0.779202 Loss 9.942543, Accuracy 62.399%\n",
      "Epoch 4, Batch 233, LR 0.779467 Loss 9.943050, Accuracy 62.393%\n",
      "Epoch 4, Batch 234, LR 0.779733 Loss 9.942412, Accuracy 62.407%\n",
      "Epoch 4, Batch 235, LR 0.779998 Loss 9.941648, Accuracy 62.437%\n",
      "Epoch 4, Batch 236, LR 0.780264 Loss 9.936959, Accuracy 62.457%\n",
      "Epoch 4, Batch 237, LR 0.780529 Loss 9.938089, Accuracy 62.454%\n",
      "Epoch 4, Batch 238, LR 0.780795 Loss 9.937762, Accuracy 62.444%\n",
      "Epoch 4, Batch 239, LR 0.781060 Loss 9.938650, Accuracy 62.444%\n",
      "Epoch 4, Batch 240, LR 0.781326 Loss 9.939875, Accuracy 62.415%\n",
      "Epoch 4, Batch 241, LR 0.781591 Loss 9.942384, Accuracy 62.400%\n",
      "Epoch 4, Batch 242, LR 0.781857 Loss 9.941847, Accuracy 62.400%\n",
      "Epoch 4, Batch 243, LR 0.782123 Loss 9.942891, Accuracy 62.381%\n",
      "Epoch 4, Batch 244, LR 0.782388 Loss 9.943360, Accuracy 62.375%\n",
      "Epoch 4, Batch 245, LR 0.782654 Loss 9.939957, Accuracy 62.388%\n",
      "Epoch 4, Batch 246, LR 0.782920 Loss 9.936792, Accuracy 62.430%\n",
      "Epoch 4, Batch 247, LR 0.783186 Loss 9.939421, Accuracy 62.408%\n",
      "Epoch 4, Batch 248, LR 0.783452 Loss 9.940989, Accuracy 62.409%\n",
      "Epoch 4, Batch 249, LR 0.783717 Loss 9.941197, Accuracy 62.396%\n",
      "Epoch 4, Batch 250, LR 0.783983 Loss 9.940660, Accuracy 62.400%\n",
      "Epoch 4, Batch 251, LR 0.784249 Loss 9.939395, Accuracy 62.388%\n",
      "Epoch 4, Batch 252, LR 0.784515 Loss 9.941813, Accuracy 62.370%\n",
      "Epoch 4, Batch 253, LR 0.784781 Loss 9.941351, Accuracy 62.358%\n",
      "Epoch 4, Batch 254, LR 0.785047 Loss 9.940255, Accuracy 62.374%\n",
      "Epoch 4, Batch 255, LR 0.785313 Loss 9.941134, Accuracy 62.371%\n",
      "Epoch 4, Batch 256, LR 0.785579 Loss 9.942948, Accuracy 62.350%\n",
      "Epoch 4, Batch 257, LR 0.785846 Loss 9.944512, Accuracy 62.342%\n",
      "Epoch 4, Batch 258, LR 0.786112 Loss 9.942957, Accuracy 62.355%\n",
      "Epoch 4, Batch 259, LR 0.786378 Loss 9.940468, Accuracy 62.391%\n",
      "Epoch 4, Batch 260, LR 0.786644 Loss 9.943704, Accuracy 62.365%\n",
      "Epoch 4, Batch 261, LR 0.786910 Loss 9.941585, Accuracy 62.374%\n",
      "Epoch 4, Batch 262, LR 0.787177 Loss 9.941627, Accuracy 62.381%\n",
      "Epoch 4, Batch 263, LR 0.787443 Loss 9.943027, Accuracy 62.354%\n",
      "Epoch 4, Batch 264, LR 0.787709 Loss 9.943213, Accuracy 62.367%\n",
      "Epoch 4, Batch 265, LR 0.787976 Loss 9.940701, Accuracy 62.382%\n",
      "Epoch 4, Batch 266, LR 0.788242 Loss 9.941333, Accuracy 62.350%\n",
      "Epoch 4, Batch 267, LR 0.788509 Loss 9.938836, Accuracy 62.357%\n",
      "Epoch 4, Batch 268, LR 0.788775 Loss 9.939315, Accuracy 62.351%\n",
      "Epoch 4, Batch 269, LR 0.789042 Loss 9.937947, Accuracy 62.358%\n",
      "Epoch 4, Batch 270, LR 0.789308 Loss 9.936159, Accuracy 62.367%\n",
      "Epoch 4, Batch 271, LR 0.789575 Loss 9.935014, Accuracy 62.376%\n",
      "Epoch 4, Batch 272, LR 0.789842 Loss 9.934837, Accuracy 62.379%\n",
      "Epoch 4, Batch 273, LR 0.790108 Loss 9.933662, Accuracy 62.391%\n",
      "Epoch 4, Batch 274, LR 0.790375 Loss 9.933384, Accuracy 62.395%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 275, LR 0.790642 Loss 9.930991, Accuracy 62.401%\n",
      "Epoch 4, Batch 276, LR 0.790908 Loss 9.929893, Accuracy 62.404%\n",
      "Epoch 4, Batch 277, LR 0.791175 Loss 9.928433, Accuracy 62.407%\n",
      "Epoch 4, Batch 278, LR 0.791442 Loss 9.927989, Accuracy 62.410%\n",
      "Epoch 4, Batch 279, LR 0.791709 Loss 9.928592, Accuracy 62.391%\n",
      "Epoch 4, Batch 280, LR 0.791976 Loss 9.926119, Accuracy 62.405%\n",
      "Epoch 4, Batch 281, LR 0.792243 Loss 9.924830, Accuracy 62.419%\n",
      "Epoch 4, Batch 282, LR 0.792510 Loss 9.925365, Accuracy 62.409%\n",
      "Epoch 4, Batch 283, LR 0.792777 Loss 9.925691, Accuracy 62.395%\n",
      "Epoch 4, Batch 284, LR 0.793044 Loss 9.926097, Accuracy 62.393%\n",
      "Epoch 4, Batch 285, LR 0.793311 Loss 9.926040, Accuracy 62.399%\n",
      "Epoch 4, Batch 286, LR 0.793578 Loss 9.927638, Accuracy 62.404%\n",
      "Epoch 4, Batch 287, LR 0.793845 Loss 9.927512, Accuracy 62.405%\n",
      "Epoch 4, Batch 288, LR 0.794112 Loss 9.927320, Accuracy 62.405%\n",
      "Epoch 4, Batch 289, LR 0.794379 Loss 9.927496, Accuracy 62.405%\n",
      "Epoch 4, Batch 290, LR 0.794646 Loss 9.928609, Accuracy 62.395%\n",
      "Epoch 4, Batch 291, LR 0.794914 Loss 9.928039, Accuracy 62.398%\n",
      "Epoch 4, Batch 292, LR 0.795181 Loss 9.927748, Accuracy 62.409%\n",
      "Epoch 4, Batch 293, LR 0.795448 Loss 9.925693, Accuracy 62.423%\n",
      "Epoch 4, Batch 294, LR 0.795716 Loss 9.923498, Accuracy 62.436%\n",
      "Epoch 4, Batch 295, LR 0.795983 Loss 9.924062, Accuracy 62.442%\n",
      "Epoch 4, Batch 296, LR 0.796250 Loss 9.923926, Accuracy 62.426%\n",
      "Epoch 4, Batch 297, LR 0.796518 Loss 9.922488, Accuracy 62.439%\n",
      "Epoch 4, Batch 298, LR 0.796785 Loss 9.918753, Accuracy 62.471%\n",
      "Epoch 4, Batch 299, LR 0.797053 Loss 9.920377, Accuracy 62.474%\n",
      "Epoch 4, Batch 300, LR 0.797320 Loss 9.921149, Accuracy 62.471%\n",
      "Epoch 4, Batch 301, LR 0.797588 Loss 9.920542, Accuracy 62.482%\n",
      "Epoch 4, Batch 302, LR 0.797856 Loss 9.918337, Accuracy 62.510%\n",
      "Epoch 4, Batch 303, LR 0.798123 Loss 9.918139, Accuracy 62.500%\n",
      "Epoch 4, Batch 304, LR 0.798391 Loss 9.918135, Accuracy 62.500%\n",
      "Epoch 4, Batch 305, LR 0.798659 Loss 9.918135, Accuracy 62.497%\n",
      "Epoch 4, Batch 306, LR 0.798926 Loss 9.920539, Accuracy 62.480%\n",
      "Epoch 4, Batch 307, LR 0.799194 Loss 9.921982, Accuracy 62.449%\n",
      "Epoch 4, Batch 308, LR 0.799462 Loss 9.923346, Accuracy 62.447%\n",
      "Epoch 4, Batch 309, LR 0.799730 Loss 9.924782, Accuracy 62.444%\n",
      "Epoch 4, Batch 310, LR 0.799998 Loss 9.921816, Accuracy 62.475%\n",
      "Epoch 4, Batch 311, LR 0.800265 Loss 9.919823, Accuracy 62.497%\n",
      "Epoch 4, Batch 312, LR 0.800533 Loss 9.921503, Accuracy 62.472%\n",
      "Epoch 4, Batch 313, LR 0.800801 Loss 9.920570, Accuracy 62.478%\n",
      "Epoch 4, Batch 314, LR 0.801069 Loss 9.921837, Accuracy 62.460%\n",
      "Epoch 4, Batch 315, LR 0.801337 Loss 9.921785, Accuracy 62.453%\n",
      "Epoch 4, Batch 316, LR 0.801605 Loss 9.921688, Accuracy 62.448%\n",
      "Epoch 4, Batch 317, LR 0.801873 Loss 9.920987, Accuracy 62.448%\n",
      "Epoch 4, Batch 318, LR 0.802142 Loss 9.920526, Accuracy 62.448%\n",
      "Epoch 4, Batch 319, LR 0.802410 Loss 9.920204, Accuracy 62.441%\n",
      "Epoch 4, Batch 320, LR 0.802678 Loss 9.920728, Accuracy 62.434%\n",
      "Epoch 4, Batch 321, LR 0.802946 Loss 9.921936, Accuracy 62.427%\n",
      "Epoch 4, Batch 322, LR 0.803214 Loss 9.919995, Accuracy 62.434%\n",
      "Epoch 4, Batch 323, LR 0.803483 Loss 9.920218, Accuracy 62.444%\n",
      "Epoch 4, Batch 324, LR 0.803751 Loss 9.920684, Accuracy 62.447%\n",
      "Epoch 4, Batch 325, LR 0.804019 Loss 9.920159, Accuracy 62.457%\n",
      "Epoch 4, Batch 326, LR 0.804288 Loss 9.920457, Accuracy 62.450%\n",
      "Epoch 4, Batch 327, LR 0.804556 Loss 9.919472, Accuracy 62.471%\n",
      "Epoch 4, Batch 328, LR 0.804824 Loss 9.917196, Accuracy 62.490%\n",
      "Epoch 4, Batch 329, LR 0.805093 Loss 9.917565, Accuracy 62.495%\n",
      "Epoch 4, Batch 330, LR 0.805361 Loss 9.918418, Accuracy 62.488%\n",
      "Epoch 4, Batch 331, LR 0.805630 Loss 9.918460, Accuracy 62.493%\n",
      "Epoch 4, Batch 332, LR 0.805899 Loss 9.919877, Accuracy 62.462%\n",
      "Epoch 4, Batch 333, LR 0.806167 Loss 9.920656, Accuracy 62.460%\n",
      "Epoch 4, Batch 334, LR 0.806436 Loss 9.919787, Accuracy 62.465%\n",
      "Epoch 4, Batch 335, LR 0.806704 Loss 9.919704, Accuracy 62.458%\n",
      "Epoch 4, Batch 336, LR 0.806973 Loss 9.919947, Accuracy 62.467%\n",
      "Epoch 4, Batch 337, LR 0.807242 Loss 9.920215, Accuracy 62.472%\n",
      "Epoch 4, Batch 338, LR 0.807511 Loss 9.919841, Accuracy 62.472%\n",
      "Epoch 4, Batch 339, LR 0.807779 Loss 9.919326, Accuracy 62.470%\n",
      "Epoch 4, Batch 340, LR 0.808048 Loss 9.920162, Accuracy 62.470%\n",
      "Epoch 4, Batch 341, LR 0.808317 Loss 9.921178, Accuracy 62.456%\n",
      "Epoch 4, Batch 342, LR 0.808586 Loss 9.917837, Accuracy 62.491%\n",
      "Epoch 4, Batch 343, LR 0.808855 Loss 9.917366, Accuracy 62.493%\n",
      "Epoch 4, Batch 344, LR 0.809124 Loss 9.918748, Accuracy 62.482%\n",
      "Epoch 4, Batch 345, LR 0.809393 Loss 9.920151, Accuracy 62.480%\n",
      "Epoch 4, Batch 346, LR 0.809662 Loss 9.920498, Accuracy 62.475%\n",
      "Epoch 4, Batch 347, LR 0.809931 Loss 9.921476, Accuracy 62.477%\n",
      "Epoch 4, Batch 348, LR 0.810200 Loss 9.922226, Accuracy 62.462%\n",
      "Epoch 4, Batch 349, LR 0.810469 Loss 9.921521, Accuracy 62.473%\n",
      "Epoch 4, Batch 350, LR 0.810738 Loss 9.920692, Accuracy 62.493%\n",
      "Epoch 4, Batch 351, LR 0.811007 Loss 9.921716, Accuracy 62.482%\n",
      "Epoch 4, Batch 352, LR 0.811276 Loss 9.919960, Accuracy 62.493%\n",
      "Epoch 4, Batch 353, LR 0.811546 Loss 9.919559, Accuracy 62.491%\n",
      "Epoch 4, Batch 354, LR 0.811815 Loss 9.918770, Accuracy 62.500%\n",
      "Epoch 4, Batch 355, LR 0.812084 Loss 9.917798, Accuracy 62.491%\n",
      "Epoch 4, Batch 356, LR 0.812353 Loss 9.917904, Accuracy 62.496%\n",
      "Epoch 4, Batch 357, LR 0.812623 Loss 9.918282, Accuracy 62.493%\n",
      "Epoch 4, Batch 358, LR 0.812892 Loss 9.917698, Accuracy 62.504%\n",
      "Epoch 4, Batch 359, LR 0.813162 Loss 9.919029, Accuracy 62.489%\n",
      "Epoch 4, Batch 360, LR 0.813431 Loss 9.918988, Accuracy 62.498%\n",
      "Epoch 4, Batch 361, LR 0.813700 Loss 9.918001, Accuracy 62.504%\n",
      "Epoch 4, Batch 362, LR 0.813970 Loss 9.918111, Accuracy 62.517%\n",
      "Epoch 4, Batch 363, LR 0.814240 Loss 9.918480, Accuracy 62.509%\n",
      "Epoch 4, Batch 364, LR 0.814509 Loss 9.917710, Accuracy 62.526%\n",
      "Epoch 4, Batch 365, LR 0.814779 Loss 9.917785, Accuracy 62.528%\n",
      "Epoch 4, Batch 366, LR 0.815048 Loss 9.918943, Accuracy 62.517%\n",
      "Epoch 4, Batch 367, LR 0.815318 Loss 9.920723, Accuracy 62.494%\n",
      "Epoch 4, Batch 368, LR 0.815588 Loss 9.920680, Accuracy 62.502%\n",
      "Epoch 4, Batch 369, LR 0.815857 Loss 9.921525, Accuracy 62.492%\n",
      "Epoch 4, Batch 370, LR 0.816127 Loss 9.920826, Accuracy 62.496%\n",
      "Epoch 4, Batch 371, LR 0.816397 Loss 9.921128, Accuracy 62.502%\n",
      "Epoch 4, Batch 372, LR 0.816667 Loss 9.920382, Accuracy 62.513%\n",
      "Epoch 4, Batch 373, LR 0.816937 Loss 9.920171, Accuracy 62.525%\n",
      "Epoch 4, Batch 374, LR 0.817206 Loss 9.919186, Accuracy 62.529%\n",
      "Epoch 4, Batch 375, LR 0.817476 Loss 9.920109, Accuracy 62.510%\n",
      "Epoch 4, Batch 376, LR 0.817746 Loss 9.921628, Accuracy 62.508%\n",
      "Epoch 4, Batch 377, LR 0.818016 Loss 9.922204, Accuracy 62.512%\n",
      "Epoch 4, Batch 378, LR 0.818286 Loss 9.923837, Accuracy 62.496%\n",
      "Epoch 4, Batch 379, LR 0.818556 Loss 9.923904, Accuracy 62.502%\n",
      "Epoch 4, Batch 380, LR 0.818826 Loss 9.922623, Accuracy 62.521%\n",
      "Epoch 4, Batch 381, LR 0.819096 Loss 9.922555, Accuracy 62.525%\n",
      "Epoch 4, Batch 382, LR 0.819367 Loss 9.923125, Accuracy 62.518%\n",
      "Epoch 4, Batch 383, LR 0.819637 Loss 9.924136, Accuracy 62.508%\n",
      "Epoch 4, Batch 384, LR 0.819907 Loss 9.924684, Accuracy 62.498%\n",
      "Epoch 4, Batch 385, LR 0.820177 Loss 9.923887, Accuracy 62.496%\n",
      "Epoch 4, Batch 386, LR 0.820447 Loss 9.923516, Accuracy 62.492%\n",
      "Epoch 4, Batch 387, LR 0.820718 Loss 9.924084, Accuracy 62.482%\n",
      "Epoch 4, Batch 388, LR 0.820988 Loss 9.924288, Accuracy 62.474%\n",
      "Epoch 4, Batch 389, LR 0.821258 Loss 9.924155, Accuracy 62.480%\n",
      "Epoch 4, Batch 390, LR 0.821529 Loss 9.923879, Accuracy 62.486%\n",
      "Epoch 4, Batch 391, LR 0.821799 Loss 9.925643, Accuracy 62.480%\n",
      "Epoch 4, Batch 392, LR 0.822070 Loss 9.926476, Accuracy 62.480%\n",
      "Epoch 4, Batch 393, LR 0.822340 Loss 9.926976, Accuracy 62.468%\n",
      "Epoch 4, Batch 394, LR 0.822610 Loss 9.926161, Accuracy 62.490%\n",
      "Epoch 4, Batch 395, LR 0.822881 Loss 9.926703, Accuracy 62.486%\n",
      "Epoch 4, Batch 396, LR 0.823152 Loss 9.926232, Accuracy 62.474%\n",
      "Epoch 4, Batch 397, LR 0.823422 Loss 9.927425, Accuracy 62.467%\n",
      "Epoch 4, Batch 398, LR 0.823693 Loss 9.927413, Accuracy 62.459%\n",
      "Epoch 4, Batch 399, LR 0.823963 Loss 9.926134, Accuracy 62.461%\n",
      "Epoch 4, Batch 400, LR 0.824234 Loss 9.927923, Accuracy 62.453%\n",
      "Epoch 4, Batch 401, LR 0.824505 Loss 9.929552, Accuracy 62.447%\n",
      "Epoch 4, Batch 402, LR 0.824775 Loss 9.930795, Accuracy 62.446%\n",
      "Epoch 4, Batch 403, LR 0.825046 Loss 9.931333, Accuracy 62.440%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 404, LR 0.825317 Loss 9.932617, Accuracy 62.428%\n",
      "Epoch 4, Batch 405, LR 0.825588 Loss 9.933285, Accuracy 62.425%\n",
      "Epoch 4, Batch 406, LR 0.825859 Loss 9.932260, Accuracy 62.429%\n",
      "Epoch 4, Batch 407, LR 0.826130 Loss 9.932847, Accuracy 62.410%\n",
      "Epoch 4, Batch 408, LR 0.826401 Loss 9.933023, Accuracy 62.406%\n",
      "Epoch 4, Batch 409, LR 0.826672 Loss 9.933588, Accuracy 62.391%\n",
      "Epoch 4, Batch 410, LR 0.826942 Loss 9.933805, Accuracy 62.391%\n",
      "Epoch 4, Batch 411, LR 0.827214 Loss 9.932750, Accuracy 62.392%\n",
      "Epoch 4, Batch 412, LR 0.827485 Loss 9.932268, Accuracy 62.396%\n",
      "Epoch 4, Batch 413, LR 0.827756 Loss 9.930835, Accuracy 62.407%\n",
      "Epoch 4, Batch 414, LR 0.828027 Loss 9.932629, Accuracy 62.391%\n",
      "Epoch 4, Batch 415, LR 0.828298 Loss 9.932898, Accuracy 62.387%\n",
      "Epoch 4, Batch 416, LR 0.828569 Loss 9.930016, Accuracy 62.419%\n",
      "Epoch 4, Batch 417, LR 0.828840 Loss 9.931091, Accuracy 62.401%\n",
      "Epoch 4, Batch 418, LR 0.829111 Loss 9.932123, Accuracy 62.390%\n",
      "Epoch 4, Batch 419, LR 0.829383 Loss 9.930173, Accuracy 62.407%\n",
      "Epoch 4, Batch 420, LR 0.829654 Loss 9.928566, Accuracy 62.416%\n",
      "Epoch 4, Batch 421, LR 0.829925 Loss 9.928076, Accuracy 62.428%\n",
      "Epoch 4, Batch 422, LR 0.830197 Loss 9.928470, Accuracy 62.428%\n",
      "Epoch 4, Batch 423, LR 0.830468 Loss 9.928494, Accuracy 62.432%\n",
      "Epoch 4, Batch 424, LR 0.830739 Loss 9.928955, Accuracy 62.421%\n",
      "Epoch 4, Batch 425, LR 0.831011 Loss 9.930638, Accuracy 62.404%\n",
      "Epoch 4, Batch 426, LR 0.831282 Loss 9.930428, Accuracy 62.417%\n",
      "Epoch 4, Batch 427, LR 0.831554 Loss 9.929440, Accuracy 62.430%\n",
      "Epoch 4, Batch 428, LR 0.831825 Loss 9.929591, Accuracy 62.431%\n",
      "Epoch 4, Batch 429, LR 0.832097 Loss 9.928820, Accuracy 62.434%\n",
      "Epoch 4, Batch 430, LR 0.832368 Loss 9.928828, Accuracy 62.426%\n",
      "Epoch 4, Batch 431, LR 0.832640 Loss 9.927765, Accuracy 62.446%\n",
      "Epoch 4, Batch 432, LR 0.832912 Loss 9.927565, Accuracy 62.444%\n",
      "Epoch 4, Batch 433, LR 0.833183 Loss 9.927946, Accuracy 62.440%\n",
      "Epoch 4, Batch 434, LR 0.833455 Loss 9.928475, Accuracy 62.437%\n",
      "Epoch 4, Batch 435, LR 0.833727 Loss 9.927492, Accuracy 62.455%\n",
      "Epoch 4, Batch 436, LR 0.833999 Loss 9.927142, Accuracy 62.461%\n",
      "Epoch 4, Batch 437, LR 0.834270 Loss 9.924585, Accuracy 62.479%\n",
      "Epoch 4, Batch 438, LR 0.834542 Loss 9.924649, Accuracy 62.480%\n",
      "Epoch 4, Batch 439, LR 0.834814 Loss 9.925423, Accuracy 62.475%\n",
      "Epoch 4, Batch 440, LR 0.835086 Loss 9.925894, Accuracy 62.470%\n",
      "Epoch 4, Batch 441, LR 0.835358 Loss 9.925730, Accuracy 62.461%\n",
      "Epoch 4, Batch 442, LR 0.835630 Loss 9.927696, Accuracy 62.449%\n",
      "Epoch 4, Batch 443, LR 0.835902 Loss 9.927317, Accuracy 62.442%\n",
      "Epoch 4, Batch 444, LR 0.836174 Loss 9.925554, Accuracy 62.460%\n",
      "Epoch 4, Batch 445, LR 0.836446 Loss 9.926617, Accuracy 62.454%\n",
      "Epoch 4, Batch 446, LR 0.836718 Loss 9.927416, Accuracy 62.440%\n",
      "Epoch 4, Batch 447, LR 0.836990 Loss 9.925654, Accuracy 62.451%\n",
      "Epoch 4, Batch 448, LR 0.837262 Loss 9.924543, Accuracy 62.448%\n",
      "Epoch 4, Batch 449, LR 0.837534 Loss 9.925546, Accuracy 62.430%\n",
      "Epoch 4, Batch 450, LR 0.837806 Loss 9.925957, Accuracy 62.427%\n",
      "Epoch 4, Batch 451, LR 0.838079 Loss 9.926062, Accuracy 62.419%\n",
      "Epoch 4, Batch 452, LR 0.838351 Loss 9.925765, Accuracy 62.429%\n",
      "Epoch 4, Batch 453, LR 0.838623 Loss 9.924794, Accuracy 62.431%\n",
      "Epoch 4, Batch 454, LR 0.838895 Loss 9.923646, Accuracy 62.438%\n",
      "Epoch 4, Batch 455, LR 0.839168 Loss 9.924666, Accuracy 62.440%\n",
      "Epoch 4, Batch 456, LR 0.839440 Loss 9.924602, Accuracy 62.445%\n",
      "Epoch 4, Batch 457, LR 0.839713 Loss 9.923499, Accuracy 62.445%\n",
      "Epoch 4, Batch 458, LR 0.839985 Loss 9.924546, Accuracy 62.440%\n",
      "Epoch 4, Batch 459, LR 0.840257 Loss 9.925239, Accuracy 62.437%\n",
      "Epoch 4, Batch 460, LR 0.840530 Loss 9.926595, Accuracy 62.412%\n",
      "Epoch 4, Batch 461, LR 0.840802 Loss 9.925573, Accuracy 62.424%\n",
      "Epoch 4, Batch 462, LR 0.841075 Loss 9.925483, Accuracy 62.431%\n",
      "Epoch 4, Batch 463, LR 0.841348 Loss 9.925843, Accuracy 62.426%\n",
      "Epoch 4, Batch 464, LR 0.841620 Loss 9.924836, Accuracy 62.434%\n",
      "Epoch 4, Batch 465, LR 0.841893 Loss 9.924617, Accuracy 62.436%\n",
      "Epoch 4, Batch 466, LR 0.842165 Loss 9.925635, Accuracy 62.435%\n",
      "Epoch 4, Batch 467, LR 0.842438 Loss 9.924770, Accuracy 62.445%\n",
      "Epoch 4, Batch 468, LR 0.842711 Loss 9.926400, Accuracy 62.428%\n",
      "Epoch 4, Batch 469, LR 0.842984 Loss 9.926830, Accuracy 62.423%\n",
      "Epoch 4, Batch 470, LR 0.843256 Loss 9.926780, Accuracy 62.424%\n",
      "Epoch 4, Batch 471, LR 0.843529 Loss 9.927522, Accuracy 62.424%\n",
      "Epoch 4, Batch 472, LR 0.843802 Loss 9.927230, Accuracy 62.426%\n",
      "Epoch 4, Batch 473, LR 0.844075 Loss 9.927326, Accuracy 62.424%\n",
      "Epoch 4, Batch 474, LR 0.844348 Loss 9.927648, Accuracy 62.427%\n",
      "Epoch 4, Batch 475, LR 0.844621 Loss 9.926470, Accuracy 62.442%\n",
      "Epoch 4, Batch 476, LR 0.844894 Loss 9.927651, Accuracy 62.428%\n",
      "Epoch 4, Batch 477, LR 0.845167 Loss 9.928036, Accuracy 62.428%\n",
      "Epoch 4, Batch 478, LR 0.845440 Loss 9.925675, Accuracy 62.444%\n",
      "Epoch 4, Batch 479, LR 0.845713 Loss 9.925992, Accuracy 62.446%\n",
      "Epoch 4, Batch 480, LR 0.845986 Loss 9.926756, Accuracy 62.443%\n",
      "Epoch 4, Batch 481, LR 0.846259 Loss 9.927037, Accuracy 62.435%\n",
      "Epoch 4, Batch 482, LR 0.846532 Loss 9.927131, Accuracy 62.443%\n",
      "Epoch 4, Batch 483, LR 0.846805 Loss 9.926920, Accuracy 62.437%\n",
      "Epoch 4, Batch 484, LR 0.847078 Loss 9.925160, Accuracy 62.450%\n",
      "Epoch 4, Batch 485, LR 0.847351 Loss 9.923802, Accuracy 62.466%\n",
      "Epoch 4, Batch 486, LR 0.847625 Loss 9.922713, Accuracy 62.471%\n",
      "Epoch 4, Batch 487, LR 0.847898 Loss 9.920890, Accuracy 62.482%\n",
      "Epoch 4, Batch 488, LR 0.848171 Loss 9.921996, Accuracy 62.470%\n",
      "Epoch 4, Batch 489, LR 0.848445 Loss 9.920722, Accuracy 62.479%\n",
      "Epoch 4, Batch 490, LR 0.848718 Loss 9.920062, Accuracy 62.487%\n",
      "Epoch 4, Batch 491, LR 0.848991 Loss 9.921090, Accuracy 62.476%\n",
      "Epoch 4, Batch 492, LR 0.849265 Loss 9.921885, Accuracy 62.471%\n",
      "Epoch 4, Batch 493, LR 0.849538 Loss 9.921403, Accuracy 62.486%\n",
      "Epoch 4, Batch 494, LR 0.849812 Loss 9.921056, Accuracy 62.486%\n",
      "Epoch 4, Batch 495, LR 0.850085 Loss 9.922402, Accuracy 62.483%\n",
      "Epoch 4, Batch 496, LR 0.850359 Loss 9.921583, Accuracy 62.480%\n",
      "Epoch 4, Batch 497, LR 0.850632 Loss 9.920758, Accuracy 62.481%\n",
      "Epoch 4, Batch 498, LR 0.850906 Loss 9.919285, Accuracy 62.498%\n",
      "Epoch 4, Batch 499, LR 0.851179 Loss 9.919024, Accuracy 62.508%\n",
      "Epoch 4, Batch 500, LR 0.851453 Loss 9.920488, Accuracy 62.500%\n",
      "Epoch 4, Batch 501, LR 0.851727 Loss 9.920347, Accuracy 62.505%\n",
      "Epoch 4, Batch 502, LR 0.852000 Loss 9.919911, Accuracy 62.497%\n",
      "Epoch 4, Batch 503, LR 0.852274 Loss 9.919378, Accuracy 62.489%\n",
      "Epoch 4, Batch 504, LR 0.852548 Loss 9.920415, Accuracy 62.483%\n",
      "Epoch 4, Batch 505, LR 0.852822 Loss 9.920557, Accuracy 62.480%\n",
      "Epoch 4, Batch 506, LR 0.853096 Loss 9.920832, Accuracy 62.475%\n",
      "Epoch 4, Batch 507, LR 0.853369 Loss 9.921604, Accuracy 62.468%\n",
      "Epoch 4, Batch 508, LR 0.853643 Loss 9.921596, Accuracy 62.462%\n",
      "Epoch 4, Batch 509, LR 0.853917 Loss 9.922052, Accuracy 62.451%\n",
      "Epoch 4, Batch 510, LR 0.854191 Loss 9.922059, Accuracy 62.456%\n",
      "Epoch 4, Batch 511, LR 0.854465 Loss 9.921561, Accuracy 62.462%\n",
      "Epoch 4, Batch 512, LR 0.854739 Loss 9.921522, Accuracy 62.465%\n",
      "Epoch 4, Batch 513, LR 0.855013 Loss 9.921856, Accuracy 62.466%\n",
      "Epoch 4, Batch 514, LR 0.855287 Loss 9.920974, Accuracy 62.471%\n",
      "Epoch 4, Batch 515, LR 0.855561 Loss 9.920446, Accuracy 62.477%\n",
      "Epoch 4, Batch 516, LR 0.855835 Loss 9.919311, Accuracy 62.486%\n",
      "Epoch 4, Batch 517, LR 0.856109 Loss 9.917837, Accuracy 62.506%\n",
      "Epoch 4, Batch 518, LR 0.856384 Loss 9.917919, Accuracy 62.511%\n",
      "Epoch 4, Batch 519, LR 0.856658 Loss 9.917339, Accuracy 62.515%\n",
      "Epoch 4, Batch 520, LR 0.856932 Loss 9.917169, Accuracy 62.518%\n",
      "Epoch 4, Batch 521, LR 0.857206 Loss 9.916723, Accuracy 62.521%\n",
      "Epoch 4, Batch 522, LR 0.857480 Loss 9.914862, Accuracy 62.537%\n",
      "Epoch 4, Batch 523, LR 0.857755 Loss 9.915463, Accuracy 62.536%\n",
      "Epoch 4, Batch 524, LR 0.858029 Loss 9.914936, Accuracy 62.540%\n",
      "Epoch 4, Batch 525, LR 0.858303 Loss 9.914492, Accuracy 62.552%\n",
      "Epoch 4, Batch 526, LR 0.858578 Loss 9.914218, Accuracy 62.553%\n",
      "Epoch 4, Batch 527, LR 0.858852 Loss 9.913116, Accuracy 62.564%\n",
      "Epoch 4, Batch 528, LR 0.859127 Loss 9.912745, Accuracy 62.558%\n",
      "Epoch 4, Batch 529, LR 0.859401 Loss 9.912697, Accuracy 62.556%\n",
      "Epoch 4, Batch 530, LR 0.859676 Loss 9.912039, Accuracy 62.556%\n",
      "Epoch 4, Batch 531, LR 0.859950 Loss 9.911773, Accuracy 62.550%\n",
      "Epoch 4, Batch 532, LR 0.860225 Loss 9.911708, Accuracy 62.547%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 533, LR 0.860499 Loss 9.912614, Accuracy 62.535%\n",
      "Epoch 4, Batch 534, LR 0.860774 Loss 9.913597, Accuracy 62.520%\n",
      "Epoch 4, Batch 535, LR 0.861049 Loss 9.913090, Accuracy 62.522%\n",
      "Epoch 4, Batch 536, LR 0.861323 Loss 9.913375, Accuracy 62.523%\n",
      "Epoch 4, Batch 537, LR 0.861598 Loss 9.913999, Accuracy 62.520%\n",
      "Epoch 4, Batch 538, LR 0.861873 Loss 9.913879, Accuracy 62.529%\n",
      "Epoch 4, Batch 539, LR 0.862148 Loss 9.913256, Accuracy 62.536%\n",
      "Epoch 4, Batch 540, LR 0.862422 Loss 9.913927, Accuracy 62.535%\n",
      "Epoch 4, Batch 541, LR 0.862697 Loss 9.914043, Accuracy 62.530%\n",
      "Epoch 4, Batch 542, LR 0.862972 Loss 9.913817, Accuracy 62.523%\n",
      "Epoch 4, Batch 543, LR 0.863247 Loss 9.913755, Accuracy 62.533%\n",
      "Epoch 4, Batch 544, LR 0.863522 Loss 9.914175, Accuracy 62.527%\n",
      "Epoch 4, Batch 545, LR 0.863797 Loss 9.912677, Accuracy 62.544%\n",
      "Epoch 4, Batch 546, LR 0.864072 Loss 9.913561, Accuracy 62.537%\n",
      "Epoch 4, Batch 547, LR 0.864347 Loss 9.913970, Accuracy 62.530%\n",
      "Epoch 4, Batch 548, LR 0.864622 Loss 9.913159, Accuracy 62.547%\n",
      "Epoch 4, Batch 549, LR 0.864897 Loss 9.913005, Accuracy 62.544%\n",
      "Epoch 4, Batch 550, LR 0.865172 Loss 9.913242, Accuracy 62.547%\n",
      "Epoch 4, Batch 551, LR 0.865447 Loss 9.912894, Accuracy 62.545%\n",
      "Epoch 4, Batch 552, LR 0.865722 Loss 9.912821, Accuracy 62.548%\n",
      "Epoch 4, Batch 553, LR 0.865997 Loss 9.912382, Accuracy 62.547%\n",
      "Epoch 4, Batch 554, LR 0.866272 Loss 9.912784, Accuracy 62.541%\n",
      "Epoch 4, Batch 555, LR 0.866547 Loss 9.911854, Accuracy 62.545%\n",
      "Epoch 4, Batch 556, LR 0.866823 Loss 9.911304, Accuracy 62.545%\n",
      "Epoch 4, Batch 557, LR 0.867098 Loss 9.911711, Accuracy 62.545%\n",
      "Epoch 4, Batch 558, LR 0.867373 Loss 9.911343, Accuracy 62.556%\n",
      "Epoch 4, Batch 559, LR 0.867649 Loss 9.911058, Accuracy 62.561%\n",
      "Epoch 4, Batch 560, LR 0.867924 Loss 9.910372, Accuracy 62.559%\n",
      "Epoch 4, Batch 561, LR 0.868199 Loss 9.909905, Accuracy 62.564%\n",
      "Epoch 4, Batch 562, LR 0.868475 Loss 9.909409, Accuracy 62.572%\n",
      "Epoch 4, Batch 563, LR 0.868750 Loss 9.909073, Accuracy 62.578%\n",
      "Epoch 4, Batch 564, LR 0.869026 Loss 9.908281, Accuracy 62.591%\n",
      "Epoch 4, Batch 565, LR 0.869301 Loss 9.908968, Accuracy 62.587%\n",
      "Epoch 4, Batch 566, LR 0.869577 Loss 9.909482, Accuracy 62.587%\n",
      "Epoch 4, Batch 567, LR 0.869852 Loss 9.909400, Accuracy 62.591%\n",
      "Epoch 4, Batch 568, LR 0.870128 Loss 9.908726, Accuracy 62.598%\n",
      "Epoch 4, Batch 569, LR 0.870403 Loss 9.907534, Accuracy 62.615%\n",
      "Epoch 4, Batch 570, LR 0.870679 Loss 9.906799, Accuracy 62.623%\n",
      "Epoch 4, Batch 571, LR 0.870955 Loss 9.907344, Accuracy 62.622%\n",
      "Epoch 4, Batch 572, LR 0.871230 Loss 9.907233, Accuracy 62.628%\n",
      "Epoch 4, Batch 573, LR 0.871506 Loss 9.907317, Accuracy 62.634%\n",
      "Epoch 4, Batch 574, LR 0.871782 Loss 9.907080, Accuracy 62.637%\n",
      "Epoch 4, Batch 575, LR 0.872057 Loss 9.906417, Accuracy 62.647%\n",
      "Epoch 4, Batch 576, LR 0.872333 Loss 9.907450, Accuracy 62.637%\n",
      "Epoch 4, Batch 577, LR 0.872609 Loss 9.906672, Accuracy 62.642%\n",
      "Epoch 4, Batch 578, LR 0.872885 Loss 9.906065, Accuracy 62.654%\n",
      "Epoch 4, Batch 579, LR 0.873161 Loss 9.905526, Accuracy 62.657%\n",
      "Epoch 4, Batch 580, LR 0.873437 Loss 9.905025, Accuracy 62.656%\n",
      "Epoch 4, Batch 581, LR 0.873713 Loss 9.905069, Accuracy 62.661%\n",
      "Epoch 4, Batch 582, LR 0.873988 Loss 9.904801, Accuracy 62.673%\n",
      "Epoch 4, Batch 583, LR 0.874264 Loss 9.905544, Accuracy 62.666%\n",
      "Epoch 4, Batch 584, LR 0.874540 Loss 9.905240, Accuracy 62.670%\n",
      "Epoch 4, Batch 585, LR 0.874817 Loss 9.904204, Accuracy 62.672%\n",
      "Epoch 4, Batch 586, LR 0.875093 Loss 9.904543, Accuracy 62.669%\n",
      "Epoch 4, Batch 587, LR 0.875369 Loss 9.903858, Accuracy 62.677%\n",
      "Epoch 4, Batch 588, LR 0.875645 Loss 9.904768, Accuracy 62.667%\n",
      "Epoch 4, Batch 589, LR 0.875921 Loss 9.904834, Accuracy 62.667%\n",
      "Epoch 4, Batch 590, LR 0.876197 Loss 9.903764, Accuracy 62.683%\n",
      "Epoch 4, Batch 591, LR 0.876473 Loss 9.903529, Accuracy 62.693%\n",
      "Epoch 4, Batch 592, LR 0.876750 Loss 9.904022, Accuracy 62.689%\n",
      "Epoch 4, Batch 593, LR 0.877026 Loss 9.903014, Accuracy 62.706%\n",
      "Epoch 4, Batch 594, LR 0.877302 Loss 9.902067, Accuracy 62.713%\n",
      "Epoch 4, Batch 595, LR 0.877578 Loss 9.902412, Accuracy 62.710%\n",
      "Epoch 4, Batch 596, LR 0.877855 Loss 9.903605, Accuracy 62.701%\n",
      "Epoch 4, Batch 597, LR 0.878131 Loss 9.902912, Accuracy 62.705%\n",
      "Epoch 4, Batch 598, LR 0.878407 Loss 9.902154, Accuracy 62.710%\n",
      "Epoch 4, Batch 599, LR 0.878684 Loss 9.902495, Accuracy 62.710%\n",
      "Epoch 4, Batch 600, LR 0.878960 Loss 9.901760, Accuracy 62.714%\n",
      "Epoch 4, Batch 601, LR 0.879237 Loss 9.902176, Accuracy 62.718%\n",
      "Epoch 4, Batch 602, LR 0.879513 Loss 9.901852, Accuracy 62.723%\n",
      "Epoch 4, Batch 603, LR 0.879790 Loss 9.902611, Accuracy 62.716%\n",
      "Epoch 4, Batch 604, LR 0.880066 Loss 9.902743, Accuracy 62.712%\n",
      "Epoch 4, Batch 605, LR 0.880343 Loss 9.901641, Accuracy 62.720%\n",
      "Epoch 4, Batch 606, LR 0.880620 Loss 9.901239, Accuracy 62.719%\n",
      "Epoch 4, Batch 607, LR 0.880896 Loss 9.902108, Accuracy 62.716%\n",
      "Epoch 4, Batch 608, LR 0.881173 Loss 9.901985, Accuracy 62.721%\n",
      "Epoch 4, Batch 609, LR 0.881450 Loss 9.902473, Accuracy 62.719%\n",
      "Epoch 4, Batch 610, LR 0.881726 Loss 9.902097, Accuracy 62.725%\n",
      "Epoch 4, Batch 611, LR 0.882003 Loss 9.902625, Accuracy 62.728%\n",
      "Epoch 4, Batch 612, LR 0.882280 Loss 9.903420, Accuracy 62.714%\n",
      "Epoch 4, Batch 613, LR 0.882557 Loss 9.902973, Accuracy 62.724%\n",
      "Epoch 4, Batch 614, LR 0.882833 Loss 9.901760, Accuracy 62.742%\n",
      "Epoch 4, Batch 615, LR 0.883110 Loss 9.901998, Accuracy 62.743%\n",
      "Epoch 4, Batch 616, LR 0.883387 Loss 9.901586, Accuracy 62.749%\n",
      "Epoch 4, Batch 617, LR 0.883664 Loss 9.901299, Accuracy 62.757%\n",
      "Epoch 4, Batch 618, LR 0.883941 Loss 9.901652, Accuracy 62.754%\n",
      "Epoch 4, Batch 619, LR 0.884218 Loss 9.900504, Accuracy 62.759%\n",
      "Epoch 4, Batch 620, LR 0.884495 Loss 9.900410, Accuracy 62.757%\n",
      "Epoch 4, Batch 621, LR 0.884772 Loss 9.900430, Accuracy 62.758%\n",
      "Epoch 4, Batch 622, LR 0.885049 Loss 9.898587, Accuracy 62.771%\n",
      "Epoch 4, Batch 623, LR 0.885326 Loss 9.898204, Accuracy 62.778%\n",
      "Epoch 4, Batch 624, LR 0.885603 Loss 9.897740, Accuracy 62.774%\n",
      "Epoch 4, Batch 625, LR 0.885880 Loss 9.898605, Accuracy 62.769%\n",
      "Epoch 4, Batch 626, LR 0.886157 Loss 9.897880, Accuracy 62.782%\n",
      "Epoch 4, Batch 627, LR 0.886434 Loss 9.896810, Accuracy 62.792%\n",
      "Epoch 4, Batch 628, LR 0.886712 Loss 9.896032, Accuracy 62.800%\n",
      "Epoch 4, Batch 629, LR 0.886989 Loss 9.895592, Accuracy 62.812%\n",
      "Epoch 4, Batch 630, LR 0.887266 Loss 9.895435, Accuracy 62.816%\n",
      "Epoch 4, Batch 631, LR 0.887543 Loss 9.896041, Accuracy 62.814%\n",
      "Epoch 4, Batch 632, LR 0.887821 Loss 9.896795, Accuracy 62.809%\n",
      "Epoch 4, Batch 633, LR 0.888098 Loss 9.896983, Accuracy 62.804%\n",
      "Epoch 4, Batch 634, LR 0.888375 Loss 9.897129, Accuracy 62.808%\n",
      "Epoch 4, Batch 635, LR 0.888653 Loss 9.897040, Accuracy 62.814%\n",
      "Epoch 4, Batch 636, LR 0.888930 Loss 9.896217, Accuracy 62.826%\n",
      "Epoch 4, Batch 637, LR 0.889208 Loss 9.896104, Accuracy 62.824%\n",
      "Epoch 4, Batch 638, LR 0.889485 Loss 9.895018, Accuracy 62.837%\n",
      "Epoch 4, Batch 639, LR 0.889763 Loss 9.895192, Accuracy 62.833%\n",
      "Epoch 4, Batch 640, LR 0.890040 Loss 9.896722, Accuracy 62.816%\n",
      "Epoch 4, Batch 641, LR 0.890318 Loss 9.897108, Accuracy 62.812%\n",
      "Epoch 4, Batch 642, LR 0.890595 Loss 9.897752, Accuracy 62.809%\n",
      "Epoch 4, Batch 643, LR 0.890873 Loss 9.898339, Accuracy 62.809%\n",
      "Epoch 4, Batch 644, LR 0.891150 Loss 9.898999, Accuracy 62.801%\n",
      "Epoch 4, Batch 645, LR 0.891428 Loss 9.899332, Accuracy 62.802%\n",
      "Epoch 4, Batch 646, LR 0.891706 Loss 9.898569, Accuracy 62.801%\n",
      "Epoch 4, Batch 647, LR 0.891983 Loss 9.899258, Accuracy 62.787%\n",
      "Epoch 4, Batch 648, LR 0.892261 Loss 9.899523, Accuracy 62.787%\n",
      "Epoch 4, Batch 649, LR 0.892539 Loss 9.900536, Accuracy 62.780%\n",
      "Epoch 4, Batch 650, LR 0.892817 Loss 9.900458, Accuracy 62.782%\n",
      "Epoch 4, Batch 651, LR 0.893094 Loss 9.901605, Accuracy 62.771%\n",
      "Epoch 4, Batch 652, LR 0.893372 Loss 9.901226, Accuracy 62.771%\n",
      "Epoch 4, Batch 653, LR 0.893650 Loss 9.901601, Accuracy 62.768%\n",
      "Epoch 4, Batch 654, LR 0.893928 Loss 9.900991, Accuracy 62.774%\n",
      "Epoch 4, Batch 655, LR 0.894206 Loss 9.901330, Accuracy 62.767%\n",
      "Epoch 4, Batch 656, LR 0.894484 Loss 9.901176, Accuracy 62.766%\n",
      "Epoch 4, Batch 657, LR 0.894762 Loss 9.900394, Accuracy 62.766%\n",
      "Epoch 4, Batch 658, LR 0.895040 Loss 9.901486, Accuracy 62.760%\n",
      "Epoch 4, Batch 659, LR 0.895318 Loss 9.900530, Accuracy 62.769%\n",
      "Epoch 4, Batch 660, LR 0.895596 Loss 9.900638, Accuracy 62.771%\n",
      "Epoch 4, Batch 661, LR 0.895874 Loss 9.900617, Accuracy 62.772%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 662, LR 0.896152 Loss 9.899740, Accuracy 62.780%\n",
      "Epoch 4, Batch 663, LR 0.896430 Loss 9.898836, Accuracy 62.789%\n",
      "Epoch 4, Batch 664, LR 0.896708 Loss 9.899331, Accuracy 62.794%\n",
      "Epoch 4, Batch 665, LR 0.896986 Loss 9.899104, Accuracy 62.794%\n",
      "Epoch 4, Batch 666, LR 0.897265 Loss 9.898206, Accuracy 62.801%\n",
      "Epoch 4, Batch 667, LR 0.897543 Loss 9.897407, Accuracy 62.809%\n",
      "Epoch 4, Batch 668, LR 0.897821 Loss 9.897606, Accuracy 62.815%\n",
      "Epoch 4, Batch 669, LR 0.898099 Loss 9.898925, Accuracy 62.794%\n",
      "Epoch 4, Batch 670, LR 0.898378 Loss 9.898672, Accuracy 62.803%\n",
      "Epoch 4, Batch 671, LR 0.898656 Loss 9.898163, Accuracy 62.802%\n",
      "Epoch 4, Batch 672, LR 0.898934 Loss 9.898153, Accuracy 62.796%\n",
      "Epoch 4, Batch 673, LR 0.899213 Loss 9.897439, Accuracy 62.796%\n",
      "Epoch 4, Batch 674, LR 0.899491 Loss 9.897568, Accuracy 62.798%\n",
      "Epoch 4, Batch 675, LR 0.899770 Loss 9.897113, Accuracy 62.803%\n",
      "Epoch 4, Batch 676, LR 0.900048 Loss 9.896427, Accuracy 62.807%\n",
      "Epoch 4, Batch 677, LR 0.900327 Loss 9.895778, Accuracy 62.814%\n",
      "Epoch 4, Batch 678, LR 0.900605 Loss 9.894159, Accuracy 62.825%\n",
      "Epoch 4, Batch 679, LR 0.900884 Loss 9.893856, Accuracy 62.833%\n",
      "Epoch 4, Batch 680, LR 0.901162 Loss 9.893606, Accuracy 62.829%\n",
      "Epoch 4, Batch 681, LR 0.901441 Loss 9.892771, Accuracy 62.842%\n",
      "Epoch 4, Batch 682, LR 0.901719 Loss 9.892655, Accuracy 62.849%\n",
      "Epoch 4, Batch 683, LR 0.901998 Loss 9.892334, Accuracy 62.844%\n",
      "Epoch 4, Batch 684, LR 0.902277 Loss 9.891566, Accuracy 62.855%\n",
      "Epoch 4, Batch 685, LR 0.902555 Loss 9.892139, Accuracy 62.856%\n",
      "Epoch 4, Batch 686, LR 0.902834 Loss 9.891951, Accuracy 62.856%\n",
      "Epoch 4, Batch 687, LR 0.903113 Loss 9.892236, Accuracy 62.853%\n",
      "Epoch 4, Batch 688, LR 0.903392 Loss 9.890315, Accuracy 62.869%\n",
      "Epoch 4, Batch 689, LR 0.903670 Loss 9.890169, Accuracy 62.872%\n",
      "Epoch 4, Batch 690, LR 0.903949 Loss 9.889351, Accuracy 62.882%\n",
      "Epoch 4, Batch 691, LR 0.904228 Loss 9.890138, Accuracy 62.876%\n",
      "Epoch 4, Batch 692, LR 0.904507 Loss 9.891078, Accuracy 62.866%\n",
      "Epoch 4, Batch 693, LR 0.904786 Loss 9.889426, Accuracy 62.872%\n",
      "Epoch 4, Batch 694, LR 0.905065 Loss 9.889138, Accuracy 62.876%\n",
      "Epoch 4, Batch 695, LR 0.905344 Loss 9.889117, Accuracy 62.881%\n",
      "Epoch 4, Batch 696, LR 0.905623 Loss 9.889087, Accuracy 62.887%\n",
      "Epoch 4, Batch 697, LR 0.905902 Loss 9.888560, Accuracy 62.890%\n",
      "Epoch 4, Batch 698, LR 0.906181 Loss 9.888297, Accuracy 62.894%\n",
      "Epoch 4, Batch 699, LR 0.906460 Loss 9.888219, Accuracy 62.892%\n",
      "Epoch 4, Batch 700, LR 0.906739 Loss 9.888456, Accuracy 62.888%\n",
      "Epoch 4, Batch 701, LR 0.907018 Loss 9.887678, Accuracy 62.892%\n",
      "Epoch 4, Batch 702, LR 0.907297 Loss 9.888431, Accuracy 62.882%\n",
      "Epoch 4, Batch 703, LR 0.907576 Loss 9.887744, Accuracy 62.885%\n",
      "Epoch 4, Batch 704, LR 0.907855 Loss 9.887210, Accuracy 62.886%\n",
      "Epoch 4, Batch 705, LR 0.908134 Loss 9.886731, Accuracy 62.893%\n",
      "Epoch 4, Batch 706, LR 0.908414 Loss 9.886085, Accuracy 62.895%\n",
      "Epoch 4, Batch 707, LR 0.908693 Loss 9.885580, Accuracy 62.903%\n",
      "Epoch 4, Batch 708, LR 0.908972 Loss 9.885944, Accuracy 62.901%\n",
      "Epoch 4, Batch 709, LR 0.909251 Loss 9.886537, Accuracy 62.886%\n",
      "Epoch 4, Batch 710, LR 0.909531 Loss 9.887036, Accuracy 62.891%\n",
      "Epoch 4, Batch 711, LR 0.909810 Loss 9.886222, Accuracy 62.898%\n",
      "Epoch 4, Batch 712, LR 0.910089 Loss 9.886842, Accuracy 62.892%\n",
      "Epoch 4, Batch 713, LR 0.910369 Loss 9.886367, Accuracy 62.892%\n",
      "Epoch 4, Batch 714, LR 0.910648 Loss 9.886224, Accuracy 62.895%\n",
      "Epoch 4, Batch 715, LR 0.910928 Loss 9.886041, Accuracy 62.893%\n",
      "Epoch 4, Batch 716, LR 0.911207 Loss 9.885362, Accuracy 62.895%\n",
      "Epoch 4, Batch 717, LR 0.911487 Loss 9.885175, Accuracy 62.896%\n",
      "Epoch 4, Batch 718, LR 0.911766 Loss 9.884317, Accuracy 62.906%\n",
      "Epoch 4, Batch 719, LR 0.912046 Loss 9.883038, Accuracy 62.918%\n",
      "Epoch 4, Batch 720, LR 0.912325 Loss 9.882503, Accuracy 62.920%\n",
      "Epoch 4, Batch 721, LR 0.912605 Loss 9.883564, Accuracy 62.917%\n",
      "Epoch 4, Batch 722, LR 0.912885 Loss 9.884167, Accuracy 62.913%\n",
      "Epoch 4, Batch 723, LR 0.913164 Loss 9.883951, Accuracy 62.918%\n",
      "Epoch 4, Batch 724, LR 0.913444 Loss 9.884643, Accuracy 62.913%\n",
      "Epoch 4, Batch 725, LR 0.913724 Loss 9.883667, Accuracy 62.917%\n",
      "Epoch 4, Batch 726, LR 0.914003 Loss 9.883388, Accuracy 62.922%\n",
      "Epoch 4, Batch 727, LR 0.914283 Loss 9.883294, Accuracy 62.923%\n",
      "Epoch 4, Batch 728, LR 0.914563 Loss 9.883018, Accuracy 62.924%\n",
      "Epoch 4, Batch 729, LR 0.914843 Loss 9.882329, Accuracy 62.937%\n",
      "Epoch 4, Batch 730, LR 0.915122 Loss 9.883609, Accuracy 62.928%\n",
      "Epoch 4, Batch 731, LR 0.915402 Loss 9.883520, Accuracy 62.932%\n",
      "Epoch 4, Batch 732, LR 0.915682 Loss 9.883095, Accuracy 62.935%\n",
      "Epoch 4, Batch 733, LR 0.915962 Loss 9.883818, Accuracy 62.932%\n",
      "Epoch 4, Batch 734, LR 0.916242 Loss 9.884304, Accuracy 62.926%\n",
      "Epoch 4, Batch 735, LR 0.916522 Loss 9.883740, Accuracy 62.927%\n",
      "Epoch 4, Batch 736, LR 0.916802 Loss 9.883583, Accuracy 62.931%\n",
      "Epoch 4, Batch 737, LR 0.917082 Loss 9.883693, Accuracy 62.934%\n",
      "Epoch 4, Batch 738, LR 0.917362 Loss 9.882806, Accuracy 62.935%\n",
      "Epoch 4, Batch 739, LR 0.917642 Loss 9.882418, Accuracy 62.936%\n",
      "Epoch 4, Batch 740, LR 0.917922 Loss 9.882019, Accuracy 62.938%\n",
      "Epoch 4, Batch 741, LR 0.918202 Loss 9.881569, Accuracy 62.935%\n",
      "Epoch 4, Batch 742, LR 0.918482 Loss 9.880976, Accuracy 62.938%\n",
      "Epoch 4, Batch 743, LR 0.918762 Loss 9.881255, Accuracy 62.943%\n",
      "Epoch 4, Batch 744, LR 0.919043 Loss 9.881292, Accuracy 62.945%\n",
      "Epoch 4, Batch 745, LR 0.919323 Loss 9.881565, Accuracy 62.948%\n",
      "Epoch 4, Batch 746, LR 0.919603 Loss 9.881377, Accuracy 62.947%\n",
      "Epoch 4, Batch 747, LR 0.919883 Loss 9.881110, Accuracy 62.944%\n",
      "Epoch 4, Batch 748, LR 0.920163 Loss 9.881597, Accuracy 62.943%\n",
      "Epoch 4, Batch 749, LR 0.920444 Loss 9.881767, Accuracy 62.942%\n",
      "Epoch 4, Batch 750, LR 0.920724 Loss 9.881547, Accuracy 62.943%\n",
      "Epoch 4, Batch 751, LR 0.921004 Loss 9.881245, Accuracy 62.943%\n",
      "Epoch 4, Batch 752, LR 0.921285 Loss 9.880142, Accuracy 62.952%\n",
      "Epoch 4, Batch 753, LR 0.921565 Loss 9.880021, Accuracy 62.954%\n",
      "Epoch 4, Batch 754, LR 0.921846 Loss 9.879637, Accuracy 62.962%\n",
      "Epoch 4, Batch 755, LR 0.922126 Loss 9.880048, Accuracy 62.959%\n",
      "Epoch 4, Batch 756, LR 0.922406 Loss 9.879545, Accuracy 62.963%\n",
      "Epoch 4, Batch 757, LR 0.922687 Loss 9.878702, Accuracy 62.971%\n",
      "Epoch 4, Batch 758, LR 0.922967 Loss 9.878736, Accuracy 62.969%\n",
      "Epoch 4, Batch 759, LR 0.923248 Loss 9.879401, Accuracy 62.964%\n",
      "Epoch 4, Batch 760, LR 0.923529 Loss 9.879532, Accuracy 62.963%\n",
      "Epoch 4, Batch 761, LR 0.923809 Loss 9.879362, Accuracy 62.962%\n",
      "Epoch 4, Batch 762, LR 0.924090 Loss 9.878441, Accuracy 62.963%\n",
      "Epoch 4, Batch 763, LR 0.924370 Loss 9.877443, Accuracy 62.970%\n",
      "Epoch 4, Batch 764, LR 0.924651 Loss 9.876837, Accuracy 62.969%\n",
      "Epoch 4, Batch 765, LR 0.924932 Loss 9.877227, Accuracy 62.966%\n",
      "Epoch 4, Batch 766, LR 0.925212 Loss 9.877567, Accuracy 62.961%\n",
      "Epoch 4, Batch 767, LR 0.925493 Loss 9.877179, Accuracy 62.964%\n",
      "Epoch 4, Batch 768, LR 0.925774 Loss 9.876494, Accuracy 62.967%\n",
      "Epoch 4, Batch 769, LR 0.926055 Loss 9.876115, Accuracy 62.970%\n",
      "Epoch 4, Batch 770, LR 0.926336 Loss 9.875630, Accuracy 62.974%\n",
      "Epoch 4, Batch 771, LR 0.926616 Loss 9.875218, Accuracy 62.972%\n",
      "Epoch 4, Batch 772, LR 0.926897 Loss 9.874784, Accuracy 62.973%\n",
      "Epoch 4, Batch 773, LR 0.927178 Loss 9.873802, Accuracy 62.978%\n",
      "Epoch 4, Batch 774, LR 0.927459 Loss 9.873382, Accuracy 62.976%\n",
      "Epoch 4, Batch 775, LR 0.927740 Loss 9.872909, Accuracy 62.981%\n",
      "Epoch 4, Batch 776, LR 0.928021 Loss 9.872497, Accuracy 62.987%\n",
      "Epoch 4, Batch 777, LR 0.928302 Loss 9.873041, Accuracy 62.979%\n",
      "Epoch 4, Batch 778, LR 0.928583 Loss 9.872446, Accuracy 62.986%\n",
      "Epoch 4, Batch 779, LR 0.928864 Loss 9.871517, Accuracy 62.997%\n",
      "Epoch 4, Batch 780, LR 0.929145 Loss 9.870084, Accuracy 63.013%\n",
      "Epoch 4, Batch 781, LR 0.929426 Loss 9.869209, Accuracy 63.019%\n",
      "Epoch 4, Batch 782, LR 0.929707 Loss 9.868726, Accuracy 63.028%\n",
      "Epoch 4, Batch 783, LR 0.929988 Loss 9.867978, Accuracy 63.038%\n",
      "Epoch 4, Batch 784, LR 0.930269 Loss 9.867331, Accuracy 63.046%\n",
      "Epoch 4, Batch 785, LR 0.930550 Loss 9.867954, Accuracy 63.040%\n",
      "Epoch 4, Batch 786, LR 0.930832 Loss 9.867963, Accuracy 63.042%\n",
      "Epoch 4, Batch 787, LR 0.931113 Loss 9.867118, Accuracy 63.051%\n",
      "Epoch 4, Batch 788, LR 0.931394 Loss 9.867053, Accuracy 63.046%\n",
      "Epoch 4, Batch 789, LR 0.931675 Loss 9.866465, Accuracy 63.052%\n",
      "Epoch 4, Batch 790, LR 0.931957 Loss 9.866130, Accuracy 63.057%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 791, LR 0.932238 Loss 9.866447, Accuracy 63.050%\n",
      "Epoch 4, Batch 792, LR 0.932519 Loss 9.866264, Accuracy 63.049%\n",
      "Epoch 4, Batch 793, LR 0.932801 Loss 9.865787, Accuracy 63.049%\n",
      "Epoch 4, Batch 794, LR 0.933082 Loss 9.864567, Accuracy 63.062%\n",
      "Epoch 4, Batch 795, LR 0.933363 Loss 9.864063, Accuracy 63.061%\n",
      "Epoch 4, Batch 796, LR 0.933645 Loss 9.864965, Accuracy 63.056%\n",
      "Epoch 4, Batch 797, LR 0.933926 Loss 9.864748, Accuracy 63.059%\n",
      "Epoch 4, Batch 798, LR 0.934208 Loss 9.863871, Accuracy 63.064%\n",
      "Epoch 4, Batch 799, LR 0.934489 Loss 9.864553, Accuracy 63.059%\n",
      "Epoch 4, Batch 800, LR 0.934771 Loss 9.866004, Accuracy 63.048%\n",
      "Epoch 4, Batch 801, LR 0.935052 Loss 9.865818, Accuracy 63.047%\n",
      "Epoch 4, Batch 802, LR 0.935334 Loss 9.865217, Accuracy 63.050%\n",
      "Epoch 4, Batch 803, LR 0.935615 Loss 9.865478, Accuracy 63.041%\n",
      "Epoch 4, Batch 804, LR 0.935897 Loss 9.864873, Accuracy 63.051%\n",
      "Epoch 4, Batch 805, LR 0.936179 Loss 9.864769, Accuracy 63.053%\n",
      "Epoch 4, Batch 806, LR 0.936460 Loss 9.864773, Accuracy 63.051%\n",
      "Epoch 4, Batch 807, LR 0.936742 Loss 9.864590, Accuracy 63.057%\n",
      "Epoch 4, Batch 808, LR 0.937024 Loss 9.864371, Accuracy 63.061%\n",
      "Epoch 4, Batch 809, LR 0.937305 Loss 9.864945, Accuracy 63.048%\n",
      "Epoch 4, Batch 810, LR 0.937587 Loss 9.864432, Accuracy 63.053%\n",
      "Epoch 4, Batch 811, LR 0.937869 Loss 9.863814, Accuracy 63.056%\n",
      "Epoch 4, Batch 812, LR 0.938151 Loss 9.863401, Accuracy 63.061%\n",
      "Epoch 4, Batch 813, LR 0.938433 Loss 9.862086, Accuracy 63.070%\n",
      "Epoch 4, Batch 814, LR 0.938715 Loss 9.861729, Accuracy 63.072%\n",
      "Epoch 4, Batch 815, LR 0.938996 Loss 9.861241, Accuracy 63.072%\n",
      "Epoch 4, Batch 816, LR 0.939278 Loss 9.861425, Accuracy 63.063%\n",
      "Epoch 4, Batch 817, LR 0.939560 Loss 9.861179, Accuracy 63.065%\n",
      "Epoch 4, Batch 818, LR 0.939842 Loss 9.860357, Accuracy 63.075%\n",
      "Epoch 4, Batch 819, LR 0.940124 Loss 9.860207, Accuracy 63.072%\n",
      "Epoch 4, Batch 820, LR 0.940406 Loss 9.859398, Accuracy 63.080%\n",
      "Epoch 4, Batch 821, LR 0.940688 Loss 9.858658, Accuracy 63.085%\n",
      "Epoch 4, Batch 822, LR 0.940970 Loss 9.858583, Accuracy 63.079%\n",
      "Epoch 4, Batch 823, LR 0.941252 Loss 9.859069, Accuracy 63.071%\n",
      "Epoch 4, Batch 824, LR 0.941534 Loss 9.858912, Accuracy 63.075%\n",
      "Epoch 4, Batch 825, LR 0.941816 Loss 9.858815, Accuracy 63.080%\n",
      "Epoch 4, Batch 826, LR 0.942099 Loss 9.858020, Accuracy 63.086%\n",
      "Epoch 4, Batch 827, LR 0.942381 Loss 9.857529, Accuracy 63.088%\n",
      "Epoch 4, Batch 828, LR 0.942663 Loss 9.857128, Accuracy 63.091%\n",
      "Epoch 4, Batch 829, LR 0.942945 Loss 9.856954, Accuracy 63.096%\n",
      "Epoch 4, Batch 830, LR 0.943227 Loss 9.855990, Accuracy 63.105%\n",
      "Epoch 4, Batch 831, LR 0.943509 Loss 9.856041, Accuracy 63.105%\n",
      "Epoch 4, Batch 832, LR 0.943792 Loss 9.855424, Accuracy 63.108%\n",
      "Epoch 4, Batch 833, LR 0.944074 Loss 9.855282, Accuracy 63.101%\n",
      "Epoch 4, Batch 834, LR 0.944356 Loss 9.855128, Accuracy 63.106%\n",
      "Epoch 4, Batch 835, LR 0.944639 Loss 9.855058, Accuracy 63.103%\n",
      "Epoch 4, Batch 836, LR 0.944921 Loss 9.855188, Accuracy 63.103%\n",
      "Epoch 4, Batch 837, LR 0.945203 Loss 9.854638, Accuracy 63.107%\n",
      "Epoch 4, Batch 838, LR 0.945486 Loss 9.854653, Accuracy 63.106%\n",
      "Epoch 4, Batch 839, LR 0.945768 Loss 9.854474, Accuracy 63.110%\n",
      "Epoch 4, Batch 840, LR 0.946051 Loss 9.854213, Accuracy 63.114%\n",
      "Epoch 4, Batch 841, LR 0.946333 Loss 9.854351, Accuracy 63.110%\n",
      "Epoch 4, Batch 842, LR 0.946616 Loss 9.854238, Accuracy 63.109%\n",
      "Epoch 4, Batch 843, LR 0.946898 Loss 9.853766, Accuracy 63.113%\n",
      "Epoch 4, Batch 844, LR 0.947181 Loss 9.854132, Accuracy 63.109%\n",
      "Epoch 4, Batch 845, LR 0.947463 Loss 9.854534, Accuracy 63.103%\n",
      "Epoch 4, Batch 846, LR 0.947746 Loss 9.854357, Accuracy 63.106%\n",
      "Epoch 4, Batch 847, LR 0.948029 Loss 9.854513, Accuracy 63.100%\n",
      "Epoch 4, Batch 848, LR 0.948311 Loss 9.854636, Accuracy 63.097%\n",
      "Epoch 4, Batch 849, LR 0.948594 Loss 9.855309, Accuracy 63.088%\n",
      "Epoch 4, Batch 850, LR 0.948876 Loss 9.855527, Accuracy 63.087%\n",
      "Epoch 4, Batch 851, LR 0.949159 Loss 9.855459, Accuracy 63.088%\n",
      "Epoch 4, Batch 852, LR 0.949442 Loss 9.856054, Accuracy 63.074%\n",
      "Epoch 4, Batch 853, LR 0.949725 Loss 9.855852, Accuracy 63.074%\n",
      "Epoch 4, Batch 854, LR 0.950007 Loss 9.855552, Accuracy 63.079%\n",
      "Epoch 4, Batch 855, LR 0.950290 Loss 9.855294, Accuracy 63.077%\n",
      "Epoch 4, Batch 856, LR 0.950573 Loss 9.855539, Accuracy 63.076%\n",
      "Epoch 4, Batch 857, LR 0.950856 Loss 9.855939, Accuracy 63.075%\n",
      "Epoch 4, Batch 858, LR 0.951139 Loss 9.855158, Accuracy 63.079%\n",
      "Epoch 4, Batch 859, LR 0.951422 Loss 9.855114, Accuracy 63.082%\n",
      "Epoch 4, Batch 860, LR 0.951705 Loss 9.855096, Accuracy 63.086%\n",
      "Epoch 4, Batch 861, LR 0.951987 Loss 9.855654, Accuracy 63.085%\n",
      "Epoch 4, Batch 862, LR 0.952270 Loss 9.854741, Accuracy 63.095%\n",
      "Epoch 4, Batch 863, LR 0.952553 Loss 9.854621, Accuracy 63.097%\n",
      "Epoch 4, Batch 864, LR 0.952836 Loss 9.854879, Accuracy 63.099%\n",
      "Epoch 4, Batch 865, LR 0.953119 Loss 9.854433, Accuracy 63.102%\n",
      "Epoch 4, Batch 866, LR 0.953402 Loss 9.853689, Accuracy 63.109%\n",
      "Epoch 4, Batch 867, LR 0.953685 Loss 9.853646, Accuracy 63.114%\n",
      "Epoch 4, Batch 868, LR 0.953969 Loss 9.853393, Accuracy 63.117%\n",
      "Epoch 4, Batch 869, LR 0.954252 Loss 9.853781, Accuracy 63.116%\n",
      "Epoch 4, Batch 870, LR 0.954535 Loss 9.853945, Accuracy 63.110%\n",
      "Epoch 4, Batch 871, LR 0.954818 Loss 9.852964, Accuracy 63.116%\n",
      "Epoch 4, Batch 872, LR 0.955101 Loss 9.853247, Accuracy 63.114%\n",
      "Epoch 4, Batch 873, LR 0.955384 Loss 9.852703, Accuracy 63.117%\n",
      "Epoch 4, Batch 874, LR 0.955668 Loss 9.853399, Accuracy 63.113%\n",
      "Epoch 4, Batch 875, LR 0.955951 Loss 9.853414, Accuracy 63.114%\n",
      "Epoch 4, Batch 876, LR 0.956234 Loss 9.853240, Accuracy 63.117%\n",
      "Epoch 4, Batch 877, LR 0.956517 Loss 9.852678, Accuracy 63.121%\n",
      "Epoch 4, Batch 878, LR 0.956801 Loss 9.852051, Accuracy 63.124%\n",
      "Epoch 4, Batch 879, LR 0.957084 Loss 9.851995, Accuracy 63.127%\n",
      "Epoch 4, Batch 880, LR 0.957367 Loss 9.851858, Accuracy 63.128%\n",
      "Epoch 4, Batch 881, LR 0.957651 Loss 9.851566, Accuracy 63.124%\n",
      "Epoch 4, Batch 882, LR 0.957934 Loss 9.850716, Accuracy 63.133%\n",
      "Epoch 4, Batch 883, LR 0.958218 Loss 9.850730, Accuracy 63.126%\n",
      "Epoch 4, Batch 884, LR 0.958501 Loss 9.850495, Accuracy 63.129%\n",
      "Epoch 4, Batch 885, LR 0.958784 Loss 9.850665, Accuracy 63.132%\n",
      "Epoch 4, Batch 886, LR 0.959068 Loss 9.849291, Accuracy 63.137%\n",
      "Epoch 4, Batch 887, LR 0.959351 Loss 9.849548, Accuracy 63.138%\n",
      "Epoch 4, Batch 888, LR 0.959635 Loss 9.849211, Accuracy 63.141%\n",
      "Epoch 4, Batch 889, LR 0.959919 Loss 9.848847, Accuracy 63.148%\n",
      "Epoch 4, Batch 890, LR 0.960202 Loss 9.848649, Accuracy 63.153%\n",
      "Epoch 4, Batch 891, LR 0.960486 Loss 9.848853, Accuracy 63.158%\n",
      "Epoch 4, Batch 892, LR 0.960769 Loss 9.848994, Accuracy 63.157%\n",
      "Epoch 4, Batch 893, LR 0.961053 Loss 9.847899, Accuracy 63.166%\n",
      "Epoch 4, Batch 894, LR 0.961337 Loss 9.847492, Accuracy 63.170%\n",
      "Epoch 4, Batch 895, LR 0.961620 Loss 9.846897, Accuracy 63.181%\n",
      "Epoch 4, Batch 896, LR 0.961904 Loss 9.846887, Accuracy 63.184%\n",
      "Epoch 4, Batch 897, LR 0.962188 Loss 9.846257, Accuracy 63.189%\n",
      "Epoch 4, Batch 898, LR 0.962471 Loss 9.845685, Accuracy 63.193%\n",
      "Epoch 4, Batch 899, LR 0.962755 Loss 9.845120, Accuracy 63.198%\n",
      "Epoch 4, Batch 900, LR 0.963039 Loss 9.844895, Accuracy 63.198%\n",
      "Epoch 4, Batch 901, LR 0.963323 Loss 9.845327, Accuracy 63.198%\n",
      "Epoch 4, Batch 902, LR 0.963607 Loss 9.844908, Accuracy 63.200%\n",
      "Epoch 4, Batch 903, LR 0.963891 Loss 9.845235, Accuracy 63.195%\n",
      "Epoch 4, Batch 904, LR 0.964174 Loss 9.845402, Accuracy 63.191%\n",
      "Epoch 4, Batch 905, LR 0.964458 Loss 9.845135, Accuracy 63.189%\n",
      "Epoch 4, Batch 906, LR 0.964742 Loss 9.845179, Accuracy 63.186%\n",
      "Epoch 4, Batch 907, LR 0.965026 Loss 9.845771, Accuracy 63.179%\n",
      "Epoch 4, Batch 908, LR 0.965310 Loss 9.846191, Accuracy 63.179%\n",
      "Epoch 4, Batch 909, LR 0.965594 Loss 9.846051, Accuracy 63.176%\n",
      "Epoch 4, Batch 910, LR 0.965878 Loss 9.845229, Accuracy 63.183%\n",
      "Epoch 4, Batch 911, LR 0.966162 Loss 9.845583, Accuracy 63.178%\n",
      "Epoch 4, Batch 912, LR 0.966446 Loss 9.844935, Accuracy 63.187%\n",
      "Epoch 4, Batch 913, LR 0.966730 Loss 9.844788, Accuracy 63.189%\n",
      "Epoch 4, Batch 914, LR 0.967014 Loss 9.844277, Accuracy 63.194%\n",
      "Epoch 4, Batch 915, LR 0.967299 Loss 9.843794, Accuracy 63.194%\n",
      "Epoch 4, Batch 916, LR 0.967583 Loss 9.844530, Accuracy 63.188%\n",
      "Epoch 4, Batch 917, LR 0.967867 Loss 9.843767, Accuracy 63.196%\n",
      "Epoch 4, Batch 918, LR 0.968151 Loss 9.843323, Accuracy 63.199%\n",
      "Epoch 4, Batch 919, LR 0.968435 Loss 9.843539, Accuracy 63.198%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 920, LR 0.968719 Loss 9.843435, Accuracy 63.201%\n",
      "Epoch 4, Batch 921, LR 0.969004 Loss 9.844004, Accuracy 63.196%\n",
      "Epoch 4, Batch 922, LR 0.969288 Loss 9.843572, Accuracy 63.201%\n",
      "Epoch 4, Batch 923, LR 0.969572 Loss 9.842882, Accuracy 63.210%\n",
      "Epoch 4, Batch 924, LR 0.969857 Loss 9.843300, Accuracy 63.208%\n",
      "Epoch 4, Batch 925, LR 0.970141 Loss 9.843034, Accuracy 63.208%\n",
      "Epoch 4, Batch 926, LR 0.970425 Loss 9.842581, Accuracy 63.213%\n",
      "Epoch 4, Batch 927, LR 0.970710 Loss 9.842046, Accuracy 63.210%\n",
      "Epoch 4, Batch 928, LR 0.970994 Loss 9.841894, Accuracy 63.212%\n",
      "Epoch 4, Batch 929, LR 0.971279 Loss 9.841314, Accuracy 63.214%\n",
      "Epoch 4, Batch 930, LR 0.971563 Loss 9.841397, Accuracy 63.212%\n",
      "Epoch 4, Batch 931, LR 0.971847 Loss 9.840648, Accuracy 63.217%\n",
      "Epoch 4, Batch 932, LR 0.972132 Loss 9.840174, Accuracy 63.221%\n",
      "Epoch 4, Batch 933, LR 0.972416 Loss 9.840336, Accuracy 63.219%\n",
      "Epoch 4, Batch 934, LR 0.972701 Loss 9.840715, Accuracy 63.217%\n",
      "Epoch 4, Batch 935, LR 0.972985 Loss 9.840855, Accuracy 63.219%\n",
      "Epoch 4, Batch 936, LR 0.973270 Loss 9.841222, Accuracy 63.213%\n",
      "Epoch 4, Batch 937, LR 0.973555 Loss 9.840970, Accuracy 63.217%\n",
      "Epoch 4, Batch 938, LR 0.973839 Loss 9.840672, Accuracy 63.216%\n",
      "Epoch 4, Batch 939, LR 0.974124 Loss 9.840694, Accuracy 63.211%\n",
      "Epoch 4, Batch 940, LR 0.974409 Loss 9.840063, Accuracy 63.217%\n",
      "Epoch 4, Batch 941, LR 0.974693 Loss 9.839938, Accuracy 63.220%\n",
      "Epoch 4, Batch 942, LR 0.974978 Loss 9.840479, Accuracy 63.220%\n",
      "Epoch 4, Batch 943, LR 0.975263 Loss 9.840206, Accuracy 63.222%\n",
      "Epoch 4, Batch 944, LR 0.975547 Loss 9.840553, Accuracy 63.218%\n",
      "Epoch 4, Batch 945, LR 0.975832 Loss 9.839811, Accuracy 63.227%\n",
      "Epoch 4, Batch 946, LR 0.976117 Loss 9.839665, Accuracy 63.228%\n",
      "Epoch 4, Batch 947, LR 0.976402 Loss 9.839741, Accuracy 63.232%\n",
      "Epoch 4, Batch 948, LR 0.976687 Loss 9.840049, Accuracy 63.230%\n",
      "Epoch 4, Batch 949, LR 0.976971 Loss 9.840250, Accuracy 63.231%\n",
      "Epoch 4, Batch 950, LR 0.977256 Loss 9.840346, Accuracy 63.227%\n",
      "Epoch 4, Batch 951, LR 0.977541 Loss 9.839813, Accuracy 63.231%\n",
      "Epoch 4, Batch 952, LR 0.977826 Loss 9.840130, Accuracy 63.227%\n",
      "Epoch 4, Batch 953, LR 0.978111 Loss 9.839544, Accuracy 63.232%\n",
      "Epoch 4, Batch 954, LR 0.978396 Loss 9.838848, Accuracy 63.239%\n",
      "Epoch 4, Batch 955, LR 0.978681 Loss 9.838491, Accuracy 63.244%\n",
      "Epoch 4, Batch 956, LR 0.978966 Loss 9.838145, Accuracy 63.250%\n",
      "Epoch 4, Batch 957, LR 0.979251 Loss 9.839033, Accuracy 63.242%\n",
      "Epoch 4, Batch 958, LR 0.979536 Loss 9.839040, Accuracy 63.241%\n",
      "Epoch 4, Batch 959, LR 0.979821 Loss 9.839273, Accuracy 63.245%\n",
      "Epoch 4, Batch 960, LR 0.980106 Loss 9.839712, Accuracy 63.245%\n",
      "Epoch 4, Batch 961, LR 0.980391 Loss 9.840057, Accuracy 63.246%\n",
      "Epoch 4, Batch 962, LR 0.980676 Loss 9.840092, Accuracy 63.249%\n",
      "Epoch 4, Batch 963, LR 0.980961 Loss 9.839665, Accuracy 63.249%\n",
      "Epoch 4, Batch 964, LR 0.981247 Loss 9.838770, Accuracy 63.253%\n",
      "Epoch 4, Batch 965, LR 0.981532 Loss 9.837935, Accuracy 63.263%\n",
      "Epoch 4, Batch 966, LR 0.981817 Loss 9.836473, Accuracy 63.269%\n",
      "Epoch 4, Batch 967, LR 0.982102 Loss 9.836332, Accuracy 63.270%\n",
      "Epoch 4, Batch 968, LR 0.982387 Loss 9.835972, Accuracy 63.272%\n",
      "Epoch 4, Batch 969, LR 0.982673 Loss 9.835895, Accuracy 63.271%\n",
      "Epoch 4, Batch 970, LR 0.982958 Loss 9.835798, Accuracy 63.270%\n",
      "Epoch 4, Batch 971, LR 0.983243 Loss 9.835409, Accuracy 63.272%\n",
      "Epoch 4, Batch 972, LR 0.983529 Loss 9.834752, Accuracy 63.279%\n",
      "Epoch 4, Batch 973, LR 0.983814 Loss 9.834800, Accuracy 63.280%\n",
      "Epoch 4, Batch 974, LR 0.984099 Loss 9.834541, Accuracy 63.278%\n",
      "Epoch 4, Batch 975, LR 0.984385 Loss 9.834484, Accuracy 63.275%\n",
      "Epoch 4, Batch 976, LR 0.984670 Loss 9.833951, Accuracy 63.276%\n",
      "Epoch 4, Batch 977, LR 0.984955 Loss 9.834058, Accuracy 63.273%\n",
      "Epoch 4, Batch 978, LR 0.985241 Loss 9.833638, Accuracy 63.275%\n",
      "Epoch 4, Batch 979, LR 0.985526 Loss 9.833507, Accuracy 63.276%\n",
      "Epoch 4, Batch 980, LR 0.985812 Loss 9.833677, Accuracy 63.277%\n",
      "Epoch 4, Batch 981, LR 0.986097 Loss 9.833448, Accuracy 63.272%\n",
      "Epoch 4, Batch 982, LR 0.986383 Loss 9.834030, Accuracy 63.271%\n",
      "Epoch 4, Batch 983, LR 0.986668 Loss 9.833613, Accuracy 63.276%\n",
      "Epoch 4, Batch 984, LR 0.986954 Loss 9.833356, Accuracy 63.274%\n",
      "Epoch 4, Batch 985, LR 0.987240 Loss 9.833825, Accuracy 63.272%\n",
      "Epoch 4, Batch 986, LR 0.987525 Loss 9.834764, Accuracy 63.265%\n",
      "Epoch 4, Batch 987, LR 0.987811 Loss 9.835350, Accuracy 63.258%\n",
      "Epoch 4, Batch 988, LR 0.988096 Loss 9.834468, Accuracy 63.264%\n",
      "Epoch 4, Batch 989, LR 0.988382 Loss 9.834398, Accuracy 63.265%\n",
      "Epoch 4, Batch 990, LR 0.988668 Loss 9.834389, Accuracy 63.269%\n",
      "Epoch 4, Batch 991, LR 0.988954 Loss 9.834009, Accuracy 63.270%\n",
      "Epoch 4, Batch 992, LR 0.989239 Loss 9.834464, Accuracy 63.265%\n",
      "Epoch 4, Batch 993, LR 0.989525 Loss 9.834377, Accuracy 63.266%\n",
      "Epoch 4, Batch 994, LR 0.989811 Loss 9.833620, Accuracy 63.269%\n",
      "Epoch 4, Batch 995, LR 0.990097 Loss 9.833936, Accuracy 63.269%\n",
      "Epoch 4, Batch 996, LR 0.990382 Loss 9.834232, Accuracy 63.263%\n",
      "Epoch 4, Batch 997, LR 0.990668 Loss 9.833768, Accuracy 63.270%\n",
      "Epoch 4, Batch 998, LR 0.990954 Loss 9.833175, Accuracy 63.270%\n",
      "Epoch 4, Batch 999, LR 0.991240 Loss 9.833271, Accuracy 63.266%\n",
      "Epoch 4, Batch 1000, LR 0.991526 Loss 9.831867, Accuracy 63.280%\n",
      "Epoch 4, Batch 1001, LR 0.991812 Loss 9.831740, Accuracy 63.281%\n",
      "Epoch 4, Batch 1002, LR 0.992098 Loss 9.831296, Accuracy 63.285%\n",
      "Epoch 4, Batch 1003, LR 0.992384 Loss 9.830914, Accuracy 63.286%\n",
      "Epoch 4, Batch 1004, LR 0.992670 Loss 9.830821, Accuracy 63.285%\n",
      "Epoch 4, Batch 1005, LR 0.992956 Loss 9.830251, Accuracy 63.293%\n",
      "Epoch 4, Batch 1006, LR 0.993242 Loss 9.830166, Accuracy 63.292%\n",
      "Epoch 4, Batch 1007, LR 0.993528 Loss 9.829918, Accuracy 63.292%\n",
      "Epoch 4, Batch 1008, LR 0.993814 Loss 9.829832, Accuracy 63.294%\n",
      "Epoch 4, Batch 1009, LR 0.994100 Loss 9.829156, Accuracy 63.300%\n",
      "Epoch 4, Batch 1010, LR 0.994386 Loss 9.828342, Accuracy 63.305%\n",
      "Epoch 4, Batch 1011, LR 0.994672 Loss 9.827820, Accuracy 63.308%\n",
      "Epoch 4, Batch 1012, LR 0.994958 Loss 9.827903, Accuracy 63.311%\n",
      "Epoch 4, Batch 1013, LR 0.995244 Loss 9.827774, Accuracy 63.313%\n",
      "Epoch 4, Batch 1014, LR 0.995530 Loss 9.827644, Accuracy 63.315%\n",
      "Epoch 4, Batch 1015, LR 0.995817 Loss 9.827646, Accuracy 63.316%\n",
      "Epoch 4, Batch 1016, LR 0.996103 Loss 9.827008, Accuracy 63.324%\n",
      "Epoch 4, Batch 1017, LR 0.996389 Loss 9.826660, Accuracy 63.328%\n",
      "Epoch 4, Batch 1018, LR 0.996675 Loss 9.826918, Accuracy 63.328%\n",
      "Epoch 4, Batch 1019, LR 0.996962 Loss 9.826304, Accuracy 63.331%\n",
      "Epoch 4, Batch 1020, LR 0.997248 Loss 9.826104, Accuracy 63.333%\n",
      "Epoch 4, Batch 1021, LR 0.997534 Loss 9.825687, Accuracy 63.333%\n",
      "Epoch 4, Batch 1022, LR 0.997820 Loss 9.825406, Accuracy 63.337%\n",
      "Epoch 4, Batch 1023, LR 0.998107 Loss 9.825339, Accuracy 63.338%\n",
      "Epoch 4, Batch 1024, LR 0.998393 Loss 9.825377, Accuracy 63.335%\n",
      "Epoch 4, Batch 1025, LR 0.998680 Loss 9.824727, Accuracy 63.339%\n",
      "Epoch 4, Batch 1026, LR 0.998966 Loss 9.823656, Accuracy 63.347%\n",
      "Epoch 4, Batch 1027, LR 0.999252 Loss 9.823774, Accuracy 63.344%\n",
      "Epoch 4, Batch 1028, LR 0.999539 Loss 9.823002, Accuracy 63.348%\n",
      "Epoch 4, Batch 1029, LR 0.999825 Loss 9.823060, Accuracy 63.349%\n",
      "Epoch 4, Batch 1030, LR 1.000112 Loss 9.822353, Accuracy 63.358%\n",
      "Epoch 4, Batch 1031, LR 1.000398 Loss 9.822462, Accuracy 63.357%\n",
      "Epoch 4, Batch 1032, LR 1.000685 Loss 9.821996, Accuracy 63.360%\n",
      "Epoch 4, Batch 1033, LR 1.000971 Loss 9.821348, Accuracy 63.361%\n",
      "Epoch 4, Batch 1034, LR 1.001258 Loss 9.820985, Accuracy 63.364%\n",
      "Epoch 4, Batch 1035, LR 1.001545 Loss 9.820883, Accuracy 63.366%\n",
      "Epoch 4, Batch 1036, LR 1.001831 Loss 9.820512, Accuracy 63.372%\n",
      "Epoch 4, Batch 1037, LR 1.002118 Loss 9.820805, Accuracy 63.369%\n",
      "Epoch 4, Batch 1038, LR 1.002404 Loss 9.820789, Accuracy 63.370%\n",
      "Epoch 4, Batch 1039, LR 1.002691 Loss 9.820406, Accuracy 63.374%\n",
      "Epoch 4, Batch 1040, LR 1.002978 Loss 9.820281, Accuracy 63.377%\n",
      "Epoch 4, Batch 1041, LR 1.003264 Loss 9.820166, Accuracy 63.385%\n",
      "Epoch 4, Batch 1042, LR 1.003551 Loss 9.819943, Accuracy 63.383%\n",
      "Epoch 4, Batch 1043, LR 1.003838 Loss 9.820220, Accuracy 63.373%\n",
      "Epoch 4, Batch 1044, LR 1.004125 Loss 9.820581, Accuracy 63.368%\n",
      "Epoch 4, Batch 1045, LR 1.004411 Loss 9.820270, Accuracy 63.368%\n",
      "Epoch 4, Batch 1046, LR 1.004698 Loss 9.820166, Accuracy 63.366%\n",
      "Epoch 4, Batch 1047, LR 1.004985 Loss 9.820527, Accuracy 63.366%\n",
      "Epoch 4, Loss (train set) 9.820527, Accuracy (train set) 63.366%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Accuracy (validation set) 4.127%\n",
      "Epoch 5, Batch 1, LR 1.005272 Loss 9.877637, Accuracy 63.281%\n",
      "Epoch 5, Batch 2, LR 1.005559 Loss 9.428193, Accuracy 66.406%\n",
      "Epoch 5, Batch 3, LR 1.005846 Loss 9.388744, Accuracy 67.448%\n",
      "Epoch 5, Batch 4, LR 1.006133 Loss 9.385828, Accuracy 67.383%\n",
      "Epoch 5, Batch 5, LR 1.006419 Loss 9.435235, Accuracy 67.969%\n",
      "Epoch 5, Batch 6, LR 1.006706 Loss 9.485783, Accuracy 66.927%\n",
      "Epoch 5, Batch 7, LR 1.006993 Loss 9.479040, Accuracy 65.737%\n",
      "Epoch 5, Batch 8, LR 1.007280 Loss 9.497626, Accuracy 65.625%\n",
      "Epoch 5, Batch 9, LR 1.007567 Loss 9.509586, Accuracy 65.191%\n",
      "Epoch 5, Batch 10, LR 1.007854 Loss 9.456630, Accuracy 65.781%\n",
      "Epoch 5, Batch 11, LR 1.008141 Loss 9.563152, Accuracy 65.199%\n",
      "Epoch 5, Batch 12, LR 1.008428 Loss 9.552295, Accuracy 65.234%\n",
      "Epoch 5, Batch 13, LR 1.008715 Loss 9.491645, Accuracy 65.805%\n",
      "Epoch 5, Batch 14, LR 1.009002 Loss 9.436022, Accuracy 66.295%\n",
      "Epoch 5, Batch 15, LR 1.009290 Loss 9.452705, Accuracy 66.250%\n",
      "Epoch 5, Batch 16, LR 1.009577 Loss 9.461717, Accuracy 66.162%\n",
      "Epoch 5, Batch 17, LR 1.009864 Loss 9.486049, Accuracy 66.039%\n",
      "Epoch 5, Batch 18, LR 1.010151 Loss 9.475687, Accuracy 66.146%\n",
      "Epoch 5, Batch 19, LR 1.010438 Loss 9.475965, Accuracy 66.160%\n",
      "Epoch 5, Batch 20, LR 1.010725 Loss 9.450473, Accuracy 66.445%\n",
      "Epoch 5, Batch 21, LR 1.011013 Loss 9.457132, Accuracy 66.220%\n",
      "Epoch 5, Batch 22, LR 1.011300 Loss 9.441287, Accuracy 66.264%\n",
      "Epoch 5, Batch 23, LR 1.011587 Loss 9.421152, Accuracy 66.508%\n",
      "Epoch 5, Batch 24, LR 1.011874 Loss 9.400176, Accuracy 66.732%\n",
      "Epoch 5, Batch 25, LR 1.012162 Loss 9.425506, Accuracy 66.500%\n",
      "Epoch 5, Batch 26, LR 1.012449 Loss 9.418502, Accuracy 66.376%\n",
      "Epoch 5, Batch 27, LR 1.012736 Loss 9.434150, Accuracy 66.319%\n",
      "Epoch 5, Batch 28, LR 1.013024 Loss 9.437469, Accuracy 66.239%\n",
      "Epoch 5, Batch 29, LR 1.013311 Loss 9.443350, Accuracy 66.218%\n",
      "Epoch 5, Batch 30, LR 1.013598 Loss 9.443987, Accuracy 66.198%\n",
      "Epoch 5, Batch 31, LR 1.013886 Loss 9.418547, Accuracy 66.305%\n",
      "Epoch 5, Batch 32, LR 1.014173 Loss 9.421425, Accuracy 66.309%\n",
      "Epoch 5, Batch 33, LR 1.014461 Loss 9.433437, Accuracy 66.075%\n",
      "Epoch 5, Batch 34, LR 1.014748 Loss 9.432877, Accuracy 66.131%\n",
      "Epoch 5, Batch 35, LR 1.015036 Loss 9.426112, Accuracy 66.205%\n",
      "Epoch 5, Batch 36, LR 1.015323 Loss 9.411523, Accuracy 66.319%\n",
      "Epoch 5, Batch 37, LR 1.015611 Loss 9.396218, Accuracy 66.406%\n",
      "Epoch 5, Batch 38, LR 1.015898 Loss 9.398733, Accuracy 66.386%\n",
      "Epoch 5, Batch 39, LR 1.016186 Loss 9.403917, Accuracy 66.366%\n",
      "Epoch 5, Batch 40, LR 1.016473 Loss 9.412702, Accuracy 66.250%\n",
      "Epoch 5, Batch 41, LR 1.016761 Loss 9.412160, Accuracy 66.254%\n",
      "Epoch 5, Batch 42, LR 1.017048 Loss 9.392736, Accuracy 66.388%\n",
      "Epoch 5, Batch 43, LR 1.017336 Loss 9.404012, Accuracy 66.279%\n",
      "Epoch 5, Batch 44, LR 1.017624 Loss 9.405494, Accuracy 66.104%\n",
      "Epoch 5, Batch 45, LR 1.017911 Loss 9.419438, Accuracy 65.955%\n",
      "Epoch 5, Batch 46, LR 1.018199 Loss 9.409203, Accuracy 66.033%\n",
      "Epoch 5, Batch 47, LR 1.018487 Loss 9.411656, Accuracy 66.090%\n",
      "Epoch 5, Batch 48, LR 1.018774 Loss 9.407286, Accuracy 66.162%\n",
      "Epoch 5, Batch 49, LR 1.019062 Loss 9.410686, Accuracy 66.279%\n",
      "Epoch 5, Batch 50, LR 1.019350 Loss 9.418120, Accuracy 66.234%\n",
      "Epoch 5, Batch 51, LR 1.019638 Loss 9.430597, Accuracy 66.146%\n",
      "Epoch 5, Batch 52, LR 1.019926 Loss 9.425850, Accuracy 66.241%\n",
      "Epoch 5, Batch 53, LR 1.020213 Loss 9.425334, Accuracy 66.200%\n",
      "Epoch 5, Batch 54, LR 1.020501 Loss 9.433819, Accuracy 66.073%\n",
      "Epoch 5, Batch 55, LR 1.020789 Loss 9.419379, Accuracy 66.222%\n",
      "Epoch 5, Batch 56, LR 1.021077 Loss 9.420968, Accuracy 66.239%\n",
      "Epoch 5, Batch 57, LR 1.021365 Loss 9.426130, Accuracy 66.187%\n",
      "Epoch 5, Batch 58, LR 1.021653 Loss 9.417059, Accuracy 66.285%\n",
      "Epoch 5, Batch 59, LR 1.021941 Loss 9.439262, Accuracy 66.115%\n",
      "Epoch 5, Batch 60, LR 1.022229 Loss 9.436777, Accuracy 66.159%\n",
      "Epoch 5, Batch 61, LR 1.022516 Loss 9.440257, Accuracy 66.176%\n",
      "Epoch 5, Batch 62, LR 1.022804 Loss 9.449184, Accuracy 66.142%\n",
      "Epoch 5, Batch 63, LR 1.023092 Loss 9.442266, Accuracy 66.270%\n",
      "Epoch 5, Batch 64, LR 1.023380 Loss 9.434240, Accuracy 66.296%\n",
      "Epoch 5, Batch 65, LR 1.023668 Loss 9.436890, Accuracy 66.334%\n",
      "Epoch 5, Batch 66, LR 1.023957 Loss 9.443446, Accuracy 66.229%\n",
      "Epoch 5, Batch 67, LR 1.024245 Loss 9.436454, Accuracy 66.313%\n",
      "Epoch 5, Batch 68, LR 1.024533 Loss 9.436067, Accuracy 66.326%\n",
      "Epoch 5, Batch 69, LR 1.024821 Loss 9.432517, Accuracy 66.429%\n",
      "Epoch 5, Batch 70, LR 1.025109 Loss 9.428816, Accuracy 66.429%\n",
      "Epoch 5, Batch 71, LR 1.025397 Loss 9.426004, Accuracy 66.461%\n",
      "Epoch 5, Batch 72, LR 1.025685 Loss 9.429843, Accuracy 66.482%\n",
      "Epoch 5, Batch 73, LR 1.025973 Loss 9.433203, Accuracy 66.503%\n",
      "Epoch 5, Batch 74, LR 1.026262 Loss 9.427565, Accuracy 66.533%\n",
      "Epoch 5, Batch 75, LR 1.026550 Loss 9.425548, Accuracy 66.562%\n",
      "Epoch 5, Batch 76, LR 1.026838 Loss 9.429790, Accuracy 66.478%\n",
      "Epoch 5, Batch 77, LR 1.027126 Loss 9.429636, Accuracy 66.518%\n",
      "Epoch 5, Batch 78, LR 1.027414 Loss 9.427940, Accuracy 66.496%\n",
      "Epoch 5, Batch 79, LR 1.027703 Loss 9.431919, Accuracy 66.416%\n",
      "Epoch 5, Batch 80, LR 1.027991 Loss 9.426785, Accuracy 66.445%\n",
      "Epoch 5, Batch 81, LR 1.028279 Loss 9.432353, Accuracy 66.416%\n",
      "Epoch 5, Batch 82, LR 1.028568 Loss 9.430174, Accuracy 66.425%\n",
      "Epoch 5, Batch 83, LR 1.028856 Loss 9.421464, Accuracy 66.453%\n",
      "Epoch 5, Batch 84, LR 1.029144 Loss 9.425097, Accuracy 66.490%\n",
      "Epoch 5, Batch 85, LR 1.029433 Loss 9.427733, Accuracy 66.471%\n",
      "Epoch 5, Batch 86, LR 1.029721 Loss 9.419675, Accuracy 66.533%\n",
      "Epoch 5, Batch 87, LR 1.030010 Loss 9.417508, Accuracy 66.514%\n",
      "Epoch 5, Batch 88, LR 1.030298 Loss 9.418684, Accuracy 66.460%\n",
      "Epoch 5, Batch 89, LR 1.030587 Loss 9.418941, Accuracy 66.459%\n",
      "Epoch 5, Batch 90, LR 1.030875 Loss 9.412244, Accuracy 66.536%\n",
      "Epoch 5, Batch 91, LR 1.031164 Loss 9.404292, Accuracy 66.638%\n",
      "Epoch 5, Batch 92, LR 1.031452 Loss 9.404384, Accuracy 66.627%\n",
      "Epoch 5, Batch 93, LR 1.031741 Loss 9.402750, Accuracy 66.675%\n",
      "Epoch 5, Batch 94, LR 1.032029 Loss 9.410864, Accuracy 66.622%\n",
      "Epoch 5, Batch 95, LR 1.032318 Loss 9.413792, Accuracy 66.587%\n",
      "Epoch 5, Batch 96, LR 1.032606 Loss 9.411886, Accuracy 66.650%\n",
      "Epoch 5, Batch 97, LR 1.032895 Loss 9.411570, Accuracy 66.656%\n",
      "Epoch 5, Batch 98, LR 1.033183 Loss 9.412691, Accuracy 66.701%\n",
      "Epoch 5, Batch 99, LR 1.033472 Loss 9.412438, Accuracy 66.714%\n",
      "Epoch 5, Batch 100, LR 1.033761 Loss 9.416731, Accuracy 66.719%\n",
      "Epoch 5, Batch 101, LR 1.034049 Loss 9.414606, Accuracy 66.716%\n",
      "Epoch 5, Batch 102, LR 1.034338 Loss 9.418633, Accuracy 66.713%\n",
      "Epoch 5, Batch 103, LR 1.034627 Loss 9.417717, Accuracy 66.702%\n",
      "Epoch 5, Batch 104, LR 1.034915 Loss 9.422827, Accuracy 66.654%\n",
      "Epoch 5, Batch 105, LR 1.035204 Loss 9.427549, Accuracy 66.607%\n",
      "Epoch 5, Batch 106, LR 1.035493 Loss 9.426622, Accuracy 66.598%\n",
      "Epoch 5, Batch 107, LR 1.035782 Loss 9.424424, Accuracy 66.603%\n",
      "Epoch 5, Batch 108, LR 1.036071 Loss 9.428962, Accuracy 66.573%\n",
      "Epoch 5, Batch 109, LR 1.036359 Loss 9.423347, Accuracy 66.628%\n",
      "Epoch 5, Batch 110, LR 1.036648 Loss 9.419629, Accuracy 66.676%\n",
      "Epoch 5, Batch 111, LR 1.036937 Loss 9.418661, Accuracy 66.695%\n",
      "Epoch 5, Batch 112, LR 1.037226 Loss 9.420007, Accuracy 66.713%\n",
      "Epoch 5, Batch 113, LR 1.037515 Loss 9.420778, Accuracy 66.704%\n",
      "Epoch 5, Batch 114, LR 1.037804 Loss 9.421027, Accuracy 66.694%\n",
      "Epoch 5, Batch 115, LR 1.038093 Loss 9.418335, Accuracy 66.712%\n",
      "Epoch 5, Batch 116, LR 1.038381 Loss 9.423293, Accuracy 66.608%\n",
      "Epoch 5, Batch 117, LR 1.038670 Loss 9.423652, Accuracy 66.633%\n",
      "Epoch 5, Batch 118, LR 1.038959 Loss 9.424672, Accuracy 66.631%\n",
      "Epoch 5, Batch 119, LR 1.039248 Loss 9.427419, Accuracy 66.656%\n",
      "Epoch 5, Batch 120, LR 1.039537 Loss 9.428911, Accuracy 66.621%\n",
      "Epoch 5, Batch 121, LR 1.039826 Loss 9.434630, Accuracy 66.587%\n",
      "Epoch 5, Batch 122, LR 1.040115 Loss 9.436720, Accuracy 66.573%\n",
      "Epoch 5, Batch 123, LR 1.040404 Loss 9.438964, Accuracy 66.552%\n",
      "Epoch 5, Batch 124, LR 1.040693 Loss 9.438017, Accuracy 66.576%\n",
      "Epoch 5, Batch 125, LR 1.040983 Loss 9.435215, Accuracy 66.612%\n",
      "Epoch 5, Batch 126, LR 1.041272 Loss 9.437147, Accuracy 66.580%\n",
      "Epoch 5, Batch 127, LR 1.041561 Loss 9.440170, Accuracy 66.560%\n",
      "Epoch 5, Batch 128, LR 1.041850 Loss 9.439734, Accuracy 66.553%\n",
      "Epoch 5, Batch 129, LR 1.042139 Loss 9.439100, Accuracy 66.576%\n",
      "Epoch 5, Batch 130, LR 1.042428 Loss 9.439926, Accuracy 66.544%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 131, LR 1.042717 Loss 9.433952, Accuracy 66.603%\n",
      "Epoch 5, Batch 132, LR 1.043007 Loss 9.433255, Accuracy 66.584%\n",
      "Epoch 5, Batch 133, LR 1.043296 Loss 9.429765, Accuracy 66.594%\n",
      "Epoch 5, Batch 134, LR 1.043585 Loss 9.432923, Accuracy 66.581%\n",
      "Epoch 5, Batch 135, LR 1.043874 Loss 9.427616, Accuracy 66.638%\n",
      "Epoch 5, Batch 136, LR 1.044164 Loss 9.427341, Accuracy 66.619%\n",
      "Epoch 5, Batch 137, LR 1.044453 Loss 9.431613, Accuracy 66.606%\n",
      "Epoch 5, Batch 138, LR 1.044742 Loss 9.433048, Accuracy 66.553%\n",
      "Epoch 5, Batch 139, LR 1.045031 Loss 9.431984, Accuracy 66.580%\n",
      "Epoch 5, Batch 140, LR 1.045321 Loss 9.429370, Accuracy 66.624%\n",
      "Epoch 5, Batch 141, LR 1.045610 Loss 9.421637, Accuracy 66.683%\n",
      "Epoch 5, Batch 142, LR 1.045900 Loss 9.420856, Accuracy 66.698%\n",
      "Epoch 5, Batch 143, LR 1.046189 Loss 9.422322, Accuracy 66.668%\n",
      "Epoch 5, Batch 144, LR 1.046478 Loss 9.426650, Accuracy 66.629%\n",
      "Epoch 5, Batch 145, LR 1.046768 Loss 9.427294, Accuracy 66.622%\n",
      "Epoch 5, Batch 146, LR 1.047057 Loss 9.426450, Accuracy 66.636%\n",
      "Epoch 5, Batch 147, LR 1.047347 Loss 9.424860, Accuracy 66.635%\n",
      "Epoch 5, Batch 148, LR 1.047636 Loss 9.422835, Accuracy 66.654%\n",
      "Epoch 5, Batch 149, LR 1.047926 Loss 9.421971, Accuracy 66.647%\n",
      "Epoch 5, Batch 150, LR 1.048215 Loss 9.418595, Accuracy 66.698%\n",
      "Epoch 5, Batch 151, LR 1.048505 Loss 9.420407, Accuracy 66.680%\n",
      "Epoch 5, Batch 152, LR 1.048794 Loss 9.422186, Accuracy 66.632%\n",
      "Epoch 5, Batch 153, LR 1.049084 Loss 9.420700, Accuracy 66.682%\n",
      "Epoch 5, Batch 154, LR 1.049373 Loss 9.418690, Accuracy 66.726%\n",
      "Epoch 5, Batch 155, LR 1.049663 Loss 9.421753, Accuracy 66.653%\n",
      "Epoch 5, Batch 156, LR 1.049952 Loss 9.416755, Accuracy 66.687%\n",
      "Epoch 5, Batch 157, LR 1.050242 Loss 9.413195, Accuracy 66.710%\n",
      "Epoch 5, Batch 158, LR 1.050532 Loss 9.413194, Accuracy 66.713%\n",
      "Epoch 5, Batch 159, LR 1.050821 Loss 9.411987, Accuracy 66.745%\n",
      "Epoch 5, Batch 160, LR 1.051111 Loss 9.418087, Accuracy 66.714%\n",
      "Epoch 5, Batch 161, LR 1.051401 Loss 9.421702, Accuracy 66.683%\n",
      "Epoch 5, Batch 162, LR 1.051690 Loss 9.422519, Accuracy 66.671%\n",
      "Epoch 5, Batch 163, LR 1.051980 Loss 9.422859, Accuracy 66.655%\n",
      "Epoch 5, Batch 164, LR 1.052270 Loss 9.423228, Accuracy 66.668%\n",
      "Epoch 5, Batch 165, LR 1.052559 Loss 9.423049, Accuracy 66.667%\n",
      "Epoch 5, Batch 166, LR 1.052849 Loss 9.423882, Accuracy 66.679%\n",
      "Epoch 5, Batch 167, LR 1.053139 Loss 9.424554, Accuracy 66.687%\n",
      "Epoch 5, Batch 168, LR 1.053429 Loss 9.430932, Accuracy 66.643%\n",
      "Epoch 5, Batch 169, LR 1.053719 Loss 9.431398, Accuracy 66.619%\n",
      "Epoch 5, Batch 170, LR 1.054008 Loss 9.435643, Accuracy 66.590%\n",
      "Epoch 5, Batch 171, LR 1.054298 Loss 9.435866, Accuracy 66.584%\n",
      "Epoch 5, Batch 172, LR 1.054588 Loss 9.432134, Accuracy 66.602%\n",
      "Epoch 5, Batch 173, LR 1.054878 Loss 9.433075, Accuracy 66.578%\n",
      "Epoch 5, Batch 174, LR 1.055168 Loss 9.433188, Accuracy 66.608%\n",
      "Epoch 5, Batch 175, LR 1.055458 Loss 9.435178, Accuracy 66.576%\n",
      "Epoch 5, Batch 176, LR 1.055748 Loss 9.432058, Accuracy 66.610%\n",
      "Epoch 5, Batch 177, LR 1.056038 Loss 9.434671, Accuracy 66.570%\n",
      "Epoch 5, Batch 178, LR 1.056328 Loss 9.437062, Accuracy 66.547%\n",
      "Epoch 5, Batch 179, LR 1.056618 Loss 9.440620, Accuracy 66.524%\n",
      "Epoch 5, Batch 180, LR 1.056908 Loss 9.442426, Accuracy 66.480%\n",
      "Epoch 5, Batch 181, LR 1.057198 Loss 9.443879, Accuracy 66.449%\n",
      "Epoch 5, Batch 182, LR 1.057488 Loss 9.440835, Accuracy 66.479%\n",
      "Epoch 5, Batch 183, LR 1.057778 Loss 9.442039, Accuracy 66.479%\n",
      "Epoch 5, Batch 184, LR 1.058068 Loss 9.442892, Accuracy 66.470%\n",
      "Epoch 5, Batch 185, LR 1.058358 Loss 9.442918, Accuracy 66.474%\n",
      "Epoch 5, Batch 186, LR 1.058648 Loss 9.441006, Accuracy 66.511%\n",
      "Epoch 5, Batch 187, LR 1.058938 Loss 9.437650, Accuracy 66.544%\n",
      "Epoch 5, Batch 188, LR 1.059228 Loss 9.440158, Accuracy 66.527%\n",
      "Epoch 5, Batch 189, LR 1.059518 Loss 9.442249, Accuracy 66.518%\n",
      "Epoch 5, Batch 190, LR 1.059808 Loss 9.445064, Accuracy 66.480%\n",
      "Epoch 5, Batch 191, LR 1.060099 Loss 9.444790, Accuracy 66.480%\n",
      "Epoch 5, Batch 192, LR 1.060389 Loss 9.447281, Accuracy 66.471%\n",
      "Epoch 5, Batch 193, LR 1.060679 Loss 9.446281, Accuracy 66.483%\n",
      "Epoch 5, Batch 194, LR 1.060969 Loss 9.446039, Accuracy 66.507%\n",
      "Epoch 5, Batch 195, LR 1.061259 Loss 9.447387, Accuracy 66.490%\n",
      "Epoch 5, Batch 196, LR 1.061550 Loss 9.446705, Accuracy 66.502%\n",
      "Epoch 5, Batch 197, LR 1.061840 Loss 9.445242, Accuracy 66.482%\n",
      "Epoch 5, Batch 198, LR 1.062130 Loss 9.445428, Accuracy 66.469%\n",
      "Epoch 5, Batch 199, LR 1.062420 Loss 9.448173, Accuracy 66.426%\n",
      "Epoch 5, Batch 200, LR 1.062711 Loss 9.447010, Accuracy 66.430%\n",
      "Epoch 5, Batch 201, LR 1.063001 Loss 9.441520, Accuracy 66.472%\n",
      "Epoch 5, Batch 202, LR 1.063291 Loss 9.444157, Accuracy 66.449%\n",
      "Epoch 5, Batch 203, LR 1.063582 Loss 9.444511, Accuracy 66.422%\n",
      "Epoch 5, Batch 204, LR 1.063872 Loss 9.442568, Accuracy 66.452%\n",
      "Epoch 5, Batch 205, LR 1.064162 Loss 9.445212, Accuracy 66.441%\n",
      "Epoch 5, Batch 206, LR 1.064453 Loss 9.445330, Accuracy 66.440%\n",
      "Epoch 5, Batch 207, LR 1.064743 Loss 9.441919, Accuracy 66.470%\n",
      "Epoch 5, Batch 208, LR 1.065034 Loss 9.442188, Accuracy 66.451%\n",
      "Epoch 5, Batch 209, LR 1.065324 Loss 9.440596, Accuracy 66.474%\n",
      "Epoch 5, Batch 210, LR 1.065615 Loss 9.440179, Accuracy 66.473%\n",
      "Epoch 5, Batch 211, LR 1.065905 Loss 9.436583, Accuracy 66.506%\n",
      "Epoch 5, Batch 212, LR 1.066196 Loss 9.437336, Accuracy 66.509%\n",
      "Epoch 5, Batch 213, LR 1.066486 Loss 9.439697, Accuracy 66.487%\n",
      "Epoch 5, Batch 214, LR 1.066777 Loss 9.436525, Accuracy 66.516%\n",
      "Epoch 5, Batch 215, LR 1.067067 Loss 9.435309, Accuracy 66.537%\n",
      "Epoch 5, Batch 216, LR 1.067358 Loss 9.438657, Accuracy 66.515%\n",
      "Epoch 5, Batch 217, LR 1.067648 Loss 9.439392, Accuracy 66.503%\n",
      "Epoch 5, Batch 218, LR 1.067939 Loss 9.437668, Accuracy 66.517%\n",
      "Epoch 5, Batch 219, LR 1.068229 Loss 9.437088, Accuracy 66.517%\n",
      "Epoch 5, Batch 220, LR 1.068520 Loss 9.435888, Accuracy 66.552%\n",
      "Epoch 5, Batch 221, LR 1.068811 Loss 9.433958, Accuracy 66.579%\n",
      "Epoch 5, Batch 222, LR 1.069101 Loss 9.436974, Accuracy 66.547%\n",
      "Epoch 5, Batch 223, LR 1.069392 Loss 9.435043, Accuracy 66.581%\n",
      "Epoch 5, Batch 224, LR 1.069683 Loss 9.435818, Accuracy 66.563%\n",
      "Epoch 5, Batch 225, LR 1.069973 Loss 9.434755, Accuracy 66.587%\n",
      "Epoch 5, Batch 226, LR 1.070264 Loss 9.435865, Accuracy 66.579%\n",
      "Epoch 5, Batch 227, LR 1.070555 Loss 9.436449, Accuracy 66.582%\n",
      "Epoch 5, Batch 228, LR 1.070845 Loss 9.436136, Accuracy 66.581%\n",
      "Epoch 5, Batch 229, LR 1.071136 Loss 9.438675, Accuracy 66.546%\n",
      "Epoch 5, Batch 230, LR 1.071427 Loss 9.440043, Accuracy 66.532%\n",
      "Epoch 5, Batch 231, LR 1.071718 Loss 9.439962, Accuracy 66.518%\n",
      "Epoch 5, Batch 232, LR 1.072009 Loss 9.440902, Accuracy 66.487%\n",
      "Epoch 5, Batch 233, LR 1.072299 Loss 9.439253, Accuracy 66.500%\n",
      "Epoch 5, Batch 234, LR 1.072590 Loss 9.439118, Accuracy 66.493%\n",
      "Epoch 5, Batch 235, LR 1.072881 Loss 9.437499, Accuracy 66.499%\n",
      "Epoch 5, Batch 236, LR 1.073172 Loss 9.438065, Accuracy 66.506%\n",
      "Epoch 5, Batch 237, LR 1.073463 Loss 9.436099, Accuracy 66.525%\n",
      "Epoch 5, Batch 238, LR 1.073754 Loss 9.437948, Accuracy 66.508%\n",
      "Epoch 5, Batch 239, LR 1.074044 Loss 9.436576, Accuracy 66.530%\n",
      "Epoch 5, Batch 240, LR 1.074335 Loss 9.437994, Accuracy 66.530%\n",
      "Epoch 5, Batch 241, LR 1.074626 Loss 9.438599, Accuracy 66.497%\n",
      "Epoch 5, Batch 242, LR 1.074917 Loss 9.437697, Accuracy 66.516%\n",
      "Epoch 5, Batch 243, LR 1.075208 Loss 9.437588, Accuracy 66.509%\n",
      "Epoch 5, Batch 244, LR 1.075499 Loss 9.435417, Accuracy 66.531%\n",
      "Epoch 5, Batch 245, LR 1.075790 Loss 9.435357, Accuracy 66.537%\n",
      "Epoch 5, Batch 246, LR 1.076081 Loss 9.438741, Accuracy 66.511%\n",
      "Epoch 5, Batch 247, LR 1.076372 Loss 9.436915, Accuracy 66.526%\n",
      "Epoch 5, Batch 248, LR 1.076663 Loss 9.437102, Accuracy 66.535%\n",
      "Epoch 5, Batch 249, LR 1.076954 Loss 9.436021, Accuracy 66.541%\n",
      "Epoch 5, Batch 250, LR 1.077245 Loss 9.434462, Accuracy 66.559%\n",
      "Epoch 5, Batch 251, LR 1.077536 Loss 9.437195, Accuracy 66.543%\n",
      "Epoch 5, Batch 252, LR 1.077828 Loss 9.438021, Accuracy 66.524%\n",
      "Epoch 5, Batch 253, LR 1.078119 Loss 9.436245, Accuracy 66.539%\n",
      "Epoch 5, Batch 254, LR 1.078410 Loss 9.434592, Accuracy 66.542%\n",
      "Epoch 5, Batch 255, LR 1.078701 Loss 9.433833, Accuracy 66.553%\n",
      "Epoch 5, Batch 256, LR 1.078992 Loss 9.435057, Accuracy 66.541%\n",
      "Epoch 5, Batch 257, LR 1.079283 Loss 9.434903, Accuracy 66.543%\n",
      "Epoch 5, Batch 258, LR 1.079574 Loss 9.432377, Accuracy 66.555%\n",
      "Epoch 5, Batch 259, LR 1.079866 Loss 9.434332, Accuracy 66.542%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 260, LR 1.080157 Loss 9.432738, Accuracy 66.553%\n",
      "Epoch 5, Batch 261, LR 1.080448 Loss 9.431647, Accuracy 66.568%\n",
      "Epoch 5, Batch 262, LR 1.080739 Loss 9.431525, Accuracy 66.576%\n",
      "Epoch 5, Batch 263, LR 1.081030 Loss 9.433708, Accuracy 66.549%\n",
      "Epoch 5, Batch 264, LR 1.081322 Loss 9.431711, Accuracy 66.548%\n",
      "Epoch 5, Batch 265, LR 1.081613 Loss 9.433163, Accuracy 66.518%\n",
      "Epoch 5, Batch 266, LR 1.081904 Loss 9.432058, Accuracy 66.530%\n",
      "Epoch 5, Batch 267, LR 1.082196 Loss 9.431942, Accuracy 66.535%\n",
      "Epoch 5, Batch 268, LR 1.082487 Loss 9.430582, Accuracy 66.529%\n",
      "Epoch 5, Batch 269, LR 1.082778 Loss 9.431281, Accuracy 66.540%\n",
      "Epoch 5, Batch 270, LR 1.083070 Loss 9.430711, Accuracy 66.551%\n",
      "Epoch 5, Batch 271, LR 1.083361 Loss 9.431492, Accuracy 66.550%\n",
      "Epoch 5, Batch 272, LR 1.083652 Loss 9.429438, Accuracy 66.550%\n",
      "Epoch 5, Batch 273, LR 1.083944 Loss 9.426624, Accuracy 66.575%\n",
      "Epoch 5, Batch 274, LR 1.084235 Loss 9.426400, Accuracy 66.574%\n",
      "Epoch 5, Batch 275, LR 1.084527 Loss 9.423888, Accuracy 66.574%\n",
      "Epoch 5, Batch 276, LR 1.084818 Loss 9.424294, Accuracy 66.576%\n",
      "Epoch 5, Batch 277, LR 1.085110 Loss 9.422251, Accuracy 66.601%\n",
      "Epoch 5, Batch 278, LR 1.085401 Loss 9.419187, Accuracy 66.609%\n",
      "Epoch 5, Batch 279, LR 1.085692 Loss 9.420536, Accuracy 66.613%\n",
      "Epoch 5, Batch 280, LR 1.085984 Loss 9.422570, Accuracy 66.596%\n",
      "Epoch 5, Batch 281, LR 1.086275 Loss 9.423497, Accuracy 66.573%\n",
      "Epoch 5, Batch 282, LR 1.086567 Loss 9.425053, Accuracy 66.572%\n",
      "Epoch 5, Batch 283, LR 1.086859 Loss 9.425205, Accuracy 66.566%\n",
      "Epoch 5, Batch 284, LR 1.087150 Loss 9.427802, Accuracy 66.541%\n",
      "Epoch 5, Batch 285, LR 1.087442 Loss 9.429739, Accuracy 66.532%\n",
      "Epoch 5, Batch 286, LR 1.087733 Loss 9.431197, Accuracy 66.502%\n",
      "Epoch 5, Batch 287, LR 1.088025 Loss 9.433160, Accuracy 66.485%\n",
      "Epoch 5, Batch 288, LR 1.088316 Loss 9.433052, Accuracy 66.488%\n",
      "Epoch 5, Batch 289, LR 1.088608 Loss 9.435314, Accuracy 66.468%\n",
      "Epoch 5, Batch 290, LR 1.088900 Loss 9.434697, Accuracy 66.463%\n",
      "Epoch 5, Batch 291, LR 1.089191 Loss 9.433518, Accuracy 66.479%\n",
      "Epoch 5, Batch 292, LR 1.089483 Loss 9.432600, Accuracy 66.492%\n",
      "Epoch 5, Batch 293, LR 1.089775 Loss 9.433500, Accuracy 66.484%\n",
      "Epoch 5, Batch 294, LR 1.090066 Loss 9.433646, Accuracy 66.465%\n",
      "Epoch 5, Batch 295, LR 1.090358 Loss 9.432187, Accuracy 66.483%\n",
      "Epoch 5, Batch 296, LR 1.090650 Loss 9.433024, Accuracy 66.472%\n",
      "Epoch 5, Batch 297, LR 1.090942 Loss 9.436925, Accuracy 66.446%\n",
      "Epoch 5, Batch 298, LR 1.091233 Loss 9.436130, Accuracy 66.459%\n",
      "Epoch 5, Batch 299, LR 1.091525 Loss 9.437830, Accuracy 66.451%\n",
      "Epoch 5, Batch 300, LR 1.091817 Loss 9.437869, Accuracy 66.461%\n",
      "Epoch 5, Batch 301, LR 1.092109 Loss 9.438210, Accuracy 66.463%\n",
      "Epoch 5, Batch 302, LR 1.092400 Loss 9.438922, Accuracy 66.453%\n",
      "Epoch 5, Batch 303, LR 1.092692 Loss 9.439131, Accuracy 66.455%\n",
      "Epoch 5, Batch 304, LR 1.092984 Loss 9.437926, Accuracy 66.458%\n",
      "Epoch 5, Batch 305, LR 1.093276 Loss 9.439348, Accuracy 66.447%\n",
      "Epoch 5, Batch 306, LR 1.093568 Loss 9.440481, Accuracy 66.434%\n",
      "Epoch 5, Batch 307, LR 1.093860 Loss 9.439343, Accuracy 66.427%\n",
      "Epoch 5, Batch 308, LR 1.094152 Loss 9.438287, Accuracy 66.442%\n",
      "Epoch 5, Batch 309, LR 1.094443 Loss 9.440445, Accuracy 66.424%\n",
      "Epoch 5, Batch 310, LR 1.094735 Loss 9.439421, Accuracy 66.436%\n",
      "Epoch 5, Batch 311, LR 1.095027 Loss 9.439619, Accuracy 66.436%\n",
      "Epoch 5, Batch 312, LR 1.095319 Loss 9.442488, Accuracy 66.404%\n",
      "Epoch 5, Batch 313, LR 1.095611 Loss 9.440412, Accuracy 66.424%\n",
      "Epoch 5, Batch 314, LR 1.095903 Loss 9.443157, Accuracy 66.379%\n",
      "Epoch 5, Batch 315, LR 1.096195 Loss 9.446302, Accuracy 66.359%\n",
      "Epoch 5, Batch 316, LR 1.096487 Loss 9.445561, Accuracy 66.372%\n",
      "Epoch 5, Batch 317, LR 1.096779 Loss 9.446858, Accuracy 66.362%\n",
      "Epoch 5, Batch 318, LR 1.097071 Loss 9.445818, Accuracy 66.369%\n",
      "Epoch 5, Batch 319, LR 1.097363 Loss 9.447300, Accuracy 66.362%\n",
      "Epoch 5, Batch 320, LR 1.097655 Loss 9.448177, Accuracy 66.350%\n",
      "Epoch 5, Batch 321, LR 1.097947 Loss 9.447765, Accuracy 66.367%\n",
      "Epoch 5, Batch 322, LR 1.098239 Loss 9.448433, Accuracy 66.367%\n",
      "Epoch 5, Batch 323, LR 1.098531 Loss 9.446583, Accuracy 66.382%\n",
      "Epoch 5, Batch 324, LR 1.098824 Loss 9.446637, Accuracy 66.368%\n",
      "Epoch 5, Batch 325, LR 1.099116 Loss 9.445013, Accuracy 66.382%\n",
      "Epoch 5, Batch 326, LR 1.099408 Loss 9.445992, Accuracy 66.368%\n",
      "Epoch 5, Batch 327, LR 1.099700 Loss 9.446428, Accuracy 66.370%\n",
      "Epoch 5, Batch 328, LR 1.099992 Loss 9.448295, Accuracy 66.368%\n",
      "Epoch 5, Batch 329, LR 1.100284 Loss 9.448866, Accuracy 66.368%\n",
      "Epoch 5, Batch 330, LR 1.100576 Loss 9.449071, Accuracy 66.378%\n",
      "Epoch 5, Batch 331, LR 1.100869 Loss 9.450059, Accuracy 66.376%\n",
      "Epoch 5, Batch 332, LR 1.101161 Loss 9.449913, Accuracy 66.378%\n",
      "Epoch 5, Batch 333, LR 1.101453 Loss 9.450191, Accuracy 66.378%\n",
      "Epoch 5, Batch 334, LR 1.101745 Loss 9.447890, Accuracy 66.402%\n",
      "Epoch 5, Batch 335, LR 1.102037 Loss 9.447378, Accuracy 66.402%\n",
      "Epoch 5, Batch 336, LR 1.102330 Loss 9.447288, Accuracy 66.395%\n",
      "Epoch 5, Batch 337, LR 1.102622 Loss 9.447974, Accuracy 66.390%\n",
      "Epoch 5, Batch 338, LR 1.102914 Loss 9.446950, Accuracy 66.411%\n",
      "Epoch 5, Batch 339, LR 1.103207 Loss 9.447848, Accuracy 66.390%\n",
      "Epoch 5, Batch 340, LR 1.103499 Loss 9.448537, Accuracy 66.383%\n",
      "Epoch 5, Batch 341, LR 1.103791 Loss 9.447739, Accuracy 66.386%\n",
      "Epoch 5, Batch 342, LR 1.104084 Loss 9.446282, Accuracy 66.393%\n",
      "Epoch 5, Batch 343, LR 1.104376 Loss 9.446680, Accuracy 66.390%\n",
      "Epoch 5, Batch 344, LR 1.104668 Loss 9.447215, Accuracy 66.384%\n",
      "Epoch 5, Batch 345, LR 1.104961 Loss 9.446141, Accuracy 66.397%\n",
      "Epoch 5, Batch 346, LR 1.105253 Loss 9.444449, Accuracy 66.406%\n",
      "Epoch 5, Batch 347, LR 1.105545 Loss 9.443995, Accuracy 66.402%\n",
      "Epoch 5, Batch 348, LR 1.105838 Loss 9.444709, Accuracy 66.413%\n",
      "Epoch 5, Batch 349, LR 1.106130 Loss 9.445240, Accuracy 66.404%\n",
      "Epoch 5, Batch 350, LR 1.106423 Loss 9.447575, Accuracy 66.375%\n",
      "Epoch 5, Batch 351, LR 1.106715 Loss 9.447739, Accuracy 66.355%\n",
      "Epoch 5, Batch 352, LR 1.107008 Loss 9.447287, Accuracy 66.353%\n",
      "Epoch 5, Batch 353, LR 1.107300 Loss 9.447265, Accuracy 66.331%\n",
      "Epoch 5, Batch 354, LR 1.107593 Loss 9.447186, Accuracy 66.342%\n",
      "Epoch 5, Batch 355, LR 1.107885 Loss 9.448315, Accuracy 66.336%\n",
      "Epoch 5, Batch 356, LR 1.108178 Loss 9.447505, Accuracy 66.351%\n",
      "Epoch 5, Batch 357, LR 1.108470 Loss 9.446374, Accuracy 66.354%\n",
      "Epoch 5, Batch 358, LR 1.108763 Loss 9.445679, Accuracy 66.365%\n",
      "Epoch 5, Batch 359, LR 1.109055 Loss 9.445331, Accuracy 66.374%\n",
      "Epoch 5, Batch 360, LR 1.109348 Loss 9.445199, Accuracy 66.369%\n",
      "Epoch 5, Batch 361, LR 1.109640 Loss 9.443767, Accuracy 66.387%\n",
      "Epoch 5, Batch 362, LR 1.109933 Loss 9.443932, Accuracy 66.385%\n",
      "Epoch 5, Batch 363, LR 1.110226 Loss 9.442619, Accuracy 66.393%\n",
      "Epoch 5, Batch 364, LR 1.110518 Loss 9.440350, Accuracy 66.408%\n",
      "Epoch 5, Batch 365, LR 1.110811 Loss 9.441327, Accuracy 66.400%\n",
      "Epoch 5, Batch 366, LR 1.111104 Loss 9.442173, Accuracy 66.389%\n",
      "Epoch 5, Batch 367, LR 1.111396 Loss 9.443692, Accuracy 66.381%\n",
      "Epoch 5, Batch 368, LR 1.111689 Loss 9.442221, Accuracy 66.387%\n",
      "Epoch 5, Batch 369, LR 1.111982 Loss 9.441209, Accuracy 66.396%\n",
      "Epoch 5, Batch 370, LR 1.112274 Loss 9.440958, Accuracy 66.398%\n",
      "Epoch 5, Batch 371, LR 1.112567 Loss 9.441178, Accuracy 66.394%\n",
      "Epoch 5, Batch 372, LR 1.112860 Loss 9.442997, Accuracy 66.383%\n",
      "Epoch 5, Batch 373, LR 1.113152 Loss 9.441579, Accuracy 66.394%\n",
      "Epoch 5, Batch 374, LR 1.113445 Loss 9.443455, Accuracy 66.387%\n",
      "Epoch 5, Batch 375, LR 1.113738 Loss 9.442842, Accuracy 66.383%\n",
      "Epoch 5, Batch 376, LR 1.114031 Loss 9.441714, Accuracy 66.406%\n",
      "Epoch 5, Batch 377, LR 1.114324 Loss 9.441831, Accuracy 66.415%\n",
      "Epoch 5, Batch 378, LR 1.114616 Loss 9.442612, Accuracy 66.408%\n",
      "Epoch 5, Batch 379, LR 1.114909 Loss 9.441299, Accuracy 66.419%\n",
      "Epoch 5, Batch 380, LR 1.115202 Loss 9.440316, Accuracy 66.431%\n",
      "Epoch 5, Batch 381, LR 1.115495 Loss 9.441711, Accuracy 66.423%\n",
      "Epoch 5, Batch 382, LR 1.115788 Loss 9.440242, Accuracy 66.431%\n",
      "Epoch 5, Batch 383, LR 1.116080 Loss 9.438986, Accuracy 66.439%\n",
      "Epoch 5, Batch 384, LR 1.116373 Loss 9.440706, Accuracy 66.425%\n",
      "Epoch 5, Batch 385, LR 1.116666 Loss 9.441934, Accuracy 66.422%\n",
      "Epoch 5, Batch 386, LR 1.116959 Loss 9.443428, Accuracy 66.410%\n",
      "Epoch 5, Batch 387, LR 1.117252 Loss 9.442310, Accuracy 66.418%\n",
      "Epoch 5, Batch 388, LR 1.117545 Loss 9.443789, Accuracy 66.408%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 389, LR 1.117838 Loss 9.442182, Accuracy 66.426%\n",
      "Epoch 5, Batch 390, LR 1.118131 Loss 9.442648, Accuracy 66.414%\n",
      "Epoch 5, Batch 391, LR 1.118424 Loss 9.442346, Accuracy 66.422%\n",
      "Epoch 5, Batch 392, LR 1.118717 Loss 9.442406, Accuracy 66.426%\n",
      "Epoch 5, Batch 393, LR 1.119010 Loss 9.440163, Accuracy 66.438%\n",
      "Epoch 5, Batch 394, LR 1.119303 Loss 9.438942, Accuracy 66.452%\n",
      "Epoch 5, Batch 395, LR 1.119596 Loss 9.439406, Accuracy 66.442%\n",
      "Epoch 5, Batch 396, LR 1.119889 Loss 9.437148, Accuracy 66.461%\n",
      "Epoch 5, Batch 397, LR 1.120182 Loss 9.438126, Accuracy 66.448%\n",
      "Epoch 5, Batch 398, LR 1.120475 Loss 9.438507, Accuracy 66.442%\n",
      "Epoch 5, Batch 399, LR 1.120768 Loss 9.437947, Accuracy 66.430%\n",
      "Epoch 5, Batch 400, LR 1.121061 Loss 9.437647, Accuracy 66.432%\n",
      "Epoch 5, Batch 401, LR 1.121354 Loss 9.436961, Accuracy 66.435%\n",
      "Epoch 5, Batch 402, LR 1.121647 Loss 9.436682, Accuracy 66.439%\n",
      "Epoch 5, Batch 403, LR 1.121940 Loss 9.436131, Accuracy 66.451%\n",
      "Epoch 5, Batch 404, LR 1.122233 Loss 9.435395, Accuracy 66.460%\n",
      "Epoch 5, Batch 405, LR 1.122526 Loss 9.432080, Accuracy 66.462%\n",
      "Epoch 5, Batch 406, LR 1.122820 Loss 9.431892, Accuracy 66.464%\n",
      "Epoch 5, Batch 407, LR 1.123113 Loss 9.430741, Accuracy 66.470%\n",
      "Epoch 5, Batch 408, LR 1.123406 Loss 9.429804, Accuracy 66.477%\n",
      "Epoch 5, Batch 409, LR 1.123699 Loss 9.428954, Accuracy 66.492%\n",
      "Epoch 5, Batch 410, LR 1.123992 Loss 9.427268, Accuracy 66.511%\n",
      "Epoch 5, Batch 411, LR 1.124285 Loss 9.427092, Accuracy 66.507%\n",
      "Epoch 5, Batch 412, LR 1.124579 Loss 9.427550, Accuracy 66.520%\n",
      "Epoch 5, Batch 413, LR 1.124872 Loss 9.428901, Accuracy 66.516%\n",
      "Epoch 5, Batch 414, LR 1.125165 Loss 9.428498, Accuracy 66.525%\n",
      "Epoch 5, Batch 415, LR 1.125458 Loss 9.428642, Accuracy 66.523%\n",
      "Epoch 5, Batch 416, LR 1.125752 Loss 9.427892, Accuracy 66.525%\n",
      "Epoch 5, Batch 417, LR 1.126045 Loss 9.427786, Accuracy 66.524%\n",
      "Epoch 5, Batch 418, LR 1.126338 Loss 9.426913, Accuracy 66.528%\n",
      "Epoch 5, Batch 419, LR 1.126631 Loss 9.425794, Accuracy 66.526%\n",
      "Epoch 5, Batch 420, LR 1.126925 Loss 9.425632, Accuracy 66.527%\n",
      "Epoch 5, Batch 421, LR 1.127218 Loss 9.426314, Accuracy 66.534%\n",
      "Epoch 5, Batch 422, LR 1.127511 Loss 9.427252, Accuracy 66.534%\n",
      "Epoch 5, Batch 423, LR 1.127805 Loss 9.427203, Accuracy 66.532%\n",
      "Epoch 5, Batch 424, LR 1.128098 Loss 9.427348, Accuracy 66.533%\n",
      "Epoch 5, Batch 425, LR 1.128391 Loss 9.427118, Accuracy 66.544%\n",
      "Epoch 5, Batch 426, LR 1.128685 Loss 9.427971, Accuracy 66.535%\n",
      "Epoch 5, Batch 427, LR 1.128978 Loss 9.427568, Accuracy 66.551%\n",
      "Epoch 5, Batch 428, LR 1.129272 Loss 9.425485, Accuracy 66.560%\n",
      "Epoch 5, Batch 429, LR 1.129565 Loss 9.426075, Accuracy 66.554%\n",
      "Epoch 5, Batch 430, LR 1.129858 Loss 9.425511, Accuracy 66.557%\n",
      "Epoch 5, Batch 431, LR 1.130152 Loss 9.425635, Accuracy 66.553%\n",
      "Epoch 5, Batch 432, LR 1.130445 Loss 9.426460, Accuracy 66.536%\n",
      "Epoch 5, Batch 433, LR 1.130739 Loss 9.427265, Accuracy 66.534%\n",
      "Epoch 5, Batch 434, LR 1.131032 Loss 9.428055, Accuracy 66.532%\n",
      "Epoch 5, Batch 435, LR 1.131326 Loss 9.427456, Accuracy 66.536%\n",
      "Epoch 5, Batch 436, LR 1.131619 Loss 9.429035, Accuracy 66.521%\n",
      "Epoch 5, Batch 437, LR 1.131913 Loss 9.427650, Accuracy 66.530%\n",
      "Epoch 5, Batch 438, LR 1.132206 Loss 9.427524, Accuracy 66.522%\n",
      "Epoch 5, Batch 439, LR 1.132500 Loss 9.428508, Accuracy 66.504%\n",
      "Epoch 5, Batch 440, LR 1.132793 Loss 9.427133, Accuracy 66.504%\n",
      "Epoch 5, Batch 441, LR 1.133087 Loss 9.425957, Accuracy 66.502%\n",
      "Epoch 5, Batch 442, LR 1.133380 Loss 9.425831, Accuracy 66.509%\n",
      "Epoch 5, Batch 443, LR 1.133674 Loss 9.425471, Accuracy 66.505%\n",
      "Epoch 5, Batch 444, LR 1.133967 Loss 9.425095, Accuracy 66.510%\n",
      "Epoch 5, Batch 445, LR 1.134261 Loss 9.424814, Accuracy 66.512%\n",
      "Epoch 5, Batch 446, LR 1.134555 Loss 9.424453, Accuracy 66.513%\n",
      "Epoch 5, Batch 447, LR 1.134848 Loss 9.423718, Accuracy 66.515%\n",
      "Epoch 5, Batch 448, LR 1.135142 Loss 9.423607, Accuracy 66.523%\n",
      "Epoch 5, Batch 449, LR 1.135435 Loss 9.422211, Accuracy 66.533%\n",
      "Epoch 5, Batch 450, LR 1.135729 Loss 9.421882, Accuracy 66.530%\n",
      "Epoch 5, Batch 451, LR 1.136023 Loss 9.422576, Accuracy 66.517%\n",
      "Epoch 5, Batch 452, LR 1.136316 Loss 9.422908, Accuracy 66.512%\n",
      "Epoch 5, Batch 453, LR 1.136610 Loss 9.422682, Accuracy 66.517%\n",
      "Epoch 5, Batch 454, LR 1.136904 Loss 9.423392, Accuracy 66.508%\n",
      "Epoch 5, Batch 455, LR 1.137197 Loss 9.424301, Accuracy 66.506%\n",
      "Epoch 5, Batch 456, LR 1.137491 Loss 9.424880, Accuracy 66.500%\n",
      "Epoch 5, Batch 457, LR 1.137785 Loss 9.423685, Accuracy 66.499%\n",
      "Epoch 5, Batch 458, LR 1.138079 Loss 9.424165, Accuracy 66.503%\n",
      "Epoch 5, Batch 459, LR 1.138372 Loss 9.422631, Accuracy 66.513%\n",
      "Epoch 5, Batch 460, LR 1.138666 Loss 9.422254, Accuracy 66.520%\n",
      "Epoch 5, Batch 461, LR 1.138960 Loss 9.421887, Accuracy 66.525%\n",
      "Epoch 5, Batch 462, LR 1.139254 Loss 9.421461, Accuracy 66.531%\n",
      "Epoch 5, Batch 463, LR 1.139547 Loss 9.419578, Accuracy 66.543%\n",
      "Epoch 5, Batch 464, LR 1.139841 Loss 9.418395, Accuracy 66.551%\n",
      "Epoch 5, Batch 465, LR 1.140135 Loss 9.416267, Accuracy 66.561%\n",
      "Epoch 5, Batch 466, LR 1.140429 Loss 9.415405, Accuracy 66.555%\n",
      "Epoch 5, Batch 467, LR 1.140723 Loss 9.415931, Accuracy 66.557%\n",
      "Epoch 5, Batch 468, LR 1.141016 Loss 9.415626, Accuracy 66.551%\n",
      "Epoch 5, Batch 469, LR 1.141310 Loss 9.416646, Accuracy 66.550%\n",
      "Epoch 5, Batch 470, LR 1.141604 Loss 9.415388, Accuracy 66.558%\n",
      "Epoch 5, Batch 471, LR 1.141898 Loss 9.416083, Accuracy 66.551%\n",
      "Epoch 5, Batch 472, LR 1.142192 Loss 9.416230, Accuracy 66.552%\n",
      "Epoch 5, Batch 473, LR 1.142486 Loss 9.414818, Accuracy 66.565%\n",
      "Epoch 5, Batch 474, LR 1.142780 Loss 9.414241, Accuracy 66.571%\n",
      "Epoch 5, Batch 475, LR 1.143074 Loss 9.413306, Accuracy 66.577%\n",
      "Epoch 5, Batch 476, LR 1.143368 Loss 9.411176, Accuracy 66.595%\n",
      "Epoch 5, Batch 477, LR 1.143661 Loss 9.410449, Accuracy 66.598%\n",
      "Epoch 5, Batch 478, LR 1.143955 Loss 9.410504, Accuracy 66.591%\n",
      "Epoch 5, Batch 479, LR 1.144249 Loss 9.410767, Accuracy 66.591%\n",
      "Epoch 5, Batch 480, LR 1.144543 Loss 9.410143, Accuracy 66.589%\n",
      "Epoch 5, Batch 481, LR 1.144837 Loss 9.410034, Accuracy 66.588%\n",
      "Epoch 5, Batch 482, LR 1.145131 Loss 9.410358, Accuracy 66.581%\n",
      "Epoch 5, Batch 483, LR 1.145425 Loss 9.409231, Accuracy 66.592%\n",
      "Epoch 5, Batch 484, LR 1.145719 Loss 9.408515, Accuracy 66.592%\n",
      "Epoch 5, Batch 485, LR 1.146013 Loss 9.407604, Accuracy 66.593%\n",
      "Epoch 5, Batch 486, LR 1.146307 Loss 9.406618, Accuracy 66.609%\n",
      "Epoch 5, Batch 487, LR 1.146601 Loss 9.404007, Accuracy 66.631%\n",
      "Epoch 5, Batch 488, LR 1.146895 Loss 9.404754, Accuracy 66.618%\n",
      "Epoch 5, Batch 489, LR 1.147189 Loss 9.404963, Accuracy 66.625%\n",
      "Epoch 5, Batch 490, LR 1.147484 Loss 9.405757, Accuracy 66.621%\n",
      "Epoch 5, Batch 491, LR 1.147778 Loss 9.405858, Accuracy 66.623%\n",
      "Epoch 5, Batch 492, LR 1.148072 Loss 9.406372, Accuracy 66.622%\n",
      "Epoch 5, Batch 493, LR 1.148366 Loss 9.406032, Accuracy 66.622%\n",
      "Epoch 5, Batch 494, LR 1.148660 Loss 9.406350, Accuracy 66.626%\n",
      "Epoch 5, Batch 495, LR 1.148954 Loss 9.405793, Accuracy 66.635%\n",
      "Epoch 5, Batch 496, LR 1.149248 Loss 9.405031, Accuracy 66.641%\n",
      "Epoch 5, Batch 497, LR 1.149542 Loss 9.404435, Accuracy 66.637%\n",
      "Epoch 5, Batch 498, LR 1.149836 Loss 9.404216, Accuracy 66.638%\n",
      "Epoch 5, Batch 499, LR 1.150131 Loss 9.406786, Accuracy 66.618%\n",
      "Epoch 5, Batch 500, LR 1.150425 Loss 9.407458, Accuracy 66.611%\n",
      "Epoch 5, Batch 501, LR 1.150719 Loss 9.409201, Accuracy 66.606%\n",
      "Epoch 5, Batch 502, LR 1.151013 Loss 9.409643, Accuracy 66.604%\n",
      "Epoch 5, Batch 503, LR 1.151307 Loss 9.409475, Accuracy 66.608%\n",
      "Epoch 5, Batch 504, LR 1.151602 Loss 9.409816, Accuracy 66.614%\n",
      "Epoch 5, Batch 505, LR 1.151896 Loss 9.409405, Accuracy 66.607%\n",
      "Epoch 5, Batch 506, LR 1.152190 Loss 9.409070, Accuracy 66.621%\n",
      "Epoch 5, Batch 507, LR 1.152484 Loss 9.409998, Accuracy 66.616%\n",
      "Epoch 5, Batch 508, LR 1.152779 Loss 9.410983, Accuracy 66.605%\n",
      "Epoch 5, Batch 509, LR 1.153073 Loss 9.410424, Accuracy 66.607%\n",
      "Epoch 5, Batch 510, LR 1.153367 Loss 9.410763, Accuracy 66.610%\n",
      "Epoch 5, Batch 511, LR 1.153661 Loss 9.410819, Accuracy 66.614%\n",
      "Epoch 5, Batch 512, LR 1.153956 Loss 9.409345, Accuracy 66.623%\n",
      "Epoch 5, Batch 513, LR 1.154250 Loss 9.410608, Accuracy 66.616%\n",
      "Epoch 5, Batch 514, LR 1.154544 Loss 9.410427, Accuracy 66.619%\n",
      "Epoch 5, Batch 515, LR 1.154839 Loss 9.410395, Accuracy 66.622%\n",
      "Epoch 5, Batch 516, LR 1.155133 Loss 9.410112, Accuracy 66.633%\n",
      "Epoch 5, Batch 517, LR 1.155427 Loss 9.409010, Accuracy 66.645%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 518, LR 1.155722 Loss 9.408546, Accuracy 66.655%\n",
      "Epoch 5, Batch 519, LR 1.156016 Loss 9.407848, Accuracy 66.658%\n",
      "Epoch 5, Batch 520, LR 1.156310 Loss 9.407800, Accuracy 66.657%\n",
      "Epoch 5, Batch 521, LR 1.156605 Loss 9.408571, Accuracy 66.646%\n",
      "Epoch 5, Batch 522, LR 1.156899 Loss 9.407193, Accuracy 66.656%\n",
      "Epoch 5, Batch 523, LR 1.157193 Loss 9.406961, Accuracy 66.653%\n",
      "Epoch 5, Batch 524, LR 1.157488 Loss 9.406470, Accuracy 66.655%\n",
      "Epoch 5, Batch 525, LR 1.157782 Loss 9.405739, Accuracy 66.661%\n",
      "Epoch 5, Batch 526, LR 1.158077 Loss 9.405444, Accuracy 66.659%\n",
      "Epoch 5, Batch 527, LR 1.158371 Loss 9.404585, Accuracy 66.664%\n",
      "Epoch 5, Batch 528, LR 1.158666 Loss 9.403559, Accuracy 66.674%\n",
      "Epoch 5, Batch 529, LR 1.158960 Loss 9.405172, Accuracy 66.660%\n",
      "Epoch 5, Batch 530, LR 1.159255 Loss 9.403331, Accuracy 66.676%\n",
      "Epoch 5, Batch 531, LR 1.159549 Loss 9.402194, Accuracy 66.680%\n",
      "Epoch 5, Batch 532, LR 1.159843 Loss 9.401950, Accuracy 66.675%\n",
      "Epoch 5, Batch 533, LR 1.160138 Loss 9.401003, Accuracy 66.685%\n",
      "Epoch 5, Batch 534, LR 1.160432 Loss 9.401864, Accuracy 66.683%\n",
      "Epoch 5, Batch 535, LR 1.160727 Loss 9.400955, Accuracy 66.685%\n",
      "Epoch 5, Batch 536, LR 1.161022 Loss 9.400075, Accuracy 66.692%\n",
      "Epoch 5, Batch 537, LR 1.161316 Loss 9.399021, Accuracy 66.693%\n",
      "Epoch 5, Batch 538, LR 1.161611 Loss 9.399633, Accuracy 66.688%\n",
      "Epoch 5, Batch 539, LR 1.161905 Loss 9.399618, Accuracy 66.685%\n",
      "Epoch 5, Batch 540, LR 1.162200 Loss 9.398888, Accuracy 66.688%\n",
      "Epoch 5, Batch 541, LR 1.162494 Loss 9.398725, Accuracy 66.684%\n",
      "Epoch 5, Batch 542, LR 1.162789 Loss 9.399873, Accuracy 66.664%\n",
      "Epoch 5, Batch 543, LR 1.163083 Loss 9.400006, Accuracy 66.665%\n",
      "Epoch 5, Batch 544, LR 1.163378 Loss 9.400665, Accuracy 66.666%\n",
      "Epoch 5, Batch 545, LR 1.163673 Loss 9.400204, Accuracy 66.671%\n",
      "Epoch 5, Batch 546, LR 1.163967 Loss 9.401321, Accuracy 66.661%\n",
      "Epoch 5, Batch 547, LR 1.164262 Loss 9.400905, Accuracy 66.673%\n",
      "Epoch 5, Batch 548, LR 1.164557 Loss 9.401384, Accuracy 66.673%\n",
      "Epoch 5, Batch 549, LR 1.164851 Loss 9.400784, Accuracy 66.678%\n",
      "Epoch 5, Batch 550, LR 1.165146 Loss 9.400874, Accuracy 66.678%\n",
      "Epoch 5, Batch 551, LR 1.165441 Loss 9.401632, Accuracy 66.670%\n",
      "Epoch 5, Batch 552, LR 1.165735 Loss 9.400510, Accuracy 66.684%\n",
      "Epoch 5, Batch 553, LR 1.166030 Loss 9.400299, Accuracy 66.693%\n",
      "Epoch 5, Batch 554, LR 1.166325 Loss 9.401349, Accuracy 66.681%\n",
      "Epoch 5, Batch 555, LR 1.166619 Loss 9.401111, Accuracy 66.692%\n",
      "Epoch 5, Batch 556, LR 1.166914 Loss 9.400920, Accuracy 66.684%\n",
      "Epoch 5, Batch 557, LR 1.167209 Loss 9.400353, Accuracy 66.694%\n",
      "Epoch 5, Batch 558, LR 1.167503 Loss 9.400490, Accuracy 66.696%\n",
      "Epoch 5, Batch 559, LR 1.167798 Loss 9.398299, Accuracy 66.715%\n",
      "Epoch 5, Batch 560, LR 1.168093 Loss 9.397767, Accuracy 66.722%\n",
      "Epoch 5, Batch 561, LR 1.168388 Loss 9.395747, Accuracy 66.739%\n",
      "Epoch 5, Batch 562, LR 1.168682 Loss 9.395544, Accuracy 66.743%\n",
      "Epoch 5, Batch 563, LR 1.168977 Loss 9.394370, Accuracy 66.755%\n",
      "Epoch 5, Batch 564, LR 1.169272 Loss 9.392718, Accuracy 66.766%\n",
      "Epoch 5, Batch 565, LR 1.169567 Loss 9.393091, Accuracy 66.773%\n",
      "Epoch 5, Batch 566, LR 1.169862 Loss 9.392827, Accuracy 66.771%\n",
      "Epoch 5, Batch 567, LR 1.170156 Loss 9.392527, Accuracy 66.769%\n",
      "Epoch 5, Batch 568, LR 1.170451 Loss 9.391270, Accuracy 66.780%\n",
      "Epoch 5, Batch 569, LR 1.170746 Loss 9.391580, Accuracy 66.777%\n",
      "Epoch 5, Batch 570, LR 1.171041 Loss 9.392141, Accuracy 66.767%\n",
      "Epoch 5, Batch 571, LR 1.171336 Loss 9.391905, Accuracy 66.765%\n",
      "Epoch 5, Batch 572, LR 1.171631 Loss 9.391398, Accuracy 66.763%\n",
      "Epoch 5, Batch 573, LR 1.171925 Loss 9.392892, Accuracy 66.751%\n",
      "Epoch 5, Batch 574, LR 1.172220 Loss 9.391777, Accuracy 66.756%\n",
      "Epoch 5, Batch 575, LR 1.172515 Loss 9.391904, Accuracy 66.749%\n",
      "Epoch 5, Batch 576, LR 1.172810 Loss 9.392543, Accuracy 66.737%\n",
      "Epoch 5, Batch 577, LR 1.173105 Loss 9.392761, Accuracy 66.726%\n",
      "Epoch 5, Batch 578, LR 1.173400 Loss 9.392259, Accuracy 66.729%\n",
      "Epoch 5, Batch 579, LR 1.173695 Loss 9.392078, Accuracy 66.730%\n",
      "Epoch 5, Batch 580, LR 1.173990 Loss 9.393297, Accuracy 66.724%\n",
      "Epoch 5, Batch 581, LR 1.174285 Loss 9.393709, Accuracy 66.716%\n",
      "Epoch 5, Batch 582, LR 1.174580 Loss 9.392789, Accuracy 66.723%\n",
      "Epoch 5, Batch 583, LR 1.174875 Loss 9.391599, Accuracy 66.740%\n",
      "Epoch 5, Batch 584, LR 1.175169 Loss 9.391890, Accuracy 66.733%\n",
      "Epoch 5, Batch 585, LR 1.175464 Loss 9.390421, Accuracy 66.745%\n",
      "Epoch 5, Batch 586, LR 1.175759 Loss 9.389764, Accuracy 66.749%\n",
      "Epoch 5, Batch 587, LR 1.176054 Loss 9.390029, Accuracy 66.751%\n",
      "Epoch 5, Batch 588, LR 1.176349 Loss 9.390093, Accuracy 66.748%\n",
      "Epoch 5, Batch 589, LR 1.176644 Loss 9.389294, Accuracy 66.758%\n",
      "Epoch 5, Batch 590, LR 1.176939 Loss 9.388849, Accuracy 66.760%\n",
      "Epoch 5, Batch 591, LR 1.177234 Loss 9.389344, Accuracy 66.754%\n",
      "Epoch 5, Batch 592, LR 1.177529 Loss 9.389799, Accuracy 66.753%\n",
      "Epoch 5, Batch 593, LR 1.177824 Loss 9.389776, Accuracy 66.759%\n",
      "Epoch 5, Batch 594, LR 1.178120 Loss 9.390799, Accuracy 66.746%\n",
      "Epoch 5, Batch 595, LR 1.178415 Loss 9.392625, Accuracy 66.725%\n",
      "Epoch 5, Batch 596, LR 1.178710 Loss 9.391599, Accuracy 66.734%\n",
      "Epoch 5, Batch 597, LR 1.179005 Loss 9.390378, Accuracy 66.744%\n",
      "Epoch 5, Batch 598, LR 1.179300 Loss 9.390254, Accuracy 66.746%\n",
      "Epoch 5, Batch 599, LR 1.179595 Loss 9.390579, Accuracy 66.740%\n",
      "Epoch 5, Batch 600, LR 1.179890 Loss 9.391331, Accuracy 66.737%\n",
      "Epoch 5, Batch 601, LR 1.180185 Loss 9.390428, Accuracy 66.739%\n",
      "Epoch 5, Batch 602, LR 1.180480 Loss 9.390071, Accuracy 66.738%\n",
      "Epoch 5, Batch 603, LR 1.180775 Loss 9.390140, Accuracy 66.738%\n",
      "Epoch 5, Batch 604, LR 1.181070 Loss 9.388871, Accuracy 66.750%\n",
      "Epoch 5, Batch 605, LR 1.181366 Loss 9.389283, Accuracy 66.743%\n",
      "Epoch 5, Batch 606, LR 1.181661 Loss 9.388189, Accuracy 66.756%\n",
      "Epoch 5, Batch 607, LR 1.181956 Loss 9.389033, Accuracy 66.741%\n",
      "Epoch 5, Batch 608, LR 1.182251 Loss 9.388101, Accuracy 66.748%\n",
      "Epoch 5, Batch 609, LR 1.182546 Loss 9.389176, Accuracy 66.744%\n",
      "Epoch 5, Batch 610, LR 1.182841 Loss 9.389546, Accuracy 66.746%\n",
      "Epoch 5, Batch 611, LR 1.183137 Loss 9.388989, Accuracy 66.746%\n",
      "Epoch 5, Batch 612, LR 1.183432 Loss 9.387756, Accuracy 66.756%\n",
      "Epoch 5, Batch 613, LR 1.183727 Loss 9.387762, Accuracy 66.754%\n",
      "Epoch 5, Batch 614, LR 1.184022 Loss 9.387526, Accuracy 66.763%\n",
      "Epoch 5, Batch 615, LR 1.184317 Loss 9.388122, Accuracy 66.767%\n",
      "Epoch 5, Batch 616, LR 1.184613 Loss 9.388083, Accuracy 66.766%\n",
      "Epoch 5, Batch 617, LR 1.184908 Loss 9.387912, Accuracy 66.770%\n",
      "Epoch 5, Batch 618, LR 1.185203 Loss 9.388114, Accuracy 66.764%\n",
      "Epoch 5, Batch 619, LR 1.185498 Loss 9.387625, Accuracy 66.770%\n",
      "Epoch 5, Batch 620, LR 1.185794 Loss 9.386468, Accuracy 66.786%\n",
      "Epoch 5, Batch 621, LR 1.186089 Loss 9.385859, Accuracy 66.794%\n",
      "Epoch 5, Batch 622, LR 1.186384 Loss 9.385830, Accuracy 66.797%\n",
      "Epoch 5, Batch 623, LR 1.186679 Loss 9.386083, Accuracy 66.790%\n",
      "Epoch 5, Batch 624, LR 1.186975 Loss 9.385979, Accuracy 66.791%\n",
      "Epoch 5, Batch 625, LR 1.187270 Loss 9.386220, Accuracy 66.786%\n",
      "Epoch 5, Batch 626, LR 1.187565 Loss 9.386412, Accuracy 66.787%\n",
      "Epoch 5, Batch 627, LR 1.187861 Loss 9.385926, Accuracy 66.789%\n",
      "Epoch 5, Batch 628, LR 1.188156 Loss 9.385994, Accuracy 66.788%\n",
      "Epoch 5, Batch 629, LR 1.188451 Loss 9.385508, Accuracy 66.790%\n",
      "Epoch 5, Batch 630, LR 1.188747 Loss 9.384864, Accuracy 66.806%\n",
      "Epoch 5, Batch 631, LR 1.189042 Loss 9.385387, Accuracy 66.805%\n",
      "Epoch 5, Batch 632, LR 1.189337 Loss 9.386429, Accuracy 66.797%\n",
      "Epoch 5, Batch 633, LR 1.189633 Loss 9.386570, Accuracy 66.793%\n",
      "Epoch 5, Batch 634, LR 1.189928 Loss 9.386473, Accuracy 66.793%\n",
      "Epoch 5, Batch 635, LR 1.190223 Loss 9.387180, Accuracy 66.789%\n",
      "Epoch 5, Batch 636, LR 1.190519 Loss 9.386598, Accuracy 66.796%\n",
      "Epoch 5, Batch 637, LR 1.190814 Loss 9.386231, Accuracy 66.804%\n",
      "Epoch 5, Batch 638, LR 1.191109 Loss 9.386546, Accuracy 66.799%\n",
      "Epoch 5, Batch 639, LR 1.191405 Loss 9.385504, Accuracy 66.806%\n",
      "Epoch 5, Batch 640, LR 1.191700 Loss 9.385667, Accuracy 66.807%\n",
      "Epoch 5, Batch 641, LR 1.191996 Loss 9.386500, Accuracy 66.794%\n",
      "Epoch 5, Batch 642, LR 1.192291 Loss 9.386767, Accuracy 66.793%\n",
      "Epoch 5, Batch 643, LR 1.192587 Loss 9.385632, Accuracy 66.796%\n",
      "Epoch 5, Batch 644, LR 1.192882 Loss 9.385883, Accuracy 66.794%\n",
      "Epoch 5, Batch 645, LR 1.193177 Loss 9.386905, Accuracy 66.796%\n",
      "Epoch 5, Batch 646, LR 1.193473 Loss 9.387163, Accuracy 66.796%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 647, LR 1.193768 Loss 9.385829, Accuracy 66.805%\n",
      "Epoch 5, Batch 648, LR 1.194064 Loss 9.385701, Accuracy 66.809%\n",
      "Epoch 5, Batch 649, LR 1.194359 Loss 9.386285, Accuracy 66.797%\n",
      "Epoch 5, Batch 650, LR 1.194655 Loss 9.386089, Accuracy 66.794%\n",
      "Epoch 5, Batch 651, LR 1.194950 Loss 9.385343, Accuracy 66.805%\n",
      "Epoch 5, Batch 652, LR 1.195246 Loss 9.386356, Accuracy 66.800%\n",
      "Epoch 5, Batch 653, LR 1.195541 Loss 9.385235, Accuracy 66.814%\n",
      "Epoch 5, Batch 654, LR 1.195837 Loss 9.384787, Accuracy 66.811%\n",
      "Epoch 5, Batch 655, LR 1.196132 Loss 9.384575, Accuracy 66.811%\n",
      "Epoch 5, Batch 656, LR 1.196428 Loss 9.384789, Accuracy 66.812%\n",
      "Epoch 5, Batch 657, LR 1.196723 Loss 9.384799, Accuracy 66.814%\n",
      "Epoch 5, Batch 658, LR 1.197019 Loss 9.386258, Accuracy 66.797%\n",
      "Epoch 5, Batch 659, LR 1.197315 Loss 9.386509, Accuracy 66.792%\n",
      "Epoch 5, Batch 660, LR 1.197610 Loss 9.387002, Accuracy 66.791%\n",
      "Epoch 5, Batch 661, LR 1.197906 Loss 9.386892, Accuracy 66.795%\n",
      "Epoch 5, Batch 662, LR 1.198201 Loss 9.386353, Accuracy 66.803%\n",
      "Epoch 5, Batch 663, LR 1.198497 Loss 9.386479, Accuracy 66.799%\n",
      "Epoch 5, Batch 664, LR 1.198792 Loss 9.386506, Accuracy 66.795%\n",
      "Epoch 5, Batch 665, LR 1.199088 Loss 9.386187, Accuracy 66.795%\n",
      "Epoch 5, Batch 666, LR 1.199384 Loss 9.386092, Accuracy 66.796%\n",
      "Epoch 5, Batch 667, LR 1.199679 Loss 9.385554, Accuracy 66.807%\n",
      "Epoch 5, Batch 668, LR 1.199975 Loss 9.385518, Accuracy 66.804%\n",
      "Epoch 5, Batch 669, LR 1.200270 Loss 9.385799, Accuracy 66.802%\n",
      "Epoch 5, Batch 670, LR 1.200566 Loss 9.385734, Accuracy 66.804%\n",
      "Epoch 5, Batch 671, LR 1.200862 Loss 9.385931, Accuracy 66.795%\n",
      "Epoch 5, Batch 672, LR 1.201157 Loss 9.385998, Accuracy 66.798%\n",
      "Epoch 5, Batch 673, LR 1.201453 Loss 9.385931, Accuracy 66.803%\n",
      "Epoch 5, Batch 674, LR 1.201749 Loss 9.384999, Accuracy 66.808%\n",
      "Epoch 5, Batch 675, LR 1.202044 Loss 9.385819, Accuracy 66.803%\n",
      "Epoch 5, Batch 676, LR 1.202340 Loss 9.385609, Accuracy 66.804%\n",
      "Epoch 5, Batch 677, LR 1.202636 Loss 9.385668, Accuracy 66.809%\n",
      "Epoch 5, Batch 678, LR 1.202931 Loss 9.384882, Accuracy 66.815%\n",
      "Epoch 5, Batch 679, LR 1.203227 Loss 9.385500, Accuracy 66.814%\n",
      "Epoch 5, Batch 680, LR 1.203523 Loss 9.384419, Accuracy 66.828%\n",
      "Epoch 5, Batch 681, LR 1.203818 Loss 9.384528, Accuracy 66.826%\n",
      "Epoch 5, Batch 682, LR 1.204114 Loss 9.384537, Accuracy 66.829%\n",
      "Epoch 5, Batch 683, LR 1.204410 Loss 9.383872, Accuracy 66.842%\n",
      "Epoch 5, Batch 684, LR 1.204706 Loss 9.384093, Accuracy 66.844%\n",
      "Epoch 5, Batch 685, LR 1.205001 Loss 9.383818, Accuracy 66.843%\n",
      "Epoch 5, Batch 686, LR 1.205297 Loss 9.382636, Accuracy 66.850%\n",
      "Epoch 5, Batch 687, LR 1.205593 Loss 9.383494, Accuracy 66.847%\n",
      "Epoch 5, Batch 688, LR 1.205889 Loss 9.383699, Accuracy 66.847%\n",
      "Epoch 5, Batch 689, LR 1.206184 Loss 9.383663, Accuracy 66.845%\n",
      "Epoch 5, Batch 690, LR 1.206480 Loss 9.383935, Accuracy 66.843%\n",
      "Epoch 5, Batch 691, LR 1.206776 Loss 9.383059, Accuracy 66.849%\n",
      "Epoch 5, Batch 692, LR 1.207072 Loss 9.382812, Accuracy 66.856%\n",
      "Epoch 5, Batch 693, LR 1.207367 Loss 9.381708, Accuracy 66.858%\n",
      "Epoch 5, Batch 694, LR 1.207663 Loss 9.381588, Accuracy 66.851%\n",
      "Epoch 5, Batch 695, LR 1.207959 Loss 9.381518, Accuracy 66.857%\n",
      "Epoch 5, Batch 696, LR 1.208255 Loss 9.381570, Accuracy 66.859%\n",
      "Epoch 5, Batch 697, LR 1.208551 Loss 9.381998, Accuracy 66.857%\n",
      "Epoch 5, Batch 698, LR 1.208846 Loss 9.381456, Accuracy 66.858%\n",
      "Epoch 5, Batch 699, LR 1.209142 Loss 9.382729, Accuracy 66.854%\n",
      "Epoch 5, Batch 700, LR 1.209438 Loss 9.383808, Accuracy 66.843%\n",
      "Epoch 5, Batch 701, LR 1.209734 Loss 9.383422, Accuracy 66.836%\n",
      "Epoch 5, Batch 702, LR 1.210030 Loss 9.382063, Accuracy 66.848%\n",
      "Epoch 5, Batch 703, LR 1.210326 Loss 9.381954, Accuracy 66.849%\n",
      "Epoch 5, Batch 704, LR 1.210621 Loss 9.381329, Accuracy 66.853%\n",
      "Epoch 5, Batch 705, LR 1.210917 Loss 9.381802, Accuracy 66.842%\n",
      "Epoch 5, Batch 706, LR 1.211213 Loss 9.382110, Accuracy 66.848%\n",
      "Epoch 5, Batch 707, LR 1.211509 Loss 9.382598, Accuracy 66.849%\n",
      "Epoch 5, Batch 708, LR 1.211805 Loss 9.382399, Accuracy 66.848%\n",
      "Epoch 5, Batch 709, LR 1.212101 Loss 9.382580, Accuracy 66.850%\n",
      "Epoch 5, Batch 710, LR 1.212397 Loss 9.381978, Accuracy 66.853%\n",
      "Epoch 5, Batch 711, LR 1.212693 Loss 9.381642, Accuracy 66.860%\n",
      "Epoch 5, Batch 712, LR 1.212989 Loss 9.381629, Accuracy 66.862%\n",
      "Epoch 5, Batch 713, LR 1.213284 Loss 9.380932, Accuracy 66.864%\n",
      "Epoch 5, Batch 714, LR 1.213580 Loss 9.380609, Accuracy 66.866%\n",
      "Epoch 5, Batch 715, LR 1.213876 Loss 9.379531, Accuracy 66.866%\n",
      "Epoch 5, Batch 716, LR 1.214172 Loss 9.379329, Accuracy 66.866%\n",
      "Epoch 5, Batch 717, LR 1.214468 Loss 9.380144, Accuracy 66.855%\n",
      "Epoch 5, Batch 718, LR 1.214764 Loss 9.379651, Accuracy 66.862%\n",
      "Epoch 5, Batch 719, LR 1.215060 Loss 9.380507, Accuracy 66.854%\n",
      "Epoch 5, Batch 720, LR 1.215356 Loss 9.379685, Accuracy 66.861%\n",
      "Epoch 5, Batch 721, LR 1.215652 Loss 9.379969, Accuracy 66.854%\n",
      "Epoch 5, Batch 722, LR 1.215948 Loss 9.379902, Accuracy 66.855%\n",
      "Epoch 5, Batch 723, LR 1.216244 Loss 9.380513, Accuracy 66.841%\n",
      "Epoch 5, Batch 724, LR 1.216540 Loss 9.379963, Accuracy 66.849%\n",
      "Epoch 5, Batch 725, LR 1.216836 Loss 9.379642, Accuracy 66.852%\n",
      "Epoch 5, Batch 726, LR 1.217132 Loss 9.379718, Accuracy 66.854%\n",
      "Epoch 5, Batch 727, LR 1.217428 Loss 9.380149, Accuracy 66.850%\n",
      "Epoch 5, Batch 728, LR 1.217724 Loss 9.380170, Accuracy 66.851%\n",
      "Epoch 5, Batch 729, LR 1.218020 Loss 9.380075, Accuracy 66.850%\n",
      "Epoch 5, Batch 730, LR 1.218316 Loss 9.379876, Accuracy 66.850%\n",
      "Epoch 5, Batch 731, LR 1.218612 Loss 9.379536, Accuracy 66.848%\n",
      "Epoch 5, Batch 732, LR 1.218908 Loss 9.378639, Accuracy 66.852%\n",
      "Epoch 5, Batch 733, LR 1.219204 Loss 9.378481, Accuracy 66.858%\n",
      "Epoch 5, Batch 734, LR 1.219500 Loss 9.379147, Accuracy 66.853%\n",
      "Epoch 5, Batch 735, LR 1.219796 Loss 9.379338, Accuracy 66.844%\n",
      "Epoch 5, Batch 736, LR 1.220092 Loss 9.378957, Accuracy 66.845%\n",
      "Epoch 5, Batch 737, LR 1.220388 Loss 9.378208, Accuracy 66.854%\n",
      "Epoch 5, Batch 738, LR 1.220684 Loss 9.378008, Accuracy 66.857%\n",
      "Epoch 5, Batch 739, LR 1.220980 Loss 9.378436, Accuracy 66.853%\n",
      "Epoch 5, Batch 740, LR 1.221276 Loss 9.378710, Accuracy 66.853%\n",
      "Epoch 5, Batch 741, LR 1.221572 Loss 9.378257, Accuracy 66.860%\n",
      "Epoch 5, Batch 742, LR 1.221869 Loss 9.377980, Accuracy 66.858%\n",
      "Epoch 5, Batch 743, LR 1.222165 Loss 9.377367, Accuracy 66.866%\n",
      "Epoch 5, Batch 744, LR 1.222461 Loss 9.377309, Accuracy 66.868%\n",
      "Epoch 5, Batch 745, LR 1.222757 Loss 9.377029, Accuracy 66.869%\n",
      "Epoch 5, Batch 746, LR 1.223053 Loss 9.376338, Accuracy 66.876%\n",
      "Epoch 5, Batch 747, LR 1.223349 Loss 9.376915, Accuracy 66.873%\n",
      "Epoch 5, Batch 748, LR 1.223645 Loss 9.377635, Accuracy 66.866%\n",
      "Epoch 5, Batch 749, LR 1.223941 Loss 9.376792, Accuracy 66.870%\n",
      "Epoch 5, Batch 750, LR 1.224237 Loss 9.376420, Accuracy 66.868%\n",
      "Epoch 5, Batch 751, LR 1.224534 Loss 9.377125, Accuracy 66.861%\n",
      "Epoch 5, Batch 752, LR 1.224830 Loss 9.377843, Accuracy 66.849%\n",
      "Epoch 5, Batch 753, LR 1.225126 Loss 9.377450, Accuracy 66.847%\n",
      "Epoch 5, Batch 754, LR 1.225422 Loss 9.377098, Accuracy 66.850%\n",
      "Epoch 5, Batch 755, LR 1.225718 Loss 9.376921, Accuracy 66.850%\n",
      "Epoch 5, Batch 756, LR 1.226014 Loss 9.376525, Accuracy 66.856%\n",
      "Epoch 5, Batch 757, LR 1.226311 Loss 9.376355, Accuracy 66.858%\n",
      "Epoch 5, Batch 758, LR 1.226607 Loss 9.376730, Accuracy 66.855%\n",
      "Epoch 5, Batch 759, LR 1.226903 Loss 9.376828, Accuracy 66.853%\n",
      "Epoch 5, Batch 760, LR 1.227199 Loss 9.376149, Accuracy 66.860%\n",
      "Epoch 5, Batch 761, LR 1.227495 Loss 9.376187, Accuracy 66.863%\n",
      "Epoch 5, Batch 762, LR 1.227791 Loss 9.376422, Accuracy 66.860%\n",
      "Epoch 5, Batch 763, LR 1.228088 Loss 9.376490, Accuracy 66.862%\n",
      "Epoch 5, Batch 764, LR 1.228384 Loss 9.375558, Accuracy 66.868%\n",
      "Epoch 5, Batch 765, LR 1.228680 Loss 9.374643, Accuracy 66.874%\n",
      "Epoch 5, Batch 766, LR 1.228976 Loss 9.374987, Accuracy 66.871%\n",
      "Epoch 5, Batch 767, LR 1.229273 Loss 9.374371, Accuracy 66.884%\n",
      "Epoch 5, Batch 768, LR 1.229569 Loss 9.374054, Accuracy 66.883%\n",
      "Epoch 5, Batch 769, LR 1.229865 Loss 9.374228, Accuracy 66.887%\n",
      "Epoch 5, Batch 770, LR 1.230161 Loss 9.373738, Accuracy 66.893%\n",
      "Epoch 5, Batch 771, LR 1.230457 Loss 9.374495, Accuracy 66.894%\n",
      "Epoch 5, Batch 772, LR 1.230754 Loss 9.374403, Accuracy 66.893%\n",
      "Epoch 5, Batch 773, LR 1.231050 Loss 9.374154, Accuracy 66.890%\n",
      "Epoch 5, Batch 774, LR 1.231346 Loss 9.373303, Accuracy 66.898%\n",
      "Epoch 5, Batch 775, LR 1.231642 Loss 9.373271, Accuracy 66.901%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 776, LR 1.231939 Loss 9.373735, Accuracy 66.900%\n",
      "Epoch 5, Batch 777, LR 1.232235 Loss 9.373551, Accuracy 66.900%\n",
      "Epoch 5, Batch 778, LR 1.232531 Loss 9.372913, Accuracy 66.906%\n",
      "Epoch 5, Batch 779, LR 1.232828 Loss 9.372556, Accuracy 66.908%\n",
      "Epoch 5, Batch 780, LR 1.233124 Loss 9.371959, Accuracy 66.911%\n",
      "Epoch 5, Batch 781, LR 1.233420 Loss 9.372064, Accuracy 66.914%\n",
      "Epoch 5, Batch 782, LR 1.233716 Loss 9.371799, Accuracy 66.922%\n",
      "Epoch 5, Batch 783, LR 1.234013 Loss 9.371567, Accuracy 66.925%\n",
      "Epoch 5, Batch 784, LR 1.234309 Loss 9.370897, Accuracy 66.936%\n",
      "Epoch 5, Batch 785, LR 1.234605 Loss 9.371432, Accuracy 66.937%\n",
      "Epoch 5, Batch 786, LR 1.234902 Loss 9.370654, Accuracy 66.948%\n",
      "Epoch 5, Batch 787, LR 1.235198 Loss 9.371337, Accuracy 66.942%\n",
      "Epoch 5, Batch 788, LR 1.235494 Loss 9.372244, Accuracy 66.937%\n",
      "Epoch 5, Batch 789, LR 1.235791 Loss 9.371897, Accuracy 66.938%\n",
      "Epoch 5, Batch 790, LR 1.236087 Loss 9.371380, Accuracy 66.945%\n",
      "Epoch 5, Batch 791, LR 1.236383 Loss 9.372420, Accuracy 66.933%\n",
      "Epoch 5, Batch 792, LR 1.236680 Loss 9.371691, Accuracy 66.938%\n",
      "Epoch 5, Batch 793, LR 1.236976 Loss 9.371437, Accuracy 66.938%\n",
      "Epoch 5, Batch 794, LR 1.237272 Loss 9.371381, Accuracy 66.941%\n",
      "Epoch 5, Batch 795, LR 1.237569 Loss 9.371177, Accuracy 66.950%\n",
      "Epoch 5, Batch 796, LR 1.237865 Loss 9.371786, Accuracy 66.942%\n",
      "Epoch 5, Batch 797, LR 1.238161 Loss 9.371940, Accuracy 66.944%\n",
      "Epoch 5, Batch 798, LR 1.238458 Loss 9.371856, Accuracy 66.943%\n",
      "Epoch 5, Batch 799, LR 1.238754 Loss 9.371041, Accuracy 66.946%\n",
      "Epoch 5, Batch 800, LR 1.239051 Loss 9.370710, Accuracy 66.951%\n",
      "Epoch 5, Batch 801, LR 1.239347 Loss 9.370891, Accuracy 66.950%\n",
      "Epoch 5, Batch 802, LR 1.239643 Loss 9.369522, Accuracy 66.961%\n",
      "Epoch 5, Batch 803, LR 1.239940 Loss 9.368310, Accuracy 66.971%\n",
      "Epoch 5, Batch 804, LR 1.240236 Loss 9.368524, Accuracy 66.968%\n",
      "Epoch 5, Batch 805, LR 1.240533 Loss 9.368719, Accuracy 66.965%\n",
      "Epoch 5, Batch 806, LR 1.240829 Loss 9.368103, Accuracy 66.964%\n",
      "Epoch 5, Batch 807, LR 1.241125 Loss 9.367549, Accuracy 66.967%\n",
      "Epoch 5, Batch 808, LR 1.241422 Loss 9.367057, Accuracy 66.969%\n",
      "Epoch 5, Batch 809, LR 1.241718 Loss 9.367254, Accuracy 66.969%\n",
      "Epoch 5, Batch 810, LR 1.242015 Loss 9.366494, Accuracy 66.974%\n",
      "Epoch 5, Batch 811, LR 1.242311 Loss 9.365905, Accuracy 66.978%\n",
      "Epoch 5, Batch 812, LR 1.242608 Loss 9.365278, Accuracy 66.983%\n",
      "Epoch 5, Batch 813, LR 1.242904 Loss 9.365351, Accuracy 66.989%\n",
      "Epoch 5, Batch 814, LR 1.243200 Loss 9.364820, Accuracy 66.988%\n",
      "Epoch 5, Batch 815, LR 1.243497 Loss 9.364359, Accuracy 66.992%\n",
      "Epoch 5, Batch 816, LR 1.243793 Loss 9.364205, Accuracy 66.991%\n",
      "Epoch 5, Batch 817, LR 1.244090 Loss 9.363348, Accuracy 66.999%\n",
      "Epoch 5, Batch 818, LR 1.244386 Loss 9.363276, Accuracy 67.002%\n",
      "Epoch 5, Batch 819, LR 1.244683 Loss 9.364024, Accuracy 67.001%\n",
      "Epoch 5, Batch 820, LR 1.244979 Loss 9.363668, Accuracy 66.998%\n",
      "Epoch 5, Batch 821, LR 1.245276 Loss 9.364013, Accuracy 66.998%\n",
      "Epoch 5, Batch 822, LR 1.245572 Loss 9.364264, Accuracy 66.991%\n",
      "Epoch 5, Batch 823, LR 1.245869 Loss 9.363912, Accuracy 66.993%\n",
      "Epoch 5, Batch 824, LR 1.246165 Loss 9.363489, Accuracy 66.994%\n",
      "Epoch 5, Batch 825, LR 1.246462 Loss 9.363907, Accuracy 66.994%\n",
      "Epoch 5, Batch 826, LR 1.246758 Loss 9.363655, Accuracy 66.995%\n",
      "Epoch 5, Batch 827, LR 1.247055 Loss 9.362985, Accuracy 66.997%\n",
      "Epoch 5, Batch 828, LR 1.247351 Loss 9.363037, Accuracy 66.991%\n",
      "Epoch 5, Batch 829, LR 1.247648 Loss 9.362399, Accuracy 66.991%\n",
      "Epoch 5, Batch 830, LR 1.247944 Loss 9.362817, Accuracy 66.987%\n",
      "Epoch 5, Batch 831, LR 1.248241 Loss 9.362766, Accuracy 66.991%\n",
      "Epoch 5, Batch 832, LR 1.248537 Loss 9.363065, Accuracy 66.990%\n",
      "Epoch 5, Batch 833, LR 1.248834 Loss 9.363211, Accuracy 66.993%\n",
      "Epoch 5, Batch 834, LR 1.249130 Loss 9.363212, Accuracy 66.991%\n",
      "Epoch 5, Batch 835, LR 1.249427 Loss 9.362614, Accuracy 66.999%\n",
      "Epoch 5, Batch 836, LR 1.249723 Loss 9.362738, Accuracy 67.000%\n",
      "Epoch 5, Batch 837, LR 1.250020 Loss 9.362370, Accuracy 67.001%\n",
      "Epoch 5, Batch 838, LR 1.250316 Loss 9.361860, Accuracy 66.998%\n",
      "Epoch 5, Batch 839, LR 1.250613 Loss 9.361322, Accuracy 67.002%\n",
      "Epoch 5, Batch 840, LR 1.250910 Loss 9.361168, Accuracy 67.004%\n",
      "Epoch 5, Batch 841, LR 1.251206 Loss 9.361769, Accuracy 66.999%\n",
      "Epoch 5, Batch 842, LR 1.251503 Loss 9.361728, Accuracy 67.002%\n",
      "Epoch 5, Batch 843, LR 1.251799 Loss 9.360787, Accuracy 67.006%\n",
      "Epoch 5, Batch 844, LR 1.252096 Loss 9.360488, Accuracy 67.010%\n",
      "Epoch 5, Batch 845, LR 1.252392 Loss 9.360638, Accuracy 67.008%\n",
      "Epoch 5, Batch 846, LR 1.252689 Loss 9.360633, Accuracy 67.009%\n",
      "Epoch 5, Batch 847, LR 1.252986 Loss 9.360939, Accuracy 67.009%\n",
      "Epoch 5, Batch 848, LR 1.253282 Loss 9.361333, Accuracy 67.012%\n",
      "Epoch 5, Batch 849, LR 1.253579 Loss 9.361259, Accuracy 67.008%\n",
      "Epoch 5, Batch 850, LR 1.253875 Loss 9.360721, Accuracy 67.012%\n",
      "Epoch 5, Batch 851, LR 1.254172 Loss 9.360986, Accuracy 67.005%\n",
      "Epoch 5, Batch 852, LR 1.254468 Loss 9.360858, Accuracy 67.005%\n",
      "Epoch 5, Batch 853, LR 1.254765 Loss 9.361860, Accuracy 66.995%\n",
      "Epoch 5, Batch 854, LR 1.255062 Loss 9.361796, Accuracy 66.997%\n",
      "Epoch 5, Batch 855, LR 1.255358 Loss 9.361977, Accuracy 66.999%\n",
      "Epoch 5, Batch 856, LR 1.255655 Loss 9.361420, Accuracy 67.001%\n",
      "Epoch 5, Batch 857, LR 1.255952 Loss 9.361107, Accuracy 67.006%\n",
      "Epoch 5, Batch 858, LR 1.256248 Loss 9.360711, Accuracy 67.012%\n",
      "Epoch 5, Batch 859, LR 1.256545 Loss 9.360436, Accuracy 67.015%\n",
      "Epoch 5, Batch 860, LR 1.256841 Loss 9.360976, Accuracy 67.009%\n",
      "Epoch 5, Batch 861, LR 1.257138 Loss 9.360203, Accuracy 67.014%\n",
      "Epoch 5, Batch 862, LR 1.257435 Loss 9.359483, Accuracy 67.023%\n",
      "Epoch 5, Batch 863, LR 1.257731 Loss 9.359470, Accuracy 67.023%\n",
      "Epoch 5, Batch 864, LR 1.258028 Loss 9.359478, Accuracy 67.024%\n",
      "Epoch 5, Batch 865, LR 1.258325 Loss 9.360168, Accuracy 67.020%\n",
      "Epoch 5, Batch 866, LR 1.258621 Loss 9.360358, Accuracy 67.013%\n",
      "Epoch 5, Batch 867, LR 1.258918 Loss 9.359799, Accuracy 67.019%\n",
      "Epoch 5, Batch 868, LR 1.259215 Loss 9.359212, Accuracy 67.023%\n",
      "Epoch 5, Batch 869, LR 1.259511 Loss 9.358790, Accuracy 67.027%\n",
      "Epoch 5, Batch 870, LR 1.259808 Loss 9.358467, Accuracy 67.033%\n",
      "Epoch 5, Batch 871, LR 1.260105 Loss 9.358337, Accuracy 67.034%\n",
      "Epoch 5, Batch 872, LR 1.260401 Loss 9.356989, Accuracy 67.046%\n",
      "Epoch 5, Batch 873, LR 1.260698 Loss 9.356583, Accuracy 67.051%\n",
      "Epoch 5, Batch 874, LR 1.260995 Loss 9.355496, Accuracy 67.062%\n",
      "Epoch 5, Batch 875, LR 1.261291 Loss 9.355322, Accuracy 67.062%\n",
      "Epoch 5, Batch 876, LR 1.261588 Loss 9.355713, Accuracy 67.059%\n",
      "Epoch 5, Batch 877, LR 1.261885 Loss 9.356042, Accuracy 67.053%\n",
      "Epoch 5, Batch 878, LR 1.262181 Loss 9.354779, Accuracy 67.063%\n",
      "Epoch 5, Batch 879, LR 1.262478 Loss 9.354802, Accuracy 67.063%\n",
      "Epoch 5, Batch 880, LR 1.262775 Loss 9.355296, Accuracy 67.060%\n",
      "Epoch 5, Batch 881, LR 1.263071 Loss 9.354578, Accuracy 67.067%\n",
      "Epoch 5, Batch 882, LR 1.263368 Loss 9.354540, Accuracy 67.066%\n",
      "Epoch 5, Batch 883, LR 1.263665 Loss 9.354715, Accuracy 67.063%\n",
      "Epoch 5, Batch 884, LR 1.263961 Loss 9.354859, Accuracy 67.058%\n",
      "Epoch 5, Batch 885, LR 1.264258 Loss 9.354668, Accuracy 67.063%\n",
      "Epoch 5, Batch 886, LR 1.264555 Loss 9.354743, Accuracy 67.057%\n",
      "Epoch 5, Batch 887, LR 1.264852 Loss 9.354497, Accuracy 67.055%\n",
      "Epoch 5, Batch 888, LR 1.265148 Loss 9.354553, Accuracy 67.053%\n",
      "Epoch 5, Batch 889, LR 1.265445 Loss 9.354063, Accuracy 67.059%\n",
      "Epoch 5, Batch 890, LR 1.265742 Loss 9.354001, Accuracy 67.061%\n",
      "Epoch 5, Batch 891, LR 1.266038 Loss 9.353951, Accuracy 67.062%\n",
      "Epoch 5, Batch 892, LR 1.266335 Loss 9.352842, Accuracy 67.071%\n",
      "Epoch 5, Batch 893, LR 1.266632 Loss 9.352200, Accuracy 67.075%\n",
      "Epoch 5, Batch 894, LR 1.266929 Loss 9.352547, Accuracy 67.072%\n",
      "Epoch 5, Batch 895, LR 1.267225 Loss 9.352497, Accuracy 67.074%\n",
      "Epoch 5, Batch 896, LR 1.267522 Loss 9.351548, Accuracy 67.078%\n",
      "Epoch 5, Batch 897, LR 1.267819 Loss 9.350976, Accuracy 67.084%\n",
      "Epoch 5, Batch 898, LR 1.268116 Loss 9.350300, Accuracy 67.088%\n",
      "Epoch 5, Batch 899, LR 1.268412 Loss 9.349994, Accuracy 67.088%\n",
      "Epoch 5, Batch 900, LR 1.268709 Loss 9.349622, Accuracy 67.089%\n",
      "Epoch 5, Batch 901, LR 1.269006 Loss 9.349565, Accuracy 67.090%\n",
      "Epoch 5, Batch 902, LR 1.269303 Loss 9.350202, Accuracy 67.088%\n",
      "Epoch 5, Batch 903, LR 1.269599 Loss 9.349824, Accuracy 67.091%\n",
      "Epoch 5, Batch 904, LR 1.269896 Loss 9.350226, Accuracy 67.083%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 905, LR 1.270193 Loss 9.350656, Accuracy 67.074%\n",
      "Epoch 5, Batch 906, LR 1.270490 Loss 9.349931, Accuracy 67.081%\n",
      "Epoch 5, Batch 907, LR 1.270786 Loss 9.350112, Accuracy 67.080%\n",
      "Epoch 5, Batch 908, LR 1.271083 Loss 9.350350, Accuracy 67.084%\n",
      "Epoch 5, Batch 909, LR 1.271380 Loss 9.350733, Accuracy 67.086%\n",
      "Epoch 5, Batch 910, LR 1.271677 Loss 9.350865, Accuracy 67.086%\n",
      "Epoch 5, Batch 911, LR 1.271974 Loss 9.350356, Accuracy 67.089%\n",
      "Epoch 5, Batch 912, LR 1.272270 Loss 9.350660, Accuracy 67.086%\n",
      "Epoch 5, Batch 913, LR 1.272567 Loss 9.350750, Accuracy 67.088%\n",
      "Epoch 5, Batch 914, LR 1.272864 Loss 9.351013, Accuracy 67.087%\n",
      "Epoch 5, Batch 915, LR 1.273161 Loss 9.350612, Accuracy 67.089%\n",
      "Epoch 5, Batch 916, LR 1.273458 Loss 9.350790, Accuracy 67.091%\n",
      "Epoch 5, Batch 917, LR 1.273754 Loss 9.350423, Accuracy 67.100%\n",
      "Epoch 5, Batch 918, LR 1.274051 Loss 9.350397, Accuracy 67.099%\n",
      "Epoch 5, Batch 919, LR 1.274348 Loss 9.350974, Accuracy 67.091%\n",
      "Epoch 5, Batch 920, LR 1.274645 Loss 9.351176, Accuracy 67.089%\n",
      "Epoch 5, Batch 921, LR 1.274942 Loss 9.351555, Accuracy 67.088%\n",
      "Epoch 5, Batch 922, LR 1.275238 Loss 9.350165, Accuracy 67.102%\n",
      "Epoch 5, Batch 923, LR 1.275535 Loss 9.350056, Accuracy 67.099%\n",
      "Epoch 5, Batch 924, LR 1.275832 Loss 9.349958, Accuracy 67.102%\n",
      "Epoch 5, Batch 925, LR 1.276129 Loss 9.349392, Accuracy 67.104%\n",
      "Epoch 5, Batch 926, LR 1.276426 Loss 9.349490, Accuracy 67.106%\n",
      "Epoch 5, Batch 927, LR 1.276722 Loss 9.348925, Accuracy 67.109%\n",
      "Epoch 5, Batch 928, LR 1.277019 Loss 9.348771, Accuracy 67.114%\n",
      "Epoch 5, Batch 929, LR 1.277316 Loss 9.347868, Accuracy 67.120%\n",
      "Epoch 5, Batch 930, LR 1.277613 Loss 9.347111, Accuracy 67.125%\n",
      "Epoch 5, Batch 931, LR 1.277910 Loss 9.346663, Accuracy 67.129%\n",
      "Epoch 5, Batch 932, LR 1.278207 Loss 9.346908, Accuracy 67.132%\n",
      "Epoch 5, Batch 933, LR 1.278503 Loss 9.347016, Accuracy 67.131%\n",
      "Epoch 5, Batch 934, LR 1.278800 Loss 9.347534, Accuracy 67.130%\n",
      "Epoch 5, Batch 935, LR 1.279097 Loss 9.347444, Accuracy 67.129%\n",
      "Epoch 5, Batch 936, LR 1.279394 Loss 9.347440, Accuracy 67.129%\n",
      "Epoch 5, Batch 937, LR 1.279691 Loss 9.347820, Accuracy 67.132%\n",
      "Epoch 5, Batch 938, LR 1.279988 Loss 9.347441, Accuracy 67.131%\n",
      "Epoch 5, Batch 939, LR 1.280284 Loss 9.347128, Accuracy 67.133%\n",
      "Epoch 5, Batch 940, LR 1.280581 Loss 9.347087, Accuracy 67.133%\n",
      "Epoch 5, Batch 941, LR 1.280878 Loss 9.347203, Accuracy 67.130%\n",
      "Epoch 5, Batch 942, LR 1.281175 Loss 9.346833, Accuracy 67.133%\n",
      "Epoch 5, Batch 943, LR 1.281472 Loss 9.346330, Accuracy 67.134%\n",
      "Epoch 5, Batch 944, LR 1.281769 Loss 9.346080, Accuracy 67.136%\n",
      "Epoch 5, Batch 945, LR 1.282065 Loss 9.346456, Accuracy 67.135%\n",
      "Epoch 5, Batch 946, LR 1.282362 Loss 9.346093, Accuracy 67.137%\n",
      "Epoch 5, Batch 947, LR 1.282659 Loss 9.345758, Accuracy 67.136%\n",
      "Epoch 5, Batch 948, LR 1.282956 Loss 9.346059, Accuracy 67.137%\n",
      "Epoch 5, Batch 949, LR 1.283253 Loss 9.346849, Accuracy 67.128%\n",
      "Epoch 5, Batch 950, LR 1.283550 Loss 9.347228, Accuracy 67.129%\n",
      "Epoch 5, Batch 951, LR 1.283847 Loss 9.347275, Accuracy 67.129%\n",
      "Epoch 5, Batch 952, LR 1.284144 Loss 9.346988, Accuracy 67.126%\n",
      "Epoch 5, Batch 953, LR 1.284440 Loss 9.346967, Accuracy 67.129%\n",
      "Epoch 5, Batch 954, LR 1.284737 Loss 9.346541, Accuracy 67.133%\n",
      "Epoch 5, Batch 955, LR 1.285034 Loss 9.347121, Accuracy 67.130%\n",
      "Epoch 5, Batch 956, LR 1.285331 Loss 9.346803, Accuracy 67.130%\n",
      "Epoch 5, Batch 957, LR 1.285628 Loss 9.346977, Accuracy 67.128%\n",
      "Epoch 5, Batch 958, LR 1.285925 Loss 9.347130, Accuracy 67.128%\n",
      "Epoch 5, Batch 959, LR 1.286222 Loss 9.346687, Accuracy 67.134%\n",
      "Epoch 5, Batch 960, LR 1.286519 Loss 9.346605, Accuracy 67.131%\n",
      "Epoch 5, Batch 961, LR 1.286815 Loss 9.346538, Accuracy 67.133%\n",
      "Epoch 5, Batch 962, LR 1.287112 Loss 9.346186, Accuracy 67.135%\n",
      "Epoch 5, Batch 963, LR 1.287409 Loss 9.345818, Accuracy 67.139%\n",
      "Epoch 5, Batch 964, LR 1.287706 Loss 9.345532, Accuracy 67.143%\n",
      "Epoch 5, Batch 965, LR 1.288003 Loss 9.345715, Accuracy 67.143%\n",
      "Epoch 5, Batch 966, LR 1.288300 Loss 9.345119, Accuracy 67.145%\n",
      "Epoch 5, Batch 967, LR 1.288597 Loss 9.344073, Accuracy 67.154%\n",
      "Epoch 5, Batch 968, LR 1.288894 Loss 9.344451, Accuracy 67.154%\n",
      "Epoch 5, Batch 969, LR 1.289191 Loss 9.344725, Accuracy 67.148%\n",
      "Epoch 5, Batch 970, LR 1.289487 Loss 9.345091, Accuracy 67.148%\n",
      "Epoch 5, Batch 971, LR 1.289784 Loss 9.344372, Accuracy 67.150%\n",
      "Epoch 5, Batch 972, LR 1.290081 Loss 9.344054, Accuracy 67.152%\n",
      "Epoch 5, Batch 973, LR 1.290378 Loss 9.343501, Accuracy 67.156%\n",
      "Epoch 5, Batch 974, LR 1.290675 Loss 9.343662, Accuracy 67.152%\n",
      "Epoch 5, Batch 975, LR 1.290972 Loss 9.343904, Accuracy 67.152%\n",
      "Epoch 5, Batch 976, LR 1.291269 Loss 9.344223, Accuracy 67.147%\n",
      "Epoch 5, Batch 977, LR 1.291566 Loss 9.343729, Accuracy 67.150%\n",
      "Epoch 5, Batch 978, LR 1.291863 Loss 9.344117, Accuracy 67.146%\n",
      "Epoch 5, Batch 979, LR 1.292160 Loss 9.344362, Accuracy 67.145%\n",
      "Epoch 5, Batch 980, LR 1.292457 Loss 9.344282, Accuracy 67.144%\n",
      "Epoch 5, Batch 981, LR 1.292753 Loss 9.344086, Accuracy 67.144%\n",
      "Epoch 5, Batch 982, LR 1.293050 Loss 9.343906, Accuracy 67.139%\n",
      "Epoch 5, Batch 983, LR 1.293347 Loss 9.344173, Accuracy 67.136%\n",
      "Epoch 5, Batch 984, LR 1.293644 Loss 9.343963, Accuracy 67.135%\n",
      "Epoch 5, Batch 985, LR 1.293941 Loss 9.344284, Accuracy 67.134%\n",
      "Epoch 5, Batch 986, LR 1.294238 Loss 9.343943, Accuracy 67.134%\n",
      "Epoch 5, Batch 987, LR 1.294535 Loss 9.343486, Accuracy 67.142%\n",
      "Epoch 5, Batch 988, LR 1.294832 Loss 9.343913, Accuracy 67.137%\n",
      "Epoch 5, Batch 989, LR 1.295129 Loss 9.343677, Accuracy 67.135%\n",
      "Epoch 5, Batch 990, LR 1.295426 Loss 9.343963, Accuracy 67.130%\n",
      "Epoch 5, Batch 991, LR 1.295723 Loss 9.344128, Accuracy 67.132%\n",
      "Epoch 5, Batch 992, LR 1.296020 Loss 9.343637, Accuracy 67.132%\n",
      "Epoch 5, Batch 993, LR 1.296316 Loss 9.342802, Accuracy 67.139%\n",
      "Epoch 5, Batch 994, LR 1.296613 Loss 9.342775, Accuracy 67.140%\n",
      "Epoch 5, Batch 995, LR 1.296910 Loss 9.342771, Accuracy 67.143%\n",
      "Epoch 5, Batch 996, LR 1.297207 Loss 9.343293, Accuracy 67.140%\n",
      "Epoch 5, Batch 997, LR 1.297504 Loss 9.343102, Accuracy 67.145%\n",
      "Epoch 5, Batch 998, LR 1.297801 Loss 9.342884, Accuracy 67.148%\n",
      "Epoch 5, Batch 999, LR 1.298098 Loss 9.342423, Accuracy 67.150%\n",
      "Epoch 5, Batch 1000, LR 1.298395 Loss 9.342763, Accuracy 67.146%\n",
      "Epoch 5, Batch 1001, LR 1.298692 Loss 9.342767, Accuracy 67.146%\n",
      "Epoch 5, Batch 1002, LR 1.298989 Loss 9.342170, Accuracy 67.151%\n",
      "Epoch 5, Batch 1003, LR 1.299286 Loss 9.341817, Accuracy 67.159%\n",
      "Epoch 5, Batch 1004, LR 1.299583 Loss 9.341807, Accuracy 67.156%\n",
      "Epoch 5, Batch 1005, LR 1.299880 Loss 9.342045, Accuracy 67.163%\n",
      "Epoch 5, Batch 1006, LR 1.300177 Loss 9.342189, Accuracy 67.159%\n",
      "Epoch 5, Batch 1007, LR 1.300474 Loss 9.342101, Accuracy 67.161%\n",
      "Epoch 5, Batch 1008, LR 1.300770 Loss 9.341164, Accuracy 67.165%\n",
      "Epoch 5, Batch 1009, LR 1.301067 Loss 9.340576, Accuracy 67.171%\n",
      "Epoch 5, Batch 1010, LR 1.301364 Loss 9.339992, Accuracy 67.180%\n",
      "Epoch 5, Batch 1011, LR 1.301661 Loss 9.339589, Accuracy 67.183%\n",
      "Epoch 5, Batch 1012, LR 1.301958 Loss 9.338959, Accuracy 67.190%\n",
      "Epoch 5, Batch 1013, LR 1.302255 Loss 9.339241, Accuracy 67.187%\n",
      "Epoch 5, Batch 1014, LR 1.302552 Loss 9.339116, Accuracy 67.194%\n",
      "Epoch 5, Batch 1015, LR 1.302849 Loss 9.338952, Accuracy 67.195%\n",
      "Epoch 5, Batch 1016, LR 1.303146 Loss 9.339320, Accuracy 67.197%\n",
      "Epoch 5, Batch 1017, LR 1.303443 Loss 9.338799, Accuracy 67.200%\n",
      "Epoch 5, Batch 1018, LR 1.303740 Loss 9.338515, Accuracy 67.198%\n",
      "Epoch 5, Batch 1019, LR 1.304037 Loss 9.338419, Accuracy 67.197%\n",
      "Epoch 5, Batch 1020, LR 1.304334 Loss 9.338416, Accuracy 67.195%\n",
      "Epoch 5, Batch 1021, LR 1.304631 Loss 9.337420, Accuracy 67.200%\n",
      "Epoch 5, Batch 1022, LR 1.304928 Loss 9.338188, Accuracy 67.196%\n",
      "Epoch 5, Batch 1023, LR 1.305225 Loss 9.338023, Accuracy 67.200%\n",
      "Epoch 5, Batch 1024, LR 1.305522 Loss 9.338531, Accuracy 67.196%\n",
      "Epoch 5, Batch 1025, LR 1.305819 Loss 9.338070, Accuracy 67.204%\n",
      "Epoch 5, Batch 1026, LR 1.306116 Loss 9.337593, Accuracy 67.203%\n",
      "Epoch 5, Batch 1027, LR 1.306412 Loss 9.337691, Accuracy 67.207%\n",
      "Epoch 5, Batch 1028, LR 1.306709 Loss 9.337782, Accuracy 67.207%\n",
      "Epoch 5, Batch 1029, LR 1.307006 Loss 9.337673, Accuracy 67.210%\n",
      "Epoch 5, Batch 1030, LR 1.307303 Loss 9.337558, Accuracy 67.206%\n",
      "Epoch 5, Batch 1031, LR 1.307600 Loss 9.336867, Accuracy 67.209%\n",
      "Epoch 5, Batch 1032, LR 1.307897 Loss 9.337185, Accuracy 67.204%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 1033, LR 1.308194 Loss 9.336482, Accuracy 67.207%\n",
      "Epoch 5, Batch 1034, LR 1.308491 Loss 9.335782, Accuracy 67.213%\n",
      "Epoch 5, Batch 1035, LR 1.308788 Loss 9.335966, Accuracy 67.215%\n",
      "Epoch 5, Batch 1036, LR 1.309085 Loss 9.335952, Accuracy 67.217%\n",
      "Epoch 5, Batch 1037, LR 1.309382 Loss 9.336222, Accuracy 67.215%\n",
      "Epoch 5, Batch 1038, LR 1.309679 Loss 9.335380, Accuracy 67.224%\n",
      "Epoch 5, Batch 1039, LR 1.309976 Loss 9.335039, Accuracy 67.224%\n",
      "Epoch 5, Batch 1040, LR 1.310273 Loss 9.335012, Accuracy 67.224%\n",
      "Epoch 5, Batch 1041, LR 1.310570 Loss 9.335212, Accuracy 67.227%\n",
      "Epoch 5, Batch 1042, LR 1.310867 Loss 9.334830, Accuracy 67.227%\n",
      "Epoch 5, Batch 1043, LR 1.311164 Loss 9.334286, Accuracy 67.230%\n",
      "Epoch 5, Batch 1044, LR 1.311461 Loss 9.333806, Accuracy 67.234%\n",
      "Epoch 5, Batch 1045, LR 1.311758 Loss 9.333713, Accuracy 67.233%\n",
      "Epoch 5, Batch 1046, LR 1.312055 Loss 9.333371, Accuracy 67.235%\n",
      "Epoch 5, Batch 1047, LR 1.312352 Loss 9.332299, Accuracy 67.246%\n",
      "Epoch 5, Loss (train set) 9.332299, Accuracy (train set) 67.246%\n",
      "Epoch 6, Batch 1, LR 1.312648 Loss 8.579310, Accuracy 72.656%\n",
      "Epoch 6, Batch 2, LR 1.312945 Loss 8.569252, Accuracy 73.047%\n",
      "Epoch 6, Batch 3, LR 1.313242 Loss 8.502561, Accuracy 73.438%\n",
      "Epoch 6, Batch 4, LR 1.313539 Loss 8.812912, Accuracy 72.461%\n",
      "Epoch 6, Batch 5, LR 1.313836 Loss 8.868148, Accuracy 71.406%\n",
      "Epoch 6, Batch 6, LR 1.314133 Loss 8.934925, Accuracy 70.833%\n",
      "Epoch 6, Batch 7, LR 1.314430 Loss 8.929765, Accuracy 70.871%\n",
      "Epoch 6, Batch 8, LR 1.314727 Loss 8.948809, Accuracy 70.703%\n",
      "Epoch 6, Batch 9, LR 1.315024 Loss 8.923026, Accuracy 71.181%\n",
      "Epoch 6, Batch 10, LR 1.315321 Loss 8.889936, Accuracy 71.094%\n",
      "Epoch 6, Batch 11, LR 1.315618 Loss 8.906632, Accuracy 71.094%\n",
      "Epoch 6, Batch 12, LR 1.315915 Loss 8.865651, Accuracy 71.745%\n",
      "Epoch 6, Batch 13, LR 1.316212 Loss 8.839543, Accuracy 71.454%\n",
      "Epoch 6, Batch 14, LR 1.316509 Loss 8.897697, Accuracy 71.150%\n",
      "Epoch 6, Batch 15, LR 1.316806 Loss 8.879910, Accuracy 71.146%\n",
      "Epoch 6, Batch 16, LR 1.317103 Loss 8.898889, Accuracy 71.240%\n",
      "Epoch 6, Batch 17, LR 1.317400 Loss 8.923734, Accuracy 70.956%\n",
      "Epoch 6, Batch 18, LR 1.317697 Loss 8.946493, Accuracy 70.833%\n",
      "Epoch 6, Batch 19, LR 1.317994 Loss 8.925659, Accuracy 71.012%\n",
      "Epoch 6, Batch 20, LR 1.318291 Loss 8.908798, Accuracy 71.172%\n",
      "Epoch 6, Batch 21, LR 1.318588 Loss 8.926188, Accuracy 70.796%\n",
      "Epoch 6, Batch 22, LR 1.318884 Loss 8.960787, Accuracy 70.597%\n",
      "Epoch 6, Batch 23, LR 1.319181 Loss 8.947431, Accuracy 70.720%\n",
      "Epoch 6, Batch 24, LR 1.319478 Loss 8.960832, Accuracy 70.768%\n",
      "Epoch 6, Batch 25, LR 1.319775 Loss 8.974644, Accuracy 70.562%\n",
      "Epoch 6, Batch 26, LR 1.320072 Loss 8.970171, Accuracy 70.493%\n",
      "Epoch 6, Batch 27, LR 1.320369 Loss 8.970212, Accuracy 70.515%\n",
      "Epoch 6, Batch 28, LR 1.320666 Loss 8.952119, Accuracy 70.759%\n",
      "Epoch 6, Batch 29, LR 1.320963 Loss 8.974995, Accuracy 70.636%\n",
      "Epoch 6, Batch 30, LR 1.321260 Loss 8.969800, Accuracy 70.729%\n",
      "Epoch 6, Batch 31, LR 1.321557 Loss 8.953149, Accuracy 70.842%\n",
      "Epoch 6, Batch 32, LR 1.321854 Loss 8.950762, Accuracy 70.728%\n",
      "Epoch 6, Batch 33, LR 1.322151 Loss 8.949485, Accuracy 70.857%\n",
      "Epoch 6, Batch 34, LR 1.322448 Loss 8.934275, Accuracy 70.933%\n",
      "Epoch 6, Batch 35, LR 1.322745 Loss 8.942078, Accuracy 70.982%\n",
      "Epoch 6, Batch 36, LR 1.323042 Loss 8.941137, Accuracy 71.094%\n",
      "Epoch 6, Batch 37, LR 1.323339 Loss 8.945056, Accuracy 71.009%\n",
      "Epoch 6, Batch 38, LR 1.323636 Loss 8.947825, Accuracy 71.032%\n",
      "Epoch 6, Batch 39, LR 1.323933 Loss 8.951164, Accuracy 70.994%\n",
      "Epoch 6, Batch 40, LR 1.324230 Loss 8.948502, Accuracy 71.055%\n",
      "Epoch 6, Batch 41, LR 1.324526 Loss 8.963725, Accuracy 71.056%\n",
      "Epoch 6, Batch 42, LR 1.324823 Loss 8.964085, Accuracy 71.057%\n",
      "Epoch 6, Batch 43, LR 1.325120 Loss 8.967836, Accuracy 70.985%\n",
      "Epoch 6, Batch 44, LR 1.325417 Loss 8.960315, Accuracy 71.023%\n",
      "Epoch 6, Batch 45, LR 1.325714 Loss 8.952224, Accuracy 71.007%\n",
      "Epoch 6, Batch 46, LR 1.326011 Loss 8.939029, Accuracy 71.043%\n",
      "Epoch 6, Batch 47, LR 1.326308 Loss 8.947978, Accuracy 71.110%\n",
      "Epoch 6, Batch 48, LR 1.326605 Loss 8.944610, Accuracy 71.012%\n",
      "Epoch 6, Batch 49, LR 1.326902 Loss 8.930566, Accuracy 71.126%\n",
      "Epoch 6, Batch 50, LR 1.327199 Loss 8.920876, Accuracy 71.078%\n",
      "Epoch 6, Batch 51, LR 1.327496 Loss 8.921402, Accuracy 71.032%\n",
      "Epoch 6, Batch 52, LR 1.327793 Loss 8.918162, Accuracy 71.034%\n",
      "Epoch 6, Batch 53, LR 1.328090 Loss 8.910222, Accuracy 71.138%\n",
      "Epoch 6, Batch 54, LR 1.328387 Loss 8.905209, Accuracy 71.166%\n",
      "Epoch 6, Batch 55, LR 1.328684 Loss 8.911411, Accuracy 71.122%\n",
      "Epoch 6, Batch 56, LR 1.328980 Loss 8.918271, Accuracy 71.010%\n",
      "Epoch 6, Batch 57, LR 1.329277 Loss 8.910427, Accuracy 71.121%\n",
      "Epoch 6, Batch 58, LR 1.329574 Loss 8.906968, Accuracy 71.121%\n",
      "Epoch 6, Batch 59, LR 1.329871 Loss 8.908167, Accuracy 71.081%\n",
      "Epoch 6, Batch 60, LR 1.330168 Loss 8.915381, Accuracy 70.964%\n",
      "Epoch 6, Batch 61, LR 1.330465 Loss 8.915080, Accuracy 70.902%\n",
      "Epoch 6, Batch 62, LR 1.330762 Loss 8.909139, Accuracy 70.892%\n",
      "Epoch 6, Batch 63, LR 1.331059 Loss 8.898489, Accuracy 70.908%\n",
      "Epoch 6, Batch 64, LR 1.331356 Loss 8.910068, Accuracy 70.837%\n",
      "Epoch 6, Batch 65, LR 1.331653 Loss 8.915466, Accuracy 70.769%\n",
      "Epoch 6, Batch 66, LR 1.331950 Loss 8.906303, Accuracy 70.810%\n",
      "Epoch 6, Batch 67, LR 1.332247 Loss 8.900718, Accuracy 70.849%\n",
      "Epoch 6, Batch 68, LR 1.332543 Loss 8.908808, Accuracy 70.795%\n",
      "Epoch 6, Batch 69, LR 1.332840 Loss 8.917781, Accuracy 70.641%\n",
      "Epoch 6, Batch 70, LR 1.333137 Loss 8.923620, Accuracy 70.636%\n",
      "Epoch 6, Batch 71, LR 1.333434 Loss 8.921666, Accuracy 70.632%\n",
      "Epoch 6, Batch 72, LR 1.333731 Loss 8.922872, Accuracy 70.671%\n",
      "Epoch 6, Batch 73, LR 1.334028 Loss 8.929646, Accuracy 70.612%\n",
      "Epoch 6, Batch 74, LR 1.334325 Loss 8.925700, Accuracy 70.693%\n",
      "Epoch 6, Batch 75, LR 1.334622 Loss 8.932494, Accuracy 70.667%\n",
      "Epoch 6, Batch 76, LR 1.334919 Loss 8.941428, Accuracy 70.600%\n",
      "Epoch 6, Batch 77, LR 1.335216 Loss 8.947584, Accuracy 70.536%\n",
      "Epoch 6, Batch 78, LR 1.335513 Loss 8.946225, Accuracy 70.523%\n",
      "Epoch 6, Batch 79, LR 1.335809 Loss 8.940958, Accuracy 70.560%\n",
      "Epoch 6, Batch 80, LR 1.336106 Loss 8.937486, Accuracy 70.586%\n",
      "Epoch 6, Batch 81, LR 1.336403 Loss 8.935609, Accuracy 70.650%\n",
      "Epoch 6, Batch 82, LR 1.336700 Loss 8.934116, Accuracy 70.589%\n",
      "Epoch 6, Batch 83, LR 1.336997 Loss 8.935259, Accuracy 70.604%\n",
      "Epoch 6, Batch 84, LR 1.337294 Loss 8.929403, Accuracy 70.619%\n",
      "Epoch 6, Batch 85, LR 1.337591 Loss 8.925537, Accuracy 70.662%\n",
      "Epoch 6, Batch 86, LR 1.337888 Loss 8.929838, Accuracy 70.621%\n",
      "Epoch 6, Batch 87, LR 1.338185 Loss 8.932972, Accuracy 70.591%\n",
      "Epoch 6, Batch 88, LR 1.338481 Loss 8.934761, Accuracy 70.605%\n",
      "Epoch 6, Batch 89, LR 1.338778 Loss 8.939385, Accuracy 70.576%\n",
      "Epoch 6, Batch 90, LR 1.339075 Loss 8.933848, Accuracy 70.608%\n",
      "Epoch 6, Batch 91, LR 1.339372 Loss 8.933848, Accuracy 70.579%\n",
      "Epoch 6, Batch 92, LR 1.339669 Loss 8.934480, Accuracy 70.593%\n",
      "Epoch 6, Batch 93, LR 1.339966 Loss 8.941299, Accuracy 70.548%\n",
      "Epoch 6, Batch 94, LR 1.340263 Loss 8.937586, Accuracy 70.562%\n",
      "Epoch 6, Batch 95, LR 1.340560 Loss 8.940062, Accuracy 70.559%\n",
      "Epoch 6, Batch 96, LR 1.340856 Loss 8.935685, Accuracy 70.573%\n",
      "Epoch 6, Batch 97, LR 1.341153 Loss 8.935646, Accuracy 70.586%\n",
      "Epoch 6, Batch 98, LR 1.341450 Loss 8.931323, Accuracy 70.592%\n",
      "Epoch 6, Batch 99, LR 1.341747 Loss 8.931337, Accuracy 70.589%\n",
      "Epoch 6, Batch 100, LR 1.342044 Loss 8.934867, Accuracy 70.500%\n",
      "Epoch 6, Batch 101, LR 1.342341 Loss 8.936228, Accuracy 70.490%\n",
      "Epoch 6, Batch 102, LR 1.342638 Loss 8.946063, Accuracy 70.427%\n",
      "Epoch 6, Batch 103, LR 1.342935 Loss 8.954738, Accuracy 70.312%\n",
      "Epoch 6, Batch 104, LR 1.343231 Loss 8.954867, Accuracy 70.335%\n",
      "Epoch 6, Batch 105, LR 1.343528 Loss 8.959503, Accuracy 70.305%\n",
      "Epoch 6, Batch 106, LR 1.343825 Loss 8.959350, Accuracy 70.261%\n",
      "Epoch 6, Batch 107, LR 1.344122 Loss 8.959660, Accuracy 70.283%\n",
      "Epoch 6, Batch 108, LR 1.344419 Loss 8.957179, Accuracy 70.312%\n",
      "Epoch 6, Batch 109, LR 1.344716 Loss 8.959584, Accuracy 70.356%\n",
      "Epoch 6, Batch 110, LR 1.345012 Loss 8.956601, Accuracy 70.376%\n",
      "Epoch 6, Batch 111, LR 1.345309 Loss 8.954282, Accuracy 70.390%\n",
      "Epoch 6, Batch 112, LR 1.345606 Loss 8.958535, Accuracy 70.354%\n",
      "Epoch 6, Batch 113, LR 1.345903 Loss 8.960531, Accuracy 70.312%\n",
      "Epoch 6, Batch 114, LR 1.346200 Loss 8.959856, Accuracy 70.306%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 115, LR 1.346497 Loss 8.956667, Accuracy 70.312%\n",
      "Epoch 6, Batch 116, LR 1.346793 Loss 8.950754, Accuracy 70.339%\n",
      "Epoch 6, Batch 117, LR 1.347090 Loss 8.949991, Accuracy 70.359%\n",
      "Epoch 6, Batch 118, LR 1.347387 Loss 8.950890, Accuracy 70.346%\n",
      "Epoch 6, Batch 119, LR 1.347684 Loss 8.950287, Accuracy 70.345%\n",
      "Epoch 6, Batch 120, LR 1.347981 Loss 8.952050, Accuracy 70.319%\n",
      "Epoch 6, Batch 121, LR 1.348278 Loss 8.950552, Accuracy 70.358%\n",
      "Epoch 6, Batch 122, LR 1.348574 Loss 8.947839, Accuracy 70.357%\n",
      "Epoch 6, Batch 123, LR 1.348871 Loss 8.950709, Accuracy 70.332%\n",
      "Epoch 6, Batch 124, LR 1.349168 Loss 8.947287, Accuracy 70.331%\n",
      "Epoch 6, Batch 125, LR 1.349465 Loss 8.947486, Accuracy 70.362%\n",
      "Epoch 6, Batch 126, LR 1.349762 Loss 8.952511, Accuracy 70.312%\n",
      "Epoch 6, Batch 127, LR 1.350058 Loss 8.952709, Accuracy 70.312%\n",
      "Epoch 6, Batch 128, LR 1.350355 Loss 8.951533, Accuracy 70.312%\n",
      "Epoch 6, Batch 129, LR 1.350652 Loss 8.951273, Accuracy 70.312%\n",
      "Epoch 6, Batch 130, LR 1.350949 Loss 8.947262, Accuracy 70.379%\n",
      "Epoch 6, Batch 131, LR 1.351246 Loss 8.948967, Accuracy 70.360%\n",
      "Epoch 6, Batch 132, LR 1.351542 Loss 8.948787, Accuracy 70.348%\n",
      "Epoch 6, Batch 133, LR 1.351839 Loss 8.947067, Accuracy 70.354%\n",
      "Epoch 6, Batch 134, LR 1.352136 Loss 8.946520, Accuracy 70.382%\n",
      "Epoch 6, Batch 135, LR 1.352433 Loss 8.947753, Accuracy 70.411%\n",
      "Epoch 6, Batch 136, LR 1.352730 Loss 8.948039, Accuracy 70.399%\n",
      "Epoch 6, Batch 137, LR 1.353026 Loss 8.948267, Accuracy 70.347%\n",
      "Epoch 6, Batch 138, LR 1.353323 Loss 8.948089, Accuracy 70.352%\n",
      "Epoch 6, Batch 139, LR 1.353620 Loss 8.947871, Accuracy 70.357%\n",
      "Epoch 6, Batch 140, LR 1.353917 Loss 8.950392, Accuracy 70.296%\n",
      "Epoch 6, Batch 141, LR 1.354214 Loss 8.950062, Accuracy 70.296%\n",
      "Epoch 6, Batch 142, LR 1.354510 Loss 8.948932, Accuracy 70.279%\n",
      "Epoch 6, Batch 143, LR 1.354807 Loss 8.943998, Accuracy 70.307%\n",
      "Epoch 6, Batch 144, LR 1.355104 Loss 8.940158, Accuracy 70.334%\n",
      "Epoch 6, Batch 145, LR 1.355401 Loss 8.938423, Accuracy 70.312%\n",
      "Epoch 6, Batch 146, LR 1.355697 Loss 8.944118, Accuracy 70.259%\n",
      "Epoch 6, Batch 147, LR 1.355994 Loss 8.942092, Accuracy 70.291%\n",
      "Epoch 6, Batch 148, LR 1.356291 Loss 8.943943, Accuracy 70.323%\n",
      "Epoch 6, Batch 149, LR 1.356588 Loss 8.939167, Accuracy 70.386%\n",
      "Epoch 6, Batch 150, LR 1.356884 Loss 8.935097, Accuracy 70.422%\n",
      "Epoch 6, Batch 151, LR 1.357181 Loss 8.933447, Accuracy 70.452%\n",
      "Epoch 6, Batch 152, LR 1.357478 Loss 8.935884, Accuracy 70.431%\n",
      "Epoch 6, Batch 153, LR 1.357775 Loss 8.936541, Accuracy 70.420%\n",
      "Epoch 6, Batch 154, LR 1.358071 Loss 8.934967, Accuracy 70.460%\n",
      "Epoch 6, Batch 155, LR 1.358368 Loss 8.938467, Accuracy 70.418%\n",
      "Epoch 6, Batch 156, LR 1.358665 Loss 8.937787, Accuracy 70.413%\n",
      "Epoch 6, Batch 157, LR 1.358962 Loss 8.938707, Accuracy 70.382%\n",
      "Epoch 6, Batch 158, LR 1.359258 Loss 8.939087, Accuracy 70.367%\n",
      "Epoch 6, Batch 159, LR 1.359555 Loss 8.935801, Accuracy 70.416%\n",
      "Epoch 6, Batch 160, LR 1.359852 Loss 8.933461, Accuracy 70.425%\n",
      "Epoch 6, Batch 161, LR 1.360148 Loss 8.933972, Accuracy 70.439%\n",
      "Epoch 6, Batch 162, LR 1.360445 Loss 8.931678, Accuracy 70.443%\n",
      "Epoch 6, Batch 163, LR 1.360742 Loss 8.933776, Accuracy 70.442%\n",
      "Epoch 6, Batch 164, LR 1.361039 Loss 8.933575, Accuracy 70.460%\n",
      "Epoch 6, Batch 165, LR 1.361335 Loss 8.932626, Accuracy 70.464%\n",
      "Epoch 6, Batch 166, LR 1.361632 Loss 8.933579, Accuracy 70.458%\n",
      "Epoch 6, Batch 167, LR 1.361929 Loss 8.936648, Accuracy 70.434%\n",
      "Epoch 6, Batch 168, LR 1.362225 Loss 8.938923, Accuracy 70.406%\n",
      "Epoch 6, Batch 169, LR 1.362522 Loss 8.937530, Accuracy 70.433%\n",
      "Epoch 6, Batch 170, LR 1.362819 Loss 8.934097, Accuracy 70.469%\n",
      "Epoch 6, Batch 171, LR 1.363115 Loss 8.934111, Accuracy 70.472%\n",
      "Epoch 6, Batch 172, LR 1.363412 Loss 8.935735, Accuracy 70.453%\n",
      "Epoch 6, Batch 173, LR 1.363709 Loss 8.934475, Accuracy 70.462%\n",
      "Epoch 6, Batch 174, LR 1.364005 Loss 8.937216, Accuracy 70.447%\n",
      "Epoch 6, Batch 175, LR 1.364302 Loss 8.935876, Accuracy 70.469%\n",
      "Epoch 6, Batch 176, LR 1.364599 Loss 8.930314, Accuracy 70.490%\n",
      "Epoch 6, Batch 177, LR 1.364895 Loss 8.928076, Accuracy 70.520%\n",
      "Epoch 6, Batch 178, LR 1.365192 Loss 8.926050, Accuracy 70.554%\n",
      "Epoch 6, Batch 179, LR 1.365489 Loss 8.927058, Accuracy 70.553%\n",
      "Epoch 6, Batch 180, LR 1.365785 Loss 8.928779, Accuracy 70.525%\n",
      "Epoch 6, Batch 181, LR 1.366082 Loss 8.930167, Accuracy 70.524%\n",
      "Epoch 6, Batch 182, LR 1.366379 Loss 8.932262, Accuracy 70.501%\n",
      "Epoch 6, Batch 183, LR 1.366675 Loss 8.934280, Accuracy 70.475%\n",
      "Epoch 6, Batch 184, LR 1.366972 Loss 8.930664, Accuracy 70.499%\n",
      "Epoch 6, Batch 185, LR 1.367269 Loss 8.932733, Accuracy 70.477%\n",
      "Epoch 6, Batch 186, LR 1.367565 Loss 8.935028, Accuracy 70.443%\n",
      "Epoch 6, Batch 187, LR 1.367862 Loss 8.931888, Accuracy 70.463%\n",
      "Epoch 6, Batch 188, LR 1.368159 Loss 8.933298, Accuracy 70.470%\n",
      "Epoch 6, Batch 189, LR 1.368455 Loss 8.936520, Accuracy 70.420%\n",
      "Epoch 6, Batch 190, LR 1.368752 Loss 8.932692, Accuracy 70.452%\n",
      "Epoch 6, Batch 191, LR 1.369048 Loss 8.931520, Accuracy 70.480%\n",
      "Epoch 6, Batch 192, LR 1.369345 Loss 8.931096, Accuracy 70.487%\n",
      "Epoch 6, Batch 193, LR 1.369642 Loss 8.934873, Accuracy 70.470%\n",
      "Epoch 6, Batch 194, LR 1.369938 Loss 8.933059, Accuracy 70.482%\n",
      "Epoch 6, Batch 195, LR 1.370235 Loss 8.934094, Accuracy 70.469%\n",
      "Epoch 6, Batch 196, LR 1.370532 Loss 8.936728, Accuracy 70.456%\n",
      "Epoch 6, Batch 197, LR 1.370828 Loss 8.940208, Accuracy 70.428%\n",
      "Epoch 6, Batch 198, LR 1.371125 Loss 8.942613, Accuracy 70.407%\n",
      "Epoch 6, Batch 199, LR 1.371421 Loss 8.944449, Accuracy 70.395%\n",
      "Epoch 6, Batch 200, LR 1.371718 Loss 8.948061, Accuracy 70.395%\n",
      "Epoch 6, Batch 201, LR 1.372014 Loss 8.946025, Accuracy 70.410%\n",
      "Epoch 6, Batch 202, LR 1.372311 Loss 8.946881, Accuracy 70.394%\n",
      "Epoch 6, Batch 203, LR 1.372608 Loss 8.949063, Accuracy 70.370%\n",
      "Epoch 6, Batch 204, LR 1.372904 Loss 8.949865, Accuracy 70.343%\n",
      "Epoch 6, Batch 205, LR 1.373201 Loss 8.949352, Accuracy 70.339%\n",
      "Epoch 6, Batch 206, LR 1.373497 Loss 8.953191, Accuracy 70.305%\n",
      "Epoch 6, Batch 207, LR 1.373794 Loss 8.947911, Accuracy 70.339%\n",
      "Epoch 6, Batch 208, LR 1.374090 Loss 8.948990, Accuracy 70.331%\n",
      "Epoch 6, Batch 209, LR 1.374387 Loss 8.947000, Accuracy 70.354%\n",
      "Epoch 6, Batch 210, LR 1.374684 Loss 8.948058, Accuracy 70.346%\n",
      "Epoch 6, Batch 211, LR 1.374980 Loss 8.948863, Accuracy 70.346%\n",
      "Epoch 6, Batch 212, LR 1.375277 Loss 8.947124, Accuracy 70.349%\n",
      "Epoch 6, Batch 213, LR 1.375573 Loss 8.947242, Accuracy 70.357%\n",
      "Epoch 6, Batch 214, LR 1.375870 Loss 8.945541, Accuracy 70.360%\n",
      "Epoch 6, Batch 215, LR 1.376166 Loss 8.944629, Accuracy 70.363%\n",
      "Epoch 6, Batch 216, LR 1.376463 Loss 8.944085, Accuracy 70.374%\n",
      "Epoch 6, Batch 217, LR 1.376759 Loss 8.942126, Accuracy 70.403%\n",
      "Epoch 6, Batch 218, LR 1.377056 Loss 8.943463, Accuracy 70.409%\n",
      "Epoch 6, Batch 219, LR 1.377352 Loss 8.943654, Accuracy 70.405%\n",
      "Epoch 6, Batch 220, LR 1.377649 Loss 8.943107, Accuracy 70.408%\n",
      "Epoch 6, Batch 221, LR 1.377945 Loss 8.944631, Accuracy 70.394%\n",
      "Epoch 6, Batch 222, LR 1.378242 Loss 8.945881, Accuracy 70.386%\n",
      "Epoch 6, Batch 223, LR 1.378538 Loss 8.947175, Accuracy 70.365%\n",
      "Epoch 6, Batch 224, LR 1.378835 Loss 8.949452, Accuracy 70.358%\n",
      "Epoch 6, Batch 225, LR 1.379131 Loss 8.951514, Accuracy 70.330%\n",
      "Epoch 6, Batch 226, LR 1.379428 Loss 8.952795, Accuracy 70.326%\n",
      "Epoch 6, Batch 227, LR 1.379724 Loss 8.954775, Accuracy 70.309%\n",
      "Epoch 6, Batch 228, LR 1.380021 Loss 8.954225, Accuracy 70.295%\n",
      "Epoch 6, Batch 229, LR 1.380317 Loss 8.953773, Accuracy 70.302%\n",
      "Epoch 6, Batch 230, LR 1.380614 Loss 8.950884, Accuracy 70.319%\n",
      "Epoch 6, Batch 231, LR 1.380910 Loss 8.955215, Accuracy 70.282%\n",
      "Epoch 6, Batch 232, LR 1.381207 Loss 8.957445, Accuracy 70.272%\n",
      "Epoch 6, Batch 233, LR 1.381503 Loss 8.956813, Accuracy 70.272%\n",
      "Epoch 6, Batch 234, LR 1.381800 Loss 8.957374, Accuracy 70.272%\n",
      "Epoch 6, Batch 235, LR 1.382096 Loss 8.957433, Accuracy 70.289%\n",
      "Epoch 6, Batch 236, LR 1.382392 Loss 8.957739, Accuracy 70.299%\n",
      "Epoch 6, Batch 237, LR 1.382689 Loss 8.959581, Accuracy 70.306%\n",
      "Epoch 6, Batch 238, LR 1.382985 Loss 8.959527, Accuracy 70.312%\n",
      "Epoch 6, Batch 239, LR 1.383282 Loss 8.958595, Accuracy 70.316%\n",
      "Epoch 6, Batch 240, LR 1.383578 Loss 8.958944, Accuracy 70.319%\n",
      "Epoch 6, Batch 241, LR 1.383875 Loss 8.959744, Accuracy 70.300%\n",
      "Epoch 6, Batch 242, LR 1.384171 Loss 8.960013, Accuracy 70.283%\n",
      "Epoch 6, Batch 243, LR 1.384467 Loss 8.960568, Accuracy 70.303%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 244, LR 1.384764 Loss 8.959734, Accuracy 70.322%\n",
      "Epoch 6, Batch 245, LR 1.385060 Loss 8.957779, Accuracy 70.335%\n",
      "Epoch 6, Batch 246, LR 1.385357 Loss 8.957506, Accuracy 70.335%\n",
      "Epoch 6, Batch 247, LR 1.385653 Loss 8.956634, Accuracy 70.347%\n",
      "Epoch 6, Batch 248, LR 1.385949 Loss 8.956545, Accuracy 70.328%\n",
      "Epoch 6, Batch 249, LR 1.386246 Loss 8.957594, Accuracy 70.319%\n",
      "Epoch 6, Batch 250, LR 1.386542 Loss 8.959169, Accuracy 70.294%\n",
      "Epoch 6, Batch 251, LR 1.386839 Loss 8.963212, Accuracy 70.263%\n",
      "Epoch 6, Batch 252, LR 1.387135 Loss 8.963974, Accuracy 70.257%\n",
      "Epoch 6, Batch 253, LR 1.387431 Loss 8.965748, Accuracy 70.245%\n",
      "Epoch 6, Batch 254, LR 1.387728 Loss 8.964750, Accuracy 70.251%\n",
      "Epoch 6, Batch 255, LR 1.388024 Loss 8.966549, Accuracy 70.245%\n",
      "Epoch 6, Batch 256, LR 1.388320 Loss 8.968475, Accuracy 70.236%\n",
      "Epoch 6, Batch 257, LR 1.388617 Loss 8.968129, Accuracy 70.246%\n",
      "Epoch 6, Batch 258, LR 1.388913 Loss 8.968513, Accuracy 70.228%\n",
      "Epoch 6, Batch 259, LR 1.389209 Loss 8.969093, Accuracy 70.201%\n",
      "Epoch 6, Batch 260, LR 1.389506 Loss 8.970660, Accuracy 70.168%\n",
      "Epoch 6, Batch 261, LR 1.389802 Loss 8.969879, Accuracy 70.172%\n",
      "Epoch 6, Batch 262, LR 1.390098 Loss 8.972133, Accuracy 70.169%\n",
      "Epoch 6, Batch 263, LR 1.390395 Loss 8.970760, Accuracy 70.161%\n",
      "Epoch 6, Batch 264, LR 1.390691 Loss 8.970354, Accuracy 70.162%\n",
      "Epoch 6, Batch 265, LR 1.390987 Loss 8.970992, Accuracy 70.142%\n",
      "Epoch 6, Batch 266, LR 1.391284 Loss 8.970616, Accuracy 70.151%\n",
      "Epoch 6, Batch 267, LR 1.391580 Loss 8.970315, Accuracy 70.143%\n",
      "Epoch 6, Batch 268, LR 1.391876 Loss 8.969562, Accuracy 70.146%\n",
      "Epoch 6, Batch 269, LR 1.392172 Loss 8.971065, Accuracy 70.112%\n",
      "Epoch 6, Batch 270, LR 1.392469 Loss 8.971485, Accuracy 70.122%\n",
      "Epoch 6, Batch 271, LR 1.392765 Loss 8.973083, Accuracy 70.108%\n",
      "Epoch 6, Batch 272, LR 1.393061 Loss 8.974951, Accuracy 70.083%\n",
      "Epoch 6, Batch 273, LR 1.393358 Loss 8.978624, Accuracy 70.064%\n",
      "Epoch 6, Batch 274, LR 1.393654 Loss 8.979134, Accuracy 70.056%\n",
      "Epoch 6, Batch 275, LR 1.393950 Loss 8.980616, Accuracy 70.048%\n",
      "Epoch 6, Batch 276, LR 1.394246 Loss 8.981403, Accuracy 70.049%\n",
      "Epoch 6, Batch 277, LR 1.394543 Loss 8.982016, Accuracy 70.047%\n",
      "Epoch 6, Batch 278, LR 1.394839 Loss 8.979786, Accuracy 70.065%\n",
      "Epoch 6, Batch 279, LR 1.395135 Loss 8.978917, Accuracy 70.063%\n",
      "Epoch 6, Batch 280, LR 1.395431 Loss 8.979181, Accuracy 70.061%\n",
      "Epoch 6, Batch 281, LR 1.395727 Loss 8.977973, Accuracy 70.071%\n",
      "Epoch 6, Batch 282, LR 1.396024 Loss 8.976784, Accuracy 70.066%\n",
      "Epoch 6, Batch 283, LR 1.396320 Loss 8.975663, Accuracy 70.067%\n",
      "Epoch 6, Batch 284, LR 1.396616 Loss 8.973036, Accuracy 70.084%\n",
      "Epoch 6, Batch 285, LR 1.396912 Loss 8.971326, Accuracy 70.101%\n",
      "Epoch 6, Batch 286, LR 1.397209 Loss 8.969872, Accuracy 70.116%\n",
      "Epoch 6, Batch 287, LR 1.397505 Loss 8.969938, Accuracy 70.127%\n",
      "Epoch 6, Batch 288, LR 1.397801 Loss 8.968443, Accuracy 70.125%\n",
      "Epoch 6, Batch 289, LR 1.398097 Loss 8.968698, Accuracy 70.126%\n",
      "Epoch 6, Batch 290, LR 1.398393 Loss 8.966938, Accuracy 70.140%\n",
      "Epoch 6, Batch 291, LR 1.398689 Loss 8.964775, Accuracy 70.141%\n",
      "Epoch 6, Batch 292, LR 1.398986 Loss 8.968312, Accuracy 70.101%\n",
      "Epoch 6, Batch 293, LR 1.399282 Loss 8.967316, Accuracy 70.115%\n",
      "Epoch 6, Batch 294, LR 1.399578 Loss 8.966025, Accuracy 70.126%\n",
      "Epoch 6, Batch 295, LR 1.399874 Loss 8.965972, Accuracy 70.127%\n",
      "Epoch 6, Batch 296, LR 1.400170 Loss 8.965059, Accuracy 70.136%\n",
      "Epoch 6, Batch 297, LR 1.400466 Loss 8.964844, Accuracy 70.134%\n",
      "Epoch 6, Batch 298, LR 1.400763 Loss 8.964379, Accuracy 70.147%\n",
      "Epoch 6, Batch 299, LR 1.401059 Loss 8.963358, Accuracy 70.140%\n",
      "Epoch 6, Batch 300, LR 1.401355 Loss 8.964170, Accuracy 70.120%\n",
      "Epoch 6, Batch 301, LR 1.401651 Loss 8.962810, Accuracy 70.128%\n",
      "Epoch 6, Batch 302, LR 1.401947 Loss 8.962580, Accuracy 70.124%\n",
      "Epoch 6, Batch 303, LR 1.402243 Loss 8.963528, Accuracy 70.111%\n",
      "Epoch 6, Batch 304, LR 1.402539 Loss 8.964444, Accuracy 70.094%\n",
      "Epoch 6, Batch 305, LR 1.402835 Loss 8.966758, Accuracy 70.095%\n",
      "Epoch 6, Batch 306, LR 1.403131 Loss 8.966551, Accuracy 70.095%\n",
      "Epoch 6, Batch 307, LR 1.403428 Loss 8.966233, Accuracy 70.106%\n",
      "Epoch 6, Batch 308, LR 1.403724 Loss 8.965688, Accuracy 70.117%\n",
      "Epoch 6, Batch 309, LR 1.404020 Loss 8.964442, Accuracy 70.128%\n",
      "Epoch 6, Batch 310, LR 1.404316 Loss 8.962827, Accuracy 70.129%\n",
      "Epoch 6, Batch 311, LR 1.404612 Loss 8.963132, Accuracy 70.112%\n",
      "Epoch 6, Batch 312, LR 1.404908 Loss 8.962601, Accuracy 70.115%\n",
      "Epoch 6, Batch 313, LR 1.405204 Loss 8.963012, Accuracy 70.113%\n",
      "Epoch 6, Batch 314, LR 1.405500 Loss 8.963765, Accuracy 70.111%\n",
      "Epoch 6, Batch 315, LR 1.405796 Loss 8.962404, Accuracy 70.114%\n",
      "Epoch 6, Batch 316, LR 1.406092 Loss 8.965717, Accuracy 70.073%\n",
      "Epoch 6, Batch 317, LR 1.406388 Loss 8.964817, Accuracy 70.098%\n",
      "Epoch 6, Batch 318, LR 1.406684 Loss 8.965041, Accuracy 70.096%\n",
      "Epoch 6, Batch 319, LR 1.406980 Loss 8.965442, Accuracy 70.092%\n",
      "Epoch 6, Batch 320, LR 1.407276 Loss 8.966724, Accuracy 70.088%\n",
      "Epoch 6, Batch 321, LR 1.407572 Loss 8.965431, Accuracy 70.074%\n",
      "Epoch 6, Batch 322, LR 1.407868 Loss 8.965530, Accuracy 70.077%\n",
      "Epoch 6, Batch 323, LR 1.408164 Loss 8.964975, Accuracy 70.095%\n",
      "Epoch 6, Batch 324, LR 1.408460 Loss 8.965367, Accuracy 70.098%\n",
      "Epoch 6, Batch 325, LR 1.408756 Loss 8.965387, Accuracy 70.101%\n",
      "Epoch 6, Batch 326, LR 1.409052 Loss 8.964488, Accuracy 70.097%\n",
      "Epoch 6, Batch 327, LR 1.409348 Loss 8.964806, Accuracy 70.105%\n",
      "Epoch 6, Batch 328, LR 1.409644 Loss 8.964266, Accuracy 70.120%\n",
      "Epoch 6, Batch 329, LR 1.409940 Loss 8.960939, Accuracy 70.153%\n",
      "Epoch 6, Batch 330, LR 1.410236 Loss 8.960554, Accuracy 70.149%\n",
      "Epoch 6, Batch 331, LR 1.410532 Loss 8.960185, Accuracy 70.152%\n",
      "Epoch 6, Batch 332, LR 1.410828 Loss 8.957382, Accuracy 70.178%\n",
      "Epoch 6, Batch 333, LR 1.411124 Loss 8.957388, Accuracy 70.183%\n",
      "Epoch 6, Batch 334, LR 1.411420 Loss 8.957191, Accuracy 70.186%\n",
      "Epoch 6, Batch 335, LR 1.411716 Loss 8.959397, Accuracy 70.166%\n",
      "Epoch 6, Batch 336, LR 1.412011 Loss 8.958350, Accuracy 70.175%\n",
      "Epoch 6, Batch 337, LR 1.412307 Loss 8.958050, Accuracy 70.169%\n",
      "Epoch 6, Batch 338, LR 1.412603 Loss 8.959325, Accuracy 70.162%\n",
      "Epoch 6, Batch 339, LR 1.412899 Loss 8.960398, Accuracy 70.158%\n",
      "Epoch 6, Batch 340, LR 1.413195 Loss 8.960441, Accuracy 70.142%\n",
      "Epoch 6, Batch 341, LR 1.413491 Loss 8.959151, Accuracy 70.152%\n",
      "Epoch 6, Batch 342, LR 1.413787 Loss 8.957799, Accuracy 70.169%\n",
      "Epoch 6, Batch 343, LR 1.414083 Loss 8.956136, Accuracy 70.167%\n",
      "Epoch 6, Batch 344, LR 1.414379 Loss 8.953137, Accuracy 70.192%\n",
      "Epoch 6, Batch 345, LR 1.414674 Loss 8.953364, Accuracy 70.179%\n",
      "Epoch 6, Batch 346, LR 1.414970 Loss 8.955209, Accuracy 70.166%\n",
      "Epoch 6, Batch 347, LR 1.415266 Loss 8.956170, Accuracy 70.164%\n",
      "Epoch 6, Batch 348, LR 1.415562 Loss 8.955908, Accuracy 70.169%\n",
      "Epoch 6, Batch 349, LR 1.415858 Loss 8.956045, Accuracy 70.165%\n",
      "Epoch 6, Batch 350, LR 1.416154 Loss 8.955315, Accuracy 70.179%\n",
      "Epoch 6, Batch 351, LR 1.416449 Loss 8.953172, Accuracy 70.201%\n",
      "Epoch 6, Batch 352, LR 1.416745 Loss 8.953350, Accuracy 70.190%\n",
      "Epoch 6, Batch 353, LR 1.417041 Loss 8.953119, Accuracy 70.191%\n",
      "Epoch 6, Batch 354, LR 1.417337 Loss 8.953667, Accuracy 70.196%\n",
      "Epoch 6, Batch 355, LR 1.417633 Loss 8.953136, Accuracy 70.207%\n",
      "Epoch 6, Batch 356, LR 1.417928 Loss 8.953823, Accuracy 70.192%\n",
      "Epoch 6, Batch 357, LR 1.418224 Loss 8.952102, Accuracy 70.201%\n",
      "Epoch 6, Batch 358, LR 1.418520 Loss 8.952230, Accuracy 70.195%\n",
      "Epoch 6, Batch 359, LR 1.418816 Loss 8.951294, Accuracy 70.204%\n",
      "Epoch 6, Batch 360, LR 1.419111 Loss 8.950757, Accuracy 70.217%\n",
      "Epoch 6, Batch 361, LR 1.419407 Loss 8.951922, Accuracy 70.202%\n",
      "Epoch 6, Batch 362, LR 1.419703 Loss 8.951174, Accuracy 70.209%\n",
      "Epoch 6, Batch 363, LR 1.419999 Loss 8.950404, Accuracy 70.209%\n",
      "Epoch 6, Batch 364, LR 1.420294 Loss 8.949311, Accuracy 70.227%\n",
      "Epoch 6, Batch 365, LR 1.420590 Loss 8.951213, Accuracy 70.218%\n",
      "Epoch 6, Batch 366, LR 1.420886 Loss 8.950435, Accuracy 70.221%\n",
      "Epoch 6, Batch 367, LR 1.421182 Loss 8.950915, Accuracy 70.227%\n",
      "Epoch 6, Batch 368, LR 1.421477 Loss 8.951072, Accuracy 70.223%\n",
      "Epoch 6, Batch 369, LR 1.421773 Loss 8.950386, Accuracy 70.228%\n",
      "Epoch 6, Batch 370, LR 1.422069 Loss 8.948615, Accuracy 70.241%\n",
      "Epoch 6, Batch 371, LR 1.422364 Loss 8.946173, Accuracy 70.251%\n",
      "Epoch 6, Batch 372, LR 1.422660 Loss 8.945435, Accuracy 70.266%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 373, LR 1.422956 Loss 8.945871, Accuracy 70.245%\n",
      "Epoch 6, Batch 374, LR 1.423251 Loss 8.946428, Accuracy 70.233%\n",
      "Epoch 6, Batch 375, LR 1.423547 Loss 8.945953, Accuracy 70.244%\n",
      "Epoch 6, Batch 376, LR 1.423843 Loss 8.945922, Accuracy 70.254%\n",
      "Epoch 6, Batch 377, LR 1.424138 Loss 8.946045, Accuracy 70.257%\n",
      "Epoch 6, Batch 378, LR 1.424434 Loss 8.946694, Accuracy 70.257%\n",
      "Epoch 6, Batch 379, LR 1.424730 Loss 8.944451, Accuracy 70.269%\n",
      "Epoch 6, Batch 380, LR 1.425025 Loss 8.945855, Accuracy 70.255%\n",
      "Epoch 6, Batch 381, LR 1.425321 Loss 8.946123, Accuracy 70.259%\n",
      "Epoch 6, Batch 382, LR 1.425616 Loss 8.944677, Accuracy 70.268%\n",
      "Epoch 6, Batch 383, LR 1.425912 Loss 8.944579, Accuracy 70.278%\n",
      "Epoch 6, Batch 384, LR 1.426208 Loss 8.945467, Accuracy 70.270%\n",
      "Epoch 6, Batch 385, LR 1.426503 Loss 8.944622, Accuracy 70.276%\n",
      "Epoch 6, Batch 386, LR 1.426799 Loss 8.944659, Accuracy 70.286%\n",
      "Epoch 6, Batch 387, LR 1.427094 Loss 8.943366, Accuracy 70.290%\n",
      "Epoch 6, Batch 388, LR 1.427390 Loss 8.944925, Accuracy 70.280%\n",
      "Epoch 6, Batch 389, LR 1.427685 Loss 8.944612, Accuracy 70.290%\n",
      "Epoch 6, Batch 390, LR 1.427981 Loss 8.944410, Accuracy 70.280%\n",
      "Epoch 6, Batch 391, LR 1.428277 Loss 8.944548, Accuracy 70.285%\n",
      "Epoch 6, Batch 392, LR 1.428572 Loss 8.944791, Accuracy 70.277%\n",
      "Epoch 6, Batch 393, LR 1.428868 Loss 8.947961, Accuracy 70.245%\n",
      "Epoch 6, Batch 394, LR 1.429163 Loss 8.949833, Accuracy 70.235%\n",
      "Epoch 6, Batch 395, LR 1.429459 Loss 8.949480, Accuracy 70.243%\n",
      "Epoch 6, Batch 396, LR 1.429754 Loss 8.948052, Accuracy 70.249%\n",
      "Epoch 6, Batch 397, LR 1.430050 Loss 8.947389, Accuracy 70.248%\n",
      "Epoch 6, Batch 398, LR 1.430345 Loss 8.948374, Accuracy 70.248%\n",
      "Epoch 6, Batch 399, LR 1.430641 Loss 8.950610, Accuracy 70.224%\n",
      "Epoch 6, Batch 400, LR 1.430936 Loss 8.948655, Accuracy 70.238%\n",
      "Epoch 6, Batch 401, LR 1.431232 Loss 8.948577, Accuracy 70.235%\n",
      "Epoch 6, Batch 402, LR 1.431527 Loss 8.946501, Accuracy 70.258%\n",
      "Epoch 6, Batch 403, LR 1.431823 Loss 8.946858, Accuracy 70.254%\n",
      "Epoch 6, Batch 404, LR 1.432118 Loss 8.948063, Accuracy 70.239%\n",
      "Epoch 6, Batch 405, LR 1.432413 Loss 8.947869, Accuracy 70.228%\n",
      "Epoch 6, Batch 406, LR 1.432709 Loss 8.946430, Accuracy 70.234%\n",
      "Epoch 6, Batch 407, LR 1.433004 Loss 8.945906, Accuracy 70.240%\n",
      "Epoch 6, Batch 408, LR 1.433300 Loss 8.946861, Accuracy 70.247%\n",
      "Epoch 6, Batch 409, LR 1.433595 Loss 8.946764, Accuracy 70.246%\n",
      "Epoch 6, Batch 410, LR 1.433891 Loss 8.946454, Accuracy 70.238%\n",
      "Epoch 6, Batch 411, LR 1.434186 Loss 8.945921, Accuracy 70.246%\n",
      "Epoch 6, Batch 412, LR 1.434481 Loss 8.946236, Accuracy 70.250%\n",
      "Epoch 6, Batch 413, LR 1.434777 Loss 8.947031, Accuracy 70.250%\n",
      "Epoch 6, Batch 414, LR 1.435072 Loss 8.949060, Accuracy 70.228%\n",
      "Epoch 6, Batch 415, LR 1.435367 Loss 8.949337, Accuracy 70.224%\n",
      "Epoch 6, Batch 416, LR 1.435663 Loss 8.949192, Accuracy 70.226%\n",
      "Epoch 6, Batch 417, LR 1.435958 Loss 8.949236, Accuracy 70.221%\n",
      "Epoch 6, Batch 418, LR 1.436253 Loss 8.949099, Accuracy 70.217%\n",
      "Epoch 6, Batch 419, LR 1.436549 Loss 8.948931, Accuracy 70.223%\n",
      "Epoch 6, Batch 420, LR 1.436844 Loss 8.949183, Accuracy 70.219%\n",
      "Epoch 6, Batch 421, LR 1.437139 Loss 8.949320, Accuracy 70.225%\n",
      "Epoch 6, Batch 422, LR 1.437435 Loss 8.950077, Accuracy 70.214%\n",
      "Epoch 6, Batch 423, LR 1.437730 Loss 8.951795, Accuracy 70.207%\n",
      "Epoch 6, Batch 424, LR 1.438025 Loss 8.953245, Accuracy 70.193%\n",
      "Epoch 6, Batch 425, LR 1.438321 Loss 8.952465, Accuracy 70.199%\n",
      "Epoch 6, Batch 426, LR 1.438616 Loss 8.951332, Accuracy 70.204%\n",
      "Epoch 6, Batch 427, LR 1.438911 Loss 8.950644, Accuracy 70.212%\n",
      "Epoch 6, Batch 428, LR 1.439206 Loss 8.952346, Accuracy 70.198%\n",
      "Epoch 6, Batch 429, LR 1.439502 Loss 8.951415, Accuracy 70.205%\n",
      "Epoch 6, Batch 430, LR 1.439797 Loss 8.950823, Accuracy 70.207%\n",
      "Epoch 6, Batch 431, LR 1.440092 Loss 8.950649, Accuracy 70.209%\n",
      "Epoch 6, Batch 432, LR 1.440387 Loss 8.950316, Accuracy 70.211%\n",
      "Epoch 6, Batch 433, LR 1.440683 Loss 8.952540, Accuracy 70.192%\n",
      "Epoch 6, Batch 434, LR 1.440978 Loss 8.953084, Accuracy 70.185%\n",
      "Epoch 6, Batch 435, LR 1.441273 Loss 8.953059, Accuracy 70.178%\n",
      "Epoch 6, Batch 436, LR 1.441568 Loss 8.951874, Accuracy 70.187%\n",
      "Epoch 6, Batch 437, LR 1.441863 Loss 8.952831, Accuracy 70.168%\n",
      "Epoch 6, Batch 438, LR 1.442159 Loss 8.952881, Accuracy 70.172%\n",
      "Epoch 6, Batch 439, LR 1.442454 Loss 8.952116, Accuracy 70.175%\n",
      "Epoch 6, Batch 440, LR 1.442749 Loss 8.952739, Accuracy 70.174%\n",
      "Epoch 6, Batch 441, LR 1.443044 Loss 8.952990, Accuracy 70.174%\n",
      "Epoch 6, Batch 442, LR 1.443339 Loss 8.952784, Accuracy 70.173%\n",
      "Epoch 6, Batch 443, LR 1.443634 Loss 8.955222, Accuracy 70.157%\n",
      "Epoch 6, Batch 444, LR 1.443930 Loss 8.955982, Accuracy 70.144%\n",
      "Epoch 6, Batch 445, LR 1.444225 Loss 8.954169, Accuracy 70.165%\n",
      "Epoch 6, Batch 446, LR 1.444520 Loss 8.954206, Accuracy 70.167%\n",
      "Epoch 6, Batch 447, LR 1.444815 Loss 8.952444, Accuracy 70.181%\n",
      "Epoch 6, Batch 448, LR 1.445110 Loss 8.950797, Accuracy 70.197%\n",
      "Epoch 6, Batch 449, LR 1.445405 Loss 8.949528, Accuracy 70.213%\n",
      "Epoch 6, Batch 450, LR 1.445700 Loss 8.949555, Accuracy 70.217%\n",
      "Epoch 6, Batch 451, LR 1.445995 Loss 8.951533, Accuracy 70.200%\n",
      "Epoch 6, Batch 452, LR 1.446290 Loss 8.952514, Accuracy 70.192%\n",
      "Epoch 6, Batch 453, LR 1.446585 Loss 8.952873, Accuracy 70.192%\n",
      "Epoch 6, Batch 454, LR 1.446880 Loss 8.952691, Accuracy 70.185%\n",
      "Epoch 6, Batch 455, LR 1.447176 Loss 8.951791, Accuracy 70.197%\n",
      "Epoch 6, Batch 456, LR 1.447471 Loss 8.952817, Accuracy 70.203%\n",
      "Epoch 6, Batch 457, LR 1.447766 Loss 8.952745, Accuracy 70.196%\n",
      "Epoch 6, Batch 458, LR 1.448061 Loss 8.953287, Accuracy 70.185%\n",
      "Epoch 6, Batch 459, LR 1.448356 Loss 8.955104, Accuracy 70.170%\n",
      "Epoch 6, Batch 460, LR 1.448651 Loss 8.955711, Accuracy 70.165%\n",
      "Epoch 6, Batch 461, LR 1.448946 Loss 8.953535, Accuracy 70.182%\n",
      "Epoch 6, Batch 462, LR 1.449241 Loss 8.955178, Accuracy 70.162%\n",
      "Epoch 6, Batch 463, LR 1.449536 Loss 8.956219, Accuracy 70.159%\n",
      "Epoch 6, Batch 464, LR 1.449831 Loss 8.955355, Accuracy 70.169%\n",
      "Epoch 6, Batch 465, LR 1.450125 Loss 8.953168, Accuracy 70.183%\n",
      "Epoch 6, Batch 466, LR 1.450420 Loss 8.953938, Accuracy 70.178%\n",
      "Epoch 6, Batch 467, LR 1.450715 Loss 8.955082, Accuracy 70.172%\n",
      "Epoch 6, Batch 468, LR 1.451010 Loss 8.953890, Accuracy 70.191%\n",
      "Epoch 6, Batch 469, LR 1.451305 Loss 8.952541, Accuracy 70.203%\n",
      "Epoch 6, Batch 470, LR 1.451600 Loss 8.953547, Accuracy 70.196%\n",
      "Epoch 6, Batch 471, LR 1.451895 Loss 8.952925, Accuracy 70.205%\n",
      "Epoch 6, Batch 472, LR 1.452190 Loss 8.952215, Accuracy 70.207%\n",
      "Epoch 6, Batch 473, LR 1.452485 Loss 8.950427, Accuracy 70.225%\n",
      "Epoch 6, Batch 474, LR 1.452780 Loss 8.951357, Accuracy 70.210%\n",
      "Epoch 6, Batch 475, LR 1.453075 Loss 8.949861, Accuracy 70.222%\n",
      "Epoch 6, Batch 476, LR 1.453369 Loss 8.950129, Accuracy 70.227%\n",
      "Epoch 6, Batch 477, LR 1.453664 Loss 8.949275, Accuracy 70.232%\n",
      "Epoch 6, Batch 478, LR 1.453959 Loss 8.948774, Accuracy 70.241%\n",
      "Epoch 6, Batch 479, LR 1.454254 Loss 8.949689, Accuracy 70.233%\n",
      "Epoch 6, Batch 480, LR 1.454549 Loss 8.948025, Accuracy 70.246%\n",
      "Epoch 6, Batch 481, LR 1.454844 Loss 8.948236, Accuracy 70.248%\n",
      "Epoch 6, Batch 482, LR 1.455138 Loss 8.949574, Accuracy 70.231%\n",
      "Epoch 6, Batch 483, LR 1.455433 Loss 8.949926, Accuracy 70.225%\n",
      "Epoch 6, Batch 484, LR 1.455728 Loss 8.950061, Accuracy 70.217%\n",
      "Epoch 6, Batch 485, LR 1.456023 Loss 8.950338, Accuracy 70.219%\n",
      "Epoch 6, Batch 486, LR 1.456318 Loss 8.950979, Accuracy 70.210%\n",
      "Epoch 6, Batch 487, LR 1.456612 Loss 8.951598, Accuracy 70.200%\n",
      "Epoch 6, Batch 488, LR 1.456907 Loss 8.952053, Accuracy 70.196%\n",
      "Epoch 6, Batch 489, LR 1.457202 Loss 8.953466, Accuracy 70.181%\n",
      "Epoch 6, Batch 490, LR 1.457497 Loss 8.953269, Accuracy 70.193%\n",
      "Epoch 6, Batch 491, LR 1.457791 Loss 8.953259, Accuracy 70.196%\n",
      "Epoch 6, Batch 492, LR 1.458086 Loss 8.952430, Accuracy 70.211%\n",
      "Epoch 6, Batch 493, LR 1.458381 Loss 8.953130, Accuracy 70.205%\n",
      "Epoch 6, Batch 494, LR 1.458675 Loss 8.952870, Accuracy 70.210%\n",
      "Epoch 6, Batch 495, LR 1.458970 Loss 8.952631, Accuracy 70.215%\n",
      "Epoch 6, Batch 496, LR 1.459265 Loss 8.953098, Accuracy 70.213%\n",
      "Epoch 6, Batch 497, LR 1.459559 Loss 8.952999, Accuracy 70.209%\n",
      "Epoch 6, Batch 498, LR 1.459854 Loss 8.953874, Accuracy 70.198%\n",
      "Epoch 6, Batch 499, LR 1.460149 Loss 8.954895, Accuracy 70.195%\n",
      "Epoch 6, Batch 500, LR 1.460443 Loss 8.955071, Accuracy 70.197%\n",
      "Epoch 6, Batch 501, LR 1.460738 Loss 8.955341, Accuracy 70.194%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 502, LR 1.461033 Loss 8.955248, Accuracy 70.190%\n",
      "Epoch 6, Batch 503, LR 1.461327 Loss 8.955599, Accuracy 70.185%\n",
      "Epoch 6, Batch 504, LR 1.461622 Loss 8.955449, Accuracy 70.184%\n",
      "Epoch 6, Batch 505, LR 1.461917 Loss 8.955879, Accuracy 70.178%\n",
      "Epoch 6, Batch 506, LR 1.462211 Loss 8.957465, Accuracy 70.169%\n",
      "Epoch 6, Batch 507, LR 1.462506 Loss 8.957159, Accuracy 70.172%\n",
      "Epoch 6, Batch 508, LR 1.462800 Loss 8.956071, Accuracy 70.177%\n",
      "Epoch 6, Batch 509, LR 1.463095 Loss 8.955782, Accuracy 70.171%\n",
      "Epoch 6, Batch 510, LR 1.463389 Loss 8.956811, Accuracy 70.156%\n",
      "Epoch 6, Batch 511, LR 1.463684 Loss 8.956402, Accuracy 70.160%\n",
      "Epoch 6, Batch 512, LR 1.463978 Loss 8.956347, Accuracy 70.158%\n",
      "Epoch 6, Batch 513, LR 1.464273 Loss 8.956867, Accuracy 70.151%\n",
      "Epoch 6, Batch 514, LR 1.464568 Loss 8.957232, Accuracy 70.151%\n",
      "Epoch 6, Batch 515, LR 1.464862 Loss 8.957107, Accuracy 70.150%\n",
      "Epoch 6, Batch 516, LR 1.465157 Loss 8.956727, Accuracy 70.146%\n",
      "Epoch 6, Batch 517, LR 1.465451 Loss 8.957005, Accuracy 70.133%\n",
      "Epoch 6, Batch 518, LR 1.465745 Loss 8.956445, Accuracy 70.142%\n",
      "Epoch 6, Batch 519, LR 1.466040 Loss 8.955873, Accuracy 70.141%\n",
      "Epoch 6, Batch 520, LR 1.466334 Loss 8.956477, Accuracy 70.146%\n",
      "Epoch 6, Batch 521, LR 1.466629 Loss 8.955717, Accuracy 70.152%\n",
      "Epoch 6, Batch 522, LR 1.466923 Loss 8.954676, Accuracy 70.158%\n",
      "Epoch 6, Batch 523, LR 1.467218 Loss 8.954838, Accuracy 70.166%\n",
      "Epoch 6, Batch 524, LR 1.467512 Loss 8.954737, Accuracy 70.163%\n",
      "Epoch 6, Batch 525, LR 1.467807 Loss 8.954141, Accuracy 70.167%\n",
      "Epoch 6, Batch 526, LR 1.468101 Loss 8.954652, Accuracy 70.164%\n",
      "Epoch 6, Batch 527, LR 1.468395 Loss 8.953752, Accuracy 70.169%\n",
      "Epoch 6, Batch 528, LR 1.468690 Loss 8.955187, Accuracy 70.162%\n",
      "Epoch 6, Batch 529, LR 1.468984 Loss 8.956307, Accuracy 70.152%\n",
      "Epoch 6, Batch 530, LR 1.469278 Loss 8.957174, Accuracy 70.143%\n",
      "Epoch 6, Batch 531, LR 1.469573 Loss 8.957198, Accuracy 70.145%\n",
      "Epoch 6, Batch 532, LR 1.469867 Loss 8.956453, Accuracy 70.155%\n",
      "Epoch 6, Batch 533, LR 1.470161 Loss 8.956469, Accuracy 70.151%\n",
      "Epoch 6, Batch 534, LR 1.470456 Loss 8.954634, Accuracy 70.157%\n",
      "Epoch 6, Batch 535, LR 1.470750 Loss 8.954143, Accuracy 70.174%\n",
      "Epoch 6, Batch 536, LR 1.471044 Loss 8.953574, Accuracy 70.175%\n",
      "Epoch 6, Batch 537, LR 1.471339 Loss 8.953510, Accuracy 70.180%\n",
      "Epoch 6, Batch 538, LR 1.471633 Loss 8.954182, Accuracy 70.175%\n",
      "Epoch 6, Batch 539, LR 1.471927 Loss 8.954699, Accuracy 70.170%\n",
      "Epoch 6, Batch 540, LR 1.472221 Loss 8.952764, Accuracy 70.172%\n",
      "Epoch 6, Batch 541, LR 1.472516 Loss 8.954303, Accuracy 70.152%\n",
      "Epoch 6, Batch 542, LR 1.472810 Loss 8.954601, Accuracy 70.151%\n",
      "Epoch 6, Batch 543, LR 1.473104 Loss 8.954609, Accuracy 70.148%\n",
      "Epoch 6, Batch 544, LR 1.473398 Loss 8.955439, Accuracy 70.143%\n",
      "Epoch 6, Batch 545, LR 1.473693 Loss 8.954549, Accuracy 70.148%\n",
      "Epoch 6, Batch 546, LR 1.473987 Loss 8.954325, Accuracy 70.148%\n",
      "Epoch 6, Batch 547, LR 1.474281 Loss 8.954567, Accuracy 70.147%\n",
      "Epoch 6, Batch 548, LR 1.474575 Loss 8.954267, Accuracy 70.143%\n",
      "Epoch 6, Batch 549, LR 1.474869 Loss 8.953219, Accuracy 70.152%\n",
      "Epoch 6, Batch 550, LR 1.475164 Loss 8.952413, Accuracy 70.155%\n",
      "Epoch 6, Batch 551, LR 1.475458 Loss 8.951210, Accuracy 70.155%\n",
      "Epoch 6, Batch 552, LR 1.475752 Loss 8.951960, Accuracy 70.153%\n",
      "Epoch 6, Batch 553, LR 1.476046 Loss 8.950546, Accuracy 70.157%\n",
      "Epoch 6, Batch 554, LR 1.476340 Loss 8.950540, Accuracy 70.163%\n",
      "Epoch 6, Batch 555, LR 1.476634 Loss 8.950756, Accuracy 70.169%\n",
      "Epoch 6, Batch 556, LR 1.476928 Loss 8.952047, Accuracy 70.164%\n",
      "Epoch 6, Batch 557, LR 1.477222 Loss 8.953087, Accuracy 70.157%\n",
      "Epoch 6, Batch 558, LR 1.477516 Loss 8.951957, Accuracy 70.164%\n",
      "Epoch 6, Batch 559, LR 1.477811 Loss 8.951920, Accuracy 70.160%\n",
      "Epoch 6, Batch 560, LR 1.478105 Loss 8.951260, Accuracy 70.169%\n",
      "Epoch 6, Batch 561, LR 1.478399 Loss 8.953160, Accuracy 70.152%\n",
      "Epoch 6, Batch 562, LR 1.478693 Loss 8.952961, Accuracy 70.153%\n",
      "Epoch 6, Batch 563, LR 1.478987 Loss 8.954052, Accuracy 70.143%\n",
      "Epoch 6, Batch 564, LR 1.479281 Loss 8.953908, Accuracy 70.146%\n",
      "Epoch 6, Batch 565, LR 1.479575 Loss 8.953791, Accuracy 70.149%\n",
      "Epoch 6, Batch 566, LR 1.479869 Loss 8.954486, Accuracy 70.144%\n",
      "Epoch 6, Batch 567, LR 1.480163 Loss 8.953619, Accuracy 70.151%\n",
      "Epoch 6, Batch 568, LR 1.480457 Loss 8.953315, Accuracy 70.158%\n",
      "Epoch 6, Batch 569, LR 1.480751 Loss 8.953086, Accuracy 70.163%\n",
      "Epoch 6, Batch 570, LR 1.481045 Loss 8.953512, Accuracy 70.164%\n",
      "Epoch 6, Batch 571, LR 1.481339 Loss 8.953426, Accuracy 70.162%\n",
      "Epoch 6, Batch 572, LR 1.481632 Loss 8.953396, Accuracy 70.166%\n",
      "Epoch 6, Batch 573, LR 1.481926 Loss 8.952924, Accuracy 70.168%\n",
      "Epoch 6, Batch 574, LR 1.482220 Loss 8.952768, Accuracy 70.172%\n",
      "Epoch 6, Batch 575, LR 1.482514 Loss 8.952207, Accuracy 70.179%\n",
      "Epoch 6, Batch 576, LR 1.482808 Loss 8.953053, Accuracy 70.174%\n",
      "Epoch 6, Batch 577, LR 1.483102 Loss 8.953566, Accuracy 70.170%\n",
      "Epoch 6, Batch 578, LR 1.483396 Loss 8.953177, Accuracy 70.165%\n",
      "Epoch 6, Batch 579, LR 1.483690 Loss 8.952310, Accuracy 70.174%\n",
      "Epoch 6, Batch 580, LR 1.483984 Loss 8.952389, Accuracy 70.176%\n",
      "Epoch 6, Batch 581, LR 1.484277 Loss 8.952865, Accuracy 70.166%\n",
      "Epoch 6, Batch 582, LR 1.484571 Loss 8.952721, Accuracy 70.168%\n",
      "Epoch 6, Batch 583, LR 1.484865 Loss 8.952841, Accuracy 70.170%\n",
      "Epoch 6, Batch 584, LR 1.485159 Loss 8.952543, Accuracy 70.173%\n",
      "Epoch 6, Batch 585, LR 1.485453 Loss 8.952121, Accuracy 70.182%\n",
      "Epoch 6, Batch 586, LR 1.485746 Loss 8.953218, Accuracy 70.165%\n",
      "Epoch 6, Batch 587, LR 1.486040 Loss 8.952315, Accuracy 70.179%\n",
      "Epoch 6, Batch 588, LR 1.486334 Loss 8.953182, Accuracy 70.178%\n",
      "Epoch 6, Batch 589, LR 1.486628 Loss 8.953526, Accuracy 70.175%\n",
      "Epoch 6, Batch 590, LR 1.486921 Loss 8.954103, Accuracy 70.172%\n",
      "Epoch 6, Batch 591, LR 1.487215 Loss 8.954886, Accuracy 70.167%\n",
      "Epoch 6, Batch 592, LR 1.487509 Loss 8.954210, Accuracy 70.174%\n",
      "Epoch 6, Batch 593, LR 1.487803 Loss 8.953121, Accuracy 70.182%\n",
      "Epoch 6, Batch 594, LR 1.488096 Loss 8.952846, Accuracy 70.190%\n",
      "Epoch 6, Batch 595, LR 1.488390 Loss 8.953373, Accuracy 70.186%\n",
      "Epoch 6, Batch 596, LR 1.488684 Loss 8.953842, Accuracy 70.183%\n",
      "Epoch 6, Batch 597, LR 1.488977 Loss 8.953841, Accuracy 70.179%\n",
      "Epoch 6, Batch 598, LR 1.489271 Loss 8.953938, Accuracy 70.186%\n",
      "Epoch 6, Batch 599, LR 1.489565 Loss 8.954119, Accuracy 70.177%\n",
      "Epoch 6, Batch 600, LR 1.489858 Loss 8.953425, Accuracy 70.180%\n",
      "Epoch 6, Batch 601, LR 1.490152 Loss 8.953203, Accuracy 70.183%\n",
      "Epoch 6, Batch 602, LR 1.490445 Loss 8.953106, Accuracy 70.179%\n",
      "Epoch 6, Batch 603, LR 1.490739 Loss 8.952254, Accuracy 70.182%\n",
      "Epoch 6, Batch 604, LR 1.491033 Loss 8.951872, Accuracy 70.187%\n",
      "Epoch 6, Batch 605, LR 1.491326 Loss 8.952951, Accuracy 70.181%\n",
      "Epoch 6, Batch 606, LR 1.491620 Loss 8.953142, Accuracy 70.178%\n",
      "Epoch 6, Batch 607, LR 1.491913 Loss 8.954525, Accuracy 70.170%\n",
      "Epoch 6, Batch 608, LR 1.492207 Loss 8.953840, Accuracy 70.175%\n",
      "Epoch 6, Batch 609, LR 1.492500 Loss 8.955282, Accuracy 70.169%\n",
      "Epoch 6, Batch 610, LR 1.492794 Loss 8.956420, Accuracy 70.168%\n",
      "Epoch 6, Batch 611, LR 1.493087 Loss 8.956351, Accuracy 70.173%\n",
      "Epoch 6, Batch 612, LR 1.493381 Loss 8.955155, Accuracy 70.184%\n",
      "Epoch 6, Batch 613, LR 1.493674 Loss 8.954613, Accuracy 70.197%\n",
      "Epoch 6, Batch 614, LR 1.493968 Loss 8.954497, Accuracy 70.199%\n",
      "Epoch 6, Batch 615, LR 1.494261 Loss 8.954194, Accuracy 70.199%\n",
      "Epoch 6, Batch 616, LR 1.494555 Loss 8.953618, Accuracy 70.200%\n",
      "Epoch 6, Batch 617, LR 1.494848 Loss 8.953385, Accuracy 70.201%\n",
      "Epoch 6, Batch 618, LR 1.495142 Loss 8.952917, Accuracy 70.203%\n",
      "Epoch 6, Batch 619, LR 1.495435 Loss 8.952450, Accuracy 70.201%\n",
      "Epoch 6, Batch 620, LR 1.495728 Loss 8.952707, Accuracy 70.205%\n",
      "Epoch 6, Batch 621, LR 1.496022 Loss 8.951423, Accuracy 70.216%\n",
      "Epoch 6, Batch 622, LR 1.496315 Loss 8.950638, Accuracy 70.218%\n",
      "Epoch 6, Batch 623, LR 1.496609 Loss 8.950318, Accuracy 70.217%\n",
      "Epoch 6, Batch 624, LR 1.496902 Loss 8.949868, Accuracy 70.225%\n",
      "Epoch 6, Batch 625, LR 1.497195 Loss 8.950440, Accuracy 70.216%\n",
      "Epoch 6, Batch 626, LR 1.497489 Loss 8.950990, Accuracy 70.214%\n",
      "Epoch 6, Batch 627, LR 1.497782 Loss 8.953427, Accuracy 70.198%\n",
      "Epoch 6, Batch 628, LR 1.498075 Loss 8.952994, Accuracy 70.204%\n",
      "Epoch 6, Batch 629, LR 1.498369 Loss 8.953626, Accuracy 70.201%\n",
      "Epoch 6, Batch 630, LR 1.498662 Loss 8.953597, Accuracy 70.206%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 631, LR 1.498955 Loss 8.951884, Accuracy 70.217%\n",
      "Epoch 6, Batch 632, LR 1.499248 Loss 8.952571, Accuracy 70.209%\n",
      "Epoch 6, Batch 633, LR 1.499542 Loss 8.952898, Accuracy 70.209%\n",
      "Epoch 6, Batch 634, LR 1.499835 Loss 8.953605, Accuracy 70.200%\n",
      "Epoch 6, Batch 635, LR 1.500128 Loss 8.952873, Accuracy 70.199%\n",
      "Epoch 6, Batch 636, LR 1.500421 Loss 8.952703, Accuracy 70.206%\n",
      "Epoch 6, Batch 637, LR 1.500715 Loss 8.952619, Accuracy 70.212%\n",
      "Epoch 6, Batch 638, LR 1.501008 Loss 8.953012, Accuracy 70.210%\n",
      "Epoch 6, Batch 639, LR 1.501301 Loss 8.952586, Accuracy 70.215%\n",
      "Epoch 6, Batch 640, LR 1.501594 Loss 8.954023, Accuracy 70.206%\n",
      "Epoch 6, Batch 641, LR 1.501887 Loss 8.954187, Accuracy 70.205%\n",
      "Epoch 6, Batch 642, LR 1.502180 Loss 8.954370, Accuracy 70.213%\n",
      "Epoch 6, Batch 643, LR 1.502474 Loss 8.953598, Accuracy 70.214%\n",
      "Epoch 6, Batch 644, LR 1.502767 Loss 8.953917, Accuracy 70.219%\n",
      "Epoch 6, Batch 645, LR 1.503060 Loss 8.954176, Accuracy 70.220%\n",
      "Epoch 6, Batch 646, LR 1.503353 Loss 8.954122, Accuracy 70.216%\n",
      "Epoch 6, Batch 647, LR 1.503646 Loss 8.954181, Accuracy 70.224%\n",
      "Epoch 6, Batch 648, LR 1.503939 Loss 8.954344, Accuracy 70.222%\n",
      "Epoch 6, Batch 649, LR 1.504232 Loss 8.955385, Accuracy 70.214%\n",
      "Epoch 6, Batch 650, LR 1.504525 Loss 8.955437, Accuracy 70.214%\n",
      "Epoch 6, Batch 651, LR 1.504818 Loss 8.955187, Accuracy 70.219%\n",
      "Epoch 6, Batch 652, LR 1.505111 Loss 8.954382, Accuracy 70.232%\n",
      "Epoch 6, Batch 653, LR 1.505404 Loss 8.954001, Accuracy 70.242%\n",
      "Epoch 6, Batch 654, LR 1.505697 Loss 8.953666, Accuracy 70.246%\n",
      "Epoch 6, Batch 655, LR 1.505990 Loss 8.953359, Accuracy 70.247%\n",
      "Epoch 6, Batch 656, LR 1.506283 Loss 8.953350, Accuracy 70.252%\n",
      "Epoch 6, Batch 657, LR 1.506576 Loss 8.952131, Accuracy 70.259%\n",
      "Epoch 6, Batch 658, LR 1.506869 Loss 8.952309, Accuracy 70.254%\n",
      "Epoch 6, Batch 659, LR 1.507162 Loss 8.951858, Accuracy 70.258%\n",
      "Epoch 6, Batch 660, LR 1.507455 Loss 8.951432, Accuracy 70.264%\n",
      "Epoch 6, Batch 661, LR 1.507748 Loss 8.952801, Accuracy 70.252%\n",
      "Epoch 6, Batch 662, LR 1.508041 Loss 8.953171, Accuracy 70.249%\n",
      "Epoch 6, Batch 663, LR 1.508334 Loss 8.952972, Accuracy 70.250%\n",
      "Epoch 6, Batch 664, LR 1.508627 Loss 8.952174, Accuracy 70.260%\n",
      "Epoch 6, Batch 665, LR 1.508920 Loss 8.952740, Accuracy 70.260%\n",
      "Epoch 6, Batch 666, LR 1.509212 Loss 8.953197, Accuracy 70.261%\n",
      "Epoch 6, Batch 667, LR 1.509505 Loss 8.953155, Accuracy 70.266%\n",
      "Epoch 6, Batch 668, LR 1.509798 Loss 8.953026, Accuracy 70.268%\n",
      "Epoch 6, Batch 669, LR 1.510091 Loss 8.954487, Accuracy 70.253%\n",
      "Epoch 6, Batch 670, LR 1.510384 Loss 8.953443, Accuracy 70.260%\n",
      "Epoch 6, Batch 671, LR 1.510676 Loss 8.952100, Accuracy 70.272%\n",
      "Epoch 6, Batch 672, LR 1.510969 Loss 8.951164, Accuracy 70.275%\n",
      "Epoch 6, Batch 673, LR 1.511262 Loss 8.951652, Accuracy 70.267%\n",
      "Epoch 6, Batch 674, LR 1.511555 Loss 8.950485, Accuracy 70.279%\n",
      "Epoch 6, Batch 675, LR 1.511848 Loss 8.950608, Accuracy 70.274%\n",
      "Epoch 6, Batch 676, LR 1.512140 Loss 8.951132, Accuracy 70.270%\n",
      "Epoch 6, Batch 677, LR 1.512433 Loss 8.951367, Accuracy 70.266%\n",
      "Epoch 6, Batch 678, LR 1.512726 Loss 8.950628, Accuracy 70.271%\n",
      "Epoch 6, Batch 679, LR 1.513018 Loss 8.951332, Accuracy 70.265%\n",
      "Epoch 6, Batch 680, LR 1.513311 Loss 8.950484, Accuracy 70.269%\n",
      "Epoch 6, Batch 681, LR 1.513604 Loss 8.949508, Accuracy 70.277%\n",
      "Epoch 6, Batch 682, LR 1.513896 Loss 8.949274, Accuracy 70.280%\n",
      "Epoch 6, Batch 683, LR 1.514189 Loss 8.949799, Accuracy 70.274%\n",
      "Epoch 6, Batch 684, LR 1.514482 Loss 8.949369, Accuracy 70.270%\n",
      "Epoch 6, Batch 685, LR 1.514774 Loss 8.949765, Accuracy 70.267%\n",
      "Epoch 6, Batch 686, LR 1.515067 Loss 8.949722, Accuracy 70.270%\n",
      "Epoch 6, Batch 687, LR 1.515360 Loss 8.949557, Accuracy 70.277%\n",
      "Epoch 6, Batch 688, LR 1.515652 Loss 8.948957, Accuracy 70.281%\n",
      "Epoch 6, Batch 689, LR 1.515945 Loss 8.948208, Accuracy 70.290%\n",
      "Epoch 6, Batch 690, LR 1.516237 Loss 8.948667, Accuracy 70.291%\n",
      "Epoch 6, Batch 691, LR 1.516530 Loss 8.948415, Accuracy 70.291%\n",
      "Epoch 6, Batch 692, LR 1.516822 Loss 8.948257, Accuracy 70.292%\n",
      "Epoch 6, Batch 693, LR 1.517115 Loss 8.948545, Accuracy 70.298%\n",
      "Epoch 6, Batch 694, LR 1.517407 Loss 8.948765, Accuracy 70.289%\n",
      "Epoch 6, Batch 695, LR 1.517700 Loss 8.948724, Accuracy 70.290%\n",
      "Epoch 6, Batch 696, LR 1.517992 Loss 8.948509, Accuracy 70.289%\n",
      "Epoch 6, Batch 697, LR 1.518285 Loss 8.948629, Accuracy 70.287%\n",
      "Epoch 6, Batch 698, LR 1.518577 Loss 8.948350, Accuracy 70.291%\n",
      "Epoch 6, Batch 699, LR 1.518870 Loss 8.948068, Accuracy 70.296%\n",
      "Epoch 6, Batch 700, LR 1.519162 Loss 8.948580, Accuracy 70.289%\n",
      "Epoch 6, Batch 701, LR 1.519455 Loss 8.949163, Accuracy 70.281%\n",
      "Epoch 6, Batch 702, LR 1.519747 Loss 8.948829, Accuracy 70.280%\n",
      "Epoch 6, Batch 703, LR 1.520039 Loss 8.949297, Accuracy 70.280%\n",
      "Epoch 6, Batch 704, LR 1.520332 Loss 8.950398, Accuracy 70.278%\n",
      "Epoch 6, Batch 705, LR 1.520624 Loss 8.950049, Accuracy 70.286%\n",
      "Epoch 6, Batch 706, LR 1.520916 Loss 8.950596, Accuracy 70.282%\n",
      "Epoch 6, Batch 707, LR 1.521209 Loss 8.950044, Accuracy 70.284%\n",
      "Epoch 6, Batch 708, LR 1.521501 Loss 8.949551, Accuracy 70.286%\n",
      "Epoch 6, Batch 709, LR 1.521793 Loss 8.949117, Accuracy 70.288%\n",
      "Epoch 6, Batch 710, LR 1.522086 Loss 8.949631, Accuracy 70.279%\n",
      "Epoch 6, Batch 711, LR 1.522378 Loss 8.949180, Accuracy 70.286%\n",
      "Epoch 6, Batch 712, LR 1.522670 Loss 8.949895, Accuracy 70.280%\n",
      "Epoch 6, Batch 713, LR 1.522963 Loss 8.950148, Accuracy 70.275%\n",
      "Epoch 6, Batch 714, LR 1.523255 Loss 8.949018, Accuracy 70.282%\n",
      "Epoch 6, Batch 715, LR 1.523547 Loss 8.949103, Accuracy 70.275%\n",
      "Epoch 6, Batch 716, LR 1.523839 Loss 8.947753, Accuracy 70.283%\n",
      "Epoch 6, Batch 717, LR 1.524131 Loss 8.947163, Accuracy 70.287%\n",
      "Epoch 6, Batch 718, LR 1.524424 Loss 8.946570, Accuracy 70.296%\n",
      "Epoch 6, Batch 719, LR 1.524716 Loss 8.946326, Accuracy 70.302%\n",
      "Epoch 6, Batch 720, LR 1.525008 Loss 8.945982, Accuracy 70.303%\n",
      "Epoch 6, Batch 721, LR 1.525300 Loss 8.946272, Accuracy 70.301%\n",
      "Epoch 6, Batch 722, LR 1.525592 Loss 8.946843, Accuracy 70.300%\n",
      "Epoch 6, Batch 723, LR 1.525884 Loss 8.946752, Accuracy 70.303%\n",
      "Epoch 6, Batch 724, LR 1.526176 Loss 8.946994, Accuracy 70.302%\n",
      "Epoch 6, Batch 725, LR 1.526469 Loss 8.945910, Accuracy 70.310%\n",
      "Epoch 6, Batch 726, LR 1.526761 Loss 8.947065, Accuracy 70.307%\n",
      "Epoch 6, Batch 727, LR 1.527053 Loss 8.947287, Accuracy 70.304%\n",
      "Epoch 6, Batch 728, LR 1.527345 Loss 8.946052, Accuracy 70.306%\n",
      "Epoch 6, Batch 729, LR 1.527637 Loss 8.946710, Accuracy 70.308%\n",
      "Epoch 6, Batch 730, LR 1.527929 Loss 8.946221, Accuracy 70.305%\n",
      "Epoch 6, Batch 731, LR 1.528221 Loss 8.945180, Accuracy 70.310%\n",
      "Epoch 6, Batch 732, LR 1.528513 Loss 8.944944, Accuracy 70.307%\n",
      "Epoch 6, Batch 733, LR 1.528805 Loss 8.943973, Accuracy 70.317%\n",
      "Epoch 6, Batch 734, LR 1.529097 Loss 8.944380, Accuracy 70.314%\n",
      "Epoch 6, Batch 735, LR 1.529389 Loss 8.944616, Accuracy 70.305%\n",
      "Epoch 6, Batch 736, LR 1.529681 Loss 8.944120, Accuracy 70.311%\n",
      "Epoch 6, Batch 737, LR 1.529973 Loss 8.944915, Accuracy 70.302%\n",
      "Epoch 6, Batch 738, LR 1.530265 Loss 8.944058, Accuracy 70.306%\n",
      "Epoch 6, Batch 739, LR 1.530557 Loss 8.944674, Accuracy 70.304%\n",
      "Epoch 6, Batch 740, LR 1.530848 Loss 8.944345, Accuracy 70.310%\n",
      "Epoch 6, Batch 741, LR 1.531140 Loss 8.943099, Accuracy 70.314%\n",
      "Epoch 6, Batch 742, LR 1.531432 Loss 8.943919, Accuracy 70.307%\n",
      "Epoch 6, Batch 743, LR 1.531724 Loss 8.944095, Accuracy 70.309%\n",
      "Epoch 6, Batch 744, LR 1.532016 Loss 8.944021, Accuracy 70.307%\n",
      "Epoch 6, Batch 745, LR 1.532308 Loss 8.944302, Accuracy 70.302%\n",
      "Epoch 6, Batch 746, LR 1.532600 Loss 8.943721, Accuracy 70.304%\n",
      "Epoch 6, Batch 747, LR 1.532891 Loss 8.943387, Accuracy 70.306%\n",
      "Epoch 6, Batch 748, LR 1.533183 Loss 8.942878, Accuracy 70.316%\n",
      "Epoch 6, Batch 749, LR 1.533475 Loss 8.942235, Accuracy 70.318%\n",
      "Epoch 6, Batch 750, LR 1.533767 Loss 8.941539, Accuracy 70.328%\n",
      "Epoch 6, Batch 751, LR 1.534058 Loss 8.941959, Accuracy 70.323%\n",
      "Epoch 6, Batch 752, LR 1.534350 Loss 8.941891, Accuracy 70.326%\n",
      "Epoch 6, Batch 753, LR 1.534642 Loss 8.941820, Accuracy 70.330%\n",
      "Epoch 6, Batch 754, LR 1.534934 Loss 8.941394, Accuracy 70.334%\n",
      "Epoch 6, Batch 755, LR 1.535225 Loss 8.940338, Accuracy 70.338%\n",
      "Epoch 6, Batch 756, LR 1.535517 Loss 8.940848, Accuracy 70.333%\n",
      "Epoch 6, Batch 757, LR 1.535809 Loss 8.940929, Accuracy 70.333%\n",
      "Epoch 6, Batch 758, LR 1.536100 Loss 8.940959, Accuracy 70.339%\n",
      "Epoch 6, Batch 759, LR 1.536392 Loss 8.940615, Accuracy 70.336%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 760, LR 1.536684 Loss 8.940480, Accuracy 70.339%\n",
      "Epoch 6, Batch 761, LR 1.536975 Loss 8.940635, Accuracy 70.342%\n",
      "Epoch 6, Batch 762, LR 1.537267 Loss 8.941004, Accuracy 70.336%\n",
      "Epoch 6, Batch 763, LR 1.537558 Loss 8.940554, Accuracy 70.341%\n",
      "Epoch 6, Batch 764, LR 1.537850 Loss 8.941269, Accuracy 70.332%\n",
      "Epoch 6, Batch 765, LR 1.538141 Loss 8.941402, Accuracy 70.332%\n",
      "Epoch 6, Batch 766, LR 1.538433 Loss 8.941380, Accuracy 70.327%\n",
      "Epoch 6, Batch 767, LR 1.538725 Loss 8.941702, Accuracy 70.321%\n",
      "Epoch 6, Batch 768, LR 1.539016 Loss 8.941885, Accuracy 70.318%\n",
      "Epoch 6, Batch 769, LR 1.539308 Loss 8.942216, Accuracy 70.308%\n",
      "Epoch 6, Batch 770, LR 1.539599 Loss 8.942017, Accuracy 70.308%\n",
      "Epoch 6, Batch 771, LR 1.539890 Loss 8.942751, Accuracy 70.305%\n",
      "Epoch 6, Batch 772, LR 1.540182 Loss 8.942699, Accuracy 70.311%\n",
      "Epoch 6, Batch 773, LR 1.540473 Loss 8.942897, Accuracy 70.305%\n",
      "Epoch 6, Batch 774, LR 1.540765 Loss 8.943633, Accuracy 70.308%\n",
      "Epoch 6, Batch 775, LR 1.541056 Loss 8.943471, Accuracy 70.314%\n",
      "Epoch 6, Batch 776, LR 1.541348 Loss 8.943953, Accuracy 70.316%\n",
      "Epoch 6, Batch 777, LR 1.541639 Loss 8.944592, Accuracy 70.307%\n",
      "Epoch 6, Batch 778, LR 1.541930 Loss 8.944700, Accuracy 70.307%\n",
      "Epoch 6, Batch 779, LR 1.542222 Loss 8.944774, Accuracy 70.306%\n",
      "Epoch 6, Batch 780, LR 1.542513 Loss 8.944474, Accuracy 70.306%\n",
      "Epoch 6, Batch 781, LR 1.542804 Loss 8.944141, Accuracy 70.311%\n",
      "Epoch 6, Batch 782, LR 1.543096 Loss 8.944637, Accuracy 70.312%\n",
      "Epoch 6, Batch 783, LR 1.543387 Loss 8.945131, Accuracy 70.308%\n",
      "Epoch 6, Batch 784, LR 1.543678 Loss 8.945589, Accuracy 70.306%\n",
      "Epoch 6, Batch 785, LR 1.543970 Loss 8.945786, Accuracy 70.302%\n",
      "Epoch 6, Batch 786, LR 1.544261 Loss 8.945371, Accuracy 70.307%\n",
      "Epoch 6, Batch 787, LR 1.544552 Loss 8.944269, Accuracy 70.317%\n",
      "Epoch 6, Batch 788, LR 1.544843 Loss 8.943736, Accuracy 70.323%\n",
      "Epoch 6, Batch 789, LR 1.545134 Loss 8.944273, Accuracy 70.319%\n",
      "Epoch 6, Batch 790, LR 1.545426 Loss 8.945309, Accuracy 70.311%\n",
      "Epoch 6, Batch 791, LR 1.545717 Loss 8.945586, Accuracy 70.308%\n",
      "Epoch 6, Batch 792, LR 1.546008 Loss 8.945517, Accuracy 70.306%\n",
      "Epoch 6, Batch 793, LR 1.546299 Loss 8.945226, Accuracy 70.309%\n",
      "Epoch 6, Batch 794, LR 1.546590 Loss 8.945150, Accuracy 70.312%\n",
      "Epoch 6, Batch 795, LR 1.546881 Loss 8.944246, Accuracy 70.314%\n",
      "Epoch 6, Batch 796, LR 1.547172 Loss 8.944515, Accuracy 70.306%\n",
      "Epoch 6, Batch 797, LR 1.547464 Loss 8.944544, Accuracy 70.303%\n",
      "Epoch 6, Batch 798, LR 1.547755 Loss 8.944431, Accuracy 70.303%\n",
      "Epoch 6, Batch 799, LR 1.548046 Loss 8.944162, Accuracy 70.312%\n",
      "Epoch 6, Batch 800, LR 1.548337 Loss 8.944130, Accuracy 70.309%\n",
      "Epoch 6, Batch 801, LR 1.548628 Loss 8.943684, Accuracy 70.312%\n",
      "Epoch 6, Batch 802, LR 1.548919 Loss 8.943262, Accuracy 70.316%\n",
      "Epoch 6, Batch 803, LR 1.549210 Loss 8.942358, Accuracy 70.320%\n",
      "Epoch 6, Batch 804, LR 1.549501 Loss 8.942022, Accuracy 70.323%\n",
      "Epoch 6, Batch 805, LR 1.549792 Loss 8.940364, Accuracy 70.337%\n",
      "Epoch 6, Batch 806, LR 1.550083 Loss 8.941210, Accuracy 70.328%\n",
      "Epoch 6, Batch 807, LR 1.550374 Loss 8.940205, Accuracy 70.335%\n",
      "Epoch 6, Batch 808, LR 1.550665 Loss 8.939623, Accuracy 70.339%\n",
      "Epoch 6, Batch 809, LR 1.550956 Loss 8.939757, Accuracy 70.330%\n",
      "Epoch 6, Batch 810, LR 1.551246 Loss 8.939115, Accuracy 70.333%\n",
      "Epoch 6, Batch 811, LR 1.551537 Loss 8.938554, Accuracy 70.346%\n",
      "Epoch 6, Batch 812, LR 1.551828 Loss 8.938655, Accuracy 70.345%\n",
      "Epoch 6, Batch 813, LR 1.552119 Loss 8.938891, Accuracy 70.339%\n",
      "Epoch 6, Batch 814, LR 1.552410 Loss 8.939250, Accuracy 70.334%\n",
      "Epoch 6, Batch 815, LR 1.552701 Loss 8.938250, Accuracy 70.337%\n",
      "Epoch 6, Batch 816, LR 1.552991 Loss 8.938333, Accuracy 70.342%\n",
      "Epoch 6, Batch 817, LR 1.553282 Loss 8.937204, Accuracy 70.349%\n",
      "Epoch 6, Batch 818, LR 1.553573 Loss 8.937187, Accuracy 70.351%\n",
      "Epoch 6, Batch 819, LR 1.553864 Loss 8.937448, Accuracy 70.349%\n",
      "Epoch 6, Batch 820, LR 1.554155 Loss 8.936837, Accuracy 70.360%\n",
      "Epoch 6, Batch 821, LR 1.554445 Loss 8.936467, Accuracy 70.364%\n",
      "Epoch 6, Batch 822, LR 1.554736 Loss 8.936668, Accuracy 70.360%\n",
      "Epoch 6, Batch 823, LR 1.555027 Loss 8.936405, Accuracy 70.361%\n",
      "Epoch 6, Batch 824, LR 1.555317 Loss 8.936243, Accuracy 70.365%\n",
      "Epoch 6, Batch 825, LR 1.555608 Loss 8.935159, Accuracy 70.372%\n",
      "Epoch 6, Batch 826, LR 1.555899 Loss 8.934280, Accuracy 70.375%\n",
      "Epoch 6, Batch 827, LR 1.556189 Loss 8.933412, Accuracy 70.387%\n",
      "Epoch 6, Batch 828, LR 1.556480 Loss 8.933426, Accuracy 70.391%\n",
      "Epoch 6, Batch 829, LR 1.556771 Loss 8.933220, Accuracy 70.391%\n",
      "Epoch 6, Batch 830, LR 1.557061 Loss 8.931958, Accuracy 70.395%\n",
      "Epoch 6, Batch 831, LR 1.557352 Loss 8.931861, Accuracy 70.399%\n",
      "Epoch 6, Batch 832, LR 1.557642 Loss 8.933145, Accuracy 70.389%\n",
      "Epoch 6, Batch 833, LR 1.557933 Loss 8.932583, Accuracy 70.395%\n",
      "Epoch 6, Batch 834, LR 1.558223 Loss 8.932496, Accuracy 70.390%\n",
      "Epoch 6, Batch 835, LR 1.558514 Loss 8.932231, Accuracy 70.391%\n",
      "Epoch 6, Batch 836, LR 1.558804 Loss 8.932631, Accuracy 70.387%\n",
      "Epoch 6, Batch 837, LR 1.559095 Loss 8.933221, Accuracy 70.385%\n",
      "Epoch 6, Batch 838, LR 1.559385 Loss 8.932643, Accuracy 70.393%\n",
      "Epoch 6, Batch 839, LR 1.559676 Loss 8.932883, Accuracy 70.388%\n",
      "Epoch 6, Batch 840, LR 1.559966 Loss 8.932320, Accuracy 70.391%\n",
      "Epoch 6, Batch 841, LR 1.560257 Loss 8.932100, Accuracy 70.391%\n",
      "Epoch 6, Batch 842, LR 1.560547 Loss 8.931797, Accuracy 70.398%\n",
      "Epoch 6, Batch 843, LR 1.560838 Loss 8.931325, Accuracy 70.399%\n",
      "Epoch 6, Batch 844, LR 1.561128 Loss 8.931327, Accuracy 70.396%\n",
      "Epoch 6, Batch 845, LR 1.561418 Loss 8.931501, Accuracy 70.398%\n",
      "Epoch 6, Batch 846, LR 1.561709 Loss 8.931671, Accuracy 70.399%\n",
      "Epoch 6, Batch 847, LR 1.561999 Loss 8.931856, Accuracy 70.397%\n",
      "Epoch 6, Batch 848, LR 1.562289 Loss 8.931282, Accuracy 70.403%\n",
      "Epoch 6, Batch 849, LR 1.562580 Loss 8.931373, Accuracy 70.400%\n",
      "Epoch 6, Batch 850, LR 1.562870 Loss 8.931647, Accuracy 70.392%\n",
      "Epoch 6, Batch 851, LR 1.563160 Loss 8.932119, Accuracy 70.396%\n",
      "Epoch 6, Batch 852, LR 1.563450 Loss 8.931535, Accuracy 70.399%\n",
      "Epoch 6, Batch 853, LR 1.563741 Loss 8.931114, Accuracy 70.403%\n",
      "Epoch 6, Batch 854, LR 1.564031 Loss 8.930610, Accuracy 70.411%\n",
      "Epoch 6, Batch 855, LR 1.564321 Loss 8.930469, Accuracy 70.413%\n",
      "Epoch 6, Batch 856, LR 1.564611 Loss 8.930006, Accuracy 70.417%\n",
      "Epoch 6, Batch 857, LR 1.564901 Loss 8.929692, Accuracy 70.416%\n",
      "Epoch 6, Batch 858, LR 1.565192 Loss 8.929281, Accuracy 70.417%\n",
      "Epoch 6, Batch 859, LR 1.565482 Loss 8.928705, Accuracy 70.426%\n",
      "Epoch 6, Batch 860, LR 1.565772 Loss 8.928192, Accuracy 70.432%\n",
      "Epoch 6, Batch 861, LR 1.566062 Loss 8.928263, Accuracy 70.430%\n",
      "Epoch 6, Batch 862, LR 1.566352 Loss 8.928154, Accuracy 70.427%\n",
      "Epoch 6, Batch 863, LR 1.566642 Loss 8.927999, Accuracy 70.424%\n",
      "Epoch 6, Batch 864, LR 1.566932 Loss 8.928357, Accuracy 70.420%\n",
      "Epoch 6, Batch 865, LR 1.567222 Loss 8.928138, Accuracy 70.421%\n",
      "Epoch 6, Batch 866, LR 1.567512 Loss 8.928925, Accuracy 70.416%\n",
      "Epoch 6, Batch 867, LR 1.567802 Loss 8.928785, Accuracy 70.419%\n",
      "Epoch 6, Batch 868, LR 1.568092 Loss 8.928948, Accuracy 70.420%\n",
      "Epoch 6, Batch 869, LR 1.568382 Loss 8.928341, Accuracy 70.426%\n",
      "Epoch 6, Batch 870, LR 1.568672 Loss 8.927442, Accuracy 70.431%\n",
      "Epoch 6, Batch 871, LR 1.568962 Loss 8.926651, Accuracy 70.439%\n",
      "Epoch 6, Batch 872, LR 1.569252 Loss 8.925823, Accuracy 70.445%\n",
      "Epoch 6, Batch 873, LR 1.569542 Loss 8.925640, Accuracy 70.445%\n",
      "Epoch 6, Batch 874, LR 1.569832 Loss 8.925472, Accuracy 70.443%\n",
      "Epoch 6, Batch 875, LR 1.570122 Loss 8.925234, Accuracy 70.444%\n",
      "Epoch 6, Batch 876, LR 1.570412 Loss 8.925123, Accuracy 70.446%\n",
      "Epoch 6, Batch 877, LR 1.570702 Loss 8.925231, Accuracy 70.445%\n",
      "Epoch 6, Batch 878, LR 1.570992 Loss 8.925021, Accuracy 70.440%\n",
      "Epoch 6, Batch 879, LR 1.571281 Loss 8.924154, Accuracy 70.448%\n",
      "Epoch 6, Batch 880, LR 1.571571 Loss 8.923780, Accuracy 70.449%\n",
      "Epoch 6, Batch 881, LR 1.571861 Loss 8.923595, Accuracy 70.452%\n",
      "Epoch 6, Batch 882, LR 1.572151 Loss 8.923900, Accuracy 70.451%\n",
      "Epoch 6, Batch 883, LR 1.572441 Loss 8.923940, Accuracy 70.447%\n",
      "Epoch 6, Batch 884, LR 1.572730 Loss 8.923276, Accuracy 70.446%\n",
      "Epoch 6, Batch 885, LR 1.573020 Loss 8.922560, Accuracy 70.448%\n",
      "Epoch 6, Batch 886, LR 1.573310 Loss 8.921836, Accuracy 70.455%\n",
      "Epoch 6, Batch 887, LR 1.573599 Loss 8.922551, Accuracy 70.453%\n",
      "Epoch 6, Batch 888, LR 1.573889 Loss 8.921853, Accuracy 70.462%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 889, LR 1.574179 Loss 8.921540, Accuracy 70.466%\n",
      "Epoch 6, Batch 890, LR 1.574468 Loss 8.921107, Accuracy 70.470%\n",
      "Epoch 6, Batch 891, LR 1.574758 Loss 8.920846, Accuracy 70.467%\n",
      "Epoch 6, Batch 892, LR 1.575048 Loss 8.919880, Accuracy 70.477%\n",
      "Epoch 6, Batch 893, LR 1.575337 Loss 8.920154, Accuracy 70.473%\n",
      "Epoch 6, Batch 894, LR 1.575627 Loss 8.920395, Accuracy 70.472%\n",
      "Epoch 6, Batch 895, LR 1.575916 Loss 8.920524, Accuracy 70.470%\n",
      "Epoch 6, Batch 896, LR 1.576206 Loss 8.919535, Accuracy 70.472%\n",
      "Epoch 6, Batch 897, LR 1.576495 Loss 8.919292, Accuracy 70.474%\n",
      "Epoch 6, Batch 898, LR 1.576785 Loss 8.918185, Accuracy 70.479%\n",
      "Epoch 6, Batch 899, LR 1.577074 Loss 8.918889, Accuracy 70.474%\n",
      "Epoch 6, Batch 900, LR 1.577364 Loss 8.918488, Accuracy 70.478%\n",
      "Epoch 6, Batch 901, LR 1.577653 Loss 8.917786, Accuracy 70.482%\n",
      "Epoch 6, Batch 902, LR 1.577943 Loss 8.918016, Accuracy 70.480%\n",
      "Epoch 6, Batch 903, LR 1.578232 Loss 8.918286, Accuracy 70.480%\n",
      "Epoch 6, Batch 904, LR 1.578522 Loss 8.919117, Accuracy 70.472%\n",
      "Epoch 6, Batch 905, LR 1.578811 Loss 8.918956, Accuracy 70.473%\n",
      "Epoch 6, Batch 906, LR 1.579100 Loss 8.918640, Accuracy 70.476%\n",
      "Epoch 6, Batch 907, LR 1.579390 Loss 8.918922, Accuracy 70.474%\n",
      "Epoch 6, Batch 908, LR 1.579679 Loss 8.918432, Accuracy 70.476%\n",
      "Epoch 6, Batch 909, LR 1.579969 Loss 8.918851, Accuracy 70.472%\n",
      "Epoch 6, Batch 910, LR 1.580258 Loss 8.918226, Accuracy 70.476%\n",
      "Epoch 6, Batch 911, LR 1.580547 Loss 8.918799, Accuracy 70.473%\n",
      "Epoch 6, Batch 912, LR 1.580836 Loss 8.918679, Accuracy 70.472%\n",
      "Epoch 6, Batch 913, LR 1.581126 Loss 8.919244, Accuracy 70.467%\n",
      "Epoch 6, Batch 914, LR 1.581415 Loss 8.919601, Accuracy 70.471%\n",
      "Epoch 6, Batch 915, LR 1.581704 Loss 8.919572, Accuracy 70.476%\n",
      "Epoch 6, Batch 916, LR 1.581993 Loss 8.918980, Accuracy 70.485%\n",
      "Epoch 6, Batch 917, LR 1.582283 Loss 8.919632, Accuracy 70.479%\n",
      "Epoch 6, Batch 918, LR 1.582572 Loss 8.919794, Accuracy 70.473%\n",
      "Epoch 6, Batch 919, LR 1.582861 Loss 8.919262, Accuracy 70.477%\n",
      "Epoch 6, Batch 920, LR 1.583150 Loss 8.919147, Accuracy 70.475%\n",
      "Epoch 6, Batch 921, LR 1.583439 Loss 8.918603, Accuracy 70.479%\n",
      "Epoch 6, Batch 922, LR 1.583728 Loss 8.918679, Accuracy 70.485%\n",
      "Epoch 6, Batch 923, LR 1.584017 Loss 8.918169, Accuracy 70.489%\n",
      "Epoch 6, Batch 924, LR 1.584307 Loss 8.918182, Accuracy 70.490%\n",
      "Epoch 6, Batch 925, LR 1.584596 Loss 8.916964, Accuracy 70.496%\n",
      "Epoch 6, Batch 926, LR 1.584885 Loss 8.916910, Accuracy 70.493%\n",
      "Epoch 6, Batch 927, LR 1.585174 Loss 8.916695, Accuracy 70.490%\n",
      "Epoch 6, Batch 928, LR 1.585463 Loss 8.916855, Accuracy 70.486%\n",
      "Epoch 6, Batch 929, LR 1.585752 Loss 8.916099, Accuracy 70.486%\n",
      "Epoch 6, Batch 930, LR 1.586041 Loss 8.915872, Accuracy 70.489%\n",
      "Epoch 6, Batch 931, LR 1.586330 Loss 8.915560, Accuracy 70.487%\n",
      "Epoch 6, Batch 932, LR 1.586619 Loss 8.915134, Accuracy 70.484%\n",
      "Epoch 6, Batch 933, LR 1.586907 Loss 8.914798, Accuracy 70.487%\n",
      "Epoch 6, Batch 934, LR 1.587196 Loss 8.914427, Accuracy 70.489%\n",
      "Epoch 6, Batch 935, LR 1.587485 Loss 8.914632, Accuracy 70.485%\n",
      "Epoch 6, Batch 936, LR 1.587774 Loss 8.913890, Accuracy 70.488%\n",
      "Epoch 6, Batch 937, LR 1.588063 Loss 8.913604, Accuracy 70.487%\n",
      "Epoch 6, Batch 938, LR 1.588352 Loss 8.913535, Accuracy 70.494%\n",
      "Epoch 6, Batch 939, LR 1.588641 Loss 8.913196, Accuracy 70.495%\n",
      "Epoch 6, Batch 940, LR 1.588929 Loss 8.913199, Accuracy 70.503%\n",
      "Epoch 6, Batch 941, LR 1.589218 Loss 8.913128, Accuracy 70.506%\n",
      "Epoch 6, Batch 942, LR 1.589507 Loss 8.911432, Accuracy 70.517%\n",
      "Epoch 6, Batch 943, LR 1.589796 Loss 8.910673, Accuracy 70.522%\n",
      "Epoch 6, Batch 944, LR 1.590085 Loss 8.911236, Accuracy 70.519%\n",
      "Epoch 6, Batch 945, LR 1.590373 Loss 8.910929, Accuracy 70.523%\n",
      "Epoch 6, Batch 946, LR 1.590662 Loss 8.910398, Accuracy 70.527%\n",
      "Epoch 6, Batch 947, LR 1.590951 Loss 8.909662, Accuracy 70.529%\n",
      "Epoch 6, Batch 948, LR 1.591239 Loss 8.909093, Accuracy 70.537%\n",
      "Epoch 6, Batch 949, LR 1.591528 Loss 8.908774, Accuracy 70.538%\n",
      "Epoch 6, Batch 950, LR 1.591817 Loss 8.908085, Accuracy 70.547%\n",
      "Epoch 6, Batch 951, LR 1.592105 Loss 8.907196, Accuracy 70.552%\n",
      "Epoch 6, Batch 952, LR 1.592394 Loss 8.906538, Accuracy 70.555%\n",
      "Epoch 6, Batch 953, LR 1.592682 Loss 8.906336, Accuracy 70.559%\n",
      "Epoch 6, Batch 954, LR 1.592971 Loss 8.906679, Accuracy 70.560%\n",
      "Epoch 6, Batch 955, LR 1.593259 Loss 8.906651, Accuracy 70.560%\n",
      "Epoch 6, Batch 956, LR 1.593548 Loss 8.906383, Accuracy 70.560%\n",
      "Epoch 6, Batch 957, LR 1.593836 Loss 8.906030, Accuracy 70.568%\n",
      "Epoch 6, Batch 958, LR 1.594125 Loss 8.905923, Accuracy 70.567%\n",
      "Epoch 6, Batch 959, LR 1.594413 Loss 8.906400, Accuracy 70.564%\n",
      "Epoch 6, Batch 960, LR 1.594702 Loss 8.905935, Accuracy 70.565%\n",
      "Epoch 6, Batch 961, LR 1.594990 Loss 8.905683, Accuracy 70.572%\n",
      "Epoch 6, Batch 962, LR 1.595279 Loss 8.905675, Accuracy 70.570%\n",
      "Epoch 6, Batch 963, LR 1.595567 Loss 8.905770, Accuracy 70.564%\n",
      "Epoch 6, Batch 964, LR 1.595856 Loss 8.905849, Accuracy 70.569%\n",
      "Epoch 6, Batch 965, LR 1.596144 Loss 8.905482, Accuracy 70.574%\n",
      "Epoch 6, Batch 966, LR 1.596432 Loss 8.905733, Accuracy 70.572%\n",
      "Epoch 6, Batch 967, LR 1.596721 Loss 8.905207, Accuracy 70.573%\n",
      "Epoch 6, Batch 968, LR 1.597009 Loss 8.904725, Accuracy 70.575%\n",
      "Epoch 6, Batch 969, LR 1.597297 Loss 8.904505, Accuracy 70.576%\n",
      "Epoch 6, Batch 970, LR 1.597586 Loss 8.904986, Accuracy 70.572%\n",
      "Epoch 6, Batch 971, LR 1.597874 Loss 8.904352, Accuracy 70.575%\n",
      "Epoch 6, Batch 972, LR 1.598162 Loss 8.904448, Accuracy 70.579%\n",
      "Epoch 6, Batch 973, LR 1.598450 Loss 8.904283, Accuracy 70.581%\n",
      "Epoch 6, Batch 974, LR 1.598738 Loss 8.903774, Accuracy 70.579%\n",
      "Epoch 6, Batch 975, LR 1.599027 Loss 8.903102, Accuracy 70.583%\n",
      "Epoch 6, Batch 976, LR 1.599315 Loss 8.902832, Accuracy 70.586%\n",
      "Epoch 6, Batch 977, LR 1.599603 Loss 8.902853, Accuracy 70.585%\n",
      "Epoch 6, Batch 978, LR 1.599891 Loss 8.901412, Accuracy 70.595%\n",
      "Epoch 6, Batch 979, LR 1.600179 Loss 8.901069, Accuracy 70.597%\n",
      "Epoch 6, Batch 980, LR 1.600467 Loss 8.900786, Accuracy 70.596%\n",
      "Epoch 6, Batch 981, LR 1.600755 Loss 8.900719, Accuracy 70.598%\n",
      "Epoch 6, Batch 982, LR 1.601043 Loss 8.900764, Accuracy 70.593%\n",
      "Epoch 6, Batch 983, LR 1.601332 Loss 8.900190, Accuracy 70.595%\n",
      "Epoch 6, Batch 984, LR 1.601620 Loss 8.899884, Accuracy 70.598%\n",
      "Epoch 6, Batch 985, LR 1.601908 Loss 8.899196, Accuracy 70.604%\n",
      "Epoch 6, Batch 986, LR 1.602196 Loss 8.898902, Accuracy 70.610%\n",
      "Epoch 6, Batch 987, LR 1.602484 Loss 8.898249, Accuracy 70.610%\n",
      "Epoch 6, Batch 988, LR 1.602771 Loss 8.898659, Accuracy 70.603%\n",
      "Epoch 6, Batch 989, LR 1.603059 Loss 8.898966, Accuracy 70.601%\n",
      "Epoch 6, Batch 990, LR 1.603347 Loss 8.898461, Accuracy 70.599%\n",
      "Epoch 6, Batch 991, LR 1.603635 Loss 8.898054, Accuracy 70.601%\n",
      "Epoch 6, Batch 992, LR 1.603923 Loss 8.897197, Accuracy 70.608%\n",
      "Epoch 6, Batch 993, LR 1.604211 Loss 8.896801, Accuracy 70.611%\n",
      "Epoch 6, Batch 994, LR 1.604499 Loss 8.897469, Accuracy 70.603%\n",
      "Epoch 6, Batch 995, LR 1.604787 Loss 8.896824, Accuracy 70.606%\n",
      "Epoch 6, Batch 996, LR 1.605074 Loss 8.896535, Accuracy 70.604%\n",
      "Epoch 6, Batch 997, LR 1.605362 Loss 8.896535, Accuracy 70.607%\n",
      "Epoch 6, Batch 998, LR 1.605650 Loss 8.896489, Accuracy 70.606%\n",
      "Epoch 6, Batch 999, LR 1.605938 Loss 8.896135, Accuracy 70.608%\n",
      "Epoch 6, Batch 1000, LR 1.606226 Loss 8.895878, Accuracy 70.609%\n",
      "Epoch 6, Batch 1001, LR 1.606513 Loss 8.895891, Accuracy 70.608%\n",
      "Epoch 6, Batch 1002, LR 1.606801 Loss 8.895535, Accuracy 70.611%\n",
      "Epoch 6, Batch 1003, LR 1.607089 Loss 8.895598, Accuracy 70.615%\n",
      "Epoch 6, Batch 1004, LR 1.607376 Loss 8.895374, Accuracy 70.621%\n",
      "Epoch 6, Batch 1005, LR 1.607664 Loss 8.895661, Accuracy 70.623%\n",
      "Epoch 6, Batch 1006, LR 1.607952 Loss 8.895443, Accuracy 70.625%\n",
      "Epoch 6, Batch 1007, LR 1.608239 Loss 8.895145, Accuracy 70.628%\n",
      "Epoch 6, Batch 1008, LR 1.608527 Loss 8.894486, Accuracy 70.630%\n",
      "Epoch 6, Batch 1009, LR 1.608814 Loss 8.895008, Accuracy 70.625%\n",
      "Epoch 6, Batch 1010, LR 1.609102 Loss 8.894824, Accuracy 70.627%\n",
      "Epoch 6, Batch 1011, LR 1.609389 Loss 8.894695, Accuracy 70.627%\n",
      "Epoch 6, Batch 1012, LR 1.609677 Loss 8.895090, Accuracy 70.626%\n",
      "Epoch 6, Batch 1013, LR 1.609964 Loss 8.894920, Accuracy 70.626%\n",
      "Epoch 6, Batch 1014, LR 1.610252 Loss 8.894516, Accuracy 70.625%\n",
      "Epoch 6, Batch 1015, LR 1.610539 Loss 8.894305, Accuracy 70.630%\n",
      "Epoch 6, Batch 1016, LR 1.610827 Loss 8.895680, Accuracy 70.620%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 1017, LR 1.611114 Loss 8.896139, Accuracy 70.617%\n",
      "Epoch 6, Batch 1018, LR 1.611402 Loss 8.896015, Accuracy 70.621%\n",
      "Epoch 6, Batch 1019, LR 1.611689 Loss 8.895537, Accuracy 70.622%\n",
      "Epoch 6, Batch 1020, LR 1.611976 Loss 8.895307, Accuracy 70.622%\n",
      "Epoch 6, Batch 1021, LR 1.612264 Loss 8.895080, Accuracy 70.628%\n",
      "Epoch 6, Batch 1022, LR 1.612551 Loss 8.894987, Accuracy 70.627%\n",
      "Epoch 6, Batch 1023, LR 1.612838 Loss 8.894937, Accuracy 70.629%\n",
      "Epoch 6, Batch 1024, LR 1.613126 Loss 8.895246, Accuracy 70.625%\n",
      "Epoch 6, Batch 1025, LR 1.613413 Loss 8.895530, Accuracy 70.627%\n",
      "Epoch 6, Batch 1026, LR 1.613700 Loss 8.895358, Accuracy 70.630%\n",
      "Epoch 6, Batch 1027, LR 1.613987 Loss 8.894937, Accuracy 70.631%\n",
      "Epoch 6, Batch 1028, LR 1.614275 Loss 8.895239, Accuracy 70.629%\n",
      "Epoch 6, Batch 1029, LR 1.614562 Loss 8.895737, Accuracy 70.625%\n",
      "Epoch 6, Batch 1030, LR 1.614849 Loss 8.895205, Accuracy 70.629%\n",
      "Epoch 6, Batch 1031, LR 1.615136 Loss 8.894844, Accuracy 70.632%\n",
      "Epoch 6, Batch 1032, LR 1.615423 Loss 8.894652, Accuracy 70.639%\n",
      "Epoch 6, Batch 1033, LR 1.615710 Loss 8.894290, Accuracy 70.641%\n",
      "Epoch 6, Batch 1034, LR 1.615998 Loss 8.893781, Accuracy 70.646%\n",
      "Epoch 6, Batch 1035, LR 1.616285 Loss 8.893774, Accuracy 70.642%\n",
      "Epoch 6, Batch 1036, LR 1.616572 Loss 8.892757, Accuracy 70.650%\n",
      "Epoch 6, Batch 1037, LR 1.616859 Loss 8.892869, Accuracy 70.650%\n",
      "Epoch 6, Batch 1038, LR 1.617146 Loss 8.893645, Accuracy 70.644%\n",
      "Epoch 6, Batch 1039, LR 1.617433 Loss 8.893608, Accuracy 70.643%\n",
      "Epoch 6, Batch 1040, LR 1.617720 Loss 8.893441, Accuracy 70.645%\n",
      "Epoch 6, Batch 1041, LR 1.618007 Loss 8.893101, Accuracy 70.648%\n",
      "Epoch 6, Batch 1042, LR 1.618294 Loss 8.893191, Accuracy 70.645%\n",
      "Epoch 6, Batch 1043, LR 1.618581 Loss 8.893284, Accuracy 70.642%\n",
      "Epoch 6, Batch 1044, LR 1.618867 Loss 8.892729, Accuracy 70.643%\n",
      "Epoch 6, Batch 1045, LR 1.619154 Loss 8.892239, Accuracy 70.647%\n",
      "Epoch 6, Batch 1046, LR 1.619441 Loss 8.892611, Accuracy 70.640%\n",
      "Epoch 6, Batch 1047, LR 1.619728 Loss 8.892398, Accuracy 70.641%\n",
      "Epoch 6, Loss (train set) 8.892398, Accuracy (train set) 70.641%\n",
      "Epoch 7, Batch 1, LR 1.620015 Loss 8.693700, Accuracy 70.312%\n",
      "Epoch 7, Batch 2, LR 1.620302 Loss 8.664131, Accuracy 72.266%\n",
      "Epoch 7, Batch 3, LR 1.620589 Loss 8.698671, Accuracy 71.615%\n",
      "Epoch 7, Batch 4, LR 1.620875 Loss 8.674404, Accuracy 72.656%\n",
      "Epoch 7, Batch 5, LR 1.621162 Loss 8.600782, Accuracy 74.062%\n",
      "Epoch 7, Batch 6, LR 1.621449 Loss 8.665675, Accuracy 73.568%\n",
      "Epoch 7, Batch 7, LR 1.621736 Loss 8.710002, Accuracy 72.768%\n",
      "Epoch 7, Batch 8, LR 1.622022 Loss 8.671974, Accuracy 72.754%\n",
      "Epoch 7, Batch 9, LR 1.622309 Loss 8.594289, Accuracy 73.003%\n",
      "Epoch 7, Batch 10, LR 1.622596 Loss 8.651063, Accuracy 72.500%\n",
      "Epoch 7, Batch 11, LR 1.622882 Loss 8.653453, Accuracy 72.727%\n",
      "Epoch 7, Batch 12, LR 1.623169 Loss 8.641833, Accuracy 72.656%\n",
      "Epoch 7, Batch 13, LR 1.623455 Loss 8.590485, Accuracy 73.137%\n",
      "Epoch 7, Batch 14, LR 1.623742 Loss 8.603387, Accuracy 72.991%\n",
      "Epoch 7, Batch 15, LR 1.624029 Loss 8.577625, Accuracy 73.125%\n",
      "Epoch 7, Batch 16, LR 1.624315 Loss 8.585688, Accuracy 72.900%\n",
      "Epoch 7, Batch 17, LR 1.624602 Loss 8.616227, Accuracy 72.840%\n",
      "Epoch 7, Batch 18, LR 1.624888 Loss 8.625727, Accuracy 72.526%\n",
      "Epoch 7, Batch 19, LR 1.625175 Loss 8.589295, Accuracy 72.821%\n",
      "Epoch 7, Batch 20, LR 1.625461 Loss 8.579710, Accuracy 72.656%\n",
      "Epoch 7, Batch 21, LR 1.625748 Loss 8.576755, Accuracy 72.545%\n",
      "Epoch 7, Batch 22, LR 1.626034 Loss 8.580334, Accuracy 72.408%\n",
      "Epoch 7, Batch 23, LR 1.626320 Loss 8.543022, Accuracy 72.588%\n",
      "Epoch 7, Batch 24, LR 1.626607 Loss 8.523765, Accuracy 72.819%\n",
      "Epoch 7, Batch 25, LR 1.626893 Loss 8.512930, Accuracy 72.781%\n",
      "Epoch 7, Batch 26, LR 1.627180 Loss 8.506813, Accuracy 72.867%\n",
      "Epoch 7, Batch 27, LR 1.627466 Loss 8.541550, Accuracy 72.685%\n",
      "Epoch 7, Batch 28, LR 1.627752 Loss 8.517173, Accuracy 72.824%\n",
      "Epoch 7, Batch 29, LR 1.628038 Loss 8.499871, Accuracy 72.845%\n",
      "Epoch 7, Batch 30, LR 1.628325 Loss 8.510912, Accuracy 72.708%\n",
      "Epoch 7, Batch 31, LR 1.628611 Loss 8.538296, Accuracy 72.606%\n",
      "Epoch 7, Batch 32, LR 1.628897 Loss 8.541120, Accuracy 72.656%\n",
      "Epoch 7, Batch 33, LR 1.629183 Loss 8.528711, Accuracy 72.751%\n",
      "Epoch 7, Batch 34, LR 1.629470 Loss 8.525549, Accuracy 72.771%\n",
      "Epoch 7, Batch 35, LR 1.629756 Loss 8.532941, Accuracy 72.768%\n",
      "Epoch 7, Batch 36, LR 1.630042 Loss 8.528579, Accuracy 72.765%\n",
      "Epoch 7, Batch 37, LR 1.630328 Loss 8.523522, Accuracy 72.825%\n",
      "Epoch 7, Batch 38, LR 1.630614 Loss 8.524291, Accuracy 72.903%\n",
      "Epoch 7, Batch 39, LR 1.630900 Loss 8.525198, Accuracy 72.857%\n",
      "Epoch 7, Batch 40, LR 1.631186 Loss 8.519148, Accuracy 72.910%\n",
      "Epoch 7, Batch 41, LR 1.631472 Loss 8.510164, Accuracy 73.056%\n",
      "Epoch 7, Batch 42, LR 1.631758 Loss 8.494730, Accuracy 73.140%\n",
      "Epoch 7, Batch 43, LR 1.632044 Loss 8.490500, Accuracy 73.201%\n",
      "Epoch 7, Batch 44, LR 1.632330 Loss 8.485110, Accuracy 73.295%\n",
      "Epoch 7, Batch 45, LR 1.632616 Loss 8.490870, Accuracy 73.247%\n",
      "Epoch 7, Batch 46, LR 1.632902 Loss 8.504658, Accuracy 73.183%\n",
      "Epoch 7, Batch 47, LR 1.633188 Loss 8.494589, Accuracy 73.321%\n",
      "Epoch 7, Batch 48, LR 1.633474 Loss 8.501743, Accuracy 73.275%\n",
      "Epoch 7, Batch 49, LR 1.633760 Loss 8.505404, Accuracy 73.310%\n",
      "Epoch 7, Batch 50, LR 1.634046 Loss 8.507314, Accuracy 73.312%\n",
      "Epoch 7, Batch 51, LR 1.634332 Loss 8.493014, Accuracy 73.361%\n",
      "Epoch 7, Batch 52, LR 1.634618 Loss 8.507168, Accuracy 73.167%\n",
      "Epoch 7, Batch 53, LR 1.634903 Loss 8.503884, Accuracy 73.128%\n",
      "Epoch 7, Batch 54, LR 1.635189 Loss 8.505414, Accuracy 73.018%\n",
      "Epoch 7, Batch 55, LR 1.635475 Loss 8.507832, Accuracy 73.011%\n",
      "Epoch 7, Batch 56, LR 1.635761 Loss 8.510249, Accuracy 72.991%\n",
      "Epoch 7, Batch 57, LR 1.636046 Loss 8.512515, Accuracy 72.971%\n",
      "Epoch 7, Batch 58, LR 1.636332 Loss 8.521609, Accuracy 72.993%\n",
      "Epoch 7, Batch 59, LR 1.636618 Loss 8.522648, Accuracy 72.974%\n",
      "Epoch 7, Batch 60, LR 1.636904 Loss 8.499622, Accuracy 73.073%\n",
      "Epoch 7, Batch 61, LR 1.637189 Loss 8.491184, Accuracy 73.130%\n",
      "Epoch 7, Batch 62, LR 1.637475 Loss 8.482212, Accuracy 73.211%\n",
      "Epoch 7, Batch 63, LR 1.637760 Loss 8.486215, Accuracy 73.177%\n",
      "Epoch 7, Batch 64, LR 1.638046 Loss 8.480043, Accuracy 73.267%\n",
      "Epoch 7, Batch 65, LR 1.638332 Loss 8.484380, Accuracy 73.281%\n",
      "Epoch 7, Batch 66, LR 1.638617 Loss 8.483917, Accuracy 73.272%\n",
      "Epoch 7, Batch 67, LR 1.638903 Loss 8.479951, Accuracy 73.391%\n",
      "Epoch 7, Batch 68, LR 1.639188 Loss 8.474102, Accuracy 73.426%\n",
      "Epoch 7, Batch 69, LR 1.639474 Loss 8.465151, Accuracy 73.483%\n",
      "Epoch 7, Batch 70, LR 1.639759 Loss 8.480905, Accuracy 73.393%\n",
      "Epoch 7, Batch 71, LR 1.640045 Loss 8.489675, Accuracy 73.327%\n",
      "Epoch 7, Batch 72, LR 1.640330 Loss 8.492770, Accuracy 73.275%\n",
      "Epoch 7, Batch 73, LR 1.640615 Loss 8.497933, Accuracy 73.256%\n",
      "Epoch 7, Batch 74, LR 1.640901 Loss 8.500001, Accuracy 73.258%\n",
      "Epoch 7, Batch 75, LR 1.641186 Loss 8.493553, Accuracy 73.281%\n",
      "Epoch 7, Batch 76, LR 1.641471 Loss 8.488436, Accuracy 73.263%\n",
      "Epoch 7, Batch 77, LR 1.641757 Loss 8.480124, Accuracy 73.336%\n",
      "Epoch 7, Batch 78, LR 1.642042 Loss 8.475310, Accuracy 73.407%\n",
      "Epoch 7, Batch 79, LR 1.642327 Loss 8.467449, Accuracy 73.447%\n",
      "Epoch 7, Batch 80, LR 1.642613 Loss 8.474680, Accuracy 73.379%\n",
      "Epoch 7, Batch 81, LR 1.642898 Loss 8.474824, Accuracy 73.370%\n",
      "Epoch 7, Batch 82, LR 1.643183 Loss 8.476744, Accuracy 73.361%\n",
      "Epoch 7, Batch 83, LR 1.643468 Loss 8.464142, Accuracy 73.466%\n",
      "Epoch 7, Batch 84, LR 1.643753 Loss 8.462229, Accuracy 73.521%\n",
      "Epoch 7, Batch 85, LR 1.644039 Loss 8.451758, Accuracy 73.603%\n",
      "Epoch 7, Batch 86, LR 1.644324 Loss 8.453341, Accuracy 73.583%\n",
      "Epoch 7, Batch 87, LR 1.644609 Loss 8.447255, Accuracy 73.644%\n",
      "Epoch 7, Batch 88, LR 1.644894 Loss 8.448336, Accuracy 73.624%\n",
      "Epoch 7, Batch 89, LR 1.645179 Loss 8.452288, Accuracy 73.613%\n",
      "Epoch 7, Batch 90, LR 1.645464 Loss 8.459734, Accuracy 73.550%\n",
      "Epoch 7, Batch 91, LR 1.645749 Loss 8.462489, Accuracy 73.532%\n",
      "Epoch 7, Batch 92, LR 1.646034 Loss 8.465398, Accuracy 73.505%\n",
      "Epoch 7, Batch 93, LR 1.646319 Loss 8.466908, Accuracy 73.488%\n",
      "Epoch 7, Batch 94, LR 1.646604 Loss 8.466842, Accuracy 73.496%\n",
      "Epoch 7, Batch 95, LR 1.646889 Loss 8.470289, Accuracy 73.454%\n",
      "Epoch 7, Batch 96, LR 1.647174 Loss 8.466583, Accuracy 73.494%\n",
      "Epoch 7, Batch 97, LR 1.647459 Loss 8.470914, Accuracy 73.494%\n",
      "Epoch 7, Batch 98, LR 1.647744 Loss 8.474064, Accuracy 73.469%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 99, LR 1.648029 Loss 8.478479, Accuracy 73.414%\n",
      "Epoch 7, Batch 100, LR 1.648313 Loss 8.479163, Accuracy 73.445%\n",
      "Epoch 7, Batch 101, LR 1.648598 Loss 8.480715, Accuracy 73.468%\n",
      "Epoch 7, Batch 102, LR 1.648883 Loss 8.486618, Accuracy 73.445%\n",
      "Epoch 7, Batch 103, LR 1.649168 Loss 8.483461, Accuracy 73.498%\n",
      "Epoch 7, Batch 104, LR 1.649453 Loss 8.481590, Accuracy 73.490%\n",
      "Epoch 7, Batch 105, LR 1.649737 Loss 8.487238, Accuracy 73.430%\n",
      "Epoch 7, Batch 106, LR 1.650022 Loss 8.486192, Accuracy 73.401%\n",
      "Epoch 7, Batch 107, LR 1.650307 Loss 8.490232, Accuracy 73.379%\n",
      "Epoch 7, Batch 108, LR 1.650591 Loss 8.488554, Accuracy 73.380%\n",
      "Epoch 7, Batch 109, LR 1.650876 Loss 8.489024, Accuracy 73.380%\n",
      "Epoch 7, Batch 110, LR 1.651161 Loss 8.490521, Accuracy 73.395%\n",
      "Epoch 7, Batch 111, LR 1.651445 Loss 8.487118, Accuracy 73.416%\n",
      "Epoch 7, Batch 112, LR 1.651730 Loss 8.490880, Accuracy 73.375%\n",
      "Epoch 7, Batch 113, LR 1.652015 Loss 8.491256, Accuracy 73.355%\n",
      "Epoch 7, Batch 114, LR 1.652299 Loss 8.495799, Accuracy 73.280%\n",
      "Epoch 7, Batch 115, LR 1.652584 Loss 8.490437, Accuracy 73.349%\n",
      "Epoch 7, Batch 116, LR 1.652868 Loss 8.489518, Accuracy 73.363%\n",
      "Epoch 7, Batch 117, LR 1.653153 Loss 8.491021, Accuracy 73.357%\n",
      "Epoch 7, Batch 118, LR 1.653437 Loss 8.490041, Accuracy 73.365%\n",
      "Epoch 7, Batch 119, LR 1.653721 Loss 8.486438, Accuracy 73.405%\n",
      "Epoch 7, Batch 120, LR 1.654006 Loss 8.486888, Accuracy 73.444%\n",
      "Epoch 7, Batch 121, LR 1.654290 Loss 8.478866, Accuracy 73.528%\n",
      "Epoch 7, Batch 122, LR 1.654575 Loss 8.483261, Accuracy 73.495%\n",
      "Epoch 7, Batch 123, LR 1.654859 Loss 8.482088, Accuracy 73.514%\n",
      "Epoch 7, Batch 124, LR 1.655143 Loss 8.484864, Accuracy 73.456%\n",
      "Epoch 7, Batch 125, LR 1.655428 Loss 8.480545, Accuracy 73.481%\n",
      "Epoch 7, Batch 126, LR 1.655712 Loss 8.477950, Accuracy 73.481%\n",
      "Epoch 7, Batch 127, LR 1.655996 Loss 8.475794, Accuracy 73.493%\n",
      "Epoch 7, Batch 128, LR 1.656281 Loss 8.477243, Accuracy 73.462%\n",
      "Epoch 7, Batch 129, LR 1.656565 Loss 8.475252, Accuracy 73.498%\n",
      "Epoch 7, Batch 130, LR 1.656849 Loss 8.482852, Accuracy 73.456%\n",
      "Epoch 7, Batch 131, LR 1.657133 Loss 8.493627, Accuracy 73.384%\n",
      "Epoch 7, Batch 132, LR 1.657417 Loss 8.494363, Accuracy 73.378%\n",
      "Epoch 7, Batch 133, LR 1.657701 Loss 8.488161, Accuracy 73.420%\n",
      "Epoch 7, Batch 134, LR 1.657986 Loss 8.487199, Accuracy 73.472%\n",
      "Epoch 7, Batch 135, LR 1.658270 Loss 8.487119, Accuracy 73.478%\n",
      "Epoch 7, Batch 136, LR 1.658554 Loss 8.486209, Accuracy 73.506%\n",
      "Epoch 7, Batch 137, LR 1.658838 Loss 8.489300, Accuracy 73.472%\n",
      "Epoch 7, Batch 138, LR 1.659122 Loss 8.489261, Accuracy 73.488%\n",
      "Epoch 7, Batch 139, LR 1.659406 Loss 8.487640, Accuracy 73.550%\n",
      "Epoch 7, Batch 140, LR 1.659690 Loss 8.482403, Accuracy 73.577%\n",
      "Epoch 7, Batch 141, LR 1.659974 Loss 8.485096, Accuracy 73.570%\n",
      "Epoch 7, Batch 142, LR 1.660258 Loss 8.484555, Accuracy 73.603%\n",
      "Epoch 7, Batch 143, LR 1.660542 Loss 8.480394, Accuracy 73.612%\n",
      "Epoch 7, Batch 144, LR 1.660826 Loss 8.474220, Accuracy 73.671%\n",
      "Epoch 7, Batch 145, LR 1.661109 Loss 8.470917, Accuracy 73.685%\n",
      "Epoch 7, Batch 146, LR 1.661393 Loss 8.474225, Accuracy 73.684%\n",
      "Epoch 7, Batch 147, LR 1.661677 Loss 8.474831, Accuracy 73.687%\n",
      "Epoch 7, Batch 148, LR 1.661961 Loss 8.480425, Accuracy 73.664%\n",
      "Epoch 7, Batch 149, LR 1.662245 Loss 8.482186, Accuracy 73.647%\n",
      "Epoch 7, Batch 150, LR 1.662529 Loss 8.480981, Accuracy 73.667%\n",
      "Epoch 7, Batch 151, LR 1.662812 Loss 8.481718, Accuracy 73.675%\n",
      "Epoch 7, Batch 152, LR 1.663096 Loss 8.482008, Accuracy 73.689%\n",
      "Epoch 7, Batch 153, LR 1.663380 Loss 8.482855, Accuracy 73.657%\n",
      "Epoch 7, Batch 154, LR 1.663663 Loss 8.486606, Accuracy 73.656%\n",
      "Epoch 7, Batch 155, LR 1.663947 Loss 8.488748, Accuracy 73.639%\n",
      "Epoch 7, Batch 156, LR 1.664231 Loss 8.490918, Accuracy 73.613%\n",
      "Epoch 7, Batch 157, LR 1.664514 Loss 8.493637, Accuracy 73.572%\n",
      "Epoch 7, Batch 158, LR 1.664798 Loss 8.495601, Accuracy 73.576%\n",
      "Epoch 7, Batch 159, LR 1.665081 Loss 8.492284, Accuracy 73.605%\n",
      "Epoch 7, Batch 160, LR 1.665365 Loss 8.491312, Accuracy 73.589%\n",
      "Epoch 7, Batch 161, LR 1.665649 Loss 8.491766, Accuracy 73.593%\n",
      "Epoch 7, Batch 162, LR 1.665932 Loss 8.494283, Accuracy 73.577%\n",
      "Epoch 7, Batch 163, LR 1.666216 Loss 8.493622, Accuracy 73.557%\n",
      "Epoch 7, Batch 164, LR 1.666499 Loss 8.495705, Accuracy 73.557%\n",
      "Epoch 7, Batch 165, LR 1.666782 Loss 8.496394, Accuracy 73.570%\n",
      "Epoch 7, Batch 166, LR 1.667066 Loss 8.501169, Accuracy 73.532%\n",
      "Epoch 7, Batch 167, LR 1.667349 Loss 8.497791, Accuracy 73.554%\n",
      "Epoch 7, Batch 168, LR 1.667633 Loss 8.498650, Accuracy 73.540%\n",
      "Epoch 7, Batch 169, LR 1.667916 Loss 8.499223, Accuracy 73.530%\n",
      "Epoch 7, Batch 170, LR 1.668199 Loss 8.501461, Accuracy 73.520%\n",
      "Epoch 7, Batch 171, LR 1.668483 Loss 8.504509, Accuracy 73.524%\n",
      "Epoch 7, Batch 172, LR 1.668766 Loss 8.506463, Accuracy 73.497%\n",
      "Epoch 7, Batch 173, LR 1.669049 Loss 8.506660, Accuracy 73.496%\n",
      "Epoch 7, Batch 174, LR 1.669332 Loss 8.505362, Accuracy 73.505%\n",
      "Epoch 7, Batch 175, LR 1.669616 Loss 8.507479, Accuracy 73.469%\n",
      "Epoch 7, Batch 176, LR 1.669899 Loss 8.506075, Accuracy 73.477%\n",
      "Epoch 7, Batch 177, LR 1.670182 Loss 8.510811, Accuracy 73.433%\n",
      "Epoch 7, Batch 178, LR 1.670465 Loss 8.515885, Accuracy 73.402%\n",
      "Epoch 7, Batch 179, LR 1.670748 Loss 8.517282, Accuracy 73.376%\n",
      "Epoch 7, Batch 180, LR 1.671031 Loss 8.512826, Accuracy 73.411%\n",
      "Epoch 7, Batch 181, LR 1.671315 Loss 8.513214, Accuracy 73.390%\n",
      "Epoch 7, Batch 182, LR 1.671598 Loss 8.511988, Accuracy 73.412%\n",
      "Epoch 7, Batch 183, LR 1.671881 Loss 8.519807, Accuracy 73.373%\n",
      "Epoch 7, Batch 184, LR 1.672164 Loss 8.519988, Accuracy 73.370%\n",
      "Epoch 7, Batch 185, LR 1.672447 Loss 8.521013, Accuracy 73.357%\n",
      "Epoch 7, Batch 186, LR 1.672730 Loss 8.522197, Accuracy 73.345%\n",
      "Epoch 7, Batch 187, LR 1.673013 Loss 8.521975, Accuracy 73.333%\n",
      "Epoch 7, Batch 188, LR 1.673295 Loss 8.521930, Accuracy 73.342%\n",
      "Epoch 7, Batch 189, LR 1.673578 Loss 8.524048, Accuracy 73.318%\n",
      "Epoch 7, Batch 190, LR 1.673861 Loss 8.524839, Accuracy 73.294%\n",
      "Epoch 7, Batch 191, LR 1.674144 Loss 8.526939, Accuracy 73.298%\n",
      "Epoch 7, Batch 192, LR 1.674427 Loss 8.528191, Accuracy 73.283%\n",
      "Epoch 7, Batch 193, LR 1.674710 Loss 8.529608, Accuracy 73.263%\n",
      "Epoch 7, Batch 194, LR 1.674993 Loss 8.531940, Accuracy 73.244%\n",
      "Epoch 7, Batch 195, LR 1.675275 Loss 8.533141, Accuracy 73.241%\n",
      "Epoch 7, Batch 196, LR 1.675558 Loss 8.529906, Accuracy 73.266%\n",
      "Epoch 7, Batch 197, LR 1.675841 Loss 8.528648, Accuracy 73.271%\n",
      "Epoch 7, Batch 198, LR 1.676124 Loss 8.530162, Accuracy 73.268%\n",
      "Epoch 7, Batch 199, LR 1.676406 Loss 8.533789, Accuracy 73.233%\n",
      "Epoch 7, Batch 200, LR 1.676689 Loss 8.536273, Accuracy 73.219%\n",
      "Epoch 7, Batch 201, LR 1.676971 Loss 8.537809, Accuracy 73.200%\n",
      "Epoch 7, Batch 202, LR 1.677254 Loss 8.536164, Accuracy 73.209%\n",
      "Epoch 7, Batch 203, LR 1.677537 Loss 8.537109, Accuracy 73.187%\n",
      "Epoch 7, Batch 204, LR 1.677819 Loss 8.539674, Accuracy 73.196%\n",
      "Epoch 7, Batch 205, LR 1.678102 Loss 8.543566, Accuracy 73.155%\n",
      "Epoch 7, Batch 206, LR 1.678384 Loss 8.543411, Accuracy 73.123%\n",
      "Epoch 7, Batch 207, LR 1.678667 Loss 8.543190, Accuracy 73.147%\n",
      "Epoch 7, Batch 208, LR 1.678949 Loss 8.542324, Accuracy 73.141%\n",
      "Epoch 7, Batch 209, LR 1.679232 Loss 8.543163, Accuracy 73.165%\n",
      "Epoch 7, Batch 210, LR 1.679514 Loss 8.543366, Accuracy 73.166%\n",
      "Epoch 7, Batch 211, LR 1.679797 Loss 8.542520, Accuracy 73.156%\n",
      "Epoch 7, Batch 212, LR 1.680079 Loss 8.541683, Accuracy 73.157%\n",
      "Epoch 7, Batch 213, LR 1.680361 Loss 8.544877, Accuracy 73.155%\n",
      "Epoch 7, Batch 214, LR 1.680644 Loss 8.543961, Accuracy 73.171%\n",
      "Epoch 7, Batch 215, LR 1.680926 Loss 8.546435, Accuracy 73.121%\n",
      "Epoch 7, Batch 216, LR 1.681208 Loss 8.547955, Accuracy 73.116%\n",
      "Epoch 7, Batch 217, LR 1.681491 Loss 8.552776, Accuracy 73.067%\n",
      "Epoch 7, Batch 218, LR 1.681773 Loss 8.553013, Accuracy 73.058%\n",
      "Epoch 7, Batch 219, LR 1.682055 Loss 8.552733, Accuracy 73.066%\n",
      "Epoch 7, Batch 220, LR 1.682337 Loss 8.553704, Accuracy 73.086%\n",
      "Epoch 7, Batch 221, LR 1.682619 Loss 8.555206, Accuracy 73.056%\n",
      "Epoch 7, Batch 222, LR 1.682901 Loss 8.552372, Accuracy 73.079%\n",
      "Epoch 7, Batch 223, LR 1.683184 Loss 8.551695, Accuracy 73.077%\n",
      "Epoch 7, Batch 224, LR 1.683466 Loss 8.550634, Accuracy 73.085%\n",
      "Epoch 7, Batch 225, LR 1.683748 Loss 8.552585, Accuracy 73.083%\n",
      "Epoch 7, Batch 226, LR 1.684030 Loss 8.554291, Accuracy 73.085%\n",
      "Epoch 7, Batch 227, LR 1.684312 Loss 8.555328, Accuracy 73.052%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 228, LR 1.684594 Loss 8.554642, Accuracy 73.061%\n",
      "Epoch 7, Batch 229, LR 1.684876 Loss 8.555724, Accuracy 73.066%\n",
      "Epoch 7, Batch 230, LR 1.685158 Loss 8.554163, Accuracy 73.084%\n",
      "Epoch 7, Batch 231, LR 1.685440 Loss 8.557342, Accuracy 73.062%\n",
      "Epoch 7, Batch 232, LR 1.685722 Loss 8.557336, Accuracy 73.060%\n",
      "Epoch 7, Batch 233, LR 1.686004 Loss 8.560340, Accuracy 73.052%\n",
      "Epoch 7, Batch 234, LR 1.686285 Loss 8.559480, Accuracy 73.057%\n",
      "Epoch 7, Batch 235, LR 1.686567 Loss 8.556146, Accuracy 73.088%\n",
      "Epoch 7, Batch 236, LR 1.686849 Loss 8.555028, Accuracy 73.090%\n",
      "Epoch 7, Batch 237, LR 1.687131 Loss 8.555591, Accuracy 73.098%\n",
      "Epoch 7, Batch 238, LR 1.687413 Loss 8.556764, Accuracy 73.086%\n",
      "Epoch 7, Batch 239, LR 1.687695 Loss 8.557451, Accuracy 73.094%\n",
      "Epoch 7, Batch 240, LR 1.687976 Loss 8.558442, Accuracy 73.099%\n",
      "Epoch 7, Batch 241, LR 1.688258 Loss 8.560044, Accuracy 73.084%\n",
      "Epoch 7, Batch 242, LR 1.688540 Loss 8.561181, Accuracy 73.063%\n",
      "Epoch 7, Batch 243, LR 1.688821 Loss 8.561576, Accuracy 73.071%\n",
      "Epoch 7, Batch 244, LR 1.689103 Loss 8.562498, Accuracy 73.044%\n",
      "Epoch 7, Batch 245, LR 1.689385 Loss 8.565313, Accuracy 73.023%\n",
      "Epoch 7, Batch 246, LR 1.689666 Loss 8.564728, Accuracy 73.034%\n",
      "Epoch 7, Batch 247, LR 1.689948 Loss 8.565695, Accuracy 73.026%\n",
      "Epoch 7, Batch 248, LR 1.690229 Loss 8.565616, Accuracy 73.009%\n",
      "Epoch 7, Batch 249, LR 1.690511 Loss 8.565047, Accuracy 73.014%\n",
      "Epoch 7, Batch 250, LR 1.690792 Loss 8.565415, Accuracy 73.019%\n",
      "Epoch 7, Batch 251, LR 1.691074 Loss 8.562547, Accuracy 73.030%\n",
      "Epoch 7, Batch 252, LR 1.691355 Loss 8.561785, Accuracy 73.025%\n",
      "Epoch 7, Batch 253, LR 1.691637 Loss 8.564398, Accuracy 73.005%\n",
      "Epoch 7, Batch 254, LR 1.691918 Loss 8.565901, Accuracy 72.992%\n",
      "Epoch 7, Batch 255, LR 1.692199 Loss 8.564821, Accuracy 72.990%\n",
      "Epoch 7, Batch 256, LR 1.692481 Loss 8.567692, Accuracy 72.968%\n",
      "Epoch 7, Batch 257, LR 1.692762 Loss 8.565712, Accuracy 72.978%\n",
      "Epoch 7, Batch 258, LR 1.693043 Loss 8.564378, Accuracy 72.995%\n",
      "Epoch 7, Batch 259, LR 1.693325 Loss 8.562392, Accuracy 73.045%\n",
      "Epoch 7, Batch 260, LR 1.693606 Loss 8.560112, Accuracy 73.050%\n",
      "Epoch 7, Batch 261, LR 1.693887 Loss 8.561321, Accuracy 73.048%\n",
      "Epoch 7, Batch 262, LR 1.694168 Loss 8.560376, Accuracy 73.032%\n",
      "Epoch 7, Batch 263, LR 1.694450 Loss 8.561609, Accuracy 73.007%\n",
      "Epoch 7, Batch 264, LR 1.694731 Loss 8.562091, Accuracy 73.023%\n",
      "Epoch 7, Batch 265, LR 1.695012 Loss 8.562234, Accuracy 73.013%\n",
      "Epoch 7, Batch 266, LR 1.695293 Loss 8.564229, Accuracy 72.994%\n",
      "Epoch 7, Batch 267, LR 1.695574 Loss 8.565799, Accuracy 72.978%\n",
      "Epoch 7, Batch 268, LR 1.695855 Loss 8.567863, Accuracy 72.974%\n",
      "Epoch 7, Batch 269, LR 1.696136 Loss 8.566066, Accuracy 72.982%\n",
      "Epoch 7, Batch 270, LR 1.696417 Loss 8.565719, Accuracy 72.986%\n",
      "Epoch 7, Batch 271, LR 1.696698 Loss 8.564500, Accuracy 73.008%\n",
      "Epoch 7, Batch 272, LR 1.696979 Loss 8.562952, Accuracy 73.012%\n",
      "Epoch 7, Batch 273, LR 1.697260 Loss 8.563099, Accuracy 73.008%\n",
      "Epoch 7, Batch 274, LR 1.697541 Loss 8.561883, Accuracy 73.030%\n",
      "Epoch 7, Batch 275, LR 1.697822 Loss 8.564087, Accuracy 73.006%\n",
      "Epoch 7, Batch 276, LR 1.698103 Loss 8.564776, Accuracy 73.007%\n",
      "Epoch 7, Batch 277, LR 1.698384 Loss 8.567526, Accuracy 72.992%\n",
      "Epoch 7, Batch 278, LR 1.698664 Loss 8.567605, Accuracy 72.979%\n",
      "Epoch 7, Batch 279, LR 1.698945 Loss 8.563063, Accuracy 73.009%\n",
      "Epoch 7, Batch 280, LR 1.699226 Loss 8.563631, Accuracy 72.983%\n",
      "Epoch 7, Batch 281, LR 1.699507 Loss 8.563274, Accuracy 72.998%\n",
      "Epoch 7, Batch 282, LR 1.699788 Loss 8.560419, Accuracy 73.014%\n",
      "Epoch 7, Batch 283, LR 1.700068 Loss 8.561725, Accuracy 72.996%\n",
      "Epoch 7, Batch 284, LR 1.700349 Loss 8.562848, Accuracy 72.984%\n",
      "Epoch 7, Batch 285, LR 1.700630 Loss 8.563103, Accuracy 72.985%\n",
      "Epoch 7, Batch 286, LR 1.700910 Loss 8.562162, Accuracy 72.992%\n",
      "Epoch 7, Batch 287, LR 1.701191 Loss 8.562568, Accuracy 72.999%\n",
      "Epoch 7, Batch 288, LR 1.701471 Loss 8.562140, Accuracy 72.998%\n",
      "Epoch 7, Batch 289, LR 1.701752 Loss 8.562074, Accuracy 72.997%\n",
      "Epoch 7, Batch 290, LR 1.702033 Loss 8.561781, Accuracy 72.993%\n",
      "Epoch 7, Batch 291, LR 1.702313 Loss 8.563581, Accuracy 72.970%\n",
      "Epoch 7, Batch 292, LR 1.702594 Loss 8.563511, Accuracy 72.972%\n",
      "Epoch 7, Batch 293, LR 1.702874 Loss 8.560660, Accuracy 72.987%\n",
      "Epoch 7, Batch 294, LR 1.703154 Loss 8.560397, Accuracy 72.988%\n",
      "Epoch 7, Batch 295, LR 1.703435 Loss 8.561365, Accuracy 72.979%\n",
      "Epoch 7, Batch 296, LR 1.703715 Loss 8.562794, Accuracy 72.970%\n",
      "Epoch 7, Batch 297, LR 1.703996 Loss 8.564153, Accuracy 72.956%\n",
      "Epoch 7, Batch 298, LR 1.704276 Loss 8.562848, Accuracy 72.971%\n",
      "Epoch 7, Batch 299, LR 1.704556 Loss 8.561708, Accuracy 72.980%\n",
      "Epoch 7, Batch 300, LR 1.704837 Loss 8.562465, Accuracy 72.961%\n",
      "Epoch 7, Batch 301, LR 1.705117 Loss 8.561287, Accuracy 72.975%\n",
      "Epoch 7, Batch 302, LR 1.705397 Loss 8.560757, Accuracy 72.990%\n",
      "Epoch 7, Batch 303, LR 1.705677 Loss 8.559787, Accuracy 73.009%\n",
      "Epoch 7, Batch 304, LR 1.705957 Loss 8.561478, Accuracy 72.990%\n",
      "Epoch 7, Batch 305, LR 1.706238 Loss 8.562373, Accuracy 72.987%\n",
      "Epoch 7, Batch 306, LR 1.706518 Loss 8.561829, Accuracy 72.988%\n",
      "Epoch 7, Batch 307, LR 1.706798 Loss 8.561020, Accuracy 72.995%\n",
      "Epoch 7, Batch 308, LR 1.707078 Loss 8.559584, Accuracy 73.006%\n",
      "Epoch 7, Batch 309, LR 1.707358 Loss 8.559585, Accuracy 73.005%\n",
      "Epoch 7, Batch 310, LR 1.707638 Loss 8.558707, Accuracy 73.007%\n",
      "Epoch 7, Batch 311, LR 1.707918 Loss 8.558452, Accuracy 73.005%\n",
      "Epoch 7, Batch 312, LR 1.708198 Loss 8.559220, Accuracy 72.992%\n",
      "Epoch 7, Batch 313, LR 1.708478 Loss 8.559130, Accuracy 72.991%\n",
      "Epoch 7, Batch 314, LR 1.708758 Loss 8.558281, Accuracy 73.010%\n",
      "Epoch 7, Batch 315, LR 1.709038 Loss 8.558678, Accuracy 73.006%\n",
      "Epoch 7, Batch 316, LR 1.709318 Loss 8.561613, Accuracy 72.983%\n",
      "Epoch 7, Batch 317, LR 1.709598 Loss 8.560942, Accuracy 72.989%\n",
      "Epoch 7, Batch 318, LR 1.709878 Loss 8.559579, Accuracy 72.995%\n",
      "Epoch 7, Batch 319, LR 1.710157 Loss 8.559951, Accuracy 72.989%\n",
      "Epoch 7, Batch 320, LR 1.710437 Loss 8.562915, Accuracy 72.966%\n",
      "Epoch 7, Batch 321, LR 1.710717 Loss 8.562287, Accuracy 72.965%\n",
      "Epoch 7, Batch 322, LR 1.710997 Loss 8.561529, Accuracy 72.967%\n",
      "Epoch 7, Batch 323, LR 1.711276 Loss 8.562082, Accuracy 72.963%\n",
      "Epoch 7, Batch 324, LR 1.711556 Loss 8.564186, Accuracy 72.948%\n",
      "Epoch 7, Batch 325, LR 1.711836 Loss 8.566503, Accuracy 72.921%\n",
      "Epoch 7, Batch 326, LR 1.712115 Loss 8.569738, Accuracy 72.894%\n",
      "Epoch 7, Batch 327, LR 1.712395 Loss 8.568755, Accuracy 72.893%\n",
      "Epoch 7, Batch 328, LR 1.712675 Loss 8.570310, Accuracy 72.871%\n",
      "Epoch 7, Batch 329, LR 1.712954 Loss 8.571051, Accuracy 72.856%\n",
      "Epoch 7, Batch 330, LR 1.713234 Loss 8.571427, Accuracy 72.843%\n",
      "Epoch 7, Batch 331, LR 1.713513 Loss 8.572105, Accuracy 72.838%\n",
      "Epoch 7, Batch 332, LR 1.713793 Loss 8.571663, Accuracy 72.840%\n",
      "Epoch 7, Batch 333, LR 1.714072 Loss 8.571479, Accuracy 72.844%\n",
      "Epoch 7, Batch 334, LR 1.714352 Loss 8.571556, Accuracy 72.839%\n",
      "Epoch 7, Batch 335, LR 1.714631 Loss 8.572319, Accuracy 72.831%\n",
      "Epoch 7, Batch 336, LR 1.714911 Loss 8.570540, Accuracy 72.840%\n",
      "Epoch 7, Batch 337, LR 1.715190 Loss 8.569612, Accuracy 72.853%\n",
      "Epoch 7, Batch 338, LR 1.715469 Loss 8.568274, Accuracy 72.869%\n",
      "Epoch 7, Batch 339, LR 1.715749 Loss 8.565820, Accuracy 72.873%\n",
      "Epoch 7, Batch 340, LR 1.716028 Loss 8.567019, Accuracy 72.868%\n",
      "Epoch 7, Batch 341, LR 1.716307 Loss 8.566275, Accuracy 72.876%\n",
      "Epoch 7, Batch 342, LR 1.716586 Loss 8.566355, Accuracy 72.878%\n",
      "Epoch 7, Batch 343, LR 1.716866 Loss 8.564288, Accuracy 72.891%\n",
      "Epoch 7, Batch 344, LR 1.717145 Loss 8.564234, Accuracy 72.890%\n",
      "Epoch 7, Batch 345, LR 1.717424 Loss 8.563292, Accuracy 72.899%\n",
      "Epoch 7, Batch 346, LR 1.717703 Loss 8.564499, Accuracy 72.882%\n",
      "Epoch 7, Batch 347, LR 1.717982 Loss 8.564386, Accuracy 72.886%\n",
      "Epoch 7, Batch 348, LR 1.718261 Loss 8.564313, Accuracy 72.883%\n",
      "Epoch 7, Batch 349, LR 1.718540 Loss 8.564413, Accuracy 72.889%\n",
      "Epoch 7, Batch 350, LR 1.718819 Loss 8.564656, Accuracy 72.886%\n",
      "Epoch 7, Batch 351, LR 1.719098 Loss 8.565401, Accuracy 72.877%\n",
      "Epoch 7, Batch 352, LR 1.719377 Loss 8.564858, Accuracy 72.874%\n",
      "Epoch 7, Batch 353, LR 1.719656 Loss 8.564916, Accuracy 72.878%\n",
      "Epoch 7, Batch 354, LR 1.719935 Loss 8.563090, Accuracy 72.890%\n",
      "Epoch 7, Batch 355, LR 1.720214 Loss 8.563110, Accuracy 72.885%\n",
      "Epoch 7, Batch 356, LR 1.720493 Loss 8.563882, Accuracy 72.880%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 357, LR 1.720772 Loss 8.562003, Accuracy 72.899%\n",
      "Epoch 7, Batch 358, LR 1.721051 Loss 8.562560, Accuracy 72.892%\n",
      "Epoch 7, Batch 359, LR 1.721330 Loss 8.560716, Accuracy 72.909%\n",
      "Epoch 7, Batch 360, LR 1.721608 Loss 8.560731, Accuracy 72.899%\n",
      "Epoch 7, Batch 361, LR 1.721887 Loss 8.561594, Accuracy 72.892%\n",
      "Epoch 7, Batch 362, LR 1.722166 Loss 8.561353, Accuracy 72.911%\n",
      "Epoch 7, Batch 363, LR 1.722445 Loss 8.562070, Accuracy 72.904%\n",
      "Epoch 7, Batch 364, LR 1.722723 Loss 8.559780, Accuracy 72.916%\n",
      "Epoch 7, Batch 365, LR 1.723002 Loss 8.560460, Accuracy 72.909%\n",
      "Epoch 7, Batch 366, LR 1.723281 Loss 8.559878, Accuracy 72.906%\n",
      "Epoch 7, Batch 367, LR 1.723559 Loss 8.558866, Accuracy 72.910%\n",
      "Epoch 7, Batch 368, LR 1.723838 Loss 8.560439, Accuracy 72.888%\n",
      "Epoch 7, Batch 369, LR 1.724116 Loss 8.561540, Accuracy 72.883%\n",
      "Epoch 7, Batch 370, LR 1.724395 Loss 8.562988, Accuracy 72.878%\n",
      "Epoch 7, Batch 371, LR 1.724673 Loss 8.562200, Accuracy 72.873%\n",
      "Epoch 7, Batch 372, LR 1.724952 Loss 8.562274, Accuracy 72.881%\n",
      "Epoch 7, Batch 373, LR 1.725230 Loss 8.559728, Accuracy 72.901%\n",
      "Epoch 7, Batch 374, LR 1.725509 Loss 8.559269, Accuracy 72.905%\n",
      "Epoch 7, Batch 375, LR 1.725787 Loss 8.561252, Accuracy 72.890%\n",
      "Epoch 7, Batch 376, LR 1.726066 Loss 8.563472, Accuracy 72.874%\n",
      "Epoch 7, Batch 377, LR 1.726344 Loss 8.560421, Accuracy 72.888%\n",
      "Epoch 7, Batch 378, LR 1.726622 Loss 8.558697, Accuracy 72.896%\n",
      "Epoch 7, Batch 379, LR 1.726901 Loss 8.557512, Accuracy 72.910%\n",
      "Epoch 7, Batch 380, LR 1.727179 Loss 8.559321, Accuracy 72.897%\n",
      "Epoch 7, Batch 381, LR 1.727457 Loss 8.559100, Accuracy 72.898%\n",
      "Epoch 7, Batch 382, LR 1.727735 Loss 8.558377, Accuracy 72.896%\n",
      "Epoch 7, Batch 383, LR 1.728014 Loss 8.557221, Accuracy 72.907%\n",
      "Epoch 7, Batch 384, LR 1.728292 Loss 8.557830, Accuracy 72.894%\n",
      "Epoch 7, Batch 385, LR 1.728570 Loss 8.557520, Accuracy 72.894%\n",
      "Epoch 7, Batch 386, LR 1.728848 Loss 8.557145, Accuracy 72.889%\n",
      "Epoch 7, Batch 387, LR 1.729126 Loss 8.557100, Accuracy 72.892%\n",
      "Epoch 7, Batch 388, LR 1.729404 Loss 8.555647, Accuracy 72.904%\n",
      "Epoch 7, Batch 389, LR 1.729682 Loss 8.555911, Accuracy 72.891%\n",
      "Epoch 7, Batch 390, LR 1.729960 Loss 8.555064, Accuracy 72.901%\n",
      "Epoch 7, Batch 391, LR 1.730238 Loss 8.554912, Accuracy 72.902%\n",
      "Epoch 7, Batch 392, LR 1.730516 Loss 8.553192, Accuracy 72.911%\n",
      "Epoch 7, Batch 393, LR 1.730794 Loss 8.553786, Accuracy 72.907%\n",
      "Epoch 7, Batch 394, LR 1.731072 Loss 8.554226, Accuracy 72.910%\n",
      "Epoch 7, Batch 395, LR 1.731350 Loss 8.554759, Accuracy 72.913%\n",
      "Epoch 7, Batch 396, LR 1.731628 Loss 8.553249, Accuracy 72.925%\n",
      "Epoch 7, Batch 397, LR 1.731906 Loss 8.552649, Accuracy 72.928%\n",
      "Epoch 7, Batch 398, LR 1.732183 Loss 8.554607, Accuracy 72.919%\n",
      "Epoch 7, Batch 399, LR 1.732461 Loss 8.553150, Accuracy 72.928%\n",
      "Epoch 7, Batch 400, LR 1.732739 Loss 8.551976, Accuracy 72.941%\n",
      "Epoch 7, Batch 401, LR 1.733017 Loss 8.552592, Accuracy 72.947%\n",
      "Epoch 7, Batch 402, LR 1.733294 Loss 8.552771, Accuracy 72.942%\n",
      "Epoch 7, Batch 403, LR 1.733572 Loss 8.552501, Accuracy 72.953%\n",
      "Epoch 7, Batch 404, LR 1.733850 Loss 8.552735, Accuracy 72.950%\n",
      "Epoch 7, Batch 405, LR 1.734127 Loss 8.551435, Accuracy 72.959%\n",
      "Epoch 7, Batch 406, LR 1.734405 Loss 8.552830, Accuracy 72.951%\n",
      "Epoch 7, Batch 407, LR 1.734682 Loss 8.553803, Accuracy 72.946%\n",
      "Epoch 7, Batch 408, LR 1.734960 Loss 8.552775, Accuracy 72.951%\n",
      "Epoch 7, Batch 409, LR 1.735237 Loss 8.552670, Accuracy 72.958%\n",
      "Epoch 7, Batch 410, LR 1.735515 Loss 8.551604, Accuracy 72.959%\n",
      "Epoch 7, Batch 411, LR 1.735792 Loss 8.551206, Accuracy 72.972%\n",
      "Epoch 7, Batch 412, LR 1.736070 Loss 8.552786, Accuracy 72.956%\n",
      "Epoch 7, Batch 413, LR 1.736347 Loss 8.552926, Accuracy 72.959%\n",
      "Epoch 7, Batch 414, LR 1.736625 Loss 8.553188, Accuracy 72.958%\n",
      "Epoch 7, Batch 415, LR 1.736902 Loss 8.552739, Accuracy 72.954%\n",
      "Epoch 7, Batch 416, LR 1.737179 Loss 8.551924, Accuracy 72.970%\n",
      "Epoch 7, Batch 417, LR 1.737457 Loss 8.552539, Accuracy 72.964%\n",
      "Epoch 7, Batch 418, LR 1.737734 Loss 8.550058, Accuracy 72.968%\n",
      "Epoch 7, Batch 419, LR 1.738011 Loss 8.551068, Accuracy 72.949%\n",
      "Epoch 7, Batch 420, LR 1.738288 Loss 8.549944, Accuracy 72.946%\n",
      "Epoch 7, Batch 421, LR 1.738566 Loss 8.550862, Accuracy 72.940%\n",
      "Epoch 7, Batch 422, LR 1.738843 Loss 8.550479, Accuracy 72.952%\n",
      "Epoch 7, Batch 423, LR 1.739120 Loss 8.550743, Accuracy 72.955%\n",
      "Epoch 7, Batch 424, LR 1.739397 Loss 8.549686, Accuracy 72.962%\n",
      "Epoch 7, Batch 425, LR 1.739674 Loss 8.550225, Accuracy 72.950%\n",
      "Epoch 7, Batch 426, LR 1.739951 Loss 8.550839, Accuracy 72.948%\n",
      "Epoch 7, Batch 427, LR 1.740228 Loss 8.550643, Accuracy 72.958%\n",
      "Epoch 7, Batch 428, LR 1.740505 Loss 8.551100, Accuracy 72.948%\n",
      "Epoch 7, Batch 429, LR 1.740782 Loss 8.550317, Accuracy 72.957%\n",
      "Epoch 7, Batch 430, LR 1.741059 Loss 8.550648, Accuracy 72.949%\n",
      "Epoch 7, Batch 431, LR 1.741336 Loss 8.549127, Accuracy 72.955%\n",
      "Epoch 7, Batch 432, LR 1.741613 Loss 8.548701, Accuracy 72.958%\n",
      "Epoch 7, Batch 433, LR 1.741890 Loss 8.549292, Accuracy 72.950%\n",
      "Epoch 7, Batch 434, LR 1.742167 Loss 8.548121, Accuracy 72.960%\n",
      "Epoch 7, Batch 435, LR 1.742443 Loss 8.547294, Accuracy 72.978%\n",
      "Epoch 7, Batch 436, LR 1.742720 Loss 8.547346, Accuracy 72.981%\n",
      "Epoch 7, Batch 437, LR 1.742997 Loss 8.546521, Accuracy 72.989%\n",
      "Epoch 7, Batch 438, LR 1.743274 Loss 8.548083, Accuracy 72.983%\n",
      "Epoch 7, Batch 439, LR 1.743550 Loss 8.548353, Accuracy 72.971%\n",
      "Epoch 7, Batch 440, LR 1.743827 Loss 8.546203, Accuracy 72.994%\n",
      "Epoch 7, Batch 441, LR 1.744104 Loss 8.545543, Accuracy 72.991%\n",
      "Epoch 7, Batch 442, LR 1.744380 Loss 8.545163, Accuracy 72.996%\n",
      "Epoch 7, Batch 443, LR 1.744657 Loss 8.545767, Accuracy 72.986%\n",
      "Epoch 7, Batch 444, LR 1.744934 Loss 8.546558, Accuracy 72.976%\n",
      "Epoch 7, Batch 445, LR 1.745210 Loss 8.546258, Accuracy 72.990%\n",
      "Epoch 7, Batch 446, LR 1.745487 Loss 8.545232, Accuracy 73.007%\n",
      "Epoch 7, Batch 447, LR 1.745763 Loss 8.544297, Accuracy 73.027%\n",
      "Epoch 7, Batch 448, LR 1.746040 Loss 8.542744, Accuracy 73.028%\n",
      "Epoch 7, Batch 449, LR 1.746316 Loss 8.543649, Accuracy 73.025%\n",
      "Epoch 7, Batch 450, LR 1.746593 Loss 8.541613, Accuracy 73.043%\n",
      "Epoch 7, Batch 451, LR 1.746869 Loss 8.542666, Accuracy 73.034%\n",
      "Epoch 7, Batch 452, LR 1.747145 Loss 8.542902, Accuracy 73.031%\n",
      "Epoch 7, Batch 453, LR 1.747422 Loss 8.542142, Accuracy 73.036%\n",
      "Epoch 7, Batch 454, LR 1.747698 Loss 8.543588, Accuracy 73.031%\n",
      "Epoch 7, Batch 455, LR 1.747974 Loss 8.544713, Accuracy 73.017%\n",
      "Epoch 7, Batch 456, LR 1.748250 Loss 8.542429, Accuracy 73.043%\n",
      "Epoch 7, Batch 457, LR 1.748527 Loss 8.543348, Accuracy 73.027%\n",
      "Epoch 7, Batch 458, LR 1.748803 Loss 8.542453, Accuracy 73.020%\n",
      "Epoch 7, Batch 459, LR 1.749079 Loss 8.542384, Accuracy 73.009%\n",
      "Epoch 7, Batch 460, LR 1.749355 Loss 8.542942, Accuracy 72.994%\n",
      "Epoch 7, Batch 461, LR 1.749631 Loss 8.541731, Accuracy 73.000%\n",
      "Epoch 7, Batch 462, LR 1.749907 Loss 8.541394, Accuracy 73.006%\n",
      "Epoch 7, Batch 463, LR 1.750183 Loss 8.540759, Accuracy 73.017%\n",
      "Epoch 7, Batch 464, LR 1.750460 Loss 8.541075, Accuracy 73.018%\n",
      "Epoch 7, Batch 465, LR 1.750736 Loss 8.541515, Accuracy 73.016%\n",
      "Epoch 7, Batch 466, LR 1.751012 Loss 8.541683, Accuracy 73.018%\n",
      "Epoch 7, Batch 467, LR 1.751287 Loss 8.541590, Accuracy 73.026%\n",
      "Epoch 7, Batch 468, LR 1.751563 Loss 8.540850, Accuracy 73.032%\n",
      "Epoch 7, Batch 469, LR 1.751839 Loss 8.541367, Accuracy 73.026%\n",
      "Epoch 7, Batch 470, LR 1.752115 Loss 8.540380, Accuracy 73.042%\n",
      "Epoch 7, Batch 471, LR 1.752391 Loss 8.541131, Accuracy 73.039%\n",
      "Epoch 7, Batch 472, LR 1.752667 Loss 8.542447, Accuracy 73.029%\n",
      "Epoch 7, Batch 473, LR 1.752943 Loss 8.542662, Accuracy 73.030%\n",
      "Epoch 7, Batch 474, LR 1.753218 Loss 8.543282, Accuracy 73.030%\n",
      "Epoch 7, Batch 475, LR 1.753494 Loss 8.543341, Accuracy 73.033%\n",
      "Epoch 7, Batch 476, LR 1.753770 Loss 8.544101, Accuracy 73.021%\n",
      "Epoch 7, Batch 477, LR 1.754045 Loss 8.545003, Accuracy 73.010%\n",
      "Epoch 7, Batch 478, LR 1.754321 Loss 8.544983, Accuracy 73.004%\n",
      "Epoch 7, Batch 479, LR 1.754597 Loss 8.544483, Accuracy 73.009%\n",
      "Epoch 7, Batch 480, LR 1.754872 Loss 8.544241, Accuracy 73.003%\n",
      "Epoch 7, Batch 481, LR 1.755148 Loss 8.545246, Accuracy 72.992%\n",
      "Epoch 7, Batch 482, LR 1.755423 Loss 8.545950, Accuracy 72.987%\n",
      "Epoch 7, Batch 483, LR 1.755699 Loss 8.545289, Accuracy 72.993%\n",
      "Epoch 7, Batch 484, LR 1.755974 Loss 8.545968, Accuracy 72.982%\n",
      "Epoch 7, Batch 485, LR 1.756250 Loss 8.545823, Accuracy 72.978%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 486, LR 1.756525 Loss 8.545927, Accuracy 72.978%\n",
      "Epoch 7, Batch 487, LR 1.756801 Loss 8.546360, Accuracy 72.979%\n",
      "Epoch 7, Batch 488, LR 1.757076 Loss 8.546658, Accuracy 72.980%\n",
      "Epoch 7, Batch 489, LR 1.757351 Loss 8.548512, Accuracy 72.974%\n",
      "Epoch 7, Batch 490, LR 1.757627 Loss 8.547403, Accuracy 72.986%\n",
      "Epoch 7, Batch 491, LR 1.757902 Loss 8.547655, Accuracy 72.984%\n",
      "Epoch 7, Batch 492, LR 1.758177 Loss 8.547393, Accuracy 72.988%\n",
      "Epoch 7, Batch 493, LR 1.758453 Loss 8.546892, Accuracy 72.984%\n",
      "Epoch 7, Batch 494, LR 1.758728 Loss 8.546239, Accuracy 72.984%\n",
      "Epoch 7, Batch 495, LR 1.759003 Loss 8.547788, Accuracy 72.959%\n",
      "Epoch 7, Batch 496, LR 1.759278 Loss 8.547697, Accuracy 72.960%\n",
      "Epoch 7, Batch 497, LR 1.759553 Loss 8.548874, Accuracy 72.952%\n",
      "Epoch 7, Batch 498, LR 1.759828 Loss 8.548176, Accuracy 72.954%\n",
      "Epoch 7, Batch 499, LR 1.760103 Loss 8.549487, Accuracy 72.936%\n",
      "Epoch 7, Batch 500, LR 1.760378 Loss 8.548814, Accuracy 72.955%\n",
      "Epoch 7, Batch 501, LR 1.760653 Loss 8.550589, Accuracy 72.938%\n",
      "Epoch 7, Batch 502, LR 1.760928 Loss 8.550117, Accuracy 72.947%\n",
      "Epoch 7, Batch 503, LR 1.761203 Loss 8.549987, Accuracy 72.958%\n",
      "Epoch 7, Batch 504, LR 1.761478 Loss 8.549331, Accuracy 72.962%\n",
      "Epoch 7, Batch 505, LR 1.761753 Loss 8.550258, Accuracy 72.959%\n",
      "Epoch 7, Batch 506, LR 1.762028 Loss 8.551079, Accuracy 72.950%\n",
      "Epoch 7, Batch 507, LR 1.762303 Loss 8.550623, Accuracy 72.947%\n",
      "Epoch 7, Batch 508, LR 1.762578 Loss 8.549594, Accuracy 72.955%\n",
      "Epoch 7, Batch 509, LR 1.762852 Loss 8.550167, Accuracy 72.951%\n",
      "Epoch 7, Batch 510, LR 1.763127 Loss 8.550513, Accuracy 72.949%\n",
      "Epoch 7, Batch 511, LR 1.763402 Loss 8.550177, Accuracy 72.942%\n",
      "Epoch 7, Batch 512, LR 1.763677 Loss 8.550006, Accuracy 72.932%\n",
      "Epoch 7, Batch 513, LR 1.763951 Loss 8.549123, Accuracy 72.929%\n",
      "Epoch 7, Batch 514, LR 1.764226 Loss 8.549471, Accuracy 72.930%\n",
      "Epoch 7, Batch 515, LR 1.764501 Loss 8.549720, Accuracy 72.925%\n",
      "Epoch 7, Batch 516, LR 1.764775 Loss 8.550433, Accuracy 72.927%\n",
      "Epoch 7, Batch 517, LR 1.765050 Loss 8.550566, Accuracy 72.921%\n",
      "Epoch 7, Batch 518, LR 1.765324 Loss 8.550979, Accuracy 72.920%\n",
      "Epoch 7, Batch 519, LR 1.765599 Loss 8.550494, Accuracy 72.926%\n",
      "Epoch 7, Batch 520, LR 1.765873 Loss 8.550868, Accuracy 72.921%\n",
      "Epoch 7, Batch 521, LR 1.766148 Loss 8.551693, Accuracy 72.904%\n",
      "Epoch 7, Batch 522, LR 1.766422 Loss 8.550927, Accuracy 72.917%\n",
      "Epoch 7, Batch 523, LR 1.766697 Loss 8.550220, Accuracy 72.918%\n",
      "Epoch 7, Batch 524, LR 1.766971 Loss 8.550766, Accuracy 72.917%\n",
      "Epoch 7, Batch 525, LR 1.767245 Loss 8.550380, Accuracy 72.905%\n",
      "Epoch 7, Batch 526, LR 1.767520 Loss 8.549362, Accuracy 72.913%\n",
      "Epoch 7, Batch 527, LR 1.767794 Loss 8.548752, Accuracy 72.920%\n",
      "Epoch 7, Batch 528, LR 1.768068 Loss 8.548382, Accuracy 72.918%\n",
      "Epoch 7, Batch 529, LR 1.768342 Loss 8.549190, Accuracy 72.913%\n",
      "Epoch 7, Batch 530, LR 1.768616 Loss 8.548940, Accuracy 72.913%\n",
      "Epoch 7, Batch 531, LR 1.768891 Loss 8.547816, Accuracy 72.918%\n",
      "Epoch 7, Batch 532, LR 1.769165 Loss 8.547167, Accuracy 72.919%\n",
      "Epoch 7, Batch 533, LR 1.769439 Loss 8.548805, Accuracy 72.911%\n",
      "Epoch 7, Batch 534, LR 1.769713 Loss 8.549277, Accuracy 72.914%\n",
      "Epoch 7, Batch 535, LR 1.769987 Loss 8.549993, Accuracy 72.915%\n",
      "Epoch 7, Batch 536, LR 1.770261 Loss 8.551355, Accuracy 72.895%\n",
      "Epoch 7, Batch 537, LR 1.770535 Loss 8.551442, Accuracy 72.899%\n",
      "Epoch 7, Batch 538, LR 1.770809 Loss 8.551704, Accuracy 72.903%\n",
      "Epoch 7, Batch 539, LR 1.771083 Loss 8.550786, Accuracy 72.904%\n",
      "Epoch 7, Batch 540, LR 1.771357 Loss 8.549944, Accuracy 72.915%\n",
      "Epoch 7, Batch 541, LR 1.771631 Loss 8.550874, Accuracy 72.908%\n",
      "Epoch 7, Batch 542, LR 1.771904 Loss 8.551127, Accuracy 72.910%\n",
      "Epoch 7, Batch 543, LR 1.772178 Loss 8.550395, Accuracy 72.911%\n",
      "Epoch 7, Batch 544, LR 1.772452 Loss 8.549121, Accuracy 72.916%\n",
      "Epoch 7, Batch 545, LR 1.772726 Loss 8.548854, Accuracy 72.920%\n",
      "Epoch 7, Batch 546, LR 1.773000 Loss 8.549028, Accuracy 72.911%\n",
      "Epoch 7, Batch 547, LR 1.773273 Loss 8.548548, Accuracy 72.910%\n",
      "Epoch 7, Batch 548, LR 1.773547 Loss 8.546721, Accuracy 72.924%\n",
      "Epoch 7, Batch 549, LR 1.773821 Loss 8.548444, Accuracy 72.920%\n",
      "Epoch 7, Batch 550, LR 1.774094 Loss 8.548803, Accuracy 72.920%\n",
      "Epoch 7, Batch 551, LR 1.774368 Loss 8.548773, Accuracy 72.923%\n",
      "Epoch 7, Batch 552, LR 1.774641 Loss 8.547832, Accuracy 72.935%\n",
      "Epoch 7, Batch 553, LR 1.774915 Loss 8.547191, Accuracy 72.944%\n",
      "Epoch 7, Batch 554, LR 1.775188 Loss 8.547142, Accuracy 72.948%\n",
      "Epoch 7, Batch 555, LR 1.775462 Loss 8.545919, Accuracy 72.959%\n",
      "Epoch 7, Batch 556, LR 1.775735 Loss 8.546719, Accuracy 72.957%\n",
      "Epoch 7, Batch 557, LR 1.776009 Loss 8.546823, Accuracy 72.956%\n",
      "Epoch 7, Batch 558, LR 1.776282 Loss 8.546160, Accuracy 72.963%\n",
      "Epoch 7, Batch 559, LR 1.776555 Loss 8.545250, Accuracy 72.975%\n",
      "Epoch 7, Batch 560, LR 1.776829 Loss 8.545131, Accuracy 72.977%\n",
      "Epoch 7, Batch 561, LR 1.777102 Loss 8.545527, Accuracy 72.978%\n",
      "Epoch 7, Batch 562, LR 1.777375 Loss 8.545492, Accuracy 72.976%\n",
      "Epoch 7, Batch 563, LR 1.777649 Loss 8.544757, Accuracy 72.988%\n",
      "Epoch 7, Batch 564, LR 1.777922 Loss 8.544446, Accuracy 72.989%\n",
      "Epoch 7, Batch 565, LR 1.778195 Loss 8.543774, Accuracy 72.991%\n",
      "Epoch 7, Batch 566, LR 1.778468 Loss 8.543529, Accuracy 72.996%\n",
      "Epoch 7, Batch 567, LR 1.778741 Loss 8.543917, Accuracy 72.994%\n",
      "Epoch 7, Batch 568, LR 1.779014 Loss 8.543680, Accuracy 72.999%\n",
      "Epoch 7, Batch 569, LR 1.779287 Loss 8.545174, Accuracy 72.994%\n",
      "Epoch 7, Batch 570, LR 1.779560 Loss 8.546328, Accuracy 72.985%\n",
      "Epoch 7, Batch 571, LR 1.779833 Loss 8.546560, Accuracy 72.981%\n",
      "Epoch 7, Batch 572, LR 1.780106 Loss 8.546059, Accuracy 72.981%\n",
      "Epoch 7, Batch 573, LR 1.780379 Loss 8.546214, Accuracy 72.983%\n",
      "Epoch 7, Batch 574, LR 1.780652 Loss 8.545179, Accuracy 72.988%\n",
      "Epoch 7, Batch 575, LR 1.780925 Loss 8.545344, Accuracy 72.988%\n",
      "Epoch 7, Batch 576, LR 1.781198 Loss 8.546082, Accuracy 72.987%\n",
      "Epoch 7, Batch 577, LR 1.781471 Loss 8.546572, Accuracy 72.985%\n",
      "Epoch 7, Batch 578, LR 1.781744 Loss 8.546094, Accuracy 72.991%\n",
      "Epoch 7, Batch 579, LR 1.782016 Loss 8.545613, Accuracy 72.987%\n",
      "Epoch 7, Batch 580, LR 1.782289 Loss 8.545789, Accuracy 72.981%\n",
      "Epoch 7, Batch 581, LR 1.782562 Loss 8.545001, Accuracy 72.987%\n",
      "Epoch 7, Batch 582, LR 1.782835 Loss 8.546092, Accuracy 72.972%\n",
      "Epoch 7, Batch 583, LR 1.783107 Loss 8.545187, Accuracy 72.983%\n",
      "Epoch 7, Batch 584, LR 1.783380 Loss 8.546109, Accuracy 72.973%\n",
      "Epoch 7, Batch 585, LR 1.783652 Loss 8.545440, Accuracy 72.967%\n",
      "Epoch 7, Batch 586, LR 1.783925 Loss 8.545402, Accuracy 72.967%\n",
      "Epoch 7, Batch 587, LR 1.784198 Loss 8.546751, Accuracy 72.958%\n",
      "Epoch 7, Batch 588, LR 1.784470 Loss 8.546122, Accuracy 72.961%\n",
      "Epoch 7, Batch 589, LR 1.784743 Loss 8.546488, Accuracy 72.961%\n",
      "Epoch 7, Batch 590, LR 1.785015 Loss 8.546213, Accuracy 72.967%\n",
      "Epoch 7, Batch 591, LR 1.785287 Loss 8.546613, Accuracy 72.972%\n",
      "Epoch 7, Batch 592, LR 1.785560 Loss 8.547157, Accuracy 72.965%\n",
      "Epoch 7, Batch 593, LR 1.785832 Loss 8.547351, Accuracy 72.967%\n",
      "Epoch 7, Batch 594, LR 1.786105 Loss 8.546592, Accuracy 72.964%\n",
      "Epoch 7, Batch 595, LR 1.786377 Loss 8.545327, Accuracy 72.977%\n",
      "Epoch 7, Batch 596, LR 1.786649 Loss 8.544288, Accuracy 72.983%\n",
      "Epoch 7, Batch 597, LR 1.786921 Loss 8.543559, Accuracy 72.987%\n",
      "Epoch 7, Batch 598, LR 1.787194 Loss 8.542983, Accuracy 72.989%\n",
      "Epoch 7, Batch 599, LR 1.787466 Loss 8.542982, Accuracy 72.985%\n",
      "Epoch 7, Batch 600, LR 1.787738 Loss 8.543690, Accuracy 72.978%\n",
      "Epoch 7, Batch 601, LR 1.788010 Loss 8.542455, Accuracy 72.984%\n",
      "Epoch 7, Batch 602, LR 1.788282 Loss 8.543401, Accuracy 72.974%\n",
      "Epoch 7, Batch 603, LR 1.788554 Loss 8.543154, Accuracy 72.979%\n",
      "Epoch 7, Batch 604, LR 1.788826 Loss 8.543239, Accuracy 72.982%\n",
      "Epoch 7, Batch 605, LR 1.789098 Loss 8.542715, Accuracy 72.986%\n",
      "Epoch 7, Batch 606, LR 1.789370 Loss 8.542633, Accuracy 72.982%\n",
      "Epoch 7, Batch 607, LR 1.789642 Loss 8.542815, Accuracy 72.981%\n",
      "Epoch 7, Batch 608, LR 1.789914 Loss 8.541671, Accuracy 72.986%\n",
      "Epoch 7, Batch 609, LR 1.790186 Loss 8.542075, Accuracy 72.986%\n",
      "Epoch 7, Batch 610, LR 1.790458 Loss 8.543319, Accuracy 72.982%\n",
      "Epoch 7, Batch 611, LR 1.790730 Loss 8.543991, Accuracy 72.975%\n",
      "Epoch 7, Batch 612, LR 1.791001 Loss 8.544403, Accuracy 72.972%\n",
      "Epoch 7, Batch 613, LR 1.791273 Loss 8.543901, Accuracy 72.977%\n",
      "Epoch 7, Batch 614, LR 1.791545 Loss 8.544249, Accuracy 72.981%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 615, LR 1.791817 Loss 8.544152, Accuracy 72.975%\n",
      "Epoch 7, Batch 616, LR 1.792088 Loss 8.543185, Accuracy 72.983%\n",
      "Epoch 7, Batch 617, LR 1.792360 Loss 8.542778, Accuracy 72.985%\n",
      "Epoch 7, Batch 618, LR 1.792632 Loss 8.541547, Accuracy 72.991%\n",
      "Epoch 7, Batch 619, LR 1.792903 Loss 8.542000, Accuracy 72.982%\n",
      "Epoch 7, Batch 620, LR 1.793175 Loss 8.542042, Accuracy 72.985%\n",
      "Epoch 7, Batch 621, LR 1.793446 Loss 8.542168, Accuracy 72.986%\n",
      "Epoch 7, Batch 622, LR 1.793718 Loss 8.542615, Accuracy 72.985%\n",
      "Epoch 7, Batch 623, LR 1.793989 Loss 8.543161, Accuracy 72.981%\n",
      "Epoch 7, Batch 624, LR 1.794261 Loss 8.543015, Accuracy 72.984%\n",
      "Epoch 7, Batch 625, LR 1.794532 Loss 8.542257, Accuracy 72.990%\n",
      "Epoch 7, Batch 626, LR 1.794803 Loss 8.544047, Accuracy 72.977%\n",
      "Epoch 7, Batch 627, LR 1.795075 Loss 8.544268, Accuracy 72.975%\n",
      "Epoch 7, Batch 628, LR 1.795346 Loss 8.545703, Accuracy 72.969%\n",
      "Epoch 7, Batch 629, LR 1.795617 Loss 8.546564, Accuracy 72.964%\n",
      "Epoch 7, Batch 630, LR 1.795889 Loss 8.546887, Accuracy 72.963%\n",
      "Epoch 7, Batch 631, LR 1.796160 Loss 8.545464, Accuracy 72.976%\n",
      "Epoch 7, Batch 632, LR 1.796431 Loss 8.545732, Accuracy 72.978%\n",
      "Epoch 7, Batch 633, LR 1.796702 Loss 8.545533, Accuracy 72.981%\n",
      "Epoch 7, Batch 634, LR 1.796973 Loss 8.545758, Accuracy 72.982%\n",
      "Epoch 7, Batch 635, LR 1.797244 Loss 8.546205, Accuracy 72.980%\n",
      "Epoch 7, Batch 636, LR 1.797515 Loss 8.546636, Accuracy 72.971%\n",
      "Epoch 7, Batch 637, LR 1.797786 Loss 8.547638, Accuracy 72.964%\n",
      "Epoch 7, Batch 638, LR 1.798058 Loss 8.548183, Accuracy 72.967%\n",
      "Epoch 7, Batch 639, LR 1.798328 Loss 8.548494, Accuracy 72.969%\n",
      "Epoch 7, Batch 640, LR 1.798599 Loss 8.547673, Accuracy 72.976%\n",
      "Epoch 7, Batch 641, LR 1.798870 Loss 8.546803, Accuracy 72.979%\n",
      "Epoch 7, Batch 642, LR 1.799141 Loss 8.547150, Accuracy 72.978%\n",
      "Epoch 7, Batch 643, LR 1.799412 Loss 8.546771, Accuracy 72.982%\n",
      "Epoch 7, Batch 644, LR 1.799683 Loss 8.545641, Accuracy 72.987%\n",
      "Epoch 7, Batch 645, LR 1.799954 Loss 8.546048, Accuracy 72.983%\n",
      "Epoch 7, Batch 646, LR 1.800225 Loss 8.546314, Accuracy 72.978%\n",
      "Epoch 7, Batch 647, LR 1.800495 Loss 8.547771, Accuracy 72.971%\n",
      "Epoch 7, Batch 648, LR 1.800766 Loss 8.548595, Accuracy 72.972%\n",
      "Epoch 7, Batch 649, LR 1.801037 Loss 8.548661, Accuracy 72.975%\n",
      "Epoch 7, Batch 650, LR 1.801307 Loss 8.547949, Accuracy 72.984%\n",
      "Epoch 7, Batch 651, LR 1.801578 Loss 8.548010, Accuracy 72.989%\n",
      "Epoch 7, Batch 652, LR 1.801848 Loss 8.548301, Accuracy 72.988%\n",
      "Epoch 7, Batch 653, LR 1.802119 Loss 8.547385, Accuracy 72.997%\n",
      "Epoch 7, Batch 654, LR 1.802390 Loss 8.547890, Accuracy 73.001%\n",
      "Epoch 7, Batch 655, LR 1.802660 Loss 8.547748, Accuracy 72.996%\n",
      "Epoch 7, Batch 656, LR 1.802930 Loss 8.547468, Accuracy 73.008%\n",
      "Epoch 7, Batch 657, LR 1.803201 Loss 8.546808, Accuracy 73.013%\n",
      "Epoch 7, Batch 658, LR 1.803471 Loss 8.546453, Accuracy 73.022%\n",
      "Epoch 7, Batch 659, LR 1.803742 Loss 8.546677, Accuracy 73.019%\n",
      "Epoch 7, Batch 660, LR 1.804012 Loss 8.545845, Accuracy 73.017%\n",
      "Epoch 7, Batch 661, LR 1.804282 Loss 8.544944, Accuracy 73.027%\n",
      "Epoch 7, Batch 662, LR 1.804553 Loss 8.544943, Accuracy 73.030%\n",
      "Epoch 7, Batch 663, LR 1.804823 Loss 8.544840, Accuracy 73.029%\n",
      "Epoch 7, Batch 664, LR 1.805093 Loss 8.544342, Accuracy 73.032%\n",
      "Epoch 7, Batch 665, LR 1.805363 Loss 8.544678, Accuracy 73.033%\n",
      "Epoch 7, Batch 666, LR 1.805633 Loss 8.543513, Accuracy 73.045%\n",
      "Epoch 7, Batch 667, LR 1.805904 Loss 8.542285, Accuracy 73.052%\n",
      "Epoch 7, Batch 668, LR 1.806174 Loss 8.541398, Accuracy 73.063%\n",
      "Epoch 7, Batch 669, LR 1.806444 Loss 8.541546, Accuracy 73.058%\n",
      "Epoch 7, Batch 670, LR 1.806714 Loss 8.541270, Accuracy 73.057%\n",
      "Epoch 7, Batch 671, LR 1.806984 Loss 8.541934, Accuracy 73.051%\n",
      "Epoch 7, Batch 672, LR 1.807254 Loss 8.543457, Accuracy 73.048%\n",
      "Epoch 7, Batch 673, LR 1.807524 Loss 8.544193, Accuracy 73.043%\n",
      "Epoch 7, Batch 674, LR 1.807794 Loss 8.544400, Accuracy 73.042%\n",
      "Epoch 7, Batch 675, LR 1.808063 Loss 8.544831, Accuracy 73.038%\n",
      "Epoch 7, Batch 676, LR 1.808333 Loss 8.545201, Accuracy 73.041%\n",
      "Epoch 7, Batch 677, LR 1.808603 Loss 8.545765, Accuracy 73.035%\n",
      "Epoch 7, Batch 678, LR 1.808873 Loss 8.545861, Accuracy 73.035%\n",
      "Epoch 7, Batch 679, LR 1.809143 Loss 8.546038, Accuracy 73.037%\n",
      "Epoch 7, Batch 680, LR 1.809412 Loss 8.545426, Accuracy 73.038%\n",
      "Epoch 7, Batch 681, LR 1.809682 Loss 8.545170, Accuracy 73.035%\n",
      "Epoch 7, Batch 682, LR 1.809952 Loss 8.545396, Accuracy 73.034%\n",
      "Epoch 7, Batch 683, LR 1.810221 Loss 8.545293, Accuracy 73.041%\n",
      "Epoch 7, Batch 684, LR 1.810491 Loss 8.544654, Accuracy 73.042%\n",
      "Epoch 7, Batch 685, LR 1.810760 Loss 8.545041, Accuracy 73.042%\n",
      "Epoch 7, Batch 686, LR 1.811030 Loss 8.544811, Accuracy 73.039%\n",
      "Epoch 7, Batch 687, LR 1.811300 Loss 8.545052, Accuracy 73.030%\n",
      "Epoch 7, Batch 688, LR 1.811569 Loss 8.544992, Accuracy 73.030%\n",
      "Epoch 7, Batch 689, LR 1.811838 Loss 8.545784, Accuracy 73.029%\n",
      "Epoch 7, Batch 690, LR 1.812108 Loss 8.547128, Accuracy 73.021%\n",
      "Epoch 7, Batch 691, LR 1.812377 Loss 8.548508, Accuracy 73.009%\n",
      "Epoch 7, Batch 692, LR 1.812647 Loss 8.549269, Accuracy 73.003%\n",
      "Epoch 7, Batch 693, LR 1.812916 Loss 8.549806, Accuracy 72.999%\n",
      "Epoch 7, Batch 694, LR 1.813185 Loss 8.548235, Accuracy 73.012%\n",
      "Epoch 7, Batch 695, LR 1.813454 Loss 8.549055, Accuracy 73.010%\n",
      "Epoch 7, Batch 696, LR 1.813724 Loss 8.548134, Accuracy 73.015%\n",
      "Epoch 7, Batch 697, LR 1.813993 Loss 8.547868, Accuracy 73.024%\n",
      "Epoch 7, Batch 698, LR 1.814262 Loss 8.547277, Accuracy 73.030%\n",
      "Epoch 7, Batch 699, LR 1.814531 Loss 8.546957, Accuracy 73.032%\n",
      "Epoch 7, Batch 700, LR 1.814800 Loss 8.546188, Accuracy 73.033%\n",
      "Epoch 7, Batch 701, LR 1.815069 Loss 8.545415, Accuracy 73.041%\n",
      "Epoch 7, Batch 702, LR 1.815338 Loss 8.545617, Accuracy 73.039%\n",
      "Epoch 7, Batch 703, LR 1.815607 Loss 8.545690, Accuracy 73.041%\n",
      "Epoch 7, Batch 704, LR 1.815876 Loss 8.545183, Accuracy 73.040%\n",
      "Epoch 7, Batch 705, LR 1.816145 Loss 8.545396, Accuracy 73.036%\n",
      "Epoch 7, Batch 706, LR 1.816414 Loss 8.545600, Accuracy 73.030%\n",
      "Epoch 7, Batch 707, LR 1.816683 Loss 8.544432, Accuracy 73.036%\n",
      "Epoch 7, Batch 708, LR 1.816952 Loss 8.543979, Accuracy 73.040%\n",
      "Epoch 7, Batch 709, LR 1.817221 Loss 8.543234, Accuracy 73.044%\n",
      "Epoch 7, Batch 710, LR 1.817489 Loss 8.543166, Accuracy 73.050%\n",
      "Epoch 7, Batch 711, LR 1.817758 Loss 8.543006, Accuracy 73.057%\n",
      "Epoch 7, Batch 712, LR 1.818027 Loss 8.542741, Accuracy 73.056%\n",
      "Epoch 7, Batch 713, LR 1.818296 Loss 8.542377, Accuracy 73.061%\n",
      "Epoch 7, Batch 714, LR 1.818564 Loss 8.542138, Accuracy 73.060%\n",
      "Epoch 7, Batch 715, LR 1.818833 Loss 8.542117, Accuracy 73.062%\n",
      "Epoch 7, Batch 716, LR 1.819101 Loss 8.541885, Accuracy 73.065%\n",
      "Epoch 7, Batch 717, LR 1.819370 Loss 8.541253, Accuracy 73.075%\n",
      "Epoch 7, Batch 718, LR 1.819639 Loss 8.541330, Accuracy 73.075%\n",
      "Epoch 7, Batch 719, LR 1.819907 Loss 8.542031, Accuracy 73.070%\n",
      "Epoch 7, Batch 720, LR 1.820176 Loss 8.541566, Accuracy 73.070%\n",
      "Epoch 7, Batch 721, LR 1.820444 Loss 8.541328, Accuracy 73.073%\n",
      "Epoch 7, Batch 722, LR 1.820712 Loss 8.541744, Accuracy 73.071%\n",
      "Epoch 7, Batch 723, LR 1.820981 Loss 8.542517, Accuracy 73.066%\n",
      "Epoch 7, Batch 724, LR 1.821249 Loss 8.544262, Accuracy 73.053%\n",
      "Epoch 7, Batch 725, LR 1.821517 Loss 8.544129, Accuracy 73.051%\n",
      "Epoch 7, Batch 726, LR 1.821786 Loss 8.543454, Accuracy 73.051%\n",
      "Epoch 7, Batch 727, LR 1.822054 Loss 8.542581, Accuracy 73.050%\n",
      "Epoch 7, Batch 728, LR 1.822322 Loss 8.541878, Accuracy 73.054%\n",
      "Epoch 7, Batch 729, LR 1.822590 Loss 8.541392, Accuracy 73.056%\n",
      "Epoch 7, Batch 730, LR 1.822858 Loss 8.542508, Accuracy 73.054%\n",
      "Epoch 7, Batch 731, LR 1.823127 Loss 8.541790, Accuracy 73.051%\n",
      "Epoch 7, Batch 732, LR 1.823395 Loss 8.541411, Accuracy 73.050%\n",
      "Epoch 7, Batch 733, LR 1.823663 Loss 8.540691, Accuracy 73.048%\n",
      "Epoch 7, Batch 734, LR 1.823931 Loss 8.540438, Accuracy 73.053%\n",
      "Epoch 7, Batch 735, LR 1.824199 Loss 8.541064, Accuracy 73.048%\n",
      "Epoch 7, Batch 736, LR 1.824467 Loss 8.541520, Accuracy 73.046%\n",
      "Epoch 7, Batch 737, LR 1.824735 Loss 8.541540, Accuracy 73.054%\n",
      "Epoch 7, Batch 738, LR 1.825002 Loss 8.541293, Accuracy 73.053%\n",
      "Epoch 7, Batch 739, LR 1.825270 Loss 8.540373, Accuracy 73.058%\n",
      "Epoch 7, Batch 740, LR 1.825538 Loss 8.540546, Accuracy 73.056%\n",
      "Epoch 7, Batch 741, LR 1.825806 Loss 8.541146, Accuracy 73.052%\n",
      "Epoch 7, Batch 742, LR 1.826074 Loss 8.541719, Accuracy 73.055%\n",
      "Epoch 7, Batch 743, LR 1.826341 Loss 8.541873, Accuracy 73.054%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 744, LR 1.826609 Loss 8.541435, Accuracy 73.055%\n",
      "Epoch 7, Batch 745, LR 1.826877 Loss 8.540717, Accuracy 73.062%\n",
      "Epoch 7, Batch 746, LR 1.827144 Loss 8.540039, Accuracy 73.064%\n",
      "Epoch 7, Batch 747, LR 1.827412 Loss 8.538008, Accuracy 73.070%\n",
      "Epoch 7, Batch 748, LR 1.827680 Loss 8.538398, Accuracy 73.069%\n",
      "Epoch 7, Batch 749, LR 1.827947 Loss 8.538697, Accuracy 73.068%\n",
      "Epoch 7, Batch 750, LR 1.828215 Loss 8.538059, Accuracy 73.069%\n",
      "Epoch 7, Batch 751, LR 1.828482 Loss 8.537377, Accuracy 73.070%\n",
      "Epoch 7, Batch 752, LR 1.828750 Loss 8.538258, Accuracy 73.066%\n",
      "Epoch 7, Batch 753, LR 1.829017 Loss 8.538407, Accuracy 73.071%\n",
      "Epoch 7, Batch 754, LR 1.829284 Loss 8.537406, Accuracy 73.075%\n",
      "Epoch 7, Batch 755, LR 1.829552 Loss 8.537732, Accuracy 73.068%\n",
      "Epoch 7, Batch 756, LR 1.829819 Loss 8.536989, Accuracy 73.073%\n",
      "Epoch 7, Batch 757, LR 1.830086 Loss 8.536907, Accuracy 73.071%\n",
      "Epoch 7, Batch 758, LR 1.830354 Loss 8.535896, Accuracy 73.080%\n",
      "Epoch 7, Batch 759, LR 1.830621 Loss 8.535999, Accuracy 73.081%\n",
      "Epoch 7, Batch 760, LR 1.830888 Loss 8.535786, Accuracy 73.081%\n",
      "Epoch 7, Batch 761, LR 1.831155 Loss 8.535824, Accuracy 73.079%\n",
      "Epoch 7, Batch 762, LR 1.831422 Loss 8.535993, Accuracy 73.076%\n",
      "Epoch 7, Batch 763, LR 1.831689 Loss 8.535105, Accuracy 73.082%\n",
      "Epoch 7, Batch 764, LR 1.831956 Loss 8.534869, Accuracy 73.081%\n",
      "Epoch 7, Batch 765, LR 1.832223 Loss 8.534558, Accuracy 73.082%\n",
      "Epoch 7, Batch 766, LR 1.832490 Loss 8.535007, Accuracy 73.078%\n",
      "Epoch 7, Batch 767, LR 1.832757 Loss 8.535283, Accuracy 73.076%\n",
      "Epoch 7, Batch 768, LR 1.833024 Loss 8.535382, Accuracy 73.074%\n",
      "Epoch 7, Batch 769, LR 1.833291 Loss 8.535425, Accuracy 73.073%\n",
      "Epoch 7, Batch 770, LR 1.833558 Loss 8.535356, Accuracy 73.070%\n",
      "Epoch 7, Batch 771, LR 1.833825 Loss 8.535984, Accuracy 73.063%\n",
      "Epoch 7, Batch 772, LR 1.834092 Loss 8.535333, Accuracy 73.065%\n",
      "Epoch 7, Batch 773, LR 1.834358 Loss 8.535477, Accuracy 73.061%\n",
      "Epoch 7, Batch 774, LR 1.834625 Loss 8.534886, Accuracy 73.059%\n",
      "Epoch 7, Batch 775, LR 1.834892 Loss 8.534352, Accuracy 73.053%\n",
      "Epoch 7, Batch 776, LR 1.835158 Loss 8.534080, Accuracy 73.055%\n",
      "Epoch 7, Batch 777, LR 1.835425 Loss 8.533088, Accuracy 73.061%\n",
      "Epoch 7, Batch 778, LR 1.835692 Loss 8.533276, Accuracy 73.061%\n",
      "Epoch 7, Batch 779, LR 1.835958 Loss 8.533179, Accuracy 73.062%\n",
      "Epoch 7, Batch 780, LR 1.836225 Loss 8.532831, Accuracy 73.064%\n",
      "Epoch 7, Batch 781, LR 1.836491 Loss 8.532934, Accuracy 73.060%\n",
      "Epoch 7, Batch 782, LR 1.836758 Loss 8.532971, Accuracy 73.063%\n",
      "Epoch 7, Batch 783, LR 1.837024 Loss 8.532502, Accuracy 73.070%\n",
      "Epoch 7, Batch 784, LR 1.837291 Loss 8.533450, Accuracy 73.066%\n",
      "Epoch 7, Batch 785, LR 1.837557 Loss 8.533628, Accuracy 73.066%\n",
      "Epoch 7, Batch 786, LR 1.837823 Loss 8.533805, Accuracy 73.065%\n",
      "Epoch 7, Batch 787, LR 1.838090 Loss 8.533606, Accuracy 73.067%\n",
      "Epoch 7, Batch 788, LR 1.838356 Loss 8.533177, Accuracy 73.075%\n",
      "Epoch 7, Batch 789, LR 1.838622 Loss 8.532739, Accuracy 73.077%\n",
      "Epoch 7, Batch 790, LR 1.838888 Loss 8.533040, Accuracy 73.076%\n",
      "Epoch 7, Batch 791, LR 1.839154 Loss 8.532462, Accuracy 73.083%\n",
      "Epoch 7, Batch 792, LR 1.839421 Loss 8.531914, Accuracy 73.091%\n",
      "Epoch 7, Batch 793, LR 1.839687 Loss 8.531851, Accuracy 73.090%\n",
      "Epoch 7, Batch 794, LR 1.839953 Loss 8.531449, Accuracy 73.095%\n",
      "Epoch 7, Batch 795, LR 1.840219 Loss 8.531365, Accuracy 73.098%\n",
      "Epoch 7, Batch 796, LR 1.840485 Loss 8.531006, Accuracy 73.099%\n",
      "Epoch 7, Batch 797, LR 1.840751 Loss 8.531619, Accuracy 73.087%\n",
      "Epoch 7, Batch 798, LR 1.841017 Loss 8.531657, Accuracy 73.091%\n",
      "Epoch 7, Batch 799, LR 1.841283 Loss 8.531790, Accuracy 73.088%\n",
      "Epoch 7, Batch 800, LR 1.841548 Loss 8.531564, Accuracy 73.082%\n",
      "Epoch 7, Batch 801, LR 1.841814 Loss 8.531672, Accuracy 73.082%\n",
      "Epoch 7, Batch 802, LR 1.842080 Loss 8.532453, Accuracy 73.081%\n",
      "Epoch 7, Batch 803, LR 1.842346 Loss 8.532550, Accuracy 73.078%\n",
      "Epoch 7, Batch 804, LR 1.842612 Loss 8.531633, Accuracy 73.082%\n",
      "Epoch 7, Batch 805, LR 1.842877 Loss 8.531360, Accuracy 73.086%\n",
      "Epoch 7, Batch 806, LR 1.843143 Loss 8.531047, Accuracy 73.088%\n",
      "Epoch 7, Batch 807, LR 1.843409 Loss 8.531538, Accuracy 73.084%\n",
      "Epoch 7, Batch 808, LR 1.843674 Loss 8.531469, Accuracy 73.088%\n",
      "Epoch 7, Batch 809, LR 1.843940 Loss 8.531816, Accuracy 73.084%\n",
      "Epoch 7, Batch 810, LR 1.844205 Loss 8.531406, Accuracy 73.081%\n",
      "Epoch 7, Batch 811, LR 1.844471 Loss 8.531700, Accuracy 73.085%\n",
      "Epoch 7, Batch 812, LR 1.844736 Loss 8.531278, Accuracy 73.086%\n",
      "Epoch 7, Batch 813, LR 1.845002 Loss 8.531424, Accuracy 73.085%\n",
      "Epoch 7, Batch 814, LR 1.845267 Loss 8.531516, Accuracy 73.081%\n",
      "Epoch 7, Batch 815, LR 1.845533 Loss 8.531520, Accuracy 73.082%\n",
      "Epoch 7, Batch 816, LR 1.845798 Loss 8.531594, Accuracy 73.078%\n",
      "Epoch 7, Batch 817, LR 1.846063 Loss 8.531031, Accuracy 73.084%\n",
      "Epoch 7, Batch 818, LR 1.846328 Loss 8.530639, Accuracy 73.086%\n",
      "Epoch 7, Batch 819, LR 1.846594 Loss 8.530686, Accuracy 73.086%\n",
      "Epoch 7, Batch 820, LR 1.846859 Loss 8.530452, Accuracy 73.088%\n",
      "Epoch 7, Batch 821, LR 1.847124 Loss 8.529930, Accuracy 73.091%\n",
      "Epoch 7, Batch 822, LR 1.847389 Loss 8.530054, Accuracy 73.095%\n",
      "Epoch 7, Batch 823, LR 1.847654 Loss 8.529713, Accuracy 73.096%\n",
      "Epoch 7, Batch 824, LR 1.847919 Loss 8.530116, Accuracy 73.092%\n",
      "Epoch 7, Batch 825, LR 1.848184 Loss 8.529522, Accuracy 73.099%\n",
      "Epoch 7, Batch 826, LR 1.848449 Loss 8.529803, Accuracy 73.096%\n",
      "Epoch 7, Batch 827, LR 1.848714 Loss 8.529200, Accuracy 73.090%\n",
      "Epoch 7, Batch 828, LR 1.848979 Loss 8.529042, Accuracy 73.091%\n",
      "Epoch 7, Batch 829, LR 1.849244 Loss 8.529095, Accuracy 73.094%\n",
      "Epoch 7, Batch 830, LR 1.849509 Loss 8.529043, Accuracy 73.097%\n",
      "Epoch 7, Batch 831, LR 1.849774 Loss 8.529414, Accuracy 73.095%\n",
      "Epoch 7, Batch 832, LR 1.850039 Loss 8.529589, Accuracy 73.091%\n",
      "Epoch 7, Batch 833, LR 1.850304 Loss 8.530242, Accuracy 73.081%\n",
      "Epoch 7, Batch 834, LR 1.850568 Loss 8.529518, Accuracy 73.088%\n",
      "Epoch 7, Batch 835, LR 1.850833 Loss 8.529301, Accuracy 73.095%\n",
      "Epoch 7, Batch 836, LR 1.851098 Loss 8.528788, Accuracy 73.095%\n",
      "Epoch 7, Batch 837, LR 1.851362 Loss 8.528193, Accuracy 73.101%\n",
      "Epoch 7, Batch 838, LR 1.851627 Loss 8.528041, Accuracy 73.102%\n",
      "Epoch 7, Batch 839, LR 1.851892 Loss 8.527946, Accuracy 73.105%\n",
      "Epoch 7, Batch 840, LR 1.852156 Loss 8.528152, Accuracy 73.105%\n",
      "Epoch 7, Batch 841, LR 1.852421 Loss 8.530386, Accuracy 73.089%\n",
      "Epoch 7, Batch 842, LR 1.852685 Loss 8.530157, Accuracy 73.093%\n",
      "Epoch 7, Batch 843, LR 1.852949 Loss 8.529742, Accuracy 73.096%\n",
      "Epoch 7, Batch 844, LR 1.853214 Loss 8.529420, Accuracy 73.092%\n",
      "Epoch 7, Batch 845, LR 1.853478 Loss 8.529426, Accuracy 73.089%\n",
      "Epoch 7, Batch 846, LR 1.853743 Loss 8.529384, Accuracy 73.095%\n",
      "Epoch 7, Batch 847, LR 1.854007 Loss 8.529916, Accuracy 73.093%\n",
      "Epoch 7, Batch 848, LR 1.854271 Loss 8.529474, Accuracy 73.094%\n",
      "Epoch 7, Batch 849, LR 1.854535 Loss 8.528831, Accuracy 73.097%\n",
      "Epoch 7, Batch 850, LR 1.854800 Loss 8.528212, Accuracy 73.103%\n",
      "Epoch 7, Batch 851, LR 1.855064 Loss 8.527502, Accuracy 73.106%\n",
      "Epoch 7, Batch 852, LR 1.855328 Loss 8.527147, Accuracy 73.106%\n",
      "Epoch 7, Batch 853, LR 1.855592 Loss 8.526964, Accuracy 73.108%\n",
      "Epoch 7, Batch 854, LR 1.855856 Loss 8.527103, Accuracy 73.112%\n",
      "Epoch 7, Batch 855, LR 1.856120 Loss 8.527303, Accuracy 73.109%\n",
      "Epoch 7, Batch 856, LR 1.856384 Loss 8.528018, Accuracy 73.101%\n",
      "Epoch 7, Batch 857, LR 1.856648 Loss 8.527884, Accuracy 73.108%\n",
      "Epoch 7, Batch 858, LR 1.856912 Loss 8.527865, Accuracy 73.108%\n",
      "Epoch 7, Batch 859, LR 1.857176 Loss 8.526812, Accuracy 73.115%\n",
      "Epoch 7, Batch 860, LR 1.857440 Loss 8.525865, Accuracy 73.117%\n",
      "Epoch 7, Batch 861, LR 1.857703 Loss 8.525354, Accuracy 73.122%\n",
      "Epoch 7, Batch 862, LR 1.857967 Loss 8.524728, Accuracy 73.128%\n",
      "Epoch 7, Batch 863, LR 1.858231 Loss 8.524354, Accuracy 73.131%\n",
      "Epoch 7, Batch 864, LR 1.858495 Loss 8.523554, Accuracy 73.139%\n",
      "Epoch 7, Batch 865, LR 1.858758 Loss 8.523446, Accuracy 73.143%\n",
      "Epoch 7, Batch 866, LR 1.859022 Loss 8.523062, Accuracy 73.143%\n",
      "Epoch 7, Batch 867, LR 1.859286 Loss 8.523345, Accuracy 73.137%\n",
      "Epoch 7, Batch 868, LR 1.859549 Loss 8.522690, Accuracy 73.142%\n",
      "Epoch 7, Batch 869, LR 1.859813 Loss 8.523198, Accuracy 73.142%\n",
      "Epoch 7, Batch 870, LR 1.860076 Loss 8.523352, Accuracy 73.140%\n",
      "Epoch 7, Batch 871, LR 1.860340 Loss 8.522899, Accuracy 73.143%\n",
      "Epoch 7, Batch 872, LR 1.860603 Loss 8.523578, Accuracy 73.143%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 873, LR 1.860867 Loss 8.524005, Accuracy 73.142%\n",
      "Epoch 7, Batch 874, LR 1.861130 Loss 8.525056, Accuracy 73.131%\n",
      "Epoch 7, Batch 875, LR 1.861393 Loss 8.525713, Accuracy 73.127%\n",
      "Epoch 7, Batch 876, LR 1.861657 Loss 8.525853, Accuracy 73.120%\n",
      "Epoch 7, Batch 877, LR 1.861920 Loss 8.526015, Accuracy 73.120%\n",
      "Epoch 7, Batch 878, LR 1.862183 Loss 8.525522, Accuracy 73.123%\n",
      "Epoch 7, Batch 879, LR 1.862446 Loss 8.524945, Accuracy 73.126%\n",
      "Epoch 7, Batch 880, LR 1.862710 Loss 8.524903, Accuracy 73.129%\n",
      "Epoch 7, Batch 881, LR 1.862973 Loss 8.525430, Accuracy 73.127%\n",
      "Epoch 7, Batch 882, LR 1.863236 Loss 8.524974, Accuracy 73.135%\n",
      "Epoch 7, Batch 883, LR 1.863499 Loss 8.524128, Accuracy 73.143%\n",
      "Epoch 7, Batch 884, LR 1.863762 Loss 8.523680, Accuracy 73.146%\n",
      "Epoch 7, Batch 885, LR 1.864025 Loss 8.522484, Accuracy 73.151%\n",
      "Epoch 7, Batch 886, LR 1.864288 Loss 8.522878, Accuracy 73.149%\n",
      "Epoch 7, Batch 887, LR 1.864551 Loss 8.522572, Accuracy 73.152%\n",
      "Epoch 7, Batch 888, LR 1.864814 Loss 8.522935, Accuracy 73.149%\n",
      "Epoch 7, Batch 889, LR 1.865077 Loss 8.523478, Accuracy 73.148%\n",
      "Epoch 7, Batch 890, LR 1.865339 Loss 8.523673, Accuracy 73.146%\n",
      "Epoch 7, Batch 891, LR 1.865602 Loss 8.523960, Accuracy 73.146%\n",
      "Epoch 7, Batch 892, LR 1.865865 Loss 8.524020, Accuracy 73.145%\n",
      "Epoch 7, Batch 893, LR 1.866128 Loss 8.523218, Accuracy 73.151%\n",
      "Epoch 7, Batch 894, LR 1.866390 Loss 8.522801, Accuracy 73.153%\n",
      "Epoch 7, Batch 895, LR 1.866653 Loss 8.522866, Accuracy 73.154%\n",
      "Epoch 7, Batch 896, LR 1.866916 Loss 8.522459, Accuracy 73.159%\n",
      "Epoch 7, Batch 897, LR 1.867178 Loss 8.522268, Accuracy 73.161%\n",
      "Epoch 7, Batch 898, LR 1.867441 Loss 8.521299, Accuracy 73.169%\n",
      "Epoch 7, Batch 899, LR 1.867703 Loss 8.521101, Accuracy 73.170%\n",
      "Epoch 7, Batch 900, LR 1.867966 Loss 8.519936, Accuracy 73.171%\n",
      "Epoch 7, Batch 901, LR 1.868228 Loss 8.518784, Accuracy 73.180%\n",
      "Epoch 7, Batch 902, LR 1.868491 Loss 8.518798, Accuracy 73.180%\n",
      "Epoch 7, Batch 903, LR 1.868753 Loss 8.518306, Accuracy 73.182%\n",
      "Epoch 7, Batch 904, LR 1.869015 Loss 8.517584, Accuracy 73.186%\n",
      "Epoch 7, Batch 905, LR 1.869278 Loss 8.516846, Accuracy 73.195%\n",
      "Epoch 7, Batch 906, LR 1.869540 Loss 8.516196, Accuracy 73.198%\n",
      "Epoch 7, Batch 907, LR 1.869802 Loss 8.516040, Accuracy 73.196%\n",
      "Epoch 7, Batch 908, LR 1.870064 Loss 8.515966, Accuracy 73.196%\n",
      "Epoch 7, Batch 909, LR 1.870327 Loss 8.515029, Accuracy 73.204%\n",
      "Epoch 7, Batch 910, LR 1.870589 Loss 8.515207, Accuracy 73.201%\n",
      "Epoch 7, Batch 911, LR 1.870851 Loss 8.515061, Accuracy 73.204%\n",
      "Epoch 7, Batch 912, LR 1.871113 Loss 8.516156, Accuracy 73.198%\n",
      "Epoch 7, Batch 913, LR 1.871375 Loss 8.516547, Accuracy 73.190%\n",
      "Epoch 7, Batch 914, LR 1.871637 Loss 8.516882, Accuracy 73.184%\n",
      "Epoch 7, Batch 915, LR 1.871899 Loss 8.516074, Accuracy 73.186%\n",
      "Epoch 7, Batch 916, LR 1.872161 Loss 8.516598, Accuracy 73.184%\n",
      "Epoch 7, Batch 917, LR 1.872423 Loss 8.516821, Accuracy 73.181%\n",
      "Epoch 7, Batch 918, LR 1.872685 Loss 8.516508, Accuracy 73.183%\n",
      "Epoch 7, Batch 919, LR 1.872946 Loss 8.516850, Accuracy 73.178%\n",
      "Epoch 7, Batch 920, LR 1.873208 Loss 8.517056, Accuracy 73.179%\n",
      "Epoch 7, Batch 921, LR 1.873470 Loss 8.516251, Accuracy 73.187%\n",
      "Epoch 7, Batch 922, LR 1.873732 Loss 8.515636, Accuracy 73.189%\n",
      "Epoch 7, Batch 923, LR 1.873993 Loss 8.515333, Accuracy 73.192%\n",
      "Epoch 7, Batch 924, LR 1.874255 Loss 8.515070, Accuracy 73.192%\n",
      "Epoch 7, Batch 925, LR 1.874517 Loss 8.515087, Accuracy 73.193%\n",
      "Epoch 7, Batch 926, LR 1.874778 Loss 8.515213, Accuracy 73.188%\n",
      "Epoch 7, Batch 927, LR 1.875040 Loss 8.514969, Accuracy 73.190%\n",
      "Epoch 7, Batch 928, LR 1.875301 Loss 8.514807, Accuracy 73.189%\n",
      "Epoch 7, Batch 929, LR 1.875563 Loss 8.513778, Accuracy 73.194%\n",
      "Epoch 7, Batch 930, LR 1.875824 Loss 8.513595, Accuracy 73.200%\n",
      "Epoch 7, Batch 931, LR 1.876086 Loss 8.513544, Accuracy 73.200%\n",
      "Epoch 7, Batch 932, LR 1.876347 Loss 8.513805, Accuracy 73.198%\n",
      "Epoch 7, Batch 933, LR 1.876608 Loss 8.513923, Accuracy 73.201%\n",
      "Epoch 7, Batch 934, LR 1.876870 Loss 8.513764, Accuracy 73.204%\n",
      "Epoch 7, Batch 935, LR 1.877131 Loss 8.514226, Accuracy 73.198%\n",
      "Epoch 7, Batch 936, LR 1.877392 Loss 8.514819, Accuracy 73.193%\n",
      "Epoch 7, Batch 937, LR 1.877653 Loss 8.514149, Accuracy 73.201%\n",
      "Epoch 7, Batch 938, LR 1.877914 Loss 8.514123, Accuracy 73.197%\n",
      "Epoch 7, Batch 939, LR 1.878175 Loss 8.513952, Accuracy 73.200%\n",
      "Epoch 7, Batch 940, LR 1.878437 Loss 8.513750, Accuracy 73.199%\n",
      "Epoch 7, Batch 941, LR 1.878698 Loss 8.513322, Accuracy 73.200%\n",
      "Epoch 7, Batch 942, LR 1.878959 Loss 8.512867, Accuracy 73.204%\n",
      "Epoch 7, Batch 943, LR 1.879220 Loss 8.511900, Accuracy 73.207%\n",
      "Epoch 7, Batch 944, LR 1.879480 Loss 8.511303, Accuracy 73.208%\n",
      "Epoch 7, Batch 945, LR 1.879741 Loss 8.511153, Accuracy 73.208%\n",
      "Epoch 7, Batch 946, LR 1.880002 Loss 8.511459, Accuracy 73.205%\n",
      "Epoch 7, Batch 947, LR 1.880263 Loss 8.510879, Accuracy 73.212%\n",
      "Epoch 7, Batch 948, LR 1.880524 Loss 8.511057, Accuracy 73.213%\n",
      "Epoch 7, Batch 949, LR 1.880785 Loss 8.510308, Accuracy 73.216%\n",
      "Epoch 7, Batch 950, LR 1.881045 Loss 8.510885, Accuracy 73.211%\n",
      "Epoch 7, Batch 951, LR 1.881306 Loss 8.510580, Accuracy 73.214%\n",
      "Epoch 7, Batch 952, LR 1.881567 Loss 8.510035, Accuracy 73.216%\n",
      "Epoch 7, Batch 953, LR 1.881827 Loss 8.511140, Accuracy 73.209%\n",
      "Epoch 7, Batch 954, LR 1.882088 Loss 8.510903, Accuracy 73.210%\n",
      "Epoch 7, Batch 955, LR 1.882348 Loss 8.511554, Accuracy 73.199%\n",
      "Epoch 7, Batch 956, LR 1.882609 Loss 8.510945, Accuracy 73.205%\n",
      "Epoch 7, Batch 957, LR 1.882869 Loss 8.511639, Accuracy 73.196%\n",
      "Epoch 7, Batch 958, LR 1.883130 Loss 8.511166, Accuracy 73.195%\n",
      "Epoch 7, Batch 959, LR 1.883390 Loss 8.510839, Accuracy 73.195%\n",
      "Epoch 7, Batch 960, LR 1.883651 Loss 8.510996, Accuracy 73.193%\n",
      "Epoch 7, Batch 961, LR 1.883911 Loss 8.510420, Accuracy 73.198%\n",
      "Epoch 7, Batch 962, LR 1.884171 Loss 8.509677, Accuracy 73.203%\n",
      "Epoch 7, Batch 963, LR 1.884432 Loss 8.509986, Accuracy 73.204%\n",
      "Epoch 7, Batch 964, LR 1.884692 Loss 8.510289, Accuracy 73.206%\n",
      "Epoch 7, Batch 965, LR 1.884952 Loss 8.509877, Accuracy 73.210%\n",
      "Epoch 7, Batch 966, LR 1.885212 Loss 8.509761, Accuracy 73.211%\n",
      "Epoch 7, Batch 967, LR 1.885472 Loss 8.509749, Accuracy 73.213%\n",
      "Epoch 7, Batch 968, LR 1.885732 Loss 8.509868, Accuracy 73.213%\n",
      "Epoch 7, Batch 969, LR 1.885992 Loss 8.508981, Accuracy 73.222%\n",
      "Epoch 7, Batch 970, LR 1.886252 Loss 8.509280, Accuracy 73.217%\n",
      "Epoch 7, Batch 971, LR 1.886512 Loss 8.508789, Accuracy 73.219%\n",
      "Epoch 7, Batch 972, LR 1.886772 Loss 8.509395, Accuracy 73.216%\n",
      "Epoch 7, Batch 973, LR 1.887032 Loss 8.509173, Accuracy 73.213%\n",
      "Epoch 7, Batch 974, LR 1.887292 Loss 8.509477, Accuracy 73.211%\n",
      "Epoch 7, Batch 975, LR 1.887552 Loss 8.509378, Accuracy 73.210%\n",
      "Epoch 7, Batch 976, LR 1.887812 Loss 8.510057, Accuracy 73.205%\n",
      "Epoch 7, Batch 977, LR 1.888071 Loss 8.510114, Accuracy 73.198%\n",
      "Epoch 7, Batch 978, LR 1.888331 Loss 8.511031, Accuracy 73.194%\n",
      "Epoch 7, Batch 979, LR 1.888591 Loss 8.510673, Accuracy 73.198%\n",
      "Epoch 7, Batch 980, LR 1.888851 Loss 8.510611, Accuracy 73.200%\n",
      "Epoch 7, Batch 981, LR 1.889110 Loss 8.510360, Accuracy 73.199%\n",
      "Epoch 7, Batch 982, LR 1.889370 Loss 8.510523, Accuracy 73.203%\n",
      "Epoch 7, Batch 983, LR 1.889629 Loss 8.510888, Accuracy 73.199%\n",
      "Epoch 7, Batch 984, LR 1.889889 Loss 8.511277, Accuracy 73.198%\n",
      "Epoch 7, Batch 985, LR 1.890148 Loss 8.511599, Accuracy 73.199%\n",
      "Epoch 7, Batch 986, LR 1.890408 Loss 8.511515, Accuracy 73.200%\n",
      "Epoch 7, Batch 987, LR 1.890667 Loss 8.511103, Accuracy 73.202%\n",
      "Epoch 7, Batch 988, LR 1.890926 Loss 8.511033, Accuracy 73.199%\n",
      "Epoch 7, Batch 989, LR 1.891186 Loss 8.510679, Accuracy 73.198%\n",
      "Epoch 7, Batch 990, LR 1.891445 Loss 8.510020, Accuracy 73.201%\n",
      "Epoch 7, Batch 991, LR 1.891704 Loss 8.509988, Accuracy 73.196%\n",
      "Epoch 7, Batch 992, LR 1.891964 Loss 8.510974, Accuracy 73.191%\n",
      "Epoch 7, Batch 993, LR 1.892223 Loss 8.510033, Accuracy 73.191%\n",
      "Epoch 7, Batch 994, LR 1.892482 Loss 8.510064, Accuracy 73.193%\n",
      "Epoch 7, Batch 995, LR 1.892741 Loss 8.509565, Accuracy 73.196%\n",
      "Epoch 7, Batch 996, LR 1.893000 Loss 8.508977, Accuracy 73.200%\n",
      "Epoch 7, Batch 997, LR 1.893259 Loss 8.508687, Accuracy 73.201%\n",
      "Epoch 7, Batch 998, LR 1.893518 Loss 8.508701, Accuracy 73.197%\n",
      "Epoch 7, Batch 999, LR 1.893777 Loss 8.508797, Accuracy 73.197%\n",
      "Epoch 7, Batch 1000, LR 1.894036 Loss 8.508393, Accuracy 73.202%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 1001, LR 1.894295 Loss 8.508159, Accuracy 73.202%\n",
      "Epoch 7, Batch 1002, LR 1.894554 Loss 8.508344, Accuracy 73.197%\n",
      "Epoch 7, Batch 1003, LR 1.894813 Loss 8.507693, Accuracy 73.200%\n",
      "Epoch 7, Batch 1004, LR 1.895071 Loss 8.507849, Accuracy 73.199%\n",
      "Epoch 7, Batch 1005, LR 1.895330 Loss 8.507497, Accuracy 73.203%\n",
      "Epoch 7, Batch 1006, LR 1.895589 Loss 8.507329, Accuracy 73.205%\n",
      "Epoch 7, Batch 1007, LR 1.895847 Loss 8.507933, Accuracy 73.202%\n",
      "Epoch 7, Batch 1008, LR 1.896106 Loss 8.508965, Accuracy 73.190%\n",
      "Epoch 7, Batch 1009, LR 1.896365 Loss 8.509473, Accuracy 73.187%\n",
      "Epoch 7, Batch 1010, LR 1.896623 Loss 8.509338, Accuracy 73.188%\n",
      "Epoch 7, Batch 1011, LR 1.896882 Loss 8.510110, Accuracy 73.185%\n",
      "Epoch 7, Batch 1012, LR 1.897140 Loss 8.509710, Accuracy 73.192%\n",
      "Epoch 7, Batch 1013, LR 1.897399 Loss 8.510362, Accuracy 73.184%\n",
      "Epoch 7, Batch 1014, LR 1.897657 Loss 8.510380, Accuracy 73.182%\n",
      "Epoch 7, Batch 1015, LR 1.897916 Loss 8.510710, Accuracy 73.183%\n",
      "Epoch 7, Batch 1016, LR 1.898174 Loss 8.509853, Accuracy 73.191%\n",
      "Epoch 7, Batch 1017, LR 1.898432 Loss 8.509874, Accuracy 73.187%\n",
      "Epoch 7, Batch 1018, LR 1.898690 Loss 8.509827, Accuracy 73.190%\n",
      "Epoch 7, Batch 1019, LR 1.898949 Loss 8.509754, Accuracy 73.191%\n",
      "Epoch 7, Batch 1020, LR 1.899207 Loss 8.509986, Accuracy 73.187%\n",
      "Epoch 7, Batch 1021, LR 1.899465 Loss 8.509870, Accuracy 73.189%\n",
      "Epoch 7, Batch 1022, LR 1.899723 Loss 8.509208, Accuracy 73.191%\n",
      "Epoch 7, Batch 1023, LR 1.899981 Loss 8.510146, Accuracy 73.180%\n",
      "Epoch 7, Batch 1024, LR 1.900239 Loss 8.509620, Accuracy 73.180%\n",
      "Epoch 7, Batch 1025, LR 1.900497 Loss 8.509486, Accuracy 73.187%\n",
      "Epoch 7, Batch 1026, LR 1.900755 Loss 8.508765, Accuracy 73.191%\n",
      "Epoch 7, Batch 1027, LR 1.901013 Loss 8.509176, Accuracy 73.189%\n",
      "Epoch 7, Batch 1028, LR 1.901271 Loss 8.509091, Accuracy 73.187%\n",
      "Epoch 7, Batch 1029, LR 1.901529 Loss 8.508821, Accuracy 73.185%\n",
      "Epoch 7, Batch 1030, LR 1.901787 Loss 8.507791, Accuracy 73.193%\n",
      "Epoch 7, Batch 1031, LR 1.902045 Loss 8.507460, Accuracy 73.198%\n",
      "Epoch 7, Batch 1032, LR 1.902302 Loss 8.507178, Accuracy 73.201%\n",
      "Epoch 7, Batch 1033, LR 1.902560 Loss 8.507236, Accuracy 73.201%\n",
      "Epoch 7, Batch 1034, LR 1.902818 Loss 8.507538, Accuracy 73.199%\n",
      "Epoch 7, Batch 1035, LR 1.903075 Loss 8.507343, Accuracy 73.204%\n",
      "Epoch 7, Batch 1036, LR 1.903333 Loss 8.507900, Accuracy 73.201%\n",
      "Epoch 7, Batch 1037, LR 1.903591 Loss 8.507385, Accuracy 73.208%\n",
      "Epoch 7, Batch 1038, LR 1.903848 Loss 8.507381, Accuracy 73.208%\n",
      "Epoch 7, Batch 1039, LR 1.904106 Loss 8.506695, Accuracy 73.210%\n",
      "Epoch 7, Batch 1040, LR 1.904363 Loss 8.507190, Accuracy 73.208%\n",
      "Epoch 7, Batch 1041, LR 1.904621 Loss 8.506455, Accuracy 73.211%\n",
      "Epoch 7, Batch 1042, LR 1.904878 Loss 8.506332, Accuracy 73.213%\n",
      "Epoch 7, Batch 1043, LR 1.905135 Loss 8.506363, Accuracy 73.211%\n",
      "Epoch 7, Batch 1044, LR 1.905393 Loss 8.506123, Accuracy 73.211%\n",
      "Epoch 7, Batch 1045, LR 1.905650 Loss 8.506245, Accuracy 73.209%\n",
      "Epoch 7, Batch 1046, LR 1.905907 Loss 8.505610, Accuracy 73.212%\n",
      "Epoch 7, Batch 1047, LR 1.906164 Loss 8.505485, Accuracy 73.218%\n",
      "Epoch 7, Loss (train set) 8.505485, Accuracy (train set) 73.218%\n",
      "Epoch 8, Batch 1, LR 1.906421 Loss 8.686604, Accuracy 70.312%\n",
      "Epoch 8, Batch 2, LR 1.906679 Loss 8.497304, Accuracy 73.828%\n",
      "Epoch 8, Batch 3, LR 1.906936 Loss 8.400201, Accuracy 73.438%\n",
      "Epoch 8, Batch 4, LR 1.907193 Loss 8.350018, Accuracy 74.414%\n",
      "Epoch 8, Batch 5, LR 1.907450 Loss 8.185056, Accuracy 75.625%\n",
      "Epoch 8, Batch 6, LR 1.907707 Loss 8.206953, Accuracy 75.651%\n",
      "Epoch 8, Batch 7, LR 1.907964 Loss 8.182896, Accuracy 76.116%\n",
      "Epoch 8, Batch 8, LR 1.908221 Loss 8.193338, Accuracy 75.781%\n",
      "Epoch 8, Batch 9, LR 1.908477 Loss 8.076840, Accuracy 76.302%\n",
      "Epoch 8, Batch 10, LR 1.908734 Loss 8.063166, Accuracy 76.797%\n",
      "Epoch 8, Batch 11, LR 1.908991 Loss 8.065761, Accuracy 76.847%\n",
      "Epoch 8, Batch 12, LR 1.909248 Loss 8.097056, Accuracy 76.628%\n",
      "Epoch 8, Batch 13, LR 1.909505 Loss 8.099250, Accuracy 76.442%\n",
      "Epoch 8, Batch 14, LR 1.909761 Loss 8.087477, Accuracy 76.674%\n",
      "Epoch 8, Batch 15, LR 1.910018 Loss 8.129005, Accuracy 76.198%\n",
      "Epoch 8, Batch 16, LR 1.910274 Loss 8.142734, Accuracy 76.270%\n",
      "Epoch 8, Batch 17, LR 1.910531 Loss 8.121836, Accuracy 76.471%\n",
      "Epoch 8, Batch 18, LR 1.910788 Loss 8.118012, Accuracy 76.259%\n",
      "Epoch 8, Batch 19, LR 1.911044 Loss 8.125632, Accuracy 76.151%\n",
      "Epoch 8, Batch 20, LR 1.911300 Loss 8.140086, Accuracy 76.016%\n",
      "Epoch 8, Batch 21, LR 1.911557 Loss 8.122485, Accuracy 76.265%\n",
      "Epoch 8, Batch 22, LR 1.911813 Loss 8.120994, Accuracy 76.207%\n",
      "Epoch 8, Batch 23, LR 1.912070 Loss 8.075085, Accuracy 76.461%\n",
      "Epoch 8, Batch 24, LR 1.912326 Loss 8.082529, Accuracy 76.595%\n",
      "Epoch 8, Batch 25, LR 1.912582 Loss 8.082700, Accuracy 76.625%\n",
      "Epoch 8, Batch 26, LR 1.912838 Loss 8.076936, Accuracy 76.653%\n",
      "Epoch 8, Batch 27, LR 1.913095 Loss 8.074015, Accuracy 76.765%\n",
      "Epoch 8, Batch 28, LR 1.913351 Loss 8.058174, Accuracy 76.702%\n",
      "Epoch 8, Batch 29, LR 1.913607 Loss 8.057497, Accuracy 76.751%\n",
      "Epoch 8, Batch 30, LR 1.913863 Loss 8.065237, Accuracy 76.667%\n",
      "Epoch 8, Batch 31, LR 1.914119 Loss 8.069659, Accuracy 76.487%\n",
      "Epoch 8, Batch 32, LR 1.914375 Loss 8.076675, Accuracy 76.465%\n",
      "Epoch 8, Batch 33, LR 1.914631 Loss 8.097579, Accuracy 76.373%\n",
      "Epoch 8, Batch 34, LR 1.914887 Loss 8.088275, Accuracy 76.356%\n",
      "Epoch 8, Batch 35, LR 1.915143 Loss 8.087071, Accuracy 76.384%\n",
      "Epoch 8, Batch 36, LR 1.915399 Loss 8.090546, Accuracy 76.280%\n",
      "Epoch 8, Batch 37, LR 1.915654 Loss 8.081880, Accuracy 76.330%\n",
      "Epoch 8, Batch 38, LR 1.915910 Loss 8.086953, Accuracy 76.357%\n",
      "Epoch 8, Batch 39, LR 1.916166 Loss 8.087400, Accuracy 76.262%\n",
      "Epoch 8, Batch 40, LR 1.916422 Loss 8.095470, Accuracy 76.152%\n",
      "Epoch 8, Batch 41, LR 1.916677 Loss 8.091470, Accuracy 76.181%\n",
      "Epoch 8, Batch 42, LR 1.916933 Loss 8.087521, Accuracy 76.228%\n",
      "Epoch 8, Batch 43, LR 1.917189 Loss 8.087781, Accuracy 76.235%\n",
      "Epoch 8, Batch 44, LR 1.917444 Loss 8.102711, Accuracy 76.172%\n",
      "Epoch 8, Batch 45, LR 1.917700 Loss 8.093380, Accuracy 76.250%\n",
      "Epoch 8, Batch 46, LR 1.917955 Loss 8.100412, Accuracy 76.206%\n",
      "Epoch 8, Batch 47, LR 1.918211 Loss 8.098047, Accuracy 76.147%\n",
      "Epoch 8, Batch 48, LR 1.918466 Loss 8.102609, Accuracy 76.172%\n",
      "Epoch 8, Batch 49, LR 1.918721 Loss 8.103393, Accuracy 76.100%\n",
      "Epoch 8, Batch 50, LR 1.918977 Loss 8.109831, Accuracy 76.109%\n",
      "Epoch 8, Batch 51, LR 1.919232 Loss 8.112271, Accuracy 76.088%\n",
      "Epoch 8, Batch 52, LR 1.919487 Loss 8.117675, Accuracy 76.022%\n",
      "Epoch 8, Batch 53, LR 1.919742 Loss 8.118406, Accuracy 76.076%\n",
      "Epoch 8, Batch 54, LR 1.919998 Loss 8.119458, Accuracy 76.042%\n",
      "Epoch 8, Batch 55, LR 1.920253 Loss 8.129940, Accuracy 75.909%\n",
      "Epoch 8, Batch 56, LR 1.920508 Loss 8.129026, Accuracy 75.963%\n",
      "Epoch 8, Batch 57, LR 1.920763 Loss 8.127829, Accuracy 75.973%\n",
      "Epoch 8, Batch 58, LR 1.921018 Loss 8.136178, Accuracy 75.943%\n",
      "Epoch 8, Batch 59, LR 1.921273 Loss 8.131594, Accuracy 75.980%\n",
      "Epoch 8, Batch 60, LR 1.921528 Loss 8.129805, Accuracy 76.016%\n",
      "Epoch 8, Batch 61, LR 1.921783 Loss 8.128899, Accuracy 76.076%\n",
      "Epoch 8, Batch 62, LR 1.922038 Loss 8.121960, Accuracy 76.121%\n",
      "Epoch 8, Batch 63, LR 1.922292 Loss 8.121154, Accuracy 76.166%\n",
      "Epoch 8, Batch 64, LR 1.922547 Loss 8.124600, Accuracy 76.123%\n",
      "Epoch 8, Batch 65, LR 1.922802 Loss 8.121902, Accuracy 76.214%\n",
      "Epoch 8, Batch 66, LR 1.923057 Loss 8.130531, Accuracy 76.196%\n",
      "Epoch 8, Batch 67, LR 1.923311 Loss 8.130165, Accuracy 76.189%\n",
      "Epoch 8, Batch 68, LR 1.923566 Loss 8.121346, Accuracy 76.287%\n",
      "Epoch 8, Batch 69, LR 1.923821 Loss 8.132295, Accuracy 76.189%\n",
      "Epoch 8, Batch 70, LR 1.924075 Loss 8.122282, Accuracy 76.295%\n",
      "Epoch 8, Batch 71, LR 1.924330 Loss 8.122614, Accuracy 76.265%\n",
      "Epoch 8, Batch 72, LR 1.924584 Loss 8.123616, Accuracy 76.204%\n",
      "Epoch 8, Batch 73, LR 1.924839 Loss 8.119644, Accuracy 76.263%\n",
      "Epoch 8, Batch 74, LR 1.925093 Loss 8.123683, Accuracy 76.161%\n",
      "Epoch 8, Batch 75, LR 1.925347 Loss 8.122705, Accuracy 76.188%\n",
      "Epoch 8, Batch 76, LR 1.925602 Loss 8.120072, Accuracy 76.223%\n",
      "Epoch 8, Batch 77, LR 1.925856 Loss 8.123214, Accuracy 76.126%\n",
      "Epoch 8, Batch 78, LR 1.926110 Loss 8.122186, Accuracy 76.142%\n",
      "Epoch 8, Batch 79, LR 1.926365 Loss 8.127371, Accuracy 76.088%\n",
      "Epoch 8, Batch 80, LR 1.926619 Loss 8.124865, Accuracy 76.123%\n",
      "Epoch 8, Batch 81, LR 1.926873 Loss 8.120317, Accuracy 76.109%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 82, LR 1.927127 Loss 8.109342, Accuracy 76.162%\n",
      "Epoch 8, Batch 83, LR 1.927381 Loss 8.104777, Accuracy 76.120%\n",
      "Epoch 8, Batch 84, LR 1.927635 Loss 8.097269, Accuracy 76.200%\n",
      "Epoch 8, Batch 85, LR 1.927889 Loss 8.105681, Accuracy 76.112%\n",
      "Epoch 8, Batch 86, LR 1.928143 Loss 8.107916, Accuracy 76.117%\n",
      "Epoch 8, Batch 87, LR 1.928397 Loss 8.103347, Accuracy 76.105%\n",
      "Epoch 8, Batch 88, LR 1.928651 Loss 8.091821, Accuracy 76.163%\n",
      "Epoch 8, Batch 89, LR 1.928905 Loss 8.092709, Accuracy 76.194%\n",
      "Epoch 8, Batch 90, LR 1.929158 Loss 8.094976, Accuracy 76.155%\n",
      "Epoch 8, Batch 91, LR 1.929412 Loss 8.094713, Accuracy 76.159%\n",
      "Epoch 8, Batch 92, LR 1.929666 Loss 8.095770, Accuracy 76.138%\n",
      "Epoch 8, Batch 93, LR 1.929920 Loss 8.093264, Accuracy 76.142%\n",
      "Epoch 8, Batch 94, LR 1.930173 Loss 8.092653, Accuracy 76.080%\n",
      "Epoch 8, Batch 95, LR 1.930427 Loss 8.097224, Accuracy 76.036%\n",
      "Epoch 8, Batch 96, LR 1.930680 Loss 8.096920, Accuracy 76.050%\n",
      "Epoch 8, Batch 97, LR 1.930934 Loss 8.105326, Accuracy 75.942%\n",
      "Epoch 8, Batch 98, LR 1.931187 Loss 8.107062, Accuracy 75.917%\n",
      "Epoch 8, Batch 99, LR 1.931441 Loss 8.114906, Accuracy 75.876%\n",
      "Epoch 8, Batch 100, LR 1.931694 Loss 8.107551, Accuracy 75.953%\n",
      "Epoch 8, Batch 101, LR 1.931948 Loss 8.104751, Accuracy 75.982%\n",
      "Epoch 8, Batch 102, LR 1.932201 Loss 8.101659, Accuracy 76.019%\n",
      "Epoch 8, Batch 103, LR 1.932454 Loss 8.100289, Accuracy 76.047%\n",
      "Epoch 8, Batch 104, LR 1.932707 Loss 8.101107, Accuracy 75.992%\n",
      "Epoch 8, Batch 105, LR 1.932961 Loss 8.107137, Accuracy 75.938%\n",
      "Epoch 8, Batch 106, LR 1.933214 Loss 8.103204, Accuracy 75.958%\n",
      "Epoch 8, Batch 107, LR 1.933467 Loss 8.101596, Accuracy 75.927%\n",
      "Epoch 8, Batch 108, LR 1.933720 Loss 8.103517, Accuracy 75.875%\n",
      "Epoch 8, Batch 109, LR 1.933973 Loss 8.105231, Accuracy 75.889%\n",
      "Epoch 8, Batch 110, LR 1.934226 Loss 8.104250, Accuracy 75.923%\n",
      "Epoch 8, Batch 111, LR 1.934479 Loss 8.102780, Accuracy 75.943%\n",
      "Epoch 8, Batch 112, LR 1.934732 Loss 8.103827, Accuracy 75.914%\n",
      "Epoch 8, Batch 113, LR 1.934985 Loss 8.105669, Accuracy 75.885%\n",
      "Epoch 8, Batch 114, LR 1.935238 Loss 8.105558, Accuracy 75.870%\n",
      "Epoch 8, Batch 115, LR 1.935491 Loss 8.110256, Accuracy 75.836%\n",
      "Epoch 8, Batch 116, LR 1.935744 Loss 8.120284, Accuracy 75.768%\n",
      "Epoch 8, Batch 117, LR 1.935996 Loss 8.123861, Accuracy 75.755%\n",
      "Epoch 8, Batch 118, LR 1.936249 Loss 8.127259, Accuracy 75.715%\n",
      "Epoch 8, Batch 119, LR 1.936502 Loss 8.128546, Accuracy 75.735%\n",
      "Epoch 8, Batch 120, LR 1.936754 Loss 8.127355, Accuracy 75.710%\n",
      "Epoch 8, Batch 121, LR 1.937007 Loss 8.132579, Accuracy 75.665%\n",
      "Epoch 8, Batch 122, LR 1.937259 Loss 8.133405, Accuracy 75.653%\n",
      "Epoch 8, Batch 123, LR 1.937512 Loss 8.131902, Accuracy 75.705%\n",
      "Epoch 8, Batch 124, LR 1.937764 Loss 8.139772, Accuracy 75.649%\n",
      "Epoch 8, Batch 125, LR 1.938017 Loss 8.139382, Accuracy 75.619%\n",
      "Epoch 8, Batch 126, LR 1.938269 Loss 8.143393, Accuracy 75.577%\n",
      "Epoch 8, Batch 127, LR 1.938522 Loss 8.139794, Accuracy 75.584%\n",
      "Epoch 8, Batch 128, LR 1.938774 Loss 8.140125, Accuracy 75.574%\n",
      "Epoch 8, Batch 129, LR 1.939026 Loss 8.140646, Accuracy 75.587%\n",
      "Epoch 8, Batch 130, LR 1.939278 Loss 8.141650, Accuracy 75.601%\n",
      "Epoch 8, Batch 131, LR 1.939531 Loss 8.142269, Accuracy 75.608%\n",
      "Epoch 8, Batch 132, LR 1.939783 Loss 8.145725, Accuracy 75.580%\n",
      "Epoch 8, Batch 133, LR 1.940035 Loss 8.143274, Accuracy 75.587%\n",
      "Epoch 8, Batch 134, LR 1.940287 Loss 8.144219, Accuracy 75.577%\n",
      "Epoch 8, Batch 135, LR 1.940539 Loss 8.148389, Accuracy 75.573%\n",
      "Epoch 8, Batch 136, LR 1.940791 Loss 8.151829, Accuracy 75.563%\n",
      "Epoch 8, Batch 137, LR 1.941043 Loss 8.150970, Accuracy 75.553%\n",
      "Epoch 8, Batch 138, LR 1.941295 Loss 8.149128, Accuracy 75.538%\n",
      "Epoch 8, Batch 139, LR 1.941547 Loss 8.149894, Accuracy 75.528%\n",
      "Epoch 8, Batch 140, LR 1.941799 Loss 8.152116, Accuracy 75.480%\n",
      "Epoch 8, Batch 141, LR 1.942051 Loss 8.155841, Accuracy 75.449%\n",
      "Epoch 8, Batch 142, LR 1.942302 Loss 8.157241, Accuracy 75.468%\n",
      "Epoch 8, Batch 143, LR 1.942554 Loss 8.155157, Accuracy 75.481%\n",
      "Epoch 8, Batch 144, LR 1.942806 Loss 8.151112, Accuracy 75.505%\n",
      "Epoch 8, Batch 145, LR 1.943057 Loss 8.157078, Accuracy 75.463%\n",
      "Epoch 8, Batch 146, LR 1.943309 Loss 8.158501, Accuracy 75.433%\n",
      "Epoch 8, Batch 147, LR 1.943561 Loss 8.159070, Accuracy 75.446%\n",
      "Epoch 8, Batch 148, LR 1.943812 Loss 8.161187, Accuracy 75.449%\n",
      "Epoch 8, Batch 149, LR 1.944064 Loss 8.163375, Accuracy 75.409%\n",
      "Epoch 8, Batch 150, LR 1.944315 Loss 8.164133, Accuracy 75.401%\n",
      "Epoch 8, Batch 151, LR 1.944566 Loss 8.164548, Accuracy 75.409%\n",
      "Epoch 8, Batch 152, LR 1.944818 Loss 8.164525, Accuracy 75.406%\n",
      "Epoch 8, Batch 153, LR 1.945069 Loss 8.165121, Accuracy 75.424%\n",
      "Epoch 8, Batch 154, LR 1.945320 Loss 8.168101, Accuracy 75.391%\n",
      "Epoch 8, Batch 155, LR 1.945572 Loss 8.166351, Accuracy 75.393%\n",
      "Epoch 8, Batch 156, LR 1.945823 Loss 8.166819, Accuracy 75.411%\n",
      "Epoch 8, Batch 157, LR 1.946074 Loss 8.167482, Accuracy 75.413%\n",
      "Epoch 8, Batch 158, LR 1.946325 Loss 8.172877, Accuracy 75.376%\n",
      "Epoch 8, Batch 159, LR 1.946576 Loss 8.173716, Accuracy 75.354%\n",
      "Epoch 8, Batch 160, LR 1.946827 Loss 8.169214, Accuracy 75.376%\n",
      "Epoch 8, Batch 161, LR 1.947078 Loss 8.168180, Accuracy 75.378%\n",
      "Epoch 8, Batch 162, LR 1.947329 Loss 8.170181, Accuracy 75.357%\n",
      "Epoch 8, Batch 163, LR 1.947580 Loss 8.170842, Accuracy 75.374%\n",
      "Epoch 8, Batch 164, LR 1.947831 Loss 8.166653, Accuracy 75.419%\n",
      "Epoch 8, Batch 165, LR 1.948082 Loss 8.171100, Accuracy 75.388%\n",
      "Epoch 8, Batch 166, LR 1.948333 Loss 8.170728, Accuracy 75.367%\n",
      "Epoch 8, Batch 167, LR 1.948584 Loss 8.172134, Accuracy 75.342%\n",
      "Epoch 8, Batch 168, LR 1.948834 Loss 8.176617, Accuracy 75.302%\n",
      "Epoch 8, Batch 169, LR 1.949085 Loss 8.174049, Accuracy 75.310%\n",
      "Epoch 8, Batch 170, LR 1.949336 Loss 8.176214, Accuracy 75.303%\n",
      "Epoch 8, Batch 171, LR 1.949586 Loss 8.178828, Accuracy 75.260%\n",
      "Epoch 8, Batch 172, LR 1.949837 Loss 8.181374, Accuracy 75.250%\n",
      "Epoch 8, Batch 173, LR 1.950088 Loss 8.177535, Accuracy 75.275%\n",
      "Epoch 8, Batch 174, LR 1.950338 Loss 8.177912, Accuracy 75.238%\n",
      "Epoch 8, Batch 175, LR 1.950589 Loss 8.177079, Accuracy 75.228%\n",
      "Epoch 8, Batch 176, LR 1.950839 Loss 8.178796, Accuracy 75.226%\n",
      "Epoch 8, Batch 177, LR 1.951089 Loss 8.181860, Accuracy 75.190%\n",
      "Epoch 8, Batch 178, LR 1.951340 Loss 8.185088, Accuracy 75.162%\n",
      "Epoch 8, Batch 179, LR 1.951590 Loss 8.177930, Accuracy 75.201%\n",
      "Epoch 8, Batch 180, LR 1.951840 Loss 8.176182, Accuracy 75.221%\n",
      "Epoch 8, Batch 181, LR 1.952090 Loss 8.173148, Accuracy 75.250%\n",
      "Epoch 8, Batch 182, LR 1.952341 Loss 8.171355, Accuracy 75.262%\n",
      "Epoch 8, Batch 183, LR 1.952591 Loss 8.172830, Accuracy 75.252%\n",
      "Epoch 8, Batch 184, LR 1.952841 Loss 8.171547, Accuracy 75.246%\n",
      "Epoch 8, Batch 185, LR 1.953091 Loss 8.175742, Accuracy 75.198%\n",
      "Epoch 8, Batch 186, LR 1.953341 Loss 8.175889, Accuracy 75.181%\n",
      "Epoch 8, Batch 187, LR 1.953591 Loss 8.173824, Accuracy 75.180%\n",
      "Epoch 8, Batch 188, LR 1.953841 Loss 8.174191, Accuracy 75.187%\n",
      "Epoch 8, Batch 189, LR 1.954091 Loss 8.177666, Accuracy 75.169%\n",
      "Epoch 8, Batch 190, LR 1.954341 Loss 8.174832, Accuracy 75.185%\n",
      "Epoch 8, Batch 191, LR 1.954590 Loss 8.176784, Accuracy 75.184%\n",
      "Epoch 8, Batch 192, LR 1.954840 Loss 8.176060, Accuracy 75.195%\n",
      "Epoch 8, Batch 193, LR 1.955090 Loss 8.172293, Accuracy 75.231%\n",
      "Epoch 8, Batch 194, LR 1.955340 Loss 8.175383, Accuracy 75.217%\n",
      "Epoch 8, Batch 195, LR 1.955589 Loss 8.176428, Accuracy 75.216%\n",
      "Epoch 8, Batch 196, LR 1.955839 Loss 8.175770, Accuracy 75.223%\n",
      "Epoch 8, Batch 197, LR 1.956089 Loss 8.172731, Accuracy 75.242%\n",
      "Epoch 8, Batch 198, LR 1.956338 Loss 8.172499, Accuracy 75.241%\n",
      "Epoch 8, Batch 199, LR 1.956588 Loss 8.174483, Accuracy 75.232%\n",
      "Epoch 8, Batch 200, LR 1.956837 Loss 8.175464, Accuracy 75.223%\n",
      "Epoch 8, Batch 201, LR 1.957086 Loss 8.175840, Accuracy 75.241%\n",
      "Epoch 8, Batch 202, LR 1.957336 Loss 8.174709, Accuracy 75.240%\n",
      "Epoch 8, Batch 203, LR 1.957585 Loss 8.173476, Accuracy 75.266%\n",
      "Epoch 8, Batch 204, LR 1.957834 Loss 8.174779, Accuracy 75.257%\n",
      "Epoch 8, Batch 205, LR 1.958084 Loss 8.173886, Accuracy 75.271%\n",
      "Epoch 8, Batch 206, LR 1.958333 Loss 8.173643, Accuracy 75.265%\n",
      "Epoch 8, Batch 207, LR 1.958582 Loss 8.173552, Accuracy 75.279%\n",
      "Epoch 8, Batch 208, LR 1.958831 Loss 8.174773, Accuracy 75.285%\n",
      "Epoch 8, Batch 209, LR 1.959080 Loss 8.173387, Accuracy 75.307%\n",
      "Epoch 8, Batch 210, LR 1.959329 Loss 8.175138, Accuracy 75.286%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 211, LR 1.959578 Loss 8.169309, Accuracy 75.311%\n",
      "Epoch 8, Batch 212, LR 1.959827 Loss 8.168505, Accuracy 75.317%\n",
      "Epoch 8, Batch 213, LR 1.960076 Loss 8.169540, Accuracy 75.304%\n",
      "Epoch 8, Batch 214, LR 1.960325 Loss 8.172366, Accuracy 75.285%\n",
      "Epoch 8, Batch 215, LR 1.960574 Loss 8.172855, Accuracy 75.269%\n",
      "Epoch 8, Batch 216, LR 1.960823 Loss 8.176703, Accuracy 75.228%\n",
      "Epoch 8, Batch 217, LR 1.961072 Loss 8.177238, Accuracy 75.238%\n",
      "Epoch 8, Batch 218, LR 1.961320 Loss 8.177786, Accuracy 75.244%\n",
      "Epoch 8, Batch 219, LR 1.961569 Loss 8.177316, Accuracy 75.235%\n",
      "Epoch 8, Batch 220, LR 1.961818 Loss 8.180522, Accuracy 75.224%\n",
      "Epoch 8, Batch 221, LR 1.962066 Loss 8.178584, Accuracy 75.237%\n",
      "Epoch 8, Batch 222, LR 1.962315 Loss 8.178162, Accuracy 75.229%\n",
      "Epoch 8, Batch 223, LR 1.962563 Loss 8.178642, Accuracy 75.228%\n",
      "Epoch 8, Batch 224, LR 1.962812 Loss 8.180528, Accuracy 75.202%\n",
      "Epoch 8, Batch 225, LR 1.963060 Loss 8.179609, Accuracy 75.226%\n",
      "Epoch 8, Batch 226, LR 1.963309 Loss 8.179324, Accuracy 75.239%\n",
      "Epoch 8, Batch 227, LR 1.963557 Loss 8.179371, Accuracy 75.251%\n",
      "Epoch 8, Batch 228, LR 1.963805 Loss 8.179613, Accuracy 75.254%\n",
      "Epoch 8, Batch 229, LR 1.964054 Loss 8.180411, Accuracy 75.256%\n",
      "Epoch 8, Batch 230, LR 1.964302 Loss 8.181030, Accuracy 75.231%\n",
      "Epoch 8, Batch 231, LR 1.964550 Loss 8.183990, Accuracy 75.213%\n",
      "Epoch 8, Batch 232, LR 1.964798 Loss 8.184752, Accuracy 75.199%\n",
      "Epoch 8, Batch 233, LR 1.965047 Loss 8.182648, Accuracy 75.228%\n",
      "Epoch 8, Batch 234, LR 1.965295 Loss 8.183525, Accuracy 75.217%\n",
      "Epoch 8, Batch 235, LR 1.965543 Loss 8.183852, Accuracy 75.223%\n",
      "Epoch 8, Batch 236, LR 1.965791 Loss 8.183418, Accuracy 75.238%\n",
      "Epoch 8, Batch 237, LR 1.966039 Loss 8.184559, Accuracy 75.244%\n",
      "Epoch 8, Batch 238, LR 1.966287 Loss 8.186093, Accuracy 75.217%\n",
      "Epoch 8, Batch 239, LR 1.966534 Loss 8.189588, Accuracy 75.196%\n",
      "Epoch 8, Batch 240, LR 1.966782 Loss 8.190621, Accuracy 75.186%\n",
      "Epoch 8, Batch 241, LR 1.967030 Loss 8.189359, Accuracy 75.198%\n",
      "Epoch 8, Batch 242, LR 1.967278 Loss 8.193128, Accuracy 75.181%\n",
      "Epoch 8, Batch 243, LR 1.967525 Loss 8.197486, Accuracy 75.158%\n",
      "Epoch 8, Batch 244, LR 1.967773 Loss 8.196111, Accuracy 75.179%\n",
      "Epoch 8, Batch 245, LR 1.968021 Loss 8.197161, Accuracy 75.179%\n",
      "Epoch 8, Batch 246, LR 1.968268 Loss 8.197222, Accuracy 75.178%\n",
      "Epoch 8, Batch 247, LR 1.968516 Loss 8.200324, Accuracy 75.149%\n",
      "Epoch 8, Batch 248, LR 1.968763 Loss 8.199890, Accuracy 75.148%\n",
      "Epoch 8, Batch 249, LR 1.969011 Loss 8.199863, Accuracy 75.169%\n",
      "Epoch 8, Batch 250, LR 1.969258 Loss 8.199440, Accuracy 75.188%\n",
      "Epoch 8, Batch 251, LR 1.969506 Loss 8.200192, Accuracy 75.168%\n",
      "Epoch 8, Batch 252, LR 1.969753 Loss 8.198341, Accuracy 75.186%\n",
      "Epoch 8, Batch 253, LR 1.970000 Loss 8.199278, Accuracy 75.164%\n",
      "Epoch 8, Batch 254, LR 1.970248 Loss 8.200308, Accuracy 75.151%\n",
      "Epoch 8, Batch 255, LR 1.970495 Loss 8.196803, Accuracy 75.162%\n",
      "Epoch 8, Batch 256, LR 1.970742 Loss 8.199660, Accuracy 75.140%\n",
      "Epoch 8, Batch 257, LR 1.970989 Loss 8.200351, Accuracy 75.140%\n",
      "Epoch 8, Batch 258, LR 1.971236 Loss 8.198929, Accuracy 75.157%\n",
      "Epoch 8, Batch 259, LR 1.971483 Loss 8.198562, Accuracy 75.157%\n",
      "Epoch 8, Batch 260, LR 1.971730 Loss 8.198415, Accuracy 75.159%\n",
      "Epoch 8, Batch 261, LR 1.971977 Loss 8.199253, Accuracy 75.159%\n",
      "Epoch 8, Batch 262, LR 1.972224 Loss 8.200035, Accuracy 75.170%\n",
      "Epoch 8, Batch 263, LR 1.972471 Loss 8.198534, Accuracy 75.175%\n",
      "Epoch 8, Batch 264, LR 1.972718 Loss 8.196871, Accuracy 75.201%\n",
      "Epoch 8, Batch 265, LR 1.972965 Loss 8.197364, Accuracy 75.186%\n",
      "Epoch 8, Batch 266, LR 1.973212 Loss 8.198011, Accuracy 75.200%\n",
      "Epoch 8, Batch 267, LR 1.973458 Loss 8.199018, Accuracy 75.196%\n",
      "Epoch 8, Batch 268, LR 1.973705 Loss 8.199125, Accuracy 75.181%\n",
      "Epoch 8, Batch 269, LR 1.973952 Loss 8.200198, Accuracy 75.166%\n",
      "Epoch 8, Batch 270, LR 1.974198 Loss 8.200447, Accuracy 75.182%\n",
      "Epoch 8, Batch 271, LR 1.974445 Loss 8.199864, Accuracy 75.196%\n",
      "Epoch 8, Batch 272, LR 1.974691 Loss 8.203364, Accuracy 75.172%\n",
      "Epoch 8, Batch 273, LR 1.974938 Loss 8.202628, Accuracy 75.183%\n",
      "Epoch 8, Batch 274, LR 1.975184 Loss 8.200486, Accuracy 75.188%\n",
      "Epoch 8, Batch 275, LR 1.975431 Loss 8.197500, Accuracy 75.213%\n",
      "Epoch 8, Batch 276, LR 1.975677 Loss 8.196594, Accuracy 75.212%\n",
      "Epoch 8, Batch 277, LR 1.975923 Loss 8.198825, Accuracy 75.195%\n",
      "Epoch 8, Batch 278, LR 1.976170 Loss 8.199293, Accuracy 75.205%\n",
      "Epoch 8, Batch 279, LR 1.976416 Loss 8.198556, Accuracy 75.207%\n",
      "Epoch 8, Batch 280, LR 1.976662 Loss 8.199138, Accuracy 75.201%\n",
      "Epoch 8, Batch 281, LR 1.976908 Loss 8.198806, Accuracy 75.192%\n",
      "Epoch 8, Batch 282, LR 1.977154 Loss 8.198931, Accuracy 75.188%\n",
      "Epoch 8, Batch 283, LR 1.977400 Loss 8.199392, Accuracy 75.188%\n",
      "Epoch 8, Batch 284, LR 1.977646 Loss 8.196799, Accuracy 75.209%\n",
      "Epoch 8, Batch 285, LR 1.977892 Loss 8.196642, Accuracy 75.189%\n",
      "Epoch 8, Batch 286, LR 1.978138 Loss 8.197261, Accuracy 75.191%\n",
      "Epoch 8, Batch 287, LR 1.978384 Loss 8.199687, Accuracy 75.161%\n",
      "Epoch 8, Batch 288, LR 1.978630 Loss 8.197883, Accuracy 75.179%\n",
      "Epoch 8, Batch 289, LR 1.978876 Loss 8.197205, Accuracy 75.189%\n",
      "Epoch 8, Batch 290, LR 1.979122 Loss 8.198313, Accuracy 75.178%\n",
      "Epoch 8, Batch 291, LR 1.979367 Loss 8.200515, Accuracy 75.158%\n",
      "Epoch 8, Batch 292, LR 1.979613 Loss 8.200809, Accuracy 75.155%\n",
      "Epoch 8, Batch 293, LR 1.979859 Loss 8.198551, Accuracy 75.171%\n",
      "Epoch 8, Batch 294, LR 1.980104 Loss 8.199975, Accuracy 75.165%\n",
      "Epoch 8, Batch 295, LR 1.980350 Loss 8.200030, Accuracy 75.162%\n",
      "Epoch 8, Batch 296, LR 1.980595 Loss 8.199509, Accuracy 75.174%\n",
      "Epoch 8, Batch 297, LR 1.980841 Loss 8.199882, Accuracy 75.179%\n",
      "Epoch 8, Batch 298, LR 1.981086 Loss 8.198188, Accuracy 75.189%\n",
      "Epoch 8, Batch 299, LR 1.981332 Loss 8.196509, Accuracy 75.188%\n",
      "Epoch 8, Batch 300, LR 1.981577 Loss 8.196761, Accuracy 75.201%\n",
      "Epoch 8, Batch 301, LR 1.981822 Loss 8.197250, Accuracy 75.197%\n",
      "Epoch 8, Batch 302, LR 1.982068 Loss 8.194740, Accuracy 75.204%\n",
      "Epoch 8, Batch 303, LR 1.982313 Loss 8.192724, Accuracy 75.211%\n",
      "Epoch 8, Batch 304, LR 1.982558 Loss 8.192094, Accuracy 75.211%\n",
      "Epoch 8, Batch 305, LR 1.982803 Loss 8.192009, Accuracy 75.205%\n",
      "Epoch 8, Batch 306, LR 1.983048 Loss 8.190745, Accuracy 75.220%\n",
      "Epoch 8, Batch 307, LR 1.983293 Loss 8.190764, Accuracy 75.232%\n",
      "Epoch 8, Batch 308, LR 1.983538 Loss 8.191631, Accuracy 75.205%\n",
      "Epoch 8, Batch 309, LR 1.983783 Loss 8.193393, Accuracy 75.197%\n",
      "Epoch 8, Batch 310, LR 1.984028 Loss 8.192780, Accuracy 75.207%\n",
      "Epoch 8, Batch 311, LR 1.984273 Loss 8.193544, Accuracy 75.201%\n",
      "Epoch 8, Batch 312, LR 1.984518 Loss 8.192962, Accuracy 75.208%\n",
      "Epoch 8, Batch 313, LR 1.984763 Loss 8.194206, Accuracy 75.205%\n",
      "Epoch 8, Batch 314, LR 1.985007 Loss 8.193545, Accuracy 75.209%\n",
      "Epoch 8, Batch 315, LR 1.985252 Loss 8.193861, Accuracy 75.211%\n",
      "Epoch 8, Batch 316, LR 1.985497 Loss 8.193726, Accuracy 75.215%\n",
      "Epoch 8, Batch 317, LR 1.985742 Loss 8.193372, Accuracy 75.214%\n",
      "Epoch 8, Batch 318, LR 1.985986 Loss 8.195184, Accuracy 75.206%\n",
      "Epoch 8, Batch 319, LR 1.986231 Loss 8.195099, Accuracy 75.201%\n",
      "Epoch 8, Batch 320, LR 1.986475 Loss 8.195678, Accuracy 75.188%\n",
      "Epoch 8, Batch 321, LR 1.986720 Loss 8.195888, Accuracy 75.178%\n",
      "Epoch 8, Batch 322, LR 1.986964 Loss 8.195049, Accuracy 75.177%\n",
      "Epoch 8, Batch 323, LR 1.987208 Loss 8.193314, Accuracy 75.177%\n",
      "Epoch 8, Batch 324, LR 1.987453 Loss 8.193758, Accuracy 75.176%\n",
      "Epoch 8, Batch 325, LR 1.987697 Loss 8.194664, Accuracy 75.156%\n",
      "Epoch 8, Batch 326, LR 1.987941 Loss 8.195026, Accuracy 75.153%\n",
      "Epoch 8, Batch 327, LR 1.988186 Loss 8.194650, Accuracy 75.167%\n",
      "Epoch 8, Batch 328, LR 1.988430 Loss 8.195463, Accuracy 75.157%\n",
      "Epoch 8, Batch 329, LR 1.988674 Loss 8.196939, Accuracy 75.140%\n",
      "Epoch 8, Batch 330, LR 1.988918 Loss 8.194740, Accuracy 75.142%\n",
      "Epoch 8, Batch 331, LR 1.989162 Loss 8.195285, Accuracy 75.127%\n",
      "Epoch 8, Batch 332, LR 1.989406 Loss 8.195842, Accuracy 75.118%\n",
      "Epoch 8, Batch 333, LR 1.989650 Loss 8.196986, Accuracy 75.122%\n",
      "Epoch 8, Batch 334, LR 1.989894 Loss 8.195535, Accuracy 75.140%\n",
      "Epoch 8, Batch 335, LR 1.990138 Loss 8.195987, Accuracy 75.147%\n",
      "Epoch 8, Batch 336, LR 1.990382 Loss 8.196328, Accuracy 75.144%\n",
      "Epoch 8, Batch 337, LR 1.990625 Loss 8.197688, Accuracy 75.123%\n",
      "Epoch 8, Batch 338, LR 1.990869 Loss 8.197729, Accuracy 75.134%\n",
      "Epoch 8, Batch 339, LR 1.991113 Loss 8.196033, Accuracy 75.145%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 340, LR 1.991357 Loss 8.195487, Accuracy 75.161%\n",
      "Epoch 8, Batch 341, LR 1.991600 Loss 8.194334, Accuracy 75.179%\n",
      "Epoch 8, Batch 342, LR 1.991844 Loss 8.195889, Accuracy 75.178%\n",
      "Epoch 8, Batch 343, LR 1.992087 Loss 8.195736, Accuracy 75.184%\n",
      "Epoch 8, Batch 344, LR 1.992331 Loss 8.197376, Accuracy 75.170%\n",
      "Epoch 8, Batch 345, LR 1.992574 Loss 8.197923, Accuracy 75.181%\n",
      "Epoch 8, Batch 346, LR 1.992818 Loss 8.198196, Accuracy 75.178%\n",
      "Epoch 8, Batch 347, LR 1.993061 Loss 8.199709, Accuracy 75.171%\n",
      "Epoch 8, Batch 348, LR 1.993304 Loss 8.199926, Accuracy 75.184%\n",
      "Epoch 8, Batch 349, LR 1.993548 Loss 8.198964, Accuracy 75.190%\n",
      "Epoch 8, Batch 350, LR 1.993791 Loss 8.197729, Accuracy 75.201%\n",
      "Epoch 8, Batch 351, LR 1.994034 Loss 8.199262, Accuracy 75.203%\n",
      "Epoch 8, Batch 352, LR 1.994277 Loss 8.198891, Accuracy 75.200%\n",
      "Epoch 8, Batch 353, LR 1.994520 Loss 8.198720, Accuracy 75.201%\n",
      "Epoch 8, Batch 354, LR 1.994763 Loss 8.200114, Accuracy 75.199%\n",
      "Epoch 8, Batch 355, LR 1.995007 Loss 8.201054, Accuracy 75.194%\n",
      "Epoch 8, Batch 356, LR 1.995249 Loss 8.200513, Accuracy 75.213%\n",
      "Epoch 8, Batch 357, LR 1.995492 Loss 8.199677, Accuracy 75.223%\n",
      "Epoch 8, Batch 358, LR 1.995735 Loss 8.200922, Accuracy 75.196%\n",
      "Epoch 8, Batch 359, LR 1.995978 Loss 8.201913, Accuracy 75.200%\n",
      "Epoch 8, Batch 360, LR 1.996221 Loss 8.199769, Accuracy 75.224%\n",
      "Epoch 8, Batch 361, LR 1.996464 Loss 8.199528, Accuracy 75.225%\n",
      "Epoch 8, Batch 362, LR 1.996707 Loss 8.199151, Accuracy 75.227%\n",
      "Epoch 8, Batch 363, LR 1.996949 Loss 8.200140, Accuracy 75.209%\n",
      "Epoch 8, Batch 364, LR 1.997192 Loss 8.201081, Accuracy 75.193%\n",
      "Epoch 8, Batch 365, LR 1.997434 Loss 8.201009, Accuracy 75.190%\n",
      "Epoch 8, Batch 366, LR 1.997677 Loss 8.199440, Accuracy 75.199%\n",
      "Epoch 8, Batch 367, LR 1.997920 Loss 8.199131, Accuracy 75.206%\n",
      "Epoch 8, Batch 368, LR 1.998162 Loss 8.199248, Accuracy 75.206%\n",
      "Epoch 8, Batch 369, LR 1.998404 Loss 8.200078, Accuracy 75.199%\n",
      "Epoch 8, Batch 370, LR 1.998647 Loss 8.199281, Accuracy 75.211%\n",
      "Epoch 8, Batch 371, LR 1.998889 Loss 8.198639, Accuracy 75.215%\n",
      "Epoch 8, Batch 372, LR 1.999131 Loss 8.198490, Accuracy 75.210%\n",
      "Epoch 8, Batch 373, LR 1.999374 Loss 8.198549, Accuracy 75.205%\n",
      "Epoch 8, Batch 374, LR 1.999616 Loss 8.197430, Accuracy 75.224%\n",
      "Epoch 8, Batch 375, LR 1.999858 Loss 8.196342, Accuracy 75.235%\n",
      "Epoch 8, Batch 376, LR 2.000100 Loss 8.195906, Accuracy 75.239%\n",
      "Epoch 8, Batch 377, LR 2.000342 Loss 8.195092, Accuracy 75.249%\n",
      "Epoch 8, Batch 378, LR 2.000584 Loss 8.195802, Accuracy 75.242%\n",
      "Epoch 8, Batch 379, LR 2.000826 Loss 8.196702, Accuracy 75.231%\n",
      "Epoch 8, Batch 380, LR 2.001068 Loss 8.194379, Accuracy 75.255%\n",
      "Epoch 8, Batch 381, LR 2.001310 Loss 8.193537, Accuracy 75.258%\n",
      "Epoch 8, Batch 382, LR 2.001552 Loss 8.192440, Accuracy 75.262%\n",
      "Epoch 8, Batch 383, LR 2.001794 Loss 8.190711, Accuracy 75.269%\n",
      "Epoch 8, Batch 384, LR 2.002036 Loss 8.190193, Accuracy 75.271%\n",
      "Epoch 8, Batch 385, LR 2.002277 Loss 8.190624, Accuracy 75.274%\n",
      "Epoch 8, Batch 386, LR 2.002519 Loss 8.189581, Accuracy 75.273%\n",
      "Epoch 8, Batch 387, LR 2.002761 Loss 8.192073, Accuracy 75.262%\n",
      "Epoch 8, Batch 388, LR 2.003002 Loss 8.191891, Accuracy 75.272%\n",
      "Epoch 8, Batch 389, LR 2.003244 Loss 8.192450, Accuracy 75.253%\n",
      "Epoch 8, Batch 390, LR 2.003486 Loss 8.190985, Accuracy 75.270%\n",
      "Epoch 8, Batch 391, LR 2.003727 Loss 8.192748, Accuracy 75.256%\n",
      "Epoch 8, Batch 392, LR 2.003968 Loss 8.190429, Accuracy 75.273%\n",
      "Epoch 8, Batch 393, LR 2.004210 Loss 8.190664, Accuracy 75.264%\n",
      "Epoch 8, Batch 394, LR 2.004451 Loss 8.190228, Accuracy 75.262%\n",
      "Epoch 8, Batch 395, LR 2.004693 Loss 8.190949, Accuracy 75.259%\n",
      "Epoch 8, Batch 396, LR 2.004934 Loss 8.191346, Accuracy 75.262%\n",
      "Epoch 8, Batch 397, LR 2.005175 Loss 8.191272, Accuracy 75.258%\n",
      "Epoch 8, Batch 398, LR 2.005416 Loss 8.189502, Accuracy 75.277%\n",
      "Epoch 8, Batch 399, LR 2.005657 Loss 8.190081, Accuracy 75.262%\n",
      "Epoch 8, Batch 400, LR 2.005898 Loss 8.190219, Accuracy 75.254%\n",
      "Epoch 8, Batch 401, LR 2.006139 Loss 8.190688, Accuracy 75.249%\n",
      "Epoch 8, Batch 402, LR 2.006380 Loss 8.191917, Accuracy 75.245%\n",
      "Epoch 8, Batch 403, LR 2.006621 Loss 8.191074, Accuracy 75.240%\n",
      "Epoch 8, Batch 404, LR 2.006862 Loss 8.191238, Accuracy 75.238%\n",
      "Epoch 8, Batch 405, LR 2.007103 Loss 8.192964, Accuracy 75.226%\n",
      "Epoch 8, Batch 406, LR 2.007344 Loss 8.192869, Accuracy 75.229%\n",
      "Epoch 8, Batch 407, LR 2.007585 Loss 8.191681, Accuracy 75.238%\n",
      "Epoch 8, Batch 408, LR 2.007826 Loss 8.192384, Accuracy 75.232%\n",
      "Epoch 8, Batch 409, LR 2.008066 Loss 8.194395, Accuracy 75.208%\n",
      "Epoch 8, Batch 410, LR 2.008307 Loss 8.195369, Accuracy 75.206%\n",
      "Epoch 8, Batch 411, LR 2.008548 Loss 8.194892, Accuracy 75.205%\n",
      "Epoch 8, Batch 412, LR 2.008788 Loss 8.193945, Accuracy 75.214%\n",
      "Epoch 8, Batch 413, LR 2.009029 Loss 8.193859, Accuracy 75.208%\n",
      "Epoch 8, Batch 414, LR 2.009269 Loss 8.193034, Accuracy 75.204%\n",
      "Epoch 8, Batch 415, LR 2.009510 Loss 8.192053, Accuracy 75.207%\n",
      "Epoch 8, Batch 416, LR 2.009750 Loss 8.191219, Accuracy 75.218%\n",
      "Epoch 8, Batch 417, LR 2.009990 Loss 8.191746, Accuracy 75.212%\n",
      "Epoch 8, Batch 418, LR 2.010231 Loss 8.190219, Accuracy 75.230%\n",
      "Epoch 8, Batch 419, LR 2.010471 Loss 8.190486, Accuracy 75.222%\n",
      "Epoch 8, Batch 420, LR 2.010711 Loss 8.193138, Accuracy 75.201%\n",
      "Epoch 8, Batch 421, LR 2.010951 Loss 8.194252, Accuracy 75.199%\n",
      "Epoch 8, Batch 422, LR 2.011192 Loss 8.193366, Accuracy 75.213%\n",
      "Epoch 8, Batch 423, LR 2.011432 Loss 8.192566, Accuracy 75.205%\n",
      "Epoch 8, Batch 424, LR 2.011672 Loss 8.194161, Accuracy 75.199%\n",
      "Epoch 8, Batch 425, LR 2.011912 Loss 8.193739, Accuracy 75.206%\n",
      "Epoch 8, Batch 426, LR 2.012152 Loss 8.194266, Accuracy 75.202%\n",
      "Epoch 8, Batch 427, LR 2.012392 Loss 8.194358, Accuracy 75.203%\n",
      "Epoch 8, Batch 428, LR 2.012631 Loss 8.194700, Accuracy 75.215%\n",
      "Epoch 8, Batch 429, LR 2.012871 Loss 8.195143, Accuracy 75.215%\n",
      "Epoch 8, Batch 430, LR 2.013111 Loss 8.192900, Accuracy 75.220%\n",
      "Epoch 8, Batch 431, LR 2.013351 Loss 8.191652, Accuracy 75.227%\n",
      "Epoch 8, Batch 432, LR 2.013590 Loss 8.191492, Accuracy 75.237%\n",
      "Epoch 8, Batch 433, LR 2.013830 Loss 8.190089, Accuracy 75.249%\n",
      "Epoch 8, Batch 434, LR 2.014070 Loss 8.189526, Accuracy 75.248%\n",
      "Epoch 8, Batch 435, LR 2.014309 Loss 8.191227, Accuracy 75.246%\n",
      "Epoch 8, Batch 436, LR 2.014549 Loss 8.191416, Accuracy 75.242%\n",
      "Epoch 8, Batch 437, LR 2.014788 Loss 8.191466, Accuracy 75.234%\n",
      "Epoch 8, Batch 438, LR 2.015028 Loss 8.191857, Accuracy 75.230%\n",
      "Epoch 8, Batch 439, LR 2.015267 Loss 8.191849, Accuracy 75.230%\n",
      "Epoch 8, Batch 440, LR 2.015506 Loss 8.192179, Accuracy 75.215%\n",
      "Epoch 8, Batch 441, LR 2.015746 Loss 8.193615, Accuracy 75.214%\n",
      "Epoch 8, Batch 442, LR 2.015985 Loss 8.193303, Accuracy 75.224%\n",
      "Epoch 8, Batch 443, LR 2.016224 Loss 8.192581, Accuracy 75.222%\n",
      "Epoch 8, Batch 444, LR 2.016463 Loss 8.193135, Accuracy 75.218%\n",
      "Epoch 8, Batch 445, LR 2.016703 Loss 8.192854, Accuracy 75.223%\n",
      "Epoch 8, Batch 446, LR 2.016942 Loss 8.193204, Accuracy 75.228%\n",
      "Epoch 8, Batch 447, LR 2.017181 Loss 8.193045, Accuracy 75.225%\n",
      "Epoch 8, Batch 448, LR 2.017420 Loss 8.192082, Accuracy 75.228%\n",
      "Epoch 8, Batch 449, LR 2.017659 Loss 8.192425, Accuracy 75.217%\n",
      "Epoch 8, Batch 450, LR 2.017898 Loss 8.191112, Accuracy 75.220%\n",
      "Epoch 8, Batch 451, LR 2.018136 Loss 8.191689, Accuracy 75.217%\n",
      "Epoch 8, Batch 452, LR 2.018375 Loss 8.191203, Accuracy 75.206%\n",
      "Epoch 8, Batch 453, LR 2.018614 Loss 8.192089, Accuracy 75.200%\n",
      "Epoch 8, Batch 454, LR 2.018853 Loss 8.191189, Accuracy 75.200%\n",
      "Epoch 8, Batch 455, LR 2.019091 Loss 8.191781, Accuracy 75.204%\n",
      "Epoch 8, Batch 456, LR 2.019330 Loss 8.191985, Accuracy 75.206%\n",
      "Epoch 8, Batch 457, LR 2.019569 Loss 8.190248, Accuracy 75.215%\n",
      "Epoch 8, Batch 458, LR 2.019807 Loss 8.189179, Accuracy 75.215%\n",
      "Epoch 8, Batch 459, LR 2.020046 Loss 8.189817, Accuracy 75.214%\n",
      "Epoch 8, Batch 460, LR 2.020284 Loss 8.191486, Accuracy 75.202%\n",
      "Epoch 8, Batch 461, LR 2.020523 Loss 8.191714, Accuracy 75.197%\n",
      "Epoch 8, Batch 462, LR 2.020761 Loss 8.191574, Accuracy 75.196%\n",
      "Epoch 8, Batch 463, LR 2.020999 Loss 8.192059, Accuracy 75.191%\n",
      "Epoch 8, Batch 464, LR 2.021238 Loss 8.192762, Accuracy 75.177%\n",
      "Epoch 8, Batch 465, LR 2.021476 Loss 8.195420, Accuracy 75.153%\n",
      "Epoch 8, Batch 466, LR 2.021714 Loss 8.195682, Accuracy 75.159%\n",
      "Epoch 8, Batch 467, LR 2.021952 Loss 8.196497, Accuracy 75.151%\n",
      "Epoch 8, Batch 468, LR 2.022190 Loss 8.195020, Accuracy 75.155%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 469, LR 2.022428 Loss 8.195697, Accuracy 75.153%\n",
      "Epoch 8, Batch 470, LR 2.022666 Loss 8.195467, Accuracy 75.155%\n",
      "Epoch 8, Batch 471, LR 2.022904 Loss 8.194445, Accuracy 75.163%\n",
      "Epoch 8, Batch 472, LR 2.023142 Loss 8.194584, Accuracy 75.162%\n",
      "Epoch 8, Batch 473, LR 2.023380 Loss 8.196096, Accuracy 75.147%\n",
      "Epoch 8, Batch 474, LR 2.023618 Loss 8.195712, Accuracy 75.150%\n",
      "Epoch 8, Batch 475, LR 2.023856 Loss 8.195358, Accuracy 75.158%\n",
      "Epoch 8, Batch 476, LR 2.024094 Loss 8.195525, Accuracy 75.164%\n",
      "Epoch 8, Batch 477, LR 2.024331 Loss 8.195588, Accuracy 75.165%\n",
      "Epoch 8, Batch 478, LR 2.024569 Loss 8.193602, Accuracy 75.173%\n",
      "Epoch 8, Batch 479, LR 2.024807 Loss 8.193691, Accuracy 75.170%\n",
      "Epoch 8, Batch 480, LR 2.025044 Loss 8.194866, Accuracy 75.164%\n",
      "Epoch 8, Batch 481, LR 2.025282 Loss 8.193852, Accuracy 75.174%\n",
      "Epoch 8, Batch 482, LR 2.025519 Loss 8.195014, Accuracy 75.165%\n",
      "Epoch 8, Batch 483, LR 2.025757 Loss 8.195633, Accuracy 75.162%\n",
      "Epoch 8, Batch 484, LR 2.025994 Loss 8.195018, Accuracy 75.165%\n",
      "Epoch 8, Batch 485, LR 2.026231 Loss 8.194460, Accuracy 75.174%\n",
      "Epoch 8, Batch 486, LR 2.026469 Loss 8.194788, Accuracy 75.169%\n",
      "Epoch 8, Batch 487, LR 2.026706 Loss 8.194272, Accuracy 75.175%\n",
      "Epoch 8, Batch 488, LR 2.026943 Loss 8.193995, Accuracy 75.184%\n",
      "Epoch 8, Batch 489, LR 2.027180 Loss 8.194706, Accuracy 75.179%\n",
      "Epoch 8, Batch 490, LR 2.027418 Loss 8.194045, Accuracy 75.179%\n",
      "Epoch 8, Batch 491, LR 2.027655 Loss 8.193890, Accuracy 75.178%\n",
      "Epoch 8, Batch 492, LR 2.027892 Loss 8.194910, Accuracy 75.167%\n",
      "Epoch 8, Batch 493, LR 2.028129 Loss 8.193520, Accuracy 75.171%\n",
      "Epoch 8, Batch 494, LR 2.028366 Loss 8.192607, Accuracy 75.180%\n",
      "Epoch 8, Batch 495, LR 2.028602 Loss 8.191580, Accuracy 75.196%\n",
      "Epoch 8, Batch 496, LR 2.028839 Loss 8.191618, Accuracy 75.189%\n",
      "Epoch 8, Batch 497, LR 2.029076 Loss 8.190973, Accuracy 75.193%\n",
      "Epoch 8, Batch 498, LR 2.029313 Loss 8.190685, Accuracy 75.195%\n",
      "Epoch 8, Batch 499, LR 2.029550 Loss 8.192242, Accuracy 75.183%\n",
      "Epoch 8, Batch 500, LR 2.029786 Loss 8.192448, Accuracy 75.177%\n",
      "Epoch 8, Batch 501, LR 2.030023 Loss 8.190419, Accuracy 75.184%\n",
      "Epoch 8, Batch 502, LR 2.030260 Loss 8.190604, Accuracy 75.187%\n",
      "Epoch 8, Batch 503, LR 2.030496 Loss 8.190383, Accuracy 75.183%\n",
      "Epoch 8, Batch 504, LR 2.030733 Loss 8.191134, Accuracy 75.178%\n",
      "Epoch 8, Batch 505, LR 2.030969 Loss 8.191334, Accuracy 75.178%\n",
      "Epoch 8, Batch 506, LR 2.031206 Loss 8.191861, Accuracy 75.178%\n",
      "Epoch 8, Batch 507, LR 2.031442 Loss 8.191837, Accuracy 75.177%\n",
      "Epoch 8, Batch 508, LR 2.031678 Loss 8.190472, Accuracy 75.185%\n",
      "Epoch 8, Batch 509, LR 2.031914 Loss 8.191813, Accuracy 75.181%\n",
      "Epoch 8, Batch 510, LR 2.032151 Loss 8.192626, Accuracy 75.170%\n",
      "Epoch 8, Batch 511, LR 2.032387 Loss 8.191843, Accuracy 75.170%\n",
      "Epoch 8, Batch 512, LR 2.032623 Loss 8.192511, Accuracy 75.165%\n",
      "Epoch 8, Batch 513, LR 2.032859 Loss 8.192828, Accuracy 75.166%\n",
      "Epoch 8, Batch 514, LR 2.033095 Loss 8.193147, Accuracy 75.161%\n",
      "Epoch 8, Batch 515, LR 2.033331 Loss 8.192595, Accuracy 75.167%\n",
      "Epoch 8, Batch 516, LR 2.033567 Loss 8.192025, Accuracy 75.168%\n",
      "Epoch 8, Batch 517, LR 2.033803 Loss 8.193239, Accuracy 75.166%\n",
      "Epoch 8, Batch 518, LR 2.034039 Loss 8.193518, Accuracy 75.166%\n",
      "Epoch 8, Batch 519, LR 2.034275 Loss 8.193049, Accuracy 75.166%\n",
      "Epoch 8, Batch 520, LR 2.034511 Loss 8.193740, Accuracy 75.158%\n",
      "Epoch 8, Batch 521, LR 2.034746 Loss 8.193898, Accuracy 75.157%\n",
      "Epoch 8, Batch 522, LR 2.034982 Loss 8.193650, Accuracy 75.157%\n",
      "Epoch 8, Batch 523, LR 2.035218 Loss 8.193649, Accuracy 75.158%\n",
      "Epoch 8, Batch 524, LR 2.035453 Loss 8.192817, Accuracy 75.168%\n",
      "Epoch 8, Batch 525, LR 2.035689 Loss 8.192879, Accuracy 75.176%\n",
      "Epoch 8, Batch 526, LR 2.035924 Loss 8.194512, Accuracy 75.166%\n",
      "Epoch 8, Batch 527, LR 2.036160 Loss 8.194283, Accuracy 75.173%\n",
      "Epoch 8, Batch 528, LR 2.036395 Loss 8.194840, Accuracy 75.164%\n",
      "Epoch 8, Batch 529, LR 2.036631 Loss 8.195160, Accuracy 75.157%\n",
      "Epoch 8, Batch 530, LR 2.036866 Loss 8.195438, Accuracy 75.153%\n",
      "Epoch 8, Batch 531, LR 2.037101 Loss 8.195519, Accuracy 75.156%\n",
      "Epoch 8, Batch 532, LR 2.037336 Loss 8.196991, Accuracy 75.144%\n",
      "Epoch 8, Batch 533, LR 2.037572 Loss 8.194497, Accuracy 75.154%\n",
      "Epoch 8, Batch 534, LR 2.037807 Loss 8.194436, Accuracy 75.154%\n",
      "Epoch 8, Batch 535, LR 2.038042 Loss 8.192797, Accuracy 75.166%\n",
      "Epoch 8, Batch 536, LR 2.038277 Loss 8.192541, Accuracy 75.168%\n",
      "Epoch 8, Batch 537, LR 2.038512 Loss 8.193317, Accuracy 75.159%\n",
      "Epoch 8, Batch 538, LR 2.038747 Loss 8.193214, Accuracy 75.161%\n",
      "Epoch 8, Batch 539, LR 2.038982 Loss 8.191715, Accuracy 75.178%\n",
      "Epoch 8, Batch 540, LR 2.039217 Loss 8.192977, Accuracy 75.168%\n",
      "Epoch 8, Batch 541, LR 2.039452 Loss 8.192377, Accuracy 75.185%\n",
      "Epoch 8, Batch 542, LR 2.039686 Loss 8.192929, Accuracy 75.189%\n",
      "Epoch 8, Batch 543, LR 2.039921 Loss 8.193099, Accuracy 75.188%\n",
      "Epoch 8, Batch 544, LR 2.040156 Loss 8.194223, Accuracy 75.178%\n",
      "Epoch 8, Batch 545, LR 2.040390 Loss 8.195151, Accuracy 75.172%\n",
      "Epoch 8, Batch 546, LR 2.040625 Loss 8.196200, Accuracy 75.165%\n",
      "Epoch 8, Batch 547, LR 2.040860 Loss 8.196732, Accuracy 75.163%\n",
      "Epoch 8, Batch 548, LR 2.041094 Loss 8.195795, Accuracy 75.174%\n",
      "Epoch 8, Batch 549, LR 2.041329 Loss 8.195076, Accuracy 75.178%\n",
      "Epoch 8, Batch 550, LR 2.041563 Loss 8.195878, Accuracy 75.182%\n",
      "Epoch 8, Batch 551, LR 2.041797 Loss 8.196201, Accuracy 75.183%\n",
      "Epoch 8, Batch 552, LR 2.042032 Loss 8.196749, Accuracy 75.173%\n",
      "Epoch 8, Batch 553, LR 2.042266 Loss 8.197090, Accuracy 75.168%\n",
      "Epoch 8, Batch 554, LR 2.042500 Loss 8.197791, Accuracy 75.168%\n",
      "Epoch 8, Batch 555, LR 2.042734 Loss 8.197672, Accuracy 75.176%\n",
      "Epoch 8, Batch 556, LR 2.042969 Loss 8.197834, Accuracy 75.183%\n",
      "Epoch 8, Batch 557, LR 2.043203 Loss 8.197858, Accuracy 75.185%\n",
      "Epoch 8, Batch 558, LR 2.043437 Loss 8.198807, Accuracy 75.176%\n",
      "Epoch 8, Batch 559, LR 2.043671 Loss 8.198814, Accuracy 75.177%\n",
      "Epoch 8, Batch 560, LR 2.043905 Loss 8.197658, Accuracy 75.186%\n",
      "Epoch 8, Batch 561, LR 2.044139 Loss 8.197560, Accuracy 75.188%\n",
      "Epoch 8, Batch 562, LR 2.044373 Loss 8.197914, Accuracy 75.172%\n",
      "Epoch 8, Batch 563, LR 2.044606 Loss 8.198237, Accuracy 75.168%\n",
      "Epoch 8, Batch 564, LR 2.044840 Loss 8.199042, Accuracy 75.163%\n",
      "Epoch 8, Batch 565, LR 2.045074 Loss 8.199071, Accuracy 75.159%\n",
      "Epoch 8, Batch 566, LR 2.045308 Loss 8.200288, Accuracy 75.153%\n",
      "Epoch 8, Batch 567, LR 2.045541 Loss 8.200759, Accuracy 75.156%\n",
      "Epoch 8, Batch 568, LR 2.045775 Loss 8.199964, Accuracy 75.171%\n",
      "Epoch 8, Batch 569, LR 2.046008 Loss 8.198259, Accuracy 75.177%\n",
      "Epoch 8, Batch 570, LR 2.046242 Loss 8.197812, Accuracy 75.181%\n",
      "Epoch 8, Batch 571, LR 2.046475 Loss 8.196543, Accuracy 75.190%\n",
      "Epoch 8, Batch 572, LR 2.046709 Loss 8.195170, Accuracy 75.197%\n",
      "Epoch 8, Batch 573, LR 2.046942 Loss 8.195710, Accuracy 75.194%\n",
      "Epoch 8, Batch 574, LR 2.047175 Loss 8.196158, Accuracy 75.185%\n",
      "Epoch 8, Batch 575, LR 2.047409 Loss 8.196495, Accuracy 75.186%\n",
      "Epoch 8, Batch 576, LR 2.047642 Loss 8.196744, Accuracy 75.189%\n",
      "Epoch 8, Batch 577, LR 2.047875 Loss 8.197312, Accuracy 75.181%\n",
      "Epoch 8, Batch 578, LR 2.048108 Loss 8.197264, Accuracy 75.181%\n",
      "Epoch 8, Batch 579, LR 2.048341 Loss 8.197758, Accuracy 75.173%\n",
      "Epoch 8, Batch 580, LR 2.048574 Loss 8.198757, Accuracy 75.162%\n",
      "Epoch 8, Batch 581, LR 2.048807 Loss 8.198386, Accuracy 75.159%\n",
      "Epoch 8, Batch 582, LR 2.049040 Loss 8.199399, Accuracy 75.158%\n",
      "Epoch 8, Batch 583, LR 2.049273 Loss 8.199913, Accuracy 75.159%\n",
      "Epoch 8, Batch 584, LR 2.049506 Loss 8.200030, Accuracy 75.166%\n",
      "Epoch 8, Batch 585, LR 2.049739 Loss 8.199160, Accuracy 75.170%\n",
      "Epoch 8, Batch 586, LR 2.049972 Loss 8.199163, Accuracy 75.165%\n",
      "Epoch 8, Batch 587, LR 2.050205 Loss 8.198943, Accuracy 75.161%\n",
      "Epoch 8, Batch 588, LR 2.050437 Loss 8.198156, Accuracy 75.173%\n",
      "Epoch 8, Batch 589, LR 2.050670 Loss 8.198009, Accuracy 75.168%\n",
      "Epoch 8, Batch 590, LR 2.050902 Loss 8.198335, Accuracy 75.168%\n",
      "Epoch 8, Batch 591, LR 2.051135 Loss 8.198391, Accuracy 75.165%\n",
      "Epoch 8, Batch 592, LR 2.051367 Loss 8.198195, Accuracy 75.170%\n",
      "Epoch 8, Batch 593, LR 2.051600 Loss 8.197446, Accuracy 75.179%\n",
      "Epoch 8, Batch 594, LR 2.051832 Loss 8.198125, Accuracy 75.166%\n",
      "Epoch 8, Batch 595, LR 2.052065 Loss 8.197917, Accuracy 75.164%\n",
      "Epoch 8, Batch 596, LR 2.052297 Loss 8.196762, Accuracy 75.166%\n",
      "Epoch 8, Batch 597, LR 2.052529 Loss 8.196150, Accuracy 75.169%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 598, LR 2.052761 Loss 8.195636, Accuracy 75.172%\n",
      "Epoch 8, Batch 599, LR 2.052994 Loss 8.195753, Accuracy 75.176%\n",
      "Epoch 8, Batch 600, LR 2.053226 Loss 8.196930, Accuracy 75.168%\n",
      "Epoch 8, Batch 601, LR 2.053458 Loss 8.196538, Accuracy 75.169%\n",
      "Epoch 8, Batch 602, LR 2.053690 Loss 8.198205, Accuracy 75.157%\n",
      "Epoch 8, Batch 603, LR 2.053922 Loss 8.197596, Accuracy 75.162%\n",
      "Epoch 8, Batch 604, LR 2.054154 Loss 8.197690, Accuracy 75.159%\n",
      "Epoch 8, Batch 605, LR 2.054386 Loss 8.198541, Accuracy 75.160%\n",
      "Epoch 8, Batch 606, LR 2.054618 Loss 8.198076, Accuracy 75.171%\n",
      "Epoch 8, Batch 607, LR 2.054849 Loss 8.200005, Accuracy 75.160%\n",
      "Epoch 8, Batch 608, LR 2.055081 Loss 8.199177, Accuracy 75.167%\n",
      "Epoch 8, Batch 609, LR 2.055313 Loss 8.199232, Accuracy 75.171%\n",
      "Epoch 8, Batch 610, LR 2.055544 Loss 8.199537, Accuracy 75.172%\n",
      "Epoch 8, Batch 611, LR 2.055776 Loss 8.198992, Accuracy 75.179%\n",
      "Epoch 8, Batch 612, LR 2.056008 Loss 8.199659, Accuracy 75.172%\n",
      "Epoch 8, Batch 613, LR 2.056239 Loss 8.199444, Accuracy 75.175%\n",
      "Epoch 8, Batch 614, LR 2.056471 Loss 8.199373, Accuracy 75.174%\n",
      "Epoch 8, Batch 615, LR 2.056702 Loss 8.199837, Accuracy 75.170%\n",
      "Epoch 8, Batch 616, LR 2.056933 Loss 8.199882, Accuracy 75.171%\n",
      "Epoch 8, Batch 617, LR 2.057165 Loss 8.199509, Accuracy 75.166%\n",
      "Epoch 8, Batch 618, LR 2.057396 Loss 8.199452, Accuracy 75.171%\n",
      "Epoch 8, Batch 619, LR 2.057627 Loss 8.199982, Accuracy 75.168%\n",
      "Epoch 8, Batch 620, LR 2.057859 Loss 8.201085, Accuracy 75.161%\n",
      "Epoch 8, Batch 621, LR 2.058090 Loss 8.201776, Accuracy 75.155%\n",
      "Epoch 8, Batch 622, LR 2.058321 Loss 8.201330, Accuracy 75.154%\n",
      "Epoch 8, Batch 623, LR 2.058552 Loss 8.201395, Accuracy 75.150%\n",
      "Epoch 8, Batch 624, LR 2.058783 Loss 8.200884, Accuracy 75.153%\n",
      "Epoch 8, Batch 625, LR 2.059014 Loss 8.200737, Accuracy 75.156%\n",
      "Epoch 8, Batch 626, LR 2.059245 Loss 8.200597, Accuracy 75.163%\n",
      "Epoch 8, Batch 627, LR 2.059476 Loss 8.199995, Accuracy 75.172%\n",
      "Epoch 8, Batch 628, LR 2.059706 Loss 8.199680, Accuracy 75.174%\n",
      "Epoch 8, Batch 629, LR 2.059937 Loss 8.198946, Accuracy 75.181%\n",
      "Epoch 8, Batch 630, LR 2.060168 Loss 8.196988, Accuracy 75.195%\n",
      "Epoch 8, Batch 631, LR 2.060399 Loss 8.197494, Accuracy 75.188%\n",
      "Epoch 8, Batch 632, LR 2.060629 Loss 8.197903, Accuracy 75.183%\n",
      "Epoch 8, Batch 633, LR 2.060860 Loss 8.197326, Accuracy 75.186%\n",
      "Epoch 8, Batch 634, LR 2.061090 Loss 8.197342, Accuracy 75.189%\n",
      "Epoch 8, Batch 635, LR 2.061321 Loss 8.198575, Accuracy 75.175%\n",
      "Epoch 8, Batch 636, LR 2.061551 Loss 8.198450, Accuracy 75.178%\n",
      "Epoch 8, Batch 637, LR 2.061782 Loss 8.197312, Accuracy 75.182%\n",
      "Epoch 8, Batch 638, LR 2.062012 Loss 8.197498, Accuracy 75.184%\n",
      "Epoch 8, Batch 639, LR 2.062242 Loss 8.197273, Accuracy 75.181%\n",
      "Epoch 8, Batch 640, LR 2.062473 Loss 8.197040, Accuracy 75.182%\n",
      "Epoch 8, Batch 641, LR 2.062703 Loss 8.196375, Accuracy 75.184%\n",
      "Epoch 8, Batch 642, LR 2.062933 Loss 8.195959, Accuracy 75.179%\n",
      "Epoch 8, Batch 643, LR 2.063163 Loss 8.195431, Accuracy 75.185%\n",
      "Epoch 8, Batch 644, LR 2.063393 Loss 8.196872, Accuracy 75.173%\n",
      "Epoch 8, Batch 645, LR 2.063623 Loss 8.198238, Accuracy 75.167%\n",
      "Epoch 8, Batch 646, LR 2.063853 Loss 8.198433, Accuracy 75.161%\n",
      "Epoch 8, Batch 647, LR 2.064083 Loss 8.197884, Accuracy 75.163%\n",
      "Epoch 8, Batch 648, LR 2.064313 Loss 8.199142, Accuracy 75.156%\n",
      "Epoch 8, Batch 649, LR 2.064543 Loss 8.200227, Accuracy 75.143%\n",
      "Epoch 8, Batch 650, LR 2.064773 Loss 8.200409, Accuracy 75.145%\n",
      "Epoch 8, Batch 651, LR 2.065002 Loss 8.199528, Accuracy 75.152%\n",
      "Epoch 8, Batch 652, LR 2.065232 Loss 8.199694, Accuracy 75.147%\n",
      "Epoch 8, Batch 653, LR 2.065462 Loss 8.199545, Accuracy 75.141%\n",
      "Epoch 8, Batch 654, LR 2.065691 Loss 8.199381, Accuracy 75.140%\n",
      "Epoch 8, Batch 655, LR 2.065921 Loss 8.199685, Accuracy 75.135%\n",
      "Epoch 8, Batch 656, LR 2.066150 Loss 8.199304, Accuracy 75.141%\n",
      "Epoch 8, Batch 657, LR 2.066380 Loss 8.199769, Accuracy 75.127%\n",
      "Epoch 8, Batch 658, LR 2.066609 Loss 8.198314, Accuracy 75.138%\n",
      "Epoch 8, Batch 659, LR 2.066839 Loss 8.198257, Accuracy 75.139%\n",
      "Epoch 8, Batch 660, LR 2.067068 Loss 8.197587, Accuracy 75.148%\n",
      "Epoch 8, Batch 661, LR 2.067297 Loss 8.196899, Accuracy 75.152%\n",
      "Epoch 8, Batch 662, LR 2.067527 Loss 8.198142, Accuracy 75.140%\n",
      "Epoch 8, Batch 663, LR 2.067756 Loss 8.197476, Accuracy 75.147%\n",
      "Epoch 8, Batch 664, LR 2.067985 Loss 8.197285, Accuracy 75.151%\n",
      "Epoch 8, Batch 665, LR 2.068214 Loss 8.197338, Accuracy 75.153%\n",
      "Epoch 8, Batch 666, LR 2.068443 Loss 8.196545, Accuracy 75.154%\n",
      "Epoch 8, Batch 667, LR 2.068672 Loss 8.197005, Accuracy 75.155%\n",
      "Epoch 8, Batch 668, LR 2.068901 Loss 8.197231, Accuracy 75.150%\n",
      "Epoch 8, Batch 669, LR 2.069130 Loss 8.195654, Accuracy 75.160%\n",
      "Epoch 8, Batch 670, LR 2.069359 Loss 8.195014, Accuracy 75.170%\n",
      "Epoch 8, Batch 671, LR 2.069587 Loss 8.194280, Accuracy 75.179%\n",
      "Epoch 8, Batch 672, LR 2.069816 Loss 8.194423, Accuracy 75.170%\n",
      "Epoch 8, Batch 673, LR 2.070045 Loss 8.194631, Accuracy 75.172%\n",
      "Epoch 8, Batch 674, LR 2.070274 Loss 8.195711, Accuracy 75.165%\n",
      "Epoch 8, Batch 675, LR 2.070502 Loss 8.196258, Accuracy 75.159%\n",
      "Epoch 8, Batch 676, LR 2.070731 Loss 8.197099, Accuracy 75.157%\n",
      "Epoch 8, Batch 677, LR 2.070959 Loss 8.198040, Accuracy 75.145%\n",
      "Epoch 8, Batch 678, LR 2.071188 Loss 8.198843, Accuracy 75.142%\n",
      "Epoch 8, Batch 679, LR 2.071416 Loss 8.198966, Accuracy 75.140%\n",
      "Epoch 8, Batch 680, LR 2.071644 Loss 8.198716, Accuracy 75.146%\n",
      "Epoch 8, Batch 681, LR 2.071873 Loss 8.197988, Accuracy 75.156%\n",
      "Epoch 8, Batch 682, LR 2.072101 Loss 8.197497, Accuracy 75.155%\n",
      "Epoch 8, Batch 683, LR 2.072329 Loss 8.197206, Accuracy 75.160%\n",
      "Epoch 8, Batch 684, LR 2.072558 Loss 8.196575, Accuracy 75.166%\n",
      "Epoch 8, Batch 685, LR 2.072786 Loss 8.197258, Accuracy 75.160%\n",
      "Epoch 8, Batch 686, LR 2.073014 Loss 8.196904, Accuracy 75.161%\n",
      "Epoch 8, Batch 687, LR 2.073242 Loss 8.197793, Accuracy 75.156%\n",
      "Epoch 8, Batch 688, LR 2.073470 Loss 8.198728, Accuracy 75.150%\n",
      "Epoch 8, Batch 689, LR 2.073698 Loss 8.197986, Accuracy 75.151%\n",
      "Epoch 8, Batch 690, LR 2.073926 Loss 8.197669, Accuracy 75.157%\n",
      "Epoch 8, Batch 691, LR 2.074153 Loss 8.198394, Accuracy 75.149%\n",
      "Epoch 8, Batch 692, LR 2.074381 Loss 8.200017, Accuracy 75.138%\n",
      "Epoch 8, Batch 693, LR 2.074609 Loss 8.200085, Accuracy 75.134%\n",
      "Epoch 8, Batch 694, LR 2.074837 Loss 8.199767, Accuracy 75.136%\n",
      "Epoch 8, Batch 695, LR 2.075064 Loss 8.199664, Accuracy 75.132%\n",
      "Epoch 8, Batch 696, LR 2.075292 Loss 8.199242, Accuracy 75.134%\n",
      "Epoch 8, Batch 697, LR 2.075520 Loss 8.199172, Accuracy 75.135%\n",
      "Epoch 8, Batch 698, LR 2.075747 Loss 8.198857, Accuracy 75.139%\n",
      "Epoch 8, Batch 699, LR 2.075975 Loss 8.198759, Accuracy 75.137%\n",
      "Epoch 8, Batch 700, LR 2.076202 Loss 8.198607, Accuracy 75.141%\n",
      "Epoch 8, Batch 701, LR 2.076429 Loss 8.198643, Accuracy 75.137%\n",
      "Epoch 8, Batch 702, LR 2.076657 Loss 8.199669, Accuracy 75.130%\n",
      "Epoch 8, Batch 703, LR 2.076884 Loss 8.200157, Accuracy 75.123%\n",
      "Epoch 8, Batch 704, LR 2.077111 Loss 8.199616, Accuracy 75.123%\n",
      "Epoch 8, Batch 705, LR 2.077338 Loss 8.199794, Accuracy 75.122%\n",
      "Epoch 8, Batch 706, LR 2.077566 Loss 8.199500, Accuracy 75.117%\n",
      "Epoch 8, Batch 707, LR 2.077793 Loss 8.199700, Accuracy 75.115%\n",
      "Epoch 8, Batch 708, LR 2.078020 Loss 8.199606, Accuracy 75.117%\n",
      "Epoch 8, Batch 709, LR 2.078247 Loss 8.200349, Accuracy 75.108%\n",
      "Epoch 8, Batch 710, LR 2.078474 Loss 8.200340, Accuracy 75.109%\n",
      "Epoch 8, Batch 711, LR 2.078700 Loss 8.200271, Accuracy 75.112%\n",
      "Epoch 8, Batch 712, LR 2.078927 Loss 8.200277, Accuracy 75.112%\n",
      "Epoch 8, Batch 713, LR 2.079154 Loss 8.199776, Accuracy 75.114%\n",
      "Epoch 8, Batch 714, LR 2.079381 Loss 8.199059, Accuracy 75.115%\n",
      "Epoch 8, Batch 715, LR 2.079608 Loss 8.199355, Accuracy 75.113%\n",
      "Epoch 8, Batch 716, LR 2.079834 Loss 8.198582, Accuracy 75.117%\n",
      "Epoch 8, Batch 717, LR 2.080061 Loss 8.199010, Accuracy 75.120%\n",
      "Epoch 8, Batch 718, LR 2.080287 Loss 8.198646, Accuracy 75.118%\n",
      "Epoch 8, Batch 719, LR 2.080514 Loss 8.199020, Accuracy 75.121%\n",
      "Epoch 8, Batch 720, LR 2.080740 Loss 8.199623, Accuracy 75.109%\n",
      "Epoch 8, Batch 721, LR 2.080967 Loss 8.198936, Accuracy 75.116%\n",
      "Epoch 8, Batch 722, LR 2.081193 Loss 8.198732, Accuracy 75.117%\n",
      "Epoch 8, Batch 723, LR 2.081419 Loss 8.197908, Accuracy 75.126%\n",
      "Epoch 8, Batch 724, LR 2.081646 Loss 8.198442, Accuracy 75.128%\n",
      "Epoch 8, Batch 725, LR 2.081872 Loss 8.198502, Accuracy 75.127%\n",
      "Epoch 8, Batch 726, LR 2.082098 Loss 8.199020, Accuracy 75.119%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 727, LR 2.082324 Loss 8.199163, Accuracy 75.117%\n",
      "Epoch 8, Batch 728, LR 2.082550 Loss 8.199306, Accuracy 75.116%\n",
      "Epoch 8, Batch 729, LR 2.082776 Loss 8.199273, Accuracy 75.121%\n",
      "Epoch 8, Batch 730, LR 2.083002 Loss 8.198400, Accuracy 75.133%\n",
      "Epoch 8, Batch 731, LR 2.083228 Loss 8.199086, Accuracy 75.129%\n",
      "Epoch 8, Batch 732, LR 2.083454 Loss 8.199133, Accuracy 75.126%\n",
      "Epoch 8, Batch 733, LR 2.083680 Loss 8.198619, Accuracy 75.132%\n",
      "Epoch 8, Batch 734, LR 2.083906 Loss 8.198582, Accuracy 75.135%\n",
      "Epoch 8, Batch 735, LR 2.084132 Loss 8.198503, Accuracy 75.139%\n",
      "Epoch 8, Batch 736, LR 2.084357 Loss 8.197974, Accuracy 75.139%\n",
      "Epoch 8, Batch 737, LR 2.084583 Loss 8.197824, Accuracy 75.139%\n",
      "Epoch 8, Batch 738, LR 2.084809 Loss 8.198638, Accuracy 75.136%\n",
      "Epoch 8, Batch 739, LR 2.085034 Loss 8.197983, Accuracy 75.141%\n",
      "Epoch 8, Batch 740, LR 2.085260 Loss 8.198004, Accuracy 75.141%\n",
      "Epoch 8, Batch 741, LR 2.085485 Loss 8.197693, Accuracy 75.139%\n",
      "Epoch 8, Batch 742, LR 2.085710 Loss 8.197616, Accuracy 75.136%\n",
      "Epoch 8, Batch 743, LR 2.085936 Loss 8.198472, Accuracy 75.128%\n",
      "Epoch 8, Batch 744, LR 2.086161 Loss 8.198148, Accuracy 75.127%\n",
      "Epoch 8, Batch 745, LR 2.086386 Loss 8.198538, Accuracy 75.121%\n",
      "Epoch 8, Batch 746, LR 2.086612 Loss 8.200000, Accuracy 75.107%\n",
      "Epoch 8, Batch 747, LR 2.086837 Loss 8.200606, Accuracy 75.101%\n",
      "Epoch 8, Batch 748, LR 2.087062 Loss 8.200558, Accuracy 75.094%\n",
      "Epoch 8, Batch 749, LR 2.087287 Loss 8.200936, Accuracy 75.092%\n",
      "Epoch 8, Batch 750, LR 2.087512 Loss 8.200646, Accuracy 75.089%\n",
      "Epoch 8, Batch 751, LR 2.087737 Loss 8.200654, Accuracy 75.089%\n",
      "Epoch 8, Batch 752, LR 2.087962 Loss 8.200165, Accuracy 75.095%\n",
      "Epoch 8, Batch 753, LR 2.088187 Loss 8.198996, Accuracy 75.102%\n",
      "Epoch 8, Batch 754, LR 2.088411 Loss 8.200179, Accuracy 75.095%\n",
      "Epoch 8, Batch 755, LR 2.088636 Loss 8.200324, Accuracy 75.096%\n",
      "Epoch 8, Batch 756, LR 2.088861 Loss 8.200271, Accuracy 75.096%\n",
      "Epoch 8, Batch 757, LR 2.089086 Loss 8.199564, Accuracy 75.101%\n",
      "Epoch 8, Batch 758, LR 2.089310 Loss 8.199127, Accuracy 75.104%\n",
      "Epoch 8, Batch 759, LR 2.089535 Loss 8.199004, Accuracy 75.103%\n",
      "Epoch 8, Batch 760, LR 2.089759 Loss 8.198766, Accuracy 75.115%\n",
      "Epoch 8, Batch 761, LR 2.089984 Loss 8.199523, Accuracy 75.112%\n",
      "Epoch 8, Batch 762, LR 2.090208 Loss 8.198195, Accuracy 75.119%\n",
      "Epoch 8, Batch 763, LR 2.090433 Loss 8.197944, Accuracy 75.119%\n",
      "Epoch 8, Batch 764, LR 2.090657 Loss 8.198183, Accuracy 75.123%\n",
      "Epoch 8, Batch 765, LR 2.090881 Loss 8.199246, Accuracy 75.114%\n",
      "Epoch 8, Batch 766, LR 2.091106 Loss 8.199991, Accuracy 75.108%\n",
      "Epoch 8, Batch 767, LR 2.091330 Loss 8.200379, Accuracy 75.110%\n",
      "Epoch 8, Batch 768, LR 2.091554 Loss 8.200159, Accuracy 75.111%\n",
      "Epoch 8, Batch 769, LR 2.091778 Loss 8.199902, Accuracy 75.114%\n",
      "Epoch 8, Batch 770, LR 2.092002 Loss 8.200009, Accuracy 75.112%\n",
      "Epoch 8, Batch 771, LR 2.092226 Loss 8.199352, Accuracy 75.112%\n",
      "Epoch 8, Batch 772, LR 2.092450 Loss 8.199263, Accuracy 75.113%\n",
      "Epoch 8, Batch 773, LR 2.092674 Loss 8.199462, Accuracy 75.110%\n",
      "Epoch 8, Batch 774, LR 2.092898 Loss 8.199183, Accuracy 75.112%\n",
      "Epoch 8, Batch 775, LR 2.093122 Loss 8.198406, Accuracy 75.118%\n",
      "Epoch 8, Batch 776, LR 2.093345 Loss 8.198795, Accuracy 75.109%\n",
      "Epoch 8, Batch 777, LR 2.093569 Loss 8.199901, Accuracy 75.107%\n",
      "Epoch 8, Batch 778, LR 2.093793 Loss 8.200042, Accuracy 75.108%\n",
      "Epoch 8, Batch 779, LR 2.094016 Loss 8.199741, Accuracy 75.112%\n",
      "Epoch 8, Batch 780, LR 2.094240 Loss 8.199805, Accuracy 75.107%\n",
      "Epoch 8, Batch 781, LR 2.094463 Loss 8.199598, Accuracy 75.107%\n",
      "Epoch 8, Batch 782, LR 2.094687 Loss 8.200232, Accuracy 75.101%\n",
      "Epoch 8, Batch 783, LR 2.094910 Loss 8.199060, Accuracy 75.114%\n",
      "Epoch 8, Batch 784, LR 2.095134 Loss 8.199192, Accuracy 75.120%\n",
      "Epoch 8, Batch 785, LR 2.095357 Loss 8.199542, Accuracy 75.114%\n",
      "Epoch 8, Batch 786, LR 2.095580 Loss 8.198797, Accuracy 75.121%\n",
      "Epoch 8, Batch 787, LR 2.095803 Loss 8.198152, Accuracy 75.117%\n",
      "Epoch 8, Batch 788, LR 2.096027 Loss 8.197436, Accuracy 75.124%\n",
      "Epoch 8, Batch 789, LR 2.096250 Loss 8.197492, Accuracy 75.119%\n",
      "Epoch 8, Batch 790, LR 2.096473 Loss 8.197099, Accuracy 75.122%\n",
      "Epoch 8, Batch 791, LR 2.096696 Loss 8.197354, Accuracy 75.116%\n",
      "Epoch 8, Batch 792, LR 2.096919 Loss 8.197494, Accuracy 75.120%\n",
      "Epoch 8, Batch 793, LR 2.097142 Loss 8.197484, Accuracy 75.122%\n",
      "Epoch 8, Batch 794, LR 2.097364 Loss 8.197232, Accuracy 75.117%\n",
      "Epoch 8, Batch 795, LR 2.097587 Loss 8.197578, Accuracy 75.114%\n",
      "Epoch 8, Batch 796, LR 2.097810 Loss 8.196552, Accuracy 75.122%\n",
      "Epoch 8, Batch 797, LR 2.098033 Loss 8.197122, Accuracy 75.118%\n",
      "Epoch 8, Batch 798, LR 2.098255 Loss 8.196681, Accuracy 75.121%\n",
      "Epoch 8, Batch 799, LR 2.098478 Loss 8.196082, Accuracy 75.127%\n",
      "Epoch 8, Batch 800, LR 2.098701 Loss 8.196410, Accuracy 75.126%\n",
      "Epoch 8, Batch 801, LR 2.098923 Loss 8.195501, Accuracy 75.129%\n",
      "Epoch 8, Batch 802, LR 2.099146 Loss 8.196434, Accuracy 75.127%\n",
      "Epoch 8, Batch 803, LR 2.099368 Loss 8.196473, Accuracy 75.126%\n",
      "Epoch 8, Batch 804, LR 2.099590 Loss 8.197289, Accuracy 75.120%\n",
      "Epoch 8, Batch 805, LR 2.099813 Loss 8.198039, Accuracy 75.114%\n",
      "Epoch 8, Batch 806, LR 2.100035 Loss 8.198317, Accuracy 75.111%\n",
      "Epoch 8, Batch 807, LR 2.100257 Loss 8.198673, Accuracy 75.108%\n",
      "Epoch 8, Batch 808, LR 2.100480 Loss 8.198558, Accuracy 75.105%\n",
      "Epoch 8, Batch 809, LR 2.100702 Loss 8.198734, Accuracy 75.105%\n",
      "Epoch 8, Batch 810, LR 2.100924 Loss 8.198962, Accuracy 75.098%\n",
      "Epoch 8, Batch 811, LR 2.101146 Loss 8.198298, Accuracy 75.107%\n",
      "Epoch 8, Batch 812, LR 2.101368 Loss 8.197932, Accuracy 75.114%\n",
      "Epoch 8, Batch 813, LR 2.101590 Loss 8.197813, Accuracy 75.114%\n",
      "Epoch 8, Batch 814, LR 2.101812 Loss 8.197411, Accuracy 75.114%\n",
      "Epoch 8, Batch 815, LR 2.102033 Loss 8.197512, Accuracy 75.116%\n",
      "Epoch 8, Batch 816, LR 2.102255 Loss 8.197893, Accuracy 75.115%\n",
      "Epoch 8, Batch 817, LR 2.102477 Loss 8.197612, Accuracy 75.114%\n",
      "Epoch 8, Batch 818, LR 2.102699 Loss 8.197712, Accuracy 75.117%\n",
      "Epoch 8, Batch 819, LR 2.102920 Loss 8.197565, Accuracy 75.114%\n",
      "Epoch 8, Batch 820, LR 2.103142 Loss 8.196850, Accuracy 75.126%\n",
      "Epoch 8, Batch 821, LR 2.103363 Loss 8.197389, Accuracy 75.119%\n",
      "Epoch 8, Batch 822, LR 2.103585 Loss 8.196843, Accuracy 75.124%\n",
      "Epoch 8, Batch 823, LR 2.103806 Loss 8.196865, Accuracy 75.126%\n",
      "Epoch 8, Batch 824, LR 2.104028 Loss 8.196780, Accuracy 75.128%\n",
      "Epoch 8, Batch 825, LR 2.104249 Loss 8.196736, Accuracy 75.134%\n",
      "Epoch 8, Batch 826, LR 2.104470 Loss 8.196965, Accuracy 75.130%\n",
      "Epoch 8, Batch 827, LR 2.104692 Loss 8.197096, Accuracy 75.132%\n",
      "Epoch 8, Batch 828, LR 2.104913 Loss 8.197075, Accuracy 75.126%\n",
      "Epoch 8, Batch 829, LR 2.105134 Loss 8.196684, Accuracy 75.127%\n",
      "Epoch 8, Batch 830, LR 2.105355 Loss 8.197071, Accuracy 75.120%\n",
      "Epoch 8, Batch 831, LR 2.105576 Loss 8.196915, Accuracy 75.121%\n",
      "Epoch 8, Batch 832, LR 2.105797 Loss 8.195738, Accuracy 75.135%\n",
      "Epoch 8, Batch 833, LR 2.106018 Loss 8.195474, Accuracy 75.136%\n",
      "Epoch 8, Batch 834, LR 2.106239 Loss 8.194840, Accuracy 75.131%\n",
      "Epoch 8, Batch 835, LR 2.106460 Loss 8.194664, Accuracy 75.124%\n",
      "Epoch 8, Batch 836, LR 2.106681 Loss 8.193977, Accuracy 75.129%\n",
      "Epoch 8, Batch 837, LR 2.106901 Loss 8.194250, Accuracy 75.122%\n",
      "Epoch 8, Batch 838, LR 2.107122 Loss 8.194559, Accuracy 75.119%\n",
      "Epoch 8, Batch 839, LR 2.107343 Loss 8.194148, Accuracy 75.124%\n",
      "Epoch 8, Batch 840, LR 2.107563 Loss 8.194956, Accuracy 75.119%\n",
      "Epoch 8, Batch 841, LR 2.107784 Loss 8.194484, Accuracy 75.125%\n",
      "Epoch 8, Batch 842, LR 2.108004 Loss 8.193634, Accuracy 75.135%\n",
      "Epoch 8, Batch 843, LR 2.108225 Loss 8.192785, Accuracy 75.140%\n",
      "Epoch 8, Batch 844, LR 2.108445 Loss 8.192466, Accuracy 75.140%\n",
      "Epoch 8, Batch 845, LR 2.108666 Loss 8.191944, Accuracy 75.146%\n",
      "Epoch 8, Batch 846, LR 2.108886 Loss 8.192175, Accuracy 75.142%\n",
      "Epoch 8, Batch 847, LR 2.109106 Loss 8.191558, Accuracy 75.149%\n",
      "Epoch 8, Batch 848, LR 2.109326 Loss 8.192780, Accuracy 75.134%\n",
      "Epoch 8, Batch 849, LR 2.109546 Loss 8.192483, Accuracy 75.133%\n",
      "Epoch 8, Batch 850, LR 2.109767 Loss 8.193125, Accuracy 75.126%\n",
      "Epoch 8, Batch 851, LR 2.109987 Loss 8.193558, Accuracy 75.128%\n",
      "Epoch 8, Batch 852, LR 2.110207 Loss 8.193606, Accuracy 75.128%\n",
      "Epoch 8, Batch 853, LR 2.110427 Loss 8.193041, Accuracy 75.129%\n",
      "Epoch 8, Batch 854, LR 2.110646 Loss 8.192252, Accuracy 75.134%\n",
      "Epoch 8, Batch 855, LR 2.110866 Loss 8.191952, Accuracy 75.136%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 856, LR 2.111086 Loss 8.192487, Accuracy 75.135%\n",
      "Epoch 8, Batch 857, LR 2.111306 Loss 8.193131, Accuracy 75.128%\n",
      "Epoch 8, Batch 858, LR 2.111526 Loss 8.193312, Accuracy 75.130%\n",
      "Epoch 8, Batch 859, LR 2.111745 Loss 8.193100, Accuracy 75.133%\n",
      "Epoch 8, Batch 860, LR 2.111965 Loss 8.192842, Accuracy 75.138%\n",
      "Epoch 8, Batch 861, LR 2.112184 Loss 8.192275, Accuracy 75.141%\n",
      "Epoch 8, Batch 862, LR 2.112404 Loss 8.192893, Accuracy 75.137%\n",
      "Epoch 8, Batch 863, LR 2.112623 Loss 8.193557, Accuracy 75.132%\n",
      "Epoch 8, Batch 864, LR 2.112843 Loss 8.192930, Accuracy 75.135%\n",
      "Epoch 8, Batch 865, LR 2.113062 Loss 8.193419, Accuracy 75.129%\n",
      "Epoch 8, Batch 866, LR 2.113281 Loss 8.193113, Accuracy 75.135%\n",
      "Epoch 8, Batch 867, LR 2.113501 Loss 8.192925, Accuracy 75.134%\n",
      "Epoch 8, Batch 868, LR 2.113720 Loss 8.192805, Accuracy 75.130%\n",
      "Epoch 8, Batch 869, LR 2.113939 Loss 8.193001, Accuracy 75.130%\n",
      "Epoch 8, Batch 870, LR 2.114158 Loss 8.192717, Accuracy 75.135%\n",
      "Epoch 8, Batch 871, LR 2.114377 Loss 8.192566, Accuracy 75.135%\n",
      "Epoch 8, Batch 872, LR 2.114596 Loss 8.192262, Accuracy 75.136%\n",
      "Epoch 8, Batch 873, LR 2.114815 Loss 8.192648, Accuracy 75.132%\n",
      "Epoch 8, Batch 874, LR 2.115034 Loss 8.192168, Accuracy 75.137%\n",
      "Epoch 8, Batch 875, LR 2.115253 Loss 8.192121, Accuracy 75.138%\n",
      "Epoch 8, Batch 876, LR 2.115472 Loss 8.191992, Accuracy 75.136%\n",
      "Epoch 8, Batch 877, LR 2.115690 Loss 8.191487, Accuracy 75.138%\n",
      "Epoch 8, Batch 878, LR 2.115909 Loss 8.190911, Accuracy 75.145%\n",
      "Epoch 8, Batch 879, LR 2.116128 Loss 8.191040, Accuracy 75.146%\n",
      "Epoch 8, Batch 880, LR 2.116346 Loss 8.190535, Accuracy 75.154%\n",
      "Epoch 8, Batch 881, LR 2.116565 Loss 8.191119, Accuracy 75.152%\n",
      "Epoch 8, Batch 882, LR 2.116783 Loss 8.190997, Accuracy 75.153%\n",
      "Epoch 8, Batch 883, LR 2.117002 Loss 8.191850, Accuracy 75.148%\n",
      "Epoch 8, Batch 884, LR 2.117220 Loss 8.191086, Accuracy 75.159%\n",
      "Epoch 8, Batch 885, LR 2.117439 Loss 8.191551, Accuracy 75.154%\n",
      "Epoch 8, Batch 886, LR 2.117657 Loss 8.190990, Accuracy 75.159%\n",
      "Epoch 8, Batch 887, LR 2.117875 Loss 8.190391, Accuracy 75.164%\n",
      "Epoch 8, Batch 888, LR 2.118093 Loss 8.190685, Accuracy 75.165%\n",
      "Epoch 8, Batch 889, LR 2.118311 Loss 8.190720, Accuracy 75.164%\n",
      "Epoch 8, Batch 890, LR 2.118530 Loss 8.191121, Accuracy 75.160%\n",
      "Epoch 8, Batch 891, LR 2.118748 Loss 8.191096, Accuracy 75.163%\n",
      "Epoch 8, Batch 892, LR 2.118966 Loss 8.190705, Accuracy 75.167%\n",
      "Epoch 8, Batch 893, LR 2.119184 Loss 8.190998, Accuracy 75.171%\n",
      "Epoch 8, Batch 894, LR 2.119401 Loss 8.190965, Accuracy 75.168%\n",
      "Epoch 8, Batch 895, LR 2.119619 Loss 8.191457, Accuracy 75.166%\n",
      "Epoch 8, Batch 896, LR 2.119837 Loss 8.192396, Accuracy 75.156%\n",
      "Epoch 8, Batch 897, LR 2.120055 Loss 8.192439, Accuracy 75.156%\n",
      "Epoch 8, Batch 898, LR 2.120273 Loss 8.192202, Accuracy 75.158%\n",
      "Epoch 8, Batch 899, LR 2.120490 Loss 8.191414, Accuracy 75.159%\n",
      "Epoch 8, Batch 900, LR 2.120708 Loss 8.191997, Accuracy 75.155%\n",
      "Epoch 8, Batch 901, LR 2.120925 Loss 8.191938, Accuracy 75.149%\n",
      "Epoch 8, Batch 902, LR 2.121143 Loss 8.191341, Accuracy 75.150%\n",
      "Epoch 8, Batch 903, LR 2.121360 Loss 8.191847, Accuracy 75.144%\n",
      "Epoch 8, Batch 904, LR 2.121578 Loss 8.192579, Accuracy 75.136%\n",
      "Epoch 8, Batch 905, LR 2.121795 Loss 8.192924, Accuracy 75.130%\n",
      "Epoch 8, Batch 906, LR 2.122012 Loss 8.192066, Accuracy 75.139%\n",
      "Epoch 8, Batch 907, LR 2.122229 Loss 8.192395, Accuracy 75.134%\n",
      "Epoch 8, Batch 908, LR 2.122447 Loss 8.192425, Accuracy 75.133%\n",
      "Epoch 8, Batch 909, LR 2.122664 Loss 8.192798, Accuracy 75.130%\n",
      "Epoch 8, Batch 910, LR 2.122881 Loss 8.192617, Accuracy 75.130%\n",
      "Epoch 8, Batch 911, LR 2.123098 Loss 8.192090, Accuracy 75.131%\n",
      "Epoch 8, Batch 912, LR 2.123315 Loss 8.192963, Accuracy 75.124%\n",
      "Epoch 8, Batch 913, LR 2.123532 Loss 8.192391, Accuracy 75.127%\n",
      "Epoch 8, Batch 914, LR 2.123749 Loss 8.192413, Accuracy 75.123%\n",
      "Epoch 8, Batch 915, LR 2.123966 Loss 8.191499, Accuracy 75.129%\n",
      "Epoch 8, Batch 916, LR 2.124182 Loss 8.192465, Accuracy 75.119%\n",
      "Epoch 8, Batch 917, LR 2.124399 Loss 8.192261, Accuracy 75.119%\n",
      "Epoch 8, Batch 918, LR 2.124616 Loss 8.191719, Accuracy 75.123%\n",
      "Epoch 8, Batch 919, LR 2.124832 Loss 8.191261, Accuracy 75.123%\n",
      "Epoch 8, Batch 920, LR 2.125049 Loss 8.191156, Accuracy 75.125%\n",
      "Epoch 8, Batch 921, LR 2.125265 Loss 8.191604, Accuracy 75.123%\n",
      "Epoch 8, Batch 922, LR 2.125482 Loss 8.191063, Accuracy 75.130%\n",
      "Epoch 8, Batch 923, LR 2.125698 Loss 8.191002, Accuracy 75.129%\n",
      "Epoch 8, Batch 924, LR 2.125915 Loss 8.190644, Accuracy 75.128%\n",
      "Epoch 8, Batch 925, LR 2.126131 Loss 8.191140, Accuracy 75.125%\n",
      "Epoch 8, Batch 926, LR 2.126347 Loss 8.191488, Accuracy 75.120%\n",
      "Epoch 8, Batch 927, LR 2.126564 Loss 8.191384, Accuracy 75.119%\n",
      "Epoch 8, Batch 928, LR 2.126780 Loss 8.191367, Accuracy 75.121%\n",
      "Epoch 8, Batch 929, LR 2.126996 Loss 8.191235, Accuracy 75.124%\n",
      "Epoch 8, Batch 930, LR 2.127212 Loss 8.190746, Accuracy 75.125%\n",
      "Epoch 8, Batch 931, LR 2.127428 Loss 8.189830, Accuracy 75.133%\n",
      "Epoch 8, Batch 932, LR 2.127644 Loss 8.188800, Accuracy 75.137%\n",
      "Epoch 8, Batch 933, LR 2.127860 Loss 8.189112, Accuracy 75.133%\n",
      "Epoch 8, Batch 934, LR 2.128076 Loss 8.189452, Accuracy 75.134%\n",
      "Epoch 8, Batch 935, LR 2.128292 Loss 8.189746, Accuracy 75.133%\n",
      "Epoch 8, Batch 936, LR 2.128507 Loss 8.189520, Accuracy 75.139%\n",
      "Epoch 8, Batch 937, LR 2.128723 Loss 8.188340, Accuracy 75.148%\n",
      "Epoch 8, Batch 938, LR 2.128939 Loss 8.188729, Accuracy 75.150%\n",
      "Epoch 8, Batch 939, LR 2.129154 Loss 8.188154, Accuracy 75.161%\n",
      "Epoch 8, Batch 940, LR 2.129370 Loss 8.187391, Accuracy 75.166%\n",
      "Epoch 8, Batch 941, LR 2.129585 Loss 8.187248, Accuracy 75.164%\n",
      "Epoch 8, Batch 942, LR 2.129801 Loss 8.187369, Accuracy 75.163%\n",
      "Epoch 8, Batch 943, LR 2.130016 Loss 8.186941, Accuracy 75.169%\n",
      "Epoch 8, Batch 944, LR 2.130232 Loss 8.186643, Accuracy 75.172%\n",
      "Epoch 8, Batch 945, LR 2.130447 Loss 8.186546, Accuracy 75.173%\n",
      "Epoch 8, Batch 946, LR 2.130662 Loss 8.187265, Accuracy 75.162%\n",
      "Epoch 8, Batch 947, LR 2.130877 Loss 8.187052, Accuracy 75.167%\n",
      "Epoch 8, Batch 948, LR 2.131092 Loss 8.187551, Accuracy 75.165%\n",
      "Epoch 8, Batch 949, LR 2.131308 Loss 8.186572, Accuracy 75.172%\n",
      "Epoch 8, Batch 950, LR 2.131523 Loss 8.186173, Accuracy 75.173%\n",
      "Epoch 8, Batch 951, LR 2.131738 Loss 8.186516, Accuracy 75.168%\n",
      "Epoch 8, Batch 952, LR 2.131953 Loss 8.186188, Accuracy 75.167%\n",
      "Epoch 8, Batch 953, LR 2.132167 Loss 8.185285, Accuracy 75.168%\n",
      "Epoch 8, Batch 954, LR 2.132382 Loss 8.184850, Accuracy 75.170%\n",
      "Epoch 8, Batch 955, LR 2.132597 Loss 8.185114, Accuracy 75.160%\n",
      "Epoch 8, Batch 956, LR 2.132812 Loss 8.184855, Accuracy 75.163%\n",
      "Epoch 8, Batch 957, LR 2.133027 Loss 8.185230, Accuracy 75.160%\n",
      "Epoch 8, Batch 958, LR 2.133241 Loss 8.185017, Accuracy 75.166%\n",
      "Epoch 8, Batch 959, LR 2.133456 Loss 8.184967, Accuracy 75.169%\n",
      "Epoch 8, Batch 960, LR 2.133670 Loss 8.185719, Accuracy 75.162%\n",
      "Epoch 8, Batch 961, LR 2.133885 Loss 8.185929, Accuracy 75.161%\n",
      "Epoch 8, Batch 962, LR 2.134099 Loss 8.185826, Accuracy 75.165%\n",
      "Epoch 8, Batch 963, LR 2.134314 Loss 8.185059, Accuracy 75.170%\n",
      "Epoch 8, Batch 964, LR 2.134528 Loss 8.184738, Accuracy 75.173%\n",
      "Epoch 8, Batch 965, LR 2.134742 Loss 8.184417, Accuracy 75.179%\n",
      "Epoch 8, Batch 966, LR 2.134956 Loss 8.184310, Accuracy 75.180%\n",
      "Epoch 8, Batch 967, LR 2.135171 Loss 8.184029, Accuracy 75.181%\n",
      "Epoch 8, Batch 968, LR 2.135385 Loss 8.183890, Accuracy 75.180%\n",
      "Epoch 8, Batch 969, LR 2.135599 Loss 8.183932, Accuracy 75.173%\n",
      "Epoch 8, Batch 970, LR 2.135813 Loss 8.184118, Accuracy 75.170%\n",
      "Epoch 8, Batch 971, LR 2.136027 Loss 8.183614, Accuracy 75.177%\n",
      "Epoch 8, Batch 972, LR 2.136241 Loss 8.183846, Accuracy 75.178%\n",
      "Epoch 8, Batch 973, LR 2.136455 Loss 8.183659, Accuracy 75.181%\n",
      "Epoch 8, Batch 974, LR 2.136668 Loss 8.183081, Accuracy 75.185%\n",
      "Epoch 8, Batch 975, LR 2.136882 Loss 8.182294, Accuracy 75.189%\n",
      "Epoch 8, Batch 976, LR 2.137096 Loss 8.181945, Accuracy 75.186%\n",
      "Epoch 8, Batch 977, LR 2.137309 Loss 8.181860, Accuracy 75.179%\n",
      "Epoch 8, Batch 978, LR 2.137523 Loss 8.182039, Accuracy 75.181%\n",
      "Epoch 8, Batch 979, LR 2.137737 Loss 8.181205, Accuracy 75.186%\n",
      "Epoch 8, Batch 980, LR 2.137950 Loss 8.181692, Accuracy 75.184%\n",
      "Epoch 8, Batch 981, LR 2.138164 Loss 8.182065, Accuracy 75.183%\n",
      "Epoch 8, Batch 982, LR 2.138377 Loss 8.181429, Accuracy 75.188%\n",
      "Epoch 8, Batch 983, LR 2.138590 Loss 8.181257, Accuracy 75.188%\n",
      "Epoch 8, Batch 984, LR 2.138804 Loss 8.181480, Accuracy 75.184%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 985, LR 2.139017 Loss 8.181082, Accuracy 75.186%\n",
      "Epoch 8, Batch 986, LR 2.139230 Loss 8.180760, Accuracy 75.189%\n",
      "Epoch 8, Batch 987, LR 2.139443 Loss 8.180744, Accuracy 75.191%\n",
      "Epoch 8, Batch 988, LR 2.139656 Loss 8.179969, Accuracy 75.192%\n",
      "Epoch 8, Batch 989, LR 2.139869 Loss 8.179735, Accuracy 75.197%\n",
      "Epoch 8, Batch 990, LR 2.140082 Loss 8.179590, Accuracy 75.199%\n",
      "Epoch 8, Batch 991, LR 2.140295 Loss 8.179745, Accuracy 75.196%\n",
      "Epoch 8, Batch 992, LR 2.140508 Loss 8.179500, Accuracy 75.198%\n",
      "Epoch 8, Batch 993, LR 2.140721 Loss 8.179410, Accuracy 75.199%\n",
      "Epoch 8, Batch 994, LR 2.140934 Loss 8.178620, Accuracy 75.204%\n",
      "Epoch 8, Batch 995, LR 2.141147 Loss 8.178655, Accuracy 75.202%\n",
      "Epoch 8, Batch 996, LR 2.141359 Loss 8.178832, Accuracy 75.201%\n",
      "Epoch 8, Batch 997, LR 2.141572 Loss 8.178667, Accuracy 75.200%\n",
      "Epoch 8, Batch 998, LR 2.141784 Loss 8.178848, Accuracy 75.200%\n",
      "Epoch 8, Batch 999, LR 2.141997 Loss 8.178546, Accuracy 75.202%\n",
      "Epoch 8, Batch 1000, LR 2.142209 Loss 8.178544, Accuracy 75.201%\n",
      "Epoch 8, Batch 1001, LR 2.142422 Loss 8.178542, Accuracy 75.200%\n",
      "Epoch 8, Batch 1002, LR 2.142634 Loss 8.179471, Accuracy 75.194%\n",
      "Epoch 8, Batch 1003, LR 2.142847 Loss 8.178969, Accuracy 75.200%\n",
      "Epoch 8, Batch 1004, LR 2.143059 Loss 8.177919, Accuracy 75.213%\n",
      "Epoch 8, Batch 1005, LR 2.143271 Loss 8.176839, Accuracy 75.221%\n",
      "Epoch 8, Batch 1006, LR 2.143483 Loss 8.176283, Accuracy 75.221%\n",
      "Epoch 8, Batch 1007, LR 2.143695 Loss 8.175805, Accuracy 75.222%\n",
      "Epoch 8, Batch 1008, LR 2.143907 Loss 8.175532, Accuracy 75.222%\n",
      "Epoch 8, Batch 1009, LR 2.144119 Loss 8.175733, Accuracy 75.221%\n",
      "Epoch 8, Batch 1010, LR 2.144331 Loss 8.175540, Accuracy 75.222%\n",
      "Epoch 8, Batch 1011, LR 2.144543 Loss 8.175861, Accuracy 75.222%\n",
      "Epoch 8, Batch 1012, LR 2.144755 Loss 8.176092, Accuracy 75.221%\n",
      "Epoch 8, Batch 1013, LR 2.144967 Loss 8.175225, Accuracy 75.230%\n",
      "Epoch 8, Batch 1014, LR 2.145179 Loss 8.174622, Accuracy 75.236%\n",
      "Epoch 8, Batch 1015, LR 2.145390 Loss 8.174389, Accuracy 75.242%\n",
      "Epoch 8, Batch 1016, LR 2.145602 Loss 8.174308, Accuracy 75.241%\n",
      "Epoch 8, Batch 1017, LR 2.145813 Loss 8.174087, Accuracy 75.244%\n",
      "Epoch 8, Batch 1018, LR 2.146025 Loss 8.174046, Accuracy 75.246%\n",
      "Epoch 8, Batch 1019, LR 2.146237 Loss 8.173742, Accuracy 75.249%\n",
      "Epoch 8, Batch 1020, LR 2.146448 Loss 8.173495, Accuracy 75.255%\n",
      "Epoch 8, Batch 1021, LR 2.146659 Loss 8.173708, Accuracy 75.256%\n",
      "Epoch 8, Batch 1022, LR 2.146871 Loss 8.173603, Accuracy 75.255%\n",
      "Epoch 8, Batch 1023, LR 2.147082 Loss 8.174011, Accuracy 75.254%\n",
      "Epoch 8, Batch 1024, LR 2.147293 Loss 8.173763, Accuracy 75.261%\n",
      "Epoch 8, Batch 1025, LR 2.147504 Loss 8.173574, Accuracy 75.263%\n",
      "Epoch 8, Batch 1026, LR 2.147715 Loss 8.173489, Accuracy 75.263%\n",
      "Epoch 8, Batch 1027, LR 2.147926 Loss 8.172577, Accuracy 75.266%\n",
      "Epoch 8, Batch 1028, LR 2.148138 Loss 8.172809, Accuracy 75.266%\n",
      "Epoch 8, Batch 1029, LR 2.148348 Loss 8.172343, Accuracy 75.269%\n",
      "Epoch 8, Batch 1030, LR 2.148559 Loss 8.172824, Accuracy 75.265%\n",
      "Epoch 8, Batch 1031, LR 2.148770 Loss 8.172851, Accuracy 75.265%\n",
      "Epoch 8, Batch 1032, LR 2.148981 Loss 8.173064, Accuracy 75.261%\n",
      "Epoch 8, Batch 1033, LR 2.149192 Loss 8.171964, Accuracy 75.268%\n",
      "Epoch 8, Batch 1034, LR 2.149402 Loss 8.171716, Accuracy 75.267%\n",
      "Epoch 8, Batch 1035, LR 2.149613 Loss 8.171333, Accuracy 75.272%\n",
      "Epoch 8, Batch 1036, LR 2.149824 Loss 8.170928, Accuracy 75.273%\n",
      "Epoch 8, Batch 1037, LR 2.150034 Loss 8.170665, Accuracy 75.276%\n",
      "Epoch 8, Batch 1038, LR 2.150245 Loss 8.169784, Accuracy 75.282%\n",
      "Epoch 8, Batch 1039, LR 2.150455 Loss 8.169331, Accuracy 75.283%\n",
      "Epoch 8, Batch 1040, LR 2.150666 Loss 8.169407, Accuracy 75.283%\n",
      "Epoch 8, Batch 1041, LR 2.150876 Loss 8.169624, Accuracy 75.279%\n",
      "Epoch 8, Batch 1042, LR 2.151086 Loss 8.169383, Accuracy 75.283%\n",
      "Epoch 8, Batch 1043, LR 2.151296 Loss 8.169507, Accuracy 75.285%\n",
      "Epoch 8, Batch 1044, LR 2.151507 Loss 8.169580, Accuracy 75.283%\n",
      "Epoch 8, Batch 1045, LR 2.151717 Loss 8.169543, Accuracy 75.286%\n",
      "Epoch 8, Batch 1046, LR 2.151927 Loss 8.169201, Accuracy 75.288%\n",
      "Epoch 8, Batch 1047, LR 2.152137 Loss 8.168976, Accuracy 75.285%\n",
      "Epoch 8, Loss (train set) 8.168976, Accuracy (train set) 75.285%\n",
      "Epoch 9, Batch 1, LR 2.152347 Loss 7.775930, Accuracy 78.125%\n",
      "Epoch 9, Batch 2, LR 2.152557 Loss 7.861785, Accuracy 78.906%\n",
      "Epoch 9, Batch 3, LR 2.152767 Loss 7.969560, Accuracy 76.302%\n",
      "Epoch 9, Batch 4, LR 2.152976 Loss 8.126243, Accuracy 75.195%\n",
      "Epoch 9, Batch 5, LR 2.153186 Loss 8.141284, Accuracy 75.938%\n",
      "Epoch 9, Batch 6, LR 2.153396 Loss 8.117684, Accuracy 76.432%\n",
      "Epoch 9, Batch 7, LR 2.153605 Loss 7.963440, Accuracy 76.897%\n",
      "Epoch 9, Batch 8, LR 2.153815 Loss 7.928357, Accuracy 77.539%\n",
      "Epoch 9, Batch 9, LR 2.154025 Loss 7.920199, Accuracy 77.344%\n",
      "Epoch 9, Batch 10, LR 2.154234 Loss 7.951298, Accuracy 76.953%\n",
      "Epoch 9, Batch 11, LR 2.154444 Loss 7.993670, Accuracy 76.420%\n",
      "Epoch 9, Batch 12, LR 2.154653 Loss 7.916791, Accuracy 76.823%\n",
      "Epoch 9, Batch 13, LR 2.154862 Loss 7.866257, Accuracy 77.103%\n",
      "Epoch 9, Batch 14, LR 2.155072 Loss 7.862577, Accuracy 77.288%\n",
      "Epoch 9, Batch 15, LR 2.155281 Loss 7.820704, Accuracy 77.500%\n",
      "Epoch 9, Batch 16, LR 2.155490 Loss 7.862513, Accuracy 76.953%\n",
      "Epoch 9, Batch 17, LR 2.155699 Loss 7.829481, Accuracy 77.482%\n",
      "Epoch 9, Batch 18, LR 2.155908 Loss 7.812408, Accuracy 77.604%\n",
      "Epoch 9, Batch 19, LR 2.156117 Loss 7.838316, Accuracy 77.714%\n",
      "Epoch 9, Batch 20, LR 2.156326 Loss 7.821564, Accuracy 77.930%\n",
      "Epoch 9, Batch 21, LR 2.156535 Loss 7.819077, Accuracy 78.051%\n",
      "Epoch 9, Batch 22, LR 2.156744 Loss 7.832655, Accuracy 77.912%\n",
      "Epoch 9, Batch 23, LR 2.156953 Loss 7.821781, Accuracy 78.159%\n",
      "Epoch 9, Batch 24, LR 2.157161 Loss 7.818375, Accuracy 78.125%\n",
      "Epoch 9, Batch 25, LR 2.157370 Loss 7.806334, Accuracy 78.281%\n",
      "Epoch 9, Batch 26, LR 2.157579 Loss 7.811586, Accuracy 78.215%\n",
      "Epoch 9, Batch 27, LR 2.157787 Loss 7.776717, Accuracy 78.356%\n",
      "Epoch 9, Batch 28, LR 2.157996 Loss 7.765427, Accuracy 78.460%\n",
      "Epoch 9, Batch 29, LR 2.158204 Loss 7.770723, Accuracy 78.287%\n",
      "Epoch 9, Batch 30, LR 2.158413 Loss 7.763147, Accuracy 78.255%\n",
      "Epoch 9, Batch 31, LR 2.158621 Loss 7.774368, Accuracy 78.175%\n",
      "Epoch 9, Batch 32, LR 2.158830 Loss 7.785230, Accuracy 78.149%\n",
      "Epoch 9, Batch 33, LR 2.159038 Loss 7.810229, Accuracy 77.912%\n",
      "Epoch 9, Batch 34, LR 2.159246 Loss 7.808712, Accuracy 77.803%\n",
      "Epoch 9, Batch 35, LR 2.159454 Loss 7.791951, Accuracy 77.969%\n",
      "Epoch 9, Batch 36, LR 2.159662 Loss 7.793472, Accuracy 77.843%\n",
      "Epoch 9, Batch 37, LR 2.159870 Loss 7.809651, Accuracy 77.724%\n",
      "Epoch 9, Batch 38, LR 2.160078 Loss 7.826018, Accuracy 77.734%\n",
      "Epoch 9, Batch 39, LR 2.160286 Loss 7.831779, Accuracy 77.744%\n",
      "Epoch 9, Batch 40, LR 2.160494 Loss 7.818627, Accuracy 77.734%\n",
      "Epoch 9, Batch 41, LR 2.160702 Loss 7.816311, Accuracy 77.706%\n",
      "Epoch 9, Batch 42, LR 2.160910 Loss 7.833305, Accuracy 77.604%\n",
      "Epoch 9, Batch 43, LR 2.161118 Loss 7.842953, Accuracy 77.616%\n",
      "Epoch 9, Batch 44, LR 2.161325 Loss 7.844858, Accuracy 77.717%\n",
      "Epoch 9, Batch 45, LR 2.161533 Loss 7.853531, Accuracy 77.639%\n",
      "Epoch 9, Batch 46, LR 2.161741 Loss 7.858519, Accuracy 77.514%\n",
      "Epoch 9, Batch 47, LR 2.161948 Loss 7.857831, Accuracy 77.527%\n",
      "Epoch 9, Batch 48, LR 2.162156 Loss 7.868915, Accuracy 77.409%\n",
      "Epoch 9, Batch 49, LR 2.162363 Loss 7.858340, Accuracy 77.487%\n",
      "Epoch 9, Batch 50, LR 2.162570 Loss 7.850129, Accuracy 77.656%\n",
      "Epoch 9, Batch 51, LR 2.162778 Loss 7.864120, Accuracy 77.558%\n",
      "Epoch 9, Batch 52, LR 2.162985 Loss 7.861831, Accuracy 77.674%\n",
      "Epoch 9, Batch 53, LR 2.163192 Loss 7.861149, Accuracy 77.639%\n",
      "Epoch 9, Batch 54, LR 2.163399 Loss 7.862056, Accuracy 77.633%\n",
      "Epoch 9, Batch 55, LR 2.163607 Loss 7.861863, Accuracy 77.727%\n",
      "Epoch 9, Batch 56, LR 2.163814 Loss 7.868629, Accuracy 77.706%\n",
      "Epoch 9, Batch 57, LR 2.164021 Loss 7.871147, Accuracy 77.741%\n",
      "Epoch 9, Batch 58, LR 2.164228 Loss 7.876691, Accuracy 77.775%\n",
      "Epoch 9, Batch 59, LR 2.164434 Loss 7.882630, Accuracy 77.728%\n",
      "Epoch 9, Batch 60, LR 2.164641 Loss 7.886015, Accuracy 77.734%\n",
      "Epoch 9, Batch 61, LR 2.164848 Loss 7.887158, Accuracy 77.715%\n",
      "Epoch 9, Batch 62, LR 2.165055 Loss 7.891460, Accuracy 77.684%\n",
      "Epoch 9, Batch 63, LR 2.165262 Loss 7.892996, Accuracy 77.629%\n",
      "Epoch 9, Batch 64, LR 2.165468 Loss 7.898478, Accuracy 77.551%\n",
      "Epoch 9, Batch 65, LR 2.165675 Loss 7.888924, Accuracy 77.596%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 66, LR 2.165881 Loss 7.877060, Accuracy 77.699%\n",
      "Epoch 9, Batch 67, LR 2.166088 Loss 7.877829, Accuracy 77.729%\n",
      "Epoch 9, Batch 68, LR 2.166294 Loss 7.877382, Accuracy 77.746%\n",
      "Epoch 9, Batch 69, LR 2.166501 Loss 7.874533, Accuracy 77.751%\n",
      "Epoch 9, Batch 70, LR 2.166707 Loss 7.869714, Accuracy 77.790%\n",
      "Epoch 9, Batch 71, LR 2.166913 Loss 7.866200, Accuracy 77.817%\n",
      "Epoch 9, Batch 72, LR 2.167119 Loss 7.870100, Accuracy 77.810%\n",
      "Epoch 9, Batch 73, LR 2.167325 Loss 7.873418, Accuracy 77.783%\n",
      "Epoch 9, Batch 74, LR 2.167532 Loss 7.876639, Accuracy 77.787%\n",
      "Epoch 9, Batch 75, LR 2.167738 Loss 7.865619, Accuracy 77.875%\n",
      "Epoch 9, Batch 76, LR 2.167944 Loss 7.863475, Accuracy 77.868%\n",
      "Epoch 9, Batch 77, LR 2.168150 Loss 7.858520, Accuracy 77.953%\n",
      "Epoch 9, Batch 78, LR 2.168355 Loss 7.850818, Accuracy 77.965%\n",
      "Epoch 9, Batch 79, LR 2.168561 Loss 7.848981, Accuracy 78.006%\n",
      "Epoch 9, Batch 80, LR 2.168767 Loss 7.848269, Accuracy 78.008%\n",
      "Epoch 9, Batch 81, LR 2.168973 Loss 7.851489, Accuracy 78.009%\n",
      "Epoch 9, Batch 82, LR 2.169178 Loss 7.848962, Accuracy 78.049%\n",
      "Epoch 9, Batch 83, LR 2.169384 Loss 7.840163, Accuracy 78.134%\n",
      "Epoch 9, Batch 84, LR 2.169590 Loss 7.839367, Accuracy 78.134%\n",
      "Epoch 9, Batch 85, LR 2.169795 Loss 7.838479, Accuracy 78.125%\n",
      "Epoch 9, Batch 86, LR 2.170001 Loss 7.844234, Accuracy 78.025%\n",
      "Epoch 9, Batch 87, LR 2.170206 Loss 7.845053, Accuracy 77.954%\n",
      "Epoch 9, Batch 88, LR 2.170411 Loss 7.844691, Accuracy 77.983%\n",
      "Epoch 9, Batch 89, LR 2.170617 Loss 7.846032, Accuracy 77.985%\n",
      "Epoch 9, Batch 90, LR 2.170822 Loss 7.855897, Accuracy 77.934%\n",
      "Epoch 9, Batch 91, LR 2.171027 Loss 7.854640, Accuracy 77.867%\n",
      "Epoch 9, Batch 92, LR 2.171232 Loss 7.853852, Accuracy 77.862%\n",
      "Epoch 9, Batch 93, LR 2.171437 Loss 7.849514, Accuracy 77.865%\n",
      "Epoch 9, Batch 94, LR 2.171642 Loss 7.847747, Accuracy 77.851%\n",
      "Epoch 9, Batch 95, LR 2.171847 Loss 7.846189, Accuracy 77.854%\n",
      "Epoch 9, Batch 96, LR 2.172052 Loss 7.846775, Accuracy 77.816%\n",
      "Epoch 9, Batch 97, LR 2.172257 Loss 7.846514, Accuracy 77.827%\n",
      "Epoch 9, Batch 98, LR 2.172462 Loss 7.848024, Accuracy 77.814%\n",
      "Epoch 9, Batch 99, LR 2.172667 Loss 7.851720, Accuracy 77.801%\n",
      "Epoch 9, Batch 100, LR 2.172871 Loss 7.859588, Accuracy 77.766%\n",
      "Epoch 9, Batch 101, LR 2.173076 Loss 7.867686, Accuracy 77.700%\n",
      "Epoch 9, Batch 102, LR 2.173281 Loss 7.866910, Accuracy 77.696%\n",
      "Epoch 9, Batch 103, LR 2.173485 Loss 7.870018, Accuracy 77.685%\n",
      "Epoch 9, Batch 104, LR 2.173690 Loss 7.863850, Accuracy 77.764%\n",
      "Epoch 9, Batch 105, LR 2.173894 Loss 7.867374, Accuracy 77.768%\n",
      "Epoch 9, Batch 106, LR 2.174098 Loss 7.866319, Accuracy 77.764%\n",
      "Epoch 9, Batch 107, LR 2.174303 Loss 7.866571, Accuracy 77.782%\n",
      "Epoch 9, Batch 108, LR 2.174507 Loss 7.872583, Accuracy 77.756%\n",
      "Epoch 9, Batch 109, LR 2.174711 Loss 7.873365, Accuracy 77.731%\n",
      "Epoch 9, Batch 110, LR 2.174915 Loss 7.872884, Accuracy 77.734%\n",
      "Epoch 9, Batch 111, LR 2.175119 Loss 7.874822, Accuracy 77.738%\n",
      "Epoch 9, Batch 112, LR 2.175324 Loss 7.875266, Accuracy 77.720%\n",
      "Epoch 9, Batch 113, LR 2.175528 Loss 7.877430, Accuracy 77.696%\n",
      "Epoch 9, Batch 114, LR 2.175731 Loss 7.880121, Accuracy 77.673%\n",
      "Epoch 9, Batch 115, LR 2.175935 Loss 7.876736, Accuracy 77.683%\n",
      "Epoch 9, Batch 116, LR 2.176139 Loss 7.881551, Accuracy 77.654%\n",
      "Epoch 9, Batch 117, LR 2.176343 Loss 7.877698, Accuracy 77.664%\n",
      "Epoch 9, Batch 118, LR 2.176547 Loss 7.875721, Accuracy 77.635%\n",
      "Epoch 9, Batch 119, LR 2.176750 Loss 7.876373, Accuracy 77.633%\n",
      "Epoch 9, Batch 120, LR 2.176954 Loss 7.878949, Accuracy 77.598%\n",
      "Epoch 9, Batch 121, LR 2.177158 Loss 7.877021, Accuracy 77.621%\n",
      "Epoch 9, Batch 122, LR 2.177361 Loss 7.877269, Accuracy 77.606%\n",
      "Epoch 9, Batch 123, LR 2.177565 Loss 7.878565, Accuracy 77.623%\n",
      "Epoch 9, Batch 124, LR 2.177768 Loss 7.878803, Accuracy 77.621%\n",
      "Epoch 9, Batch 125, LR 2.177971 Loss 7.880308, Accuracy 77.594%\n",
      "Epoch 9, Batch 126, LR 2.178175 Loss 7.880647, Accuracy 77.567%\n",
      "Epoch 9, Batch 127, LR 2.178378 Loss 7.885237, Accuracy 77.522%\n",
      "Epoch 9, Batch 128, LR 2.178581 Loss 7.883081, Accuracy 77.545%\n",
      "Epoch 9, Batch 129, LR 2.178784 Loss 7.884828, Accuracy 77.531%\n",
      "Epoch 9, Batch 130, LR 2.178987 Loss 7.882308, Accuracy 77.542%\n",
      "Epoch 9, Batch 131, LR 2.179190 Loss 7.884351, Accuracy 77.517%\n",
      "Epoch 9, Batch 132, LR 2.179393 Loss 7.887293, Accuracy 77.474%\n",
      "Epoch 9, Batch 133, LR 2.179596 Loss 7.880497, Accuracy 77.514%\n",
      "Epoch 9, Batch 134, LR 2.179799 Loss 7.879754, Accuracy 77.519%\n",
      "Epoch 9, Batch 135, LR 2.180002 Loss 7.878202, Accuracy 77.541%\n",
      "Epoch 9, Batch 136, LR 2.180205 Loss 7.879028, Accuracy 77.562%\n",
      "Epoch 9, Batch 137, LR 2.180407 Loss 7.882828, Accuracy 77.509%\n",
      "Epoch 9, Batch 138, LR 2.180610 Loss 7.886649, Accuracy 77.468%\n",
      "Epoch 9, Batch 139, LR 2.180813 Loss 7.881131, Accuracy 77.501%\n",
      "Epoch 9, Batch 140, LR 2.181015 Loss 7.880017, Accuracy 77.522%\n",
      "Epoch 9, Batch 141, LR 2.181218 Loss 7.881777, Accuracy 77.477%\n",
      "Epoch 9, Batch 142, LR 2.181420 Loss 7.882411, Accuracy 77.443%\n",
      "Epoch 9, Batch 143, LR 2.181622 Loss 7.886204, Accuracy 77.404%\n",
      "Epoch 9, Batch 144, LR 2.181825 Loss 7.883223, Accuracy 77.436%\n",
      "Epoch 9, Batch 145, LR 2.182027 Loss 7.880959, Accuracy 77.419%\n",
      "Epoch 9, Batch 146, LR 2.182229 Loss 7.877273, Accuracy 77.429%\n",
      "Epoch 9, Batch 147, LR 2.182431 Loss 7.879497, Accuracy 77.392%\n",
      "Epoch 9, Batch 148, LR 2.182634 Loss 7.882401, Accuracy 77.381%\n",
      "Epoch 9, Batch 149, LR 2.182836 Loss 7.877530, Accuracy 77.438%\n",
      "Epoch 9, Batch 150, LR 2.183038 Loss 7.874065, Accuracy 77.479%\n",
      "Epoch 9, Batch 151, LR 2.183240 Loss 7.876039, Accuracy 77.468%\n",
      "Epoch 9, Batch 152, LR 2.183441 Loss 7.873443, Accuracy 77.467%\n",
      "Epoch 9, Batch 153, LR 2.183643 Loss 7.872005, Accuracy 77.497%\n",
      "Epoch 9, Batch 154, LR 2.183845 Loss 7.871005, Accuracy 77.471%\n",
      "Epoch 9, Batch 155, LR 2.184047 Loss 7.867809, Accuracy 77.510%\n",
      "Epoch 9, Batch 156, LR 2.184248 Loss 7.868077, Accuracy 77.509%\n",
      "Epoch 9, Batch 157, LR 2.184450 Loss 7.868639, Accuracy 77.503%\n",
      "Epoch 9, Batch 158, LR 2.184652 Loss 7.865868, Accuracy 77.537%\n",
      "Epoch 9, Batch 159, LR 2.184853 Loss 7.863351, Accuracy 77.570%\n",
      "Epoch 9, Batch 160, LR 2.185055 Loss 7.864806, Accuracy 77.559%\n",
      "Epoch 9, Batch 161, LR 2.185256 Loss 7.862579, Accuracy 77.586%\n",
      "Epoch 9, Batch 162, LR 2.185457 Loss 7.864649, Accuracy 77.551%\n",
      "Epoch 9, Batch 163, LR 2.185659 Loss 7.861838, Accuracy 77.569%\n",
      "Epoch 9, Batch 164, LR 2.185860 Loss 7.864208, Accuracy 77.558%\n",
      "Epoch 9, Batch 165, LR 2.186061 Loss 7.865471, Accuracy 77.557%\n",
      "Epoch 9, Batch 166, LR 2.186262 Loss 7.865494, Accuracy 77.551%\n",
      "Epoch 9, Batch 167, LR 2.186463 Loss 7.862252, Accuracy 77.564%\n",
      "Epoch 9, Batch 168, LR 2.186664 Loss 7.862259, Accuracy 77.572%\n",
      "Epoch 9, Batch 169, LR 2.186865 Loss 7.854500, Accuracy 77.626%\n",
      "Epoch 9, Batch 170, LR 2.187066 Loss 7.855056, Accuracy 77.629%\n",
      "Epoch 9, Batch 171, LR 2.187267 Loss 7.855746, Accuracy 77.632%\n",
      "Epoch 9, Batch 172, LR 2.187468 Loss 7.853771, Accuracy 77.644%\n",
      "Epoch 9, Batch 173, LR 2.187668 Loss 7.850691, Accuracy 77.655%\n",
      "Epoch 9, Batch 174, LR 2.187869 Loss 7.849375, Accuracy 77.672%\n",
      "Epoch 9, Batch 175, LR 2.188070 Loss 7.847915, Accuracy 77.696%\n",
      "Epoch 9, Batch 176, LR 2.188270 Loss 7.849346, Accuracy 77.681%\n",
      "Epoch 9, Batch 177, LR 2.188471 Loss 7.843938, Accuracy 77.710%\n",
      "Epoch 9, Batch 178, LR 2.188671 Loss 7.840616, Accuracy 77.717%\n",
      "Epoch 9, Batch 179, LR 2.188872 Loss 7.840568, Accuracy 77.715%\n",
      "Epoch 9, Batch 180, LR 2.189072 Loss 7.843560, Accuracy 77.687%\n",
      "Epoch 9, Batch 181, LR 2.189272 Loss 7.847581, Accuracy 77.659%\n",
      "Epoch 9, Batch 182, LR 2.189473 Loss 7.847837, Accuracy 77.661%\n",
      "Epoch 9, Batch 183, LR 2.189673 Loss 7.849446, Accuracy 77.660%\n",
      "Epoch 9, Batch 184, LR 2.189873 Loss 7.851798, Accuracy 77.658%\n",
      "Epoch 9, Batch 185, LR 2.190073 Loss 7.850047, Accuracy 77.665%\n",
      "Epoch 9, Batch 186, LR 2.190273 Loss 7.852851, Accuracy 77.638%\n",
      "Epoch 9, Batch 187, LR 2.190473 Loss 7.853167, Accuracy 77.615%\n",
      "Epoch 9, Batch 188, LR 2.190673 Loss 7.856742, Accuracy 77.581%\n",
      "Epoch 9, Batch 189, LR 2.190873 Loss 7.858983, Accuracy 77.546%\n",
      "Epoch 9, Batch 190, LR 2.191073 Loss 7.859666, Accuracy 77.553%\n",
      "Epoch 9, Batch 191, LR 2.191272 Loss 7.860733, Accuracy 77.548%\n",
      "Epoch 9, Batch 192, LR 2.191472 Loss 7.858083, Accuracy 77.568%\n",
      "Epoch 9, Batch 193, LR 2.191672 Loss 7.853374, Accuracy 77.595%\n",
      "Epoch 9, Batch 194, LR 2.191871 Loss 7.856990, Accuracy 77.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 195, LR 2.192071 Loss 7.854930, Accuracy 77.564%\n",
      "Epoch 9, Batch 196, LR 2.192270 Loss 7.857279, Accuracy 77.555%\n",
      "Epoch 9, Batch 197, LR 2.192470 Loss 7.853738, Accuracy 77.574%\n",
      "Epoch 9, Batch 198, LR 2.192669 Loss 7.851547, Accuracy 77.588%\n",
      "Epoch 9, Batch 199, LR 2.192869 Loss 7.849817, Accuracy 77.615%\n",
      "Epoch 9, Batch 200, LR 2.193068 Loss 7.852380, Accuracy 77.602%\n",
      "Epoch 9, Batch 201, LR 2.193267 Loss 7.855925, Accuracy 77.589%\n",
      "Epoch 9, Batch 202, LR 2.193466 Loss 7.859743, Accuracy 77.572%\n",
      "Epoch 9, Batch 203, LR 2.193665 Loss 7.858817, Accuracy 77.579%\n",
      "Epoch 9, Batch 204, LR 2.193864 Loss 7.857319, Accuracy 77.600%\n",
      "Epoch 9, Batch 205, LR 2.194063 Loss 7.859343, Accuracy 77.569%\n",
      "Epoch 9, Batch 206, LR 2.194262 Loss 7.859375, Accuracy 77.575%\n",
      "Epoch 9, Batch 207, LR 2.194461 Loss 7.858003, Accuracy 77.582%\n",
      "Epoch 9, Batch 208, LR 2.194660 Loss 7.856131, Accuracy 77.588%\n",
      "Epoch 9, Batch 209, LR 2.194859 Loss 7.854261, Accuracy 77.590%\n",
      "Epoch 9, Batch 210, LR 2.195057 Loss 7.853517, Accuracy 77.586%\n",
      "Epoch 9, Batch 211, LR 2.195256 Loss 7.853459, Accuracy 77.577%\n",
      "Epoch 9, Batch 212, LR 2.195455 Loss 7.853186, Accuracy 77.561%\n",
      "Epoch 9, Batch 213, LR 2.195653 Loss 7.854950, Accuracy 77.545%\n",
      "Epoch 9, Batch 214, LR 2.195852 Loss 7.851448, Accuracy 77.559%\n",
      "Epoch 9, Batch 215, LR 2.196050 Loss 7.850457, Accuracy 77.584%\n",
      "Epoch 9, Batch 216, LR 2.196248 Loss 7.853765, Accuracy 77.561%\n",
      "Epoch 9, Batch 217, LR 2.196447 Loss 7.850391, Accuracy 77.596%\n",
      "Epoch 9, Batch 218, LR 2.196645 Loss 7.847320, Accuracy 77.605%\n",
      "Epoch 9, Batch 219, LR 2.196843 Loss 7.847707, Accuracy 77.604%\n",
      "Epoch 9, Batch 220, LR 2.197041 Loss 7.845485, Accuracy 77.610%\n",
      "Epoch 9, Batch 221, LR 2.197239 Loss 7.847076, Accuracy 77.595%\n",
      "Epoch 9, Batch 222, LR 2.197438 Loss 7.847943, Accuracy 77.597%\n",
      "Epoch 9, Batch 223, LR 2.197636 Loss 7.850897, Accuracy 77.575%\n",
      "Epoch 9, Batch 224, LR 2.197833 Loss 7.850187, Accuracy 77.591%\n",
      "Epoch 9, Batch 225, LR 2.198031 Loss 7.848702, Accuracy 77.608%\n",
      "Epoch 9, Batch 226, LR 2.198229 Loss 7.849296, Accuracy 77.603%\n",
      "Epoch 9, Batch 227, LR 2.198427 Loss 7.847416, Accuracy 77.616%\n",
      "Epoch 9, Batch 228, LR 2.198625 Loss 7.843872, Accuracy 77.628%\n",
      "Epoch 9, Batch 229, LR 2.198822 Loss 7.842367, Accuracy 77.671%\n",
      "Epoch 9, Batch 230, LR 2.199020 Loss 7.844426, Accuracy 77.663%\n",
      "Epoch 9, Batch 231, LR 2.199217 Loss 7.844476, Accuracy 77.675%\n",
      "Epoch 9, Batch 232, LR 2.199415 Loss 7.843760, Accuracy 77.654%\n",
      "Epoch 9, Batch 233, LR 2.199612 Loss 7.842713, Accuracy 77.689%\n",
      "Epoch 9, Batch 234, LR 2.199810 Loss 7.843449, Accuracy 77.664%\n",
      "Epoch 9, Batch 235, LR 2.200007 Loss 7.846018, Accuracy 77.623%\n",
      "Epoch 9, Batch 236, LR 2.200204 Loss 7.848161, Accuracy 77.595%\n",
      "Epoch 9, Batch 237, LR 2.200402 Loss 7.847448, Accuracy 77.601%\n",
      "Epoch 9, Batch 238, LR 2.200599 Loss 7.847806, Accuracy 77.603%\n",
      "Epoch 9, Batch 239, LR 2.200796 Loss 7.847747, Accuracy 77.605%\n",
      "Epoch 9, Batch 240, LR 2.200993 Loss 7.850606, Accuracy 77.575%\n",
      "Epoch 9, Batch 241, LR 2.201190 Loss 7.851635, Accuracy 77.577%\n",
      "Epoch 9, Batch 242, LR 2.201387 Loss 7.851839, Accuracy 77.576%\n",
      "Epoch 9, Batch 243, LR 2.201584 Loss 7.852073, Accuracy 77.575%\n",
      "Epoch 9, Batch 244, LR 2.201781 Loss 7.852475, Accuracy 77.568%\n",
      "Epoch 9, Batch 245, LR 2.201977 Loss 7.854522, Accuracy 77.564%\n",
      "Epoch 9, Batch 246, LR 2.202174 Loss 7.855495, Accuracy 77.563%\n",
      "Epoch 9, Batch 247, LR 2.202371 Loss 7.857726, Accuracy 77.562%\n",
      "Epoch 9, Batch 248, LR 2.202567 Loss 7.857871, Accuracy 77.567%\n",
      "Epoch 9, Batch 249, LR 2.202764 Loss 7.859942, Accuracy 77.545%\n",
      "Epoch 9, Batch 250, LR 2.202960 Loss 7.862050, Accuracy 77.528%\n",
      "Epoch 9, Batch 251, LR 2.203157 Loss 7.860531, Accuracy 77.518%\n",
      "Epoch 9, Batch 252, LR 2.203353 Loss 7.861174, Accuracy 77.524%\n",
      "Epoch 9, Batch 253, LR 2.203549 Loss 7.856800, Accuracy 77.541%\n",
      "Epoch 9, Batch 254, LR 2.203746 Loss 7.856558, Accuracy 77.550%\n",
      "Epoch 9, Batch 255, LR 2.203942 Loss 7.854559, Accuracy 77.570%\n",
      "Epoch 9, Batch 256, LR 2.204138 Loss 7.857140, Accuracy 77.554%\n",
      "Epoch 9, Batch 257, LR 2.204334 Loss 7.858033, Accuracy 77.554%\n",
      "Epoch 9, Batch 258, LR 2.204530 Loss 7.859198, Accuracy 77.535%\n",
      "Epoch 9, Batch 259, LR 2.204726 Loss 7.858942, Accuracy 77.546%\n",
      "Epoch 9, Batch 260, LR 2.204922 Loss 7.859171, Accuracy 77.536%\n",
      "Epoch 9, Batch 261, LR 2.205118 Loss 7.861238, Accuracy 77.514%\n",
      "Epoch 9, Batch 262, LR 2.205314 Loss 7.861090, Accuracy 77.526%\n",
      "Epoch 9, Batch 263, LR 2.205510 Loss 7.861827, Accuracy 77.546%\n",
      "Epoch 9, Batch 264, LR 2.205705 Loss 7.864922, Accuracy 77.530%\n",
      "Epoch 9, Batch 265, LR 2.205901 Loss 7.864004, Accuracy 77.529%\n",
      "Epoch 9, Batch 266, LR 2.206097 Loss 7.863335, Accuracy 77.538%\n",
      "Epoch 9, Batch 267, LR 2.206292 Loss 7.862691, Accuracy 77.525%\n",
      "Epoch 9, Batch 268, LR 2.206488 Loss 7.861895, Accuracy 77.530%\n",
      "Epoch 9, Batch 269, LR 2.206683 Loss 7.862895, Accuracy 77.518%\n",
      "Epoch 9, Batch 270, LR 2.206878 Loss 7.863145, Accuracy 77.514%\n",
      "Epoch 9, Batch 271, LR 2.207074 Loss 7.865302, Accuracy 77.479%\n",
      "Epoch 9, Batch 272, LR 2.207269 Loss 7.865869, Accuracy 77.484%\n",
      "Epoch 9, Batch 273, LR 2.207464 Loss 7.867345, Accuracy 77.470%\n",
      "Epoch 9, Batch 274, LR 2.207659 Loss 7.866161, Accuracy 77.475%\n",
      "Epoch 9, Batch 275, LR 2.207854 Loss 7.867570, Accuracy 77.469%\n",
      "Epoch 9, Batch 276, LR 2.208049 Loss 7.868612, Accuracy 77.463%\n",
      "Epoch 9, Batch 277, LR 2.208244 Loss 7.868219, Accuracy 77.473%\n",
      "Epoch 9, Batch 278, LR 2.208439 Loss 7.869183, Accuracy 77.470%\n",
      "Epoch 9, Batch 279, LR 2.208634 Loss 7.868738, Accuracy 77.467%\n",
      "Epoch 9, Batch 280, LR 2.208829 Loss 7.871716, Accuracy 77.455%\n",
      "Epoch 9, Batch 281, LR 2.209024 Loss 7.868660, Accuracy 77.461%\n",
      "Epoch 9, Batch 282, LR 2.209219 Loss 7.869452, Accuracy 77.463%\n",
      "Epoch 9, Batch 283, LR 2.209413 Loss 7.870261, Accuracy 77.462%\n",
      "Epoch 9, Batch 284, LR 2.209608 Loss 7.870535, Accuracy 77.459%\n",
      "Epoch 9, Batch 285, LR 2.209802 Loss 7.875918, Accuracy 77.423%\n",
      "Epoch 9, Batch 286, LR 2.209997 Loss 7.874029, Accuracy 77.434%\n",
      "Epoch 9, Batch 287, LR 2.210191 Loss 7.874891, Accuracy 77.428%\n",
      "Epoch 9, Batch 288, LR 2.210386 Loss 7.874920, Accuracy 77.401%\n",
      "Epoch 9, Batch 289, LR 2.210580 Loss 7.875251, Accuracy 77.390%\n",
      "Epoch 9, Batch 290, LR 2.210774 Loss 7.875370, Accuracy 77.398%\n",
      "Epoch 9, Batch 291, LR 2.210968 Loss 7.877086, Accuracy 77.387%\n",
      "Epoch 9, Batch 292, LR 2.211163 Loss 7.876980, Accuracy 77.397%\n",
      "Epoch 9, Batch 293, LR 2.211357 Loss 7.877289, Accuracy 77.373%\n",
      "Epoch 9, Batch 294, LR 2.211551 Loss 7.873585, Accuracy 77.402%\n",
      "Epoch 9, Batch 295, LR 2.211745 Loss 7.873322, Accuracy 77.410%\n",
      "Epoch 9, Batch 296, LR 2.211939 Loss 7.871935, Accuracy 77.412%\n",
      "Epoch 9, Batch 297, LR 2.212132 Loss 7.873446, Accuracy 77.388%\n",
      "Epoch 9, Batch 298, LR 2.212326 Loss 7.874727, Accuracy 77.378%\n",
      "Epoch 9, Batch 299, LR 2.212520 Loss 7.873272, Accuracy 77.386%\n",
      "Epoch 9, Batch 300, LR 2.212714 Loss 7.871808, Accuracy 77.398%\n",
      "Epoch 9, Batch 301, LR 2.212907 Loss 7.871664, Accuracy 77.403%\n",
      "Epoch 9, Batch 302, LR 2.213101 Loss 7.872293, Accuracy 77.403%\n",
      "Epoch 9, Batch 303, LR 2.213294 Loss 7.872042, Accuracy 77.413%\n",
      "Epoch 9, Batch 304, LR 2.213488 Loss 7.873813, Accuracy 77.400%\n",
      "Epoch 9, Batch 305, LR 2.213681 Loss 7.874057, Accuracy 77.385%\n",
      "Epoch 9, Batch 306, LR 2.213875 Loss 7.875844, Accuracy 77.367%\n",
      "Epoch 9, Batch 307, LR 2.214068 Loss 7.876039, Accuracy 77.364%\n",
      "Epoch 9, Batch 308, LR 2.214261 Loss 7.875736, Accuracy 77.369%\n",
      "Epoch 9, Batch 309, LR 2.214454 Loss 7.874448, Accuracy 77.387%\n",
      "Epoch 9, Batch 310, LR 2.214647 Loss 7.874002, Accuracy 77.384%\n",
      "Epoch 9, Batch 311, LR 2.214841 Loss 7.874739, Accuracy 77.384%\n",
      "Epoch 9, Batch 312, LR 2.215034 Loss 7.874144, Accuracy 77.401%\n",
      "Epoch 9, Batch 313, LR 2.215226 Loss 7.873790, Accuracy 77.409%\n",
      "Epoch 9, Batch 314, LR 2.215419 Loss 7.873768, Accuracy 77.401%\n",
      "Epoch 9, Batch 315, LR 2.215612 Loss 7.873271, Accuracy 77.396%\n",
      "Epoch 9, Batch 316, LR 2.215805 Loss 7.875142, Accuracy 77.381%\n",
      "Epoch 9, Batch 317, LR 2.215998 Loss 7.875348, Accuracy 77.371%\n",
      "Epoch 9, Batch 318, LR 2.216190 Loss 7.875719, Accuracy 77.366%\n",
      "Epoch 9, Batch 319, LR 2.216383 Loss 7.876004, Accuracy 77.363%\n",
      "Epoch 9, Batch 320, LR 2.216576 Loss 7.874369, Accuracy 77.383%\n",
      "Epoch 9, Batch 321, LR 2.216768 Loss 7.876241, Accuracy 77.371%\n",
      "Epoch 9, Batch 322, LR 2.216961 Loss 7.875112, Accuracy 77.366%\n",
      "Epoch 9, Batch 323, LR 2.217153 Loss 7.876130, Accuracy 77.370%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 324, LR 2.217345 Loss 7.874681, Accuracy 77.387%\n",
      "Epoch 9, Batch 325, LR 2.217538 Loss 7.877690, Accuracy 77.375%\n",
      "Epoch 9, Batch 326, LR 2.217730 Loss 7.875916, Accuracy 77.389%\n",
      "Epoch 9, Batch 327, LR 2.217922 Loss 7.875689, Accuracy 77.408%\n",
      "Epoch 9, Batch 328, LR 2.218114 Loss 7.876936, Accuracy 77.403%\n",
      "Epoch 9, Batch 329, LR 2.218306 Loss 7.876783, Accuracy 77.401%\n",
      "Epoch 9, Batch 330, LR 2.218498 Loss 7.875437, Accuracy 77.410%\n",
      "Epoch 9, Batch 331, LR 2.218690 Loss 7.875073, Accuracy 77.403%\n",
      "Epoch 9, Batch 332, LR 2.218882 Loss 7.875490, Accuracy 77.400%\n",
      "Epoch 9, Batch 333, LR 2.219074 Loss 7.875990, Accuracy 77.398%\n",
      "Epoch 9, Batch 334, LR 2.219266 Loss 7.875200, Accuracy 77.407%\n",
      "Epoch 9, Batch 335, LR 2.219457 Loss 7.874077, Accuracy 77.416%\n",
      "Epoch 9, Batch 336, LR 2.219649 Loss 7.873481, Accuracy 77.427%\n",
      "Epoch 9, Batch 337, LR 2.219841 Loss 7.873775, Accuracy 77.432%\n",
      "Epoch 9, Batch 338, LR 2.220032 Loss 7.871876, Accuracy 77.450%\n",
      "Epoch 9, Batch 339, LR 2.220224 Loss 7.871153, Accuracy 77.445%\n",
      "Epoch 9, Batch 340, LR 2.220415 Loss 7.869760, Accuracy 77.449%\n",
      "Epoch 9, Batch 341, LR 2.220606 Loss 7.869502, Accuracy 77.438%\n",
      "Epoch 9, Batch 342, LR 2.220798 Loss 7.869866, Accuracy 77.440%\n",
      "Epoch 9, Batch 343, LR 2.220989 Loss 7.868297, Accuracy 77.446%\n",
      "Epoch 9, Batch 344, LR 2.221180 Loss 7.866968, Accuracy 77.448%\n",
      "Epoch 9, Batch 345, LR 2.221371 Loss 7.866493, Accuracy 77.430%\n",
      "Epoch 9, Batch 346, LR 2.221562 Loss 7.866560, Accuracy 77.439%\n",
      "Epoch 9, Batch 347, LR 2.221753 Loss 7.866235, Accuracy 77.438%\n",
      "Epoch 9, Batch 348, LR 2.221944 Loss 7.866778, Accuracy 77.429%\n",
      "Epoch 9, Batch 349, LR 2.222135 Loss 7.866329, Accuracy 77.433%\n",
      "Epoch 9, Batch 350, LR 2.222326 Loss 7.868139, Accuracy 77.415%\n",
      "Epoch 9, Batch 351, LR 2.222517 Loss 7.868119, Accuracy 77.413%\n",
      "Epoch 9, Batch 352, LR 2.222708 Loss 7.870742, Accuracy 77.397%\n",
      "Epoch 9, Batch 353, LR 2.222898 Loss 7.870026, Accuracy 77.397%\n",
      "Epoch 9, Batch 354, LR 2.223089 Loss 7.872132, Accuracy 77.383%\n",
      "Epoch 9, Batch 355, LR 2.223280 Loss 7.870514, Accuracy 77.397%\n",
      "Epoch 9, Batch 356, LR 2.223470 Loss 7.871359, Accuracy 77.390%\n",
      "Epoch 9, Batch 357, LR 2.223661 Loss 7.872155, Accuracy 77.383%\n",
      "Epoch 9, Batch 358, LR 2.223851 Loss 7.871157, Accuracy 77.390%\n",
      "Epoch 9, Batch 359, LR 2.224041 Loss 7.871372, Accuracy 77.389%\n",
      "Epoch 9, Batch 360, LR 2.224232 Loss 7.869694, Accuracy 77.391%\n",
      "Epoch 9, Batch 361, LR 2.224422 Loss 7.870108, Accuracy 77.387%\n",
      "Epoch 9, Batch 362, LR 2.224612 Loss 7.870281, Accuracy 77.393%\n",
      "Epoch 9, Batch 363, LR 2.224802 Loss 7.870934, Accuracy 77.385%\n",
      "Epoch 9, Batch 364, LR 2.224992 Loss 7.868982, Accuracy 77.402%\n",
      "Epoch 9, Batch 365, LR 2.225182 Loss 7.868915, Accuracy 77.410%\n",
      "Epoch 9, Batch 366, LR 2.225372 Loss 7.866743, Accuracy 77.421%\n",
      "Epoch 9, Batch 367, LR 2.225562 Loss 7.865902, Accuracy 77.435%\n",
      "Epoch 9, Batch 368, LR 2.225752 Loss 7.864949, Accuracy 77.439%\n",
      "Epoch 9, Batch 369, LR 2.225942 Loss 7.865130, Accuracy 77.433%\n",
      "Epoch 9, Batch 370, LR 2.226132 Loss 7.862519, Accuracy 77.456%\n",
      "Epoch 9, Batch 371, LR 2.226321 Loss 7.862088, Accuracy 77.445%\n",
      "Epoch 9, Batch 372, LR 2.226511 Loss 7.864028, Accuracy 77.421%\n",
      "Epoch 9, Batch 373, LR 2.226700 Loss 7.863897, Accuracy 77.434%\n",
      "Epoch 9, Batch 374, LR 2.226890 Loss 7.863678, Accuracy 77.442%\n",
      "Epoch 9, Batch 375, LR 2.227079 Loss 7.863217, Accuracy 77.444%\n",
      "Epoch 9, Batch 376, LR 2.227269 Loss 7.863311, Accuracy 77.433%\n",
      "Epoch 9, Batch 377, LR 2.227458 Loss 7.862193, Accuracy 77.439%\n",
      "Epoch 9, Batch 378, LR 2.227647 Loss 7.859625, Accuracy 77.462%\n",
      "Epoch 9, Batch 379, LR 2.227836 Loss 7.859098, Accuracy 77.467%\n",
      "Epoch 9, Batch 380, LR 2.228026 Loss 7.858753, Accuracy 77.475%\n",
      "Epoch 9, Batch 381, LR 2.228215 Loss 7.857506, Accuracy 77.479%\n",
      "Epoch 9, Batch 382, LR 2.228404 Loss 7.857953, Accuracy 77.471%\n",
      "Epoch 9, Batch 383, LR 2.228593 Loss 7.858111, Accuracy 77.468%\n",
      "Epoch 9, Batch 384, LR 2.228782 Loss 7.857837, Accuracy 77.476%\n",
      "Epoch 9, Batch 385, LR 2.228970 Loss 7.855922, Accuracy 77.496%\n",
      "Epoch 9, Batch 386, LR 2.229159 Loss 7.854722, Accuracy 77.496%\n",
      "Epoch 9, Batch 387, LR 2.229348 Loss 7.854043, Accuracy 77.495%\n",
      "Epoch 9, Batch 388, LR 2.229537 Loss 7.853625, Accuracy 77.497%\n",
      "Epoch 9, Batch 389, LR 2.229725 Loss 7.854303, Accuracy 77.498%\n",
      "Epoch 9, Batch 390, LR 2.229914 Loss 7.854914, Accuracy 77.494%\n",
      "Epoch 9, Batch 391, LR 2.230102 Loss 7.855479, Accuracy 77.488%\n",
      "Epoch 9, Batch 392, LR 2.230291 Loss 7.855798, Accuracy 77.477%\n",
      "Epoch 9, Batch 393, LR 2.230479 Loss 7.856412, Accuracy 77.483%\n",
      "Epoch 9, Batch 394, LR 2.230668 Loss 7.856030, Accuracy 77.479%\n",
      "Epoch 9, Batch 395, LR 2.230856 Loss 7.855801, Accuracy 77.482%\n",
      "Epoch 9, Batch 396, LR 2.231044 Loss 7.854822, Accuracy 77.488%\n",
      "Epoch 9, Batch 397, LR 2.231232 Loss 7.854076, Accuracy 77.499%\n",
      "Epoch 9, Batch 398, LR 2.231421 Loss 7.854697, Accuracy 77.489%\n",
      "Epoch 9, Batch 399, LR 2.231609 Loss 7.856423, Accuracy 77.477%\n",
      "Epoch 9, Batch 400, LR 2.231797 Loss 7.856375, Accuracy 77.475%\n",
      "Epoch 9, Batch 401, LR 2.231985 Loss 7.856300, Accuracy 77.468%\n",
      "Epoch 9, Batch 402, LR 2.232172 Loss 7.856215, Accuracy 77.468%\n",
      "Epoch 9, Batch 403, LR 2.232360 Loss 7.856633, Accuracy 77.472%\n",
      "Epoch 9, Batch 404, LR 2.232548 Loss 7.854878, Accuracy 77.481%\n",
      "Epoch 9, Batch 405, LR 2.232736 Loss 7.856548, Accuracy 77.479%\n",
      "Epoch 9, Batch 406, LR 2.232923 Loss 7.856048, Accuracy 77.478%\n",
      "Epoch 9, Batch 407, LR 2.233111 Loss 7.855402, Accuracy 77.492%\n",
      "Epoch 9, Batch 408, LR 2.233299 Loss 7.854919, Accuracy 77.489%\n",
      "Epoch 9, Batch 409, LR 2.233486 Loss 7.854831, Accuracy 77.483%\n",
      "Epoch 9, Batch 410, LR 2.233673 Loss 7.853614, Accuracy 77.487%\n",
      "Epoch 9, Batch 411, LR 2.233861 Loss 7.855541, Accuracy 77.479%\n",
      "Epoch 9, Batch 412, LR 2.234048 Loss 7.855059, Accuracy 77.473%\n",
      "Epoch 9, Batch 413, LR 2.234235 Loss 7.855957, Accuracy 77.470%\n",
      "Epoch 9, Batch 414, LR 2.234423 Loss 7.855339, Accuracy 77.476%\n",
      "Epoch 9, Batch 415, LR 2.234610 Loss 7.854304, Accuracy 77.472%\n",
      "Epoch 9, Batch 416, LR 2.234797 Loss 7.854929, Accuracy 77.468%\n",
      "Epoch 9, Batch 417, LR 2.234984 Loss 7.854821, Accuracy 77.466%\n",
      "Epoch 9, Batch 418, LR 2.235171 Loss 7.855217, Accuracy 77.465%\n",
      "Epoch 9, Batch 419, LR 2.235358 Loss 7.855567, Accuracy 77.457%\n",
      "Epoch 9, Batch 420, LR 2.235545 Loss 7.854648, Accuracy 77.463%\n",
      "Epoch 9, Batch 421, LR 2.235731 Loss 7.854710, Accuracy 77.464%\n",
      "Epoch 9, Batch 422, LR 2.235918 Loss 7.855687, Accuracy 77.449%\n",
      "Epoch 9, Batch 423, LR 2.236105 Loss 7.857511, Accuracy 77.434%\n",
      "Epoch 9, Batch 424, LR 2.236291 Loss 7.858069, Accuracy 77.429%\n",
      "Epoch 9, Batch 425, LR 2.236478 Loss 7.858260, Accuracy 77.426%\n",
      "Epoch 9, Batch 426, LR 2.236664 Loss 7.858631, Accuracy 77.424%\n",
      "Epoch 9, Batch 427, LR 2.236851 Loss 7.857672, Accuracy 77.435%\n",
      "Epoch 9, Batch 428, LR 2.237037 Loss 7.858582, Accuracy 77.430%\n",
      "Epoch 9, Batch 429, LR 2.237224 Loss 7.856592, Accuracy 77.433%\n",
      "Epoch 9, Batch 430, LR 2.237410 Loss 7.857972, Accuracy 77.426%\n",
      "Epoch 9, Batch 431, LR 2.237596 Loss 7.857515, Accuracy 77.427%\n",
      "Epoch 9, Batch 432, LR 2.237782 Loss 7.857732, Accuracy 77.427%\n",
      "Epoch 9, Batch 433, LR 2.237968 Loss 7.856813, Accuracy 77.432%\n",
      "Epoch 9, Batch 434, LR 2.238154 Loss 7.857207, Accuracy 77.425%\n",
      "Epoch 9, Batch 435, LR 2.238340 Loss 7.856762, Accuracy 77.428%\n",
      "Epoch 9, Batch 436, LR 2.238526 Loss 7.856459, Accuracy 77.426%\n",
      "Epoch 9, Batch 437, LR 2.238712 Loss 7.855691, Accuracy 77.431%\n",
      "Epoch 9, Batch 438, LR 2.238898 Loss 7.856606, Accuracy 77.422%\n",
      "Epoch 9, Batch 439, LR 2.239084 Loss 7.857785, Accuracy 77.415%\n",
      "Epoch 9, Batch 440, LR 2.239269 Loss 7.857710, Accuracy 77.415%\n",
      "Epoch 9, Batch 441, LR 2.239455 Loss 7.857937, Accuracy 77.399%\n",
      "Epoch 9, Batch 442, LR 2.239641 Loss 7.858502, Accuracy 77.397%\n",
      "Epoch 9, Batch 443, LR 2.239826 Loss 7.857539, Accuracy 77.402%\n",
      "Epoch 9, Batch 444, LR 2.240012 Loss 7.855753, Accuracy 77.414%\n",
      "Epoch 9, Batch 445, LR 2.240197 Loss 7.855677, Accuracy 77.405%\n",
      "Epoch 9, Batch 446, LR 2.240382 Loss 7.855837, Accuracy 77.409%\n",
      "Epoch 9, Batch 447, LR 2.240568 Loss 7.857746, Accuracy 77.396%\n",
      "Epoch 9, Batch 448, LR 2.240753 Loss 7.858932, Accuracy 77.386%\n",
      "Epoch 9, Batch 449, LR 2.240938 Loss 7.856469, Accuracy 77.406%\n",
      "Epoch 9, Batch 450, LR 2.241123 Loss 7.854887, Accuracy 77.415%\n",
      "Epoch 9, Batch 451, LR 2.241308 Loss 7.854121, Accuracy 77.415%\n",
      "Epoch 9, Batch 452, LR 2.241493 Loss 7.853635, Accuracy 77.409%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 453, LR 2.241678 Loss 7.853873, Accuracy 77.397%\n",
      "Epoch 9, Batch 454, LR 2.241863 Loss 7.854277, Accuracy 77.404%\n",
      "Epoch 9, Batch 455, LR 2.242048 Loss 7.854525, Accuracy 77.404%\n",
      "Epoch 9, Batch 456, LR 2.242233 Loss 7.853072, Accuracy 77.417%\n",
      "Epoch 9, Batch 457, LR 2.242417 Loss 7.853839, Accuracy 77.410%\n",
      "Epoch 9, Batch 458, LR 2.242602 Loss 7.852332, Accuracy 77.421%\n",
      "Epoch 9, Batch 459, LR 2.242787 Loss 7.850430, Accuracy 77.431%\n",
      "Epoch 9, Batch 460, LR 2.242971 Loss 7.849734, Accuracy 77.447%\n",
      "Epoch 9, Batch 461, LR 2.243156 Loss 7.849846, Accuracy 77.449%\n",
      "Epoch 9, Batch 462, LR 2.243340 Loss 7.849579, Accuracy 77.445%\n",
      "Epoch 9, Batch 463, LR 2.243524 Loss 7.850323, Accuracy 77.442%\n",
      "Epoch 9, Batch 464, LR 2.243709 Loss 7.850275, Accuracy 77.443%\n",
      "Epoch 9, Batch 465, LR 2.243893 Loss 7.848603, Accuracy 77.458%\n",
      "Epoch 9, Batch 466, LR 2.244077 Loss 7.848441, Accuracy 77.454%\n",
      "Epoch 9, Batch 467, LR 2.244261 Loss 7.849067, Accuracy 77.452%\n",
      "Epoch 9, Batch 468, LR 2.244445 Loss 7.849370, Accuracy 77.459%\n",
      "Epoch 9, Batch 469, LR 2.244629 Loss 7.849663, Accuracy 77.459%\n",
      "Epoch 9, Batch 470, LR 2.244813 Loss 7.849696, Accuracy 77.462%\n",
      "Epoch 9, Batch 471, LR 2.244997 Loss 7.848686, Accuracy 77.470%\n",
      "Epoch 9, Batch 472, LR 2.245181 Loss 7.850413, Accuracy 77.458%\n",
      "Epoch 9, Batch 473, LR 2.245365 Loss 7.850064, Accuracy 77.461%\n",
      "Epoch 9, Batch 474, LR 2.245549 Loss 7.849980, Accuracy 77.451%\n",
      "Epoch 9, Batch 475, LR 2.245732 Loss 7.850480, Accuracy 77.441%\n",
      "Epoch 9, Batch 476, LR 2.245916 Loss 7.851825, Accuracy 77.431%\n",
      "Epoch 9, Batch 477, LR 2.246099 Loss 7.851048, Accuracy 77.429%\n",
      "Epoch 9, Batch 478, LR 2.246283 Loss 7.850087, Accuracy 77.437%\n",
      "Epoch 9, Batch 479, LR 2.246466 Loss 7.849296, Accuracy 77.442%\n",
      "Epoch 9, Batch 480, LR 2.246650 Loss 7.849778, Accuracy 77.437%\n",
      "Epoch 9, Batch 481, LR 2.246833 Loss 7.850235, Accuracy 77.436%\n",
      "Epoch 9, Batch 482, LR 2.247016 Loss 7.851261, Accuracy 77.430%\n",
      "Epoch 9, Batch 483, LR 2.247199 Loss 7.852863, Accuracy 77.426%\n",
      "Epoch 9, Batch 484, LR 2.247383 Loss 7.854257, Accuracy 77.410%\n",
      "Epoch 9, Batch 485, LR 2.247566 Loss 7.855595, Accuracy 77.403%\n",
      "Epoch 9, Batch 486, LR 2.247749 Loss 7.854800, Accuracy 77.405%\n",
      "Epoch 9, Batch 487, LR 2.247932 Loss 7.855153, Accuracy 77.395%\n",
      "Epoch 9, Batch 488, LR 2.248114 Loss 7.855783, Accuracy 77.390%\n",
      "Epoch 9, Batch 489, LR 2.248297 Loss 7.855213, Accuracy 77.390%\n",
      "Epoch 9, Batch 490, LR 2.248480 Loss 7.853391, Accuracy 77.393%\n",
      "Epoch 9, Batch 491, LR 2.248663 Loss 7.852751, Accuracy 77.398%\n",
      "Epoch 9, Batch 492, LR 2.248845 Loss 7.851331, Accuracy 77.406%\n",
      "Epoch 9, Batch 493, LR 2.249028 Loss 7.852122, Accuracy 77.394%\n",
      "Epoch 9, Batch 494, LR 2.249211 Loss 7.851819, Accuracy 77.391%\n",
      "Epoch 9, Batch 495, LR 2.249393 Loss 7.851589, Accuracy 77.388%\n",
      "Epoch 9, Batch 496, LR 2.249576 Loss 7.851811, Accuracy 77.382%\n",
      "Epoch 9, Batch 497, LR 2.249758 Loss 7.854075, Accuracy 77.366%\n",
      "Epoch 9, Batch 498, LR 2.249940 Loss 7.855827, Accuracy 77.347%\n",
      "Epoch 9, Batch 499, LR 2.250122 Loss 7.856486, Accuracy 77.347%\n",
      "Epoch 9, Batch 500, LR 2.250305 Loss 7.856093, Accuracy 77.356%\n",
      "Epoch 9, Batch 501, LR 2.250487 Loss 7.856439, Accuracy 77.359%\n",
      "Epoch 9, Batch 502, LR 2.250669 Loss 7.856639, Accuracy 77.350%\n",
      "Epoch 9, Batch 503, LR 2.250851 Loss 7.856159, Accuracy 77.362%\n",
      "Epoch 9, Batch 504, LR 2.251033 Loss 7.855206, Accuracy 77.376%\n",
      "Epoch 9, Batch 505, LR 2.251215 Loss 7.855984, Accuracy 77.364%\n",
      "Epoch 9, Batch 506, LR 2.251397 Loss 7.855997, Accuracy 77.373%\n",
      "Epoch 9, Batch 507, LR 2.251578 Loss 7.853726, Accuracy 77.387%\n",
      "Epoch 9, Batch 508, LR 2.251760 Loss 7.853811, Accuracy 77.384%\n",
      "Epoch 9, Batch 509, LR 2.251942 Loss 7.853661, Accuracy 77.388%\n",
      "Epoch 9, Batch 510, LR 2.252123 Loss 7.852129, Accuracy 77.394%\n",
      "Epoch 9, Batch 511, LR 2.252305 Loss 7.852067, Accuracy 77.394%\n",
      "Epoch 9, Batch 512, LR 2.252486 Loss 7.853502, Accuracy 77.379%\n",
      "Epoch 9, Batch 513, LR 2.252668 Loss 7.853998, Accuracy 77.373%\n",
      "Epoch 9, Batch 514, LR 2.252849 Loss 7.854348, Accuracy 77.373%\n",
      "Epoch 9, Batch 515, LR 2.253031 Loss 7.854342, Accuracy 77.370%\n",
      "Epoch 9, Batch 516, LR 2.253212 Loss 7.853967, Accuracy 77.371%\n",
      "Epoch 9, Batch 517, LR 2.253393 Loss 7.853578, Accuracy 77.375%\n",
      "Epoch 9, Batch 518, LR 2.253574 Loss 7.852686, Accuracy 77.383%\n",
      "Epoch 9, Batch 519, LR 2.253755 Loss 7.852326, Accuracy 77.378%\n",
      "Epoch 9, Batch 520, LR 2.253936 Loss 7.852465, Accuracy 77.381%\n",
      "Epoch 9, Batch 521, LR 2.254117 Loss 7.851521, Accuracy 77.393%\n",
      "Epoch 9, Batch 522, LR 2.254298 Loss 7.850689, Accuracy 77.402%\n",
      "Epoch 9, Batch 523, LR 2.254479 Loss 7.850212, Accuracy 77.405%\n",
      "Epoch 9, Batch 524, LR 2.254660 Loss 7.849936, Accuracy 77.403%\n",
      "Epoch 9, Batch 525, LR 2.254841 Loss 7.849786, Accuracy 77.405%\n",
      "Epoch 9, Batch 526, LR 2.255021 Loss 7.849930, Accuracy 77.403%\n",
      "Epoch 9, Batch 527, LR 2.255202 Loss 7.848835, Accuracy 77.412%\n",
      "Epoch 9, Batch 528, LR 2.255382 Loss 7.847616, Accuracy 77.419%\n",
      "Epoch 9, Batch 529, LR 2.255563 Loss 7.848131, Accuracy 77.421%\n",
      "Epoch 9, Batch 530, LR 2.255743 Loss 7.848545, Accuracy 77.420%\n",
      "Epoch 9, Batch 531, LR 2.255924 Loss 7.848724, Accuracy 77.432%\n",
      "Epoch 9, Batch 532, LR 2.256104 Loss 7.847965, Accuracy 77.433%\n",
      "Epoch 9, Batch 533, LR 2.256284 Loss 7.848469, Accuracy 77.436%\n",
      "Epoch 9, Batch 534, LR 2.256464 Loss 7.848509, Accuracy 77.427%\n",
      "Epoch 9, Batch 535, LR 2.256645 Loss 7.848120, Accuracy 77.434%\n",
      "Epoch 9, Batch 536, LR 2.256825 Loss 7.847425, Accuracy 77.441%\n",
      "Epoch 9, Batch 537, LR 2.257005 Loss 7.847145, Accuracy 77.443%\n",
      "Epoch 9, Batch 538, LR 2.257185 Loss 7.847292, Accuracy 77.444%\n",
      "Epoch 9, Batch 539, LR 2.257365 Loss 7.847482, Accuracy 77.442%\n",
      "Epoch 9, Batch 540, LR 2.257544 Loss 7.847682, Accuracy 77.438%\n",
      "Epoch 9, Batch 541, LR 2.257724 Loss 7.848588, Accuracy 77.422%\n",
      "Epoch 9, Batch 542, LR 2.257904 Loss 7.849719, Accuracy 77.422%\n",
      "Epoch 9, Batch 543, LR 2.258084 Loss 7.850104, Accuracy 77.420%\n",
      "Epoch 9, Batch 544, LR 2.258263 Loss 7.849104, Accuracy 77.428%\n",
      "Epoch 9, Batch 545, LR 2.258443 Loss 7.849872, Accuracy 77.427%\n",
      "Epoch 9, Batch 546, LR 2.258622 Loss 7.850597, Accuracy 77.425%\n",
      "Epoch 9, Batch 547, LR 2.258802 Loss 7.850241, Accuracy 77.432%\n",
      "Epoch 9, Batch 548, LR 2.258981 Loss 7.850759, Accuracy 77.422%\n",
      "Epoch 9, Batch 549, LR 2.259160 Loss 7.850526, Accuracy 77.423%\n",
      "Epoch 9, Batch 550, LR 2.259340 Loss 7.851623, Accuracy 77.412%\n",
      "Epoch 9, Batch 551, LR 2.259519 Loss 7.853133, Accuracy 77.402%\n",
      "Epoch 9, Batch 552, LR 2.259698 Loss 7.853956, Accuracy 77.392%\n",
      "Epoch 9, Batch 553, LR 2.259877 Loss 7.854279, Accuracy 77.388%\n",
      "Epoch 9, Batch 554, LR 2.260056 Loss 7.853799, Accuracy 77.386%\n",
      "Epoch 9, Batch 555, LR 2.260235 Loss 7.852767, Accuracy 77.393%\n",
      "Epoch 9, Batch 556, LR 2.260414 Loss 7.853619, Accuracy 77.386%\n",
      "Epoch 9, Batch 557, LR 2.260593 Loss 7.853815, Accuracy 77.380%\n",
      "Epoch 9, Batch 558, LR 2.260771 Loss 7.853916, Accuracy 77.375%\n",
      "Epoch 9, Batch 559, LR 2.260950 Loss 7.854664, Accuracy 77.373%\n",
      "Epoch 9, Batch 560, LR 2.261129 Loss 7.855371, Accuracy 77.366%\n",
      "Epoch 9, Batch 561, LR 2.261307 Loss 7.855216, Accuracy 77.365%\n",
      "Epoch 9, Batch 562, LR 2.261486 Loss 7.855007, Accuracy 77.369%\n",
      "Epoch 9, Batch 563, LR 2.261664 Loss 7.853928, Accuracy 77.372%\n",
      "Epoch 9, Batch 564, LR 2.261843 Loss 7.853267, Accuracy 77.369%\n",
      "Epoch 9, Batch 565, LR 2.262021 Loss 7.852207, Accuracy 77.378%\n",
      "Epoch 9, Batch 566, LR 2.262200 Loss 7.853861, Accuracy 77.363%\n",
      "Epoch 9, Batch 567, LR 2.262378 Loss 7.851893, Accuracy 77.377%\n",
      "Epoch 9, Batch 568, LR 2.262556 Loss 7.852377, Accuracy 77.375%\n",
      "Epoch 9, Batch 569, LR 2.262734 Loss 7.852512, Accuracy 77.375%\n",
      "Epoch 9, Batch 570, LR 2.262912 Loss 7.851482, Accuracy 77.386%\n",
      "Epoch 9, Batch 571, LR 2.263090 Loss 7.851907, Accuracy 77.385%\n",
      "Epoch 9, Batch 572, LR 2.263268 Loss 7.853110, Accuracy 77.375%\n",
      "Epoch 9, Batch 573, LR 2.263446 Loss 7.852480, Accuracy 77.387%\n",
      "Epoch 9, Batch 574, LR 2.263624 Loss 7.851451, Accuracy 77.393%\n",
      "Epoch 9, Batch 575, LR 2.263802 Loss 7.851920, Accuracy 77.390%\n",
      "Epoch 9, Batch 576, LR 2.263979 Loss 7.851400, Accuracy 77.387%\n",
      "Epoch 9, Batch 577, LR 2.264157 Loss 7.851112, Accuracy 77.392%\n",
      "Epoch 9, Batch 578, LR 2.264335 Loss 7.850473, Accuracy 77.387%\n",
      "Epoch 9, Batch 579, LR 2.264512 Loss 7.850132, Accuracy 77.388%\n",
      "Epoch 9, Batch 580, LR 2.264690 Loss 7.850327, Accuracy 77.379%\n",
      "Epoch 9, Batch 581, LR 2.264867 Loss 7.850092, Accuracy 77.376%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 582, LR 2.265044 Loss 7.850491, Accuracy 77.371%\n",
      "Epoch 9, Batch 583, LR 2.265222 Loss 7.849903, Accuracy 77.375%\n",
      "Epoch 9, Batch 584, LR 2.265399 Loss 7.850651, Accuracy 77.371%\n",
      "Epoch 9, Batch 585, LR 2.265576 Loss 7.852046, Accuracy 77.357%\n",
      "Epoch 9, Batch 586, LR 2.265753 Loss 7.851916, Accuracy 77.356%\n",
      "Epoch 9, Batch 587, LR 2.265930 Loss 7.851273, Accuracy 77.362%\n",
      "Epoch 9, Batch 588, LR 2.266107 Loss 7.851257, Accuracy 77.364%\n",
      "Epoch 9, Batch 589, LR 2.266284 Loss 7.852024, Accuracy 77.354%\n",
      "Epoch 9, Batch 590, LR 2.266461 Loss 7.851821, Accuracy 77.356%\n",
      "Epoch 9, Batch 591, LR 2.266638 Loss 7.851498, Accuracy 77.368%\n",
      "Epoch 9, Batch 592, LR 2.266815 Loss 7.852412, Accuracy 77.357%\n",
      "Epoch 9, Batch 593, LR 2.266991 Loss 7.852482, Accuracy 77.358%\n",
      "Epoch 9, Batch 594, LR 2.267168 Loss 7.852882, Accuracy 77.352%\n",
      "Epoch 9, Batch 595, LR 2.267344 Loss 7.852056, Accuracy 77.357%\n",
      "Epoch 9, Batch 596, LR 2.267521 Loss 7.853039, Accuracy 77.348%\n",
      "Epoch 9, Batch 597, LR 2.267697 Loss 7.854037, Accuracy 77.348%\n",
      "Epoch 9, Batch 598, LR 2.267874 Loss 7.853478, Accuracy 77.358%\n",
      "Epoch 9, Batch 599, LR 2.268050 Loss 7.853615, Accuracy 77.363%\n",
      "Epoch 9, Batch 600, LR 2.268226 Loss 7.854953, Accuracy 77.353%\n",
      "Epoch 9, Batch 601, LR 2.268403 Loss 7.854386, Accuracy 77.358%\n",
      "Epoch 9, Batch 602, LR 2.268579 Loss 7.854483, Accuracy 77.361%\n",
      "Epoch 9, Batch 603, LR 2.268755 Loss 7.855157, Accuracy 77.354%\n",
      "Epoch 9, Batch 604, LR 2.268931 Loss 7.854293, Accuracy 77.363%\n",
      "Epoch 9, Batch 605, LR 2.269107 Loss 7.854338, Accuracy 77.357%\n",
      "Epoch 9, Batch 606, LR 2.269283 Loss 7.855135, Accuracy 77.354%\n",
      "Epoch 9, Batch 607, LR 2.269459 Loss 7.855344, Accuracy 77.354%\n",
      "Epoch 9, Batch 608, LR 2.269634 Loss 7.854151, Accuracy 77.362%\n",
      "Epoch 9, Batch 609, LR 2.269810 Loss 7.854429, Accuracy 77.366%\n",
      "Epoch 9, Batch 610, LR 2.269986 Loss 7.854098, Accuracy 77.368%\n",
      "Epoch 9, Batch 611, LR 2.270162 Loss 7.854250, Accuracy 77.371%\n",
      "Epoch 9, Batch 612, LR 2.270337 Loss 7.853195, Accuracy 77.373%\n",
      "Epoch 9, Batch 613, LR 2.270513 Loss 7.852361, Accuracy 77.369%\n",
      "Epoch 9, Batch 614, LR 2.270688 Loss 7.853019, Accuracy 77.362%\n",
      "Epoch 9, Batch 615, LR 2.270863 Loss 7.851766, Accuracy 77.368%\n",
      "Epoch 9, Batch 616, LR 2.271039 Loss 7.851776, Accuracy 77.368%\n",
      "Epoch 9, Batch 617, LR 2.271214 Loss 7.851631, Accuracy 77.367%\n",
      "Epoch 9, Batch 618, LR 2.271389 Loss 7.851773, Accuracy 77.368%\n",
      "Epoch 9, Batch 619, LR 2.271564 Loss 7.851594, Accuracy 77.368%\n",
      "Epoch 9, Batch 620, LR 2.271739 Loss 7.851131, Accuracy 77.378%\n",
      "Epoch 9, Batch 621, LR 2.271914 Loss 7.851668, Accuracy 77.369%\n",
      "Epoch 9, Batch 622, LR 2.272089 Loss 7.852308, Accuracy 77.361%\n",
      "Epoch 9, Batch 623, LR 2.272264 Loss 7.852289, Accuracy 77.360%\n",
      "Epoch 9, Batch 624, LR 2.272439 Loss 7.852522, Accuracy 77.361%\n",
      "Epoch 9, Batch 625, LR 2.272614 Loss 7.852197, Accuracy 77.365%\n",
      "Epoch 9, Batch 626, LR 2.272789 Loss 7.852009, Accuracy 77.366%\n",
      "Epoch 9, Batch 627, LR 2.272963 Loss 7.851135, Accuracy 77.374%\n",
      "Epoch 9, Batch 628, LR 2.273138 Loss 7.850912, Accuracy 77.384%\n",
      "Epoch 9, Batch 629, LR 2.273312 Loss 7.850842, Accuracy 77.392%\n",
      "Epoch 9, Batch 630, LR 2.273487 Loss 7.851048, Accuracy 77.392%\n",
      "Epoch 9, Batch 631, LR 2.273661 Loss 7.850777, Accuracy 77.388%\n",
      "Epoch 9, Batch 632, LR 2.273836 Loss 7.851190, Accuracy 77.386%\n",
      "Epoch 9, Batch 633, LR 2.274010 Loss 7.850758, Accuracy 77.393%\n",
      "Epoch 9, Batch 634, LR 2.274184 Loss 7.850985, Accuracy 77.392%\n",
      "Epoch 9, Batch 635, LR 2.274358 Loss 7.851513, Accuracy 77.388%\n",
      "Epoch 9, Batch 636, LR 2.274532 Loss 7.851197, Accuracy 77.392%\n",
      "Epoch 9, Batch 637, LR 2.274707 Loss 7.849819, Accuracy 77.396%\n",
      "Epoch 9, Batch 638, LR 2.274881 Loss 7.849841, Accuracy 77.394%\n",
      "Epoch 9, Batch 639, LR 2.275054 Loss 7.849301, Accuracy 77.395%\n",
      "Epoch 9, Batch 640, LR 2.275228 Loss 7.849329, Accuracy 77.396%\n",
      "Epoch 9, Batch 641, LR 2.275402 Loss 7.848401, Accuracy 77.401%\n",
      "Epoch 9, Batch 642, LR 2.275576 Loss 7.847836, Accuracy 77.407%\n",
      "Epoch 9, Batch 643, LR 2.275750 Loss 7.848373, Accuracy 77.408%\n",
      "Epoch 9, Batch 644, LR 2.275923 Loss 7.849370, Accuracy 77.402%\n",
      "Epoch 9, Batch 645, LR 2.276097 Loss 7.848562, Accuracy 77.404%\n",
      "Epoch 9, Batch 646, LR 2.276270 Loss 7.848077, Accuracy 77.414%\n",
      "Epoch 9, Batch 647, LR 2.276444 Loss 7.848097, Accuracy 77.415%\n",
      "Epoch 9, Batch 648, LR 2.276617 Loss 7.848907, Accuracy 77.410%\n",
      "Epoch 9, Batch 649, LR 2.276791 Loss 7.848490, Accuracy 77.410%\n",
      "Epoch 9, Batch 650, LR 2.276964 Loss 7.847418, Accuracy 77.412%\n",
      "Epoch 9, Batch 651, LR 2.277137 Loss 7.847873, Accuracy 77.401%\n",
      "Epoch 9, Batch 652, LR 2.277310 Loss 7.848273, Accuracy 77.398%\n",
      "Epoch 9, Batch 653, LR 2.277483 Loss 7.848136, Accuracy 77.392%\n",
      "Epoch 9, Batch 654, LR 2.277656 Loss 7.848082, Accuracy 77.393%\n",
      "Epoch 9, Batch 655, LR 2.277829 Loss 7.847313, Accuracy 77.390%\n",
      "Epoch 9, Batch 656, LR 2.278002 Loss 7.848220, Accuracy 77.379%\n",
      "Epoch 9, Batch 657, LR 2.278175 Loss 7.847602, Accuracy 77.382%\n",
      "Epoch 9, Batch 658, LR 2.278348 Loss 7.847430, Accuracy 77.383%\n",
      "Epoch 9, Batch 659, LR 2.278521 Loss 7.846413, Accuracy 77.390%\n",
      "Epoch 9, Batch 660, LR 2.278693 Loss 7.846639, Accuracy 77.391%\n",
      "Epoch 9, Batch 661, LR 2.278866 Loss 7.846160, Accuracy 77.395%\n",
      "Epoch 9, Batch 662, LR 2.279038 Loss 7.845256, Accuracy 77.404%\n",
      "Epoch 9, Batch 663, LR 2.279211 Loss 7.845289, Accuracy 77.406%\n",
      "Epoch 9, Batch 664, LR 2.279383 Loss 7.844926, Accuracy 77.408%\n",
      "Epoch 9, Batch 665, LR 2.279556 Loss 7.844240, Accuracy 77.414%\n",
      "Epoch 9, Batch 666, LR 2.279728 Loss 7.844187, Accuracy 77.413%\n",
      "Epoch 9, Batch 667, LR 2.279900 Loss 7.844130, Accuracy 77.411%\n",
      "Epoch 9, Batch 668, LR 2.280072 Loss 7.843970, Accuracy 77.410%\n",
      "Epoch 9, Batch 669, LR 2.280245 Loss 7.844398, Accuracy 77.406%\n",
      "Epoch 9, Batch 670, LR 2.280417 Loss 7.844840, Accuracy 77.402%\n",
      "Epoch 9, Batch 671, LR 2.280589 Loss 7.845209, Accuracy 77.404%\n",
      "Epoch 9, Batch 672, LR 2.280761 Loss 7.845613, Accuracy 77.403%\n",
      "Epoch 9, Batch 673, LR 2.280933 Loss 7.845694, Accuracy 77.405%\n",
      "Epoch 9, Batch 674, LR 2.281104 Loss 7.846705, Accuracy 77.398%\n",
      "Epoch 9, Batch 675, LR 2.281276 Loss 7.846690, Accuracy 77.392%\n",
      "Epoch 9, Batch 676, LR 2.281448 Loss 7.846396, Accuracy 77.402%\n",
      "Epoch 9, Batch 677, LR 2.281619 Loss 7.846733, Accuracy 77.397%\n",
      "Epoch 9, Batch 678, LR 2.281791 Loss 7.846840, Accuracy 77.401%\n",
      "Epoch 9, Batch 679, LR 2.281963 Loss 7.845651, Accuracy 77.413%\n",
      "Epoch 9, Batch 680, LR 2.282134 Loss 7.847118, Accuracy 77.397%\n",
      "Epoch 9, Batch 681, LR 2.282305 Loss 7.846374, Accuracy 77.399%\n",
      "Epoch 9, Batch 682, LR 2.282477 Loss 7.847259, Accuracy 77.390%\n",
      "Epoch 9, Batch 683, LR 2.282648 Loss 7.847731, Accuracy 77.385%\n",
      "Epoch 9, Batch 684, LR 2.282819 Loss 7.847993, Accuracy 77.383%\n",
      "Epoch 9, Batch 685, LR 2.282990 Loss 7.848448, Accuracy 77.376%\n",
      "Epoch 9, Batch 686, LR 2.283161 Loss 7.847806, Accuracy 77.382%\n",
      "Epoch 9, Batch 687, LR 2.283333 Loss 7.848050, Accuracy 77.379%\n",
      "Epoch 9, Batch 688, LR 2.283504 Loss 7.846566, Accuracy 77.390%\n",
      "Epoch 9, Batch 689, LR 2.283674 Loss 7.845734, Accuracy 77.402%\n",
      "Epoch 9, Batch 690, LR 2.283845 Loss 7.846467, Accuracy 77.392%\n",
      "Epoch 9, Batch 691, LR 2.284016 Loss 7.846751, Accuracy 77.388%\n",
      "Epoch 9, Batch 692, LR 2.284187 Loss 7.847481, Accuracy 77.382%\n",
      "Epoch 9, Batch 693, LR 2.284357 Loss 7.847748, Accuracy 77.384%\n",
      "Epoch 9, Batch 694, LR 2.284528 Loss 7.847866, Accuracy 77.379%\n",
      "Epoch 9, Batch 695, LR 2.284699 Loss 7.848280, Accuracy 77.373%\n",
      "Epoch 9, Batch 696, LR 2.284869 Loss 7.848621, Accuracy 77.374%\n",
      "Epoch 9, Batch 697, LR 2.285040 Loss 7.848231, Accuracy 77.382%\n",
      "Epoch 9, Batch 698, LR 2.285210 Loss 7.847891, Accuracy 77.385%\n",
      "Epoch 9, Batch 699, LR 2.285380 Loss 7.847579, Accuracy 77.387%\n",
      "Epoch 9, Batch 700, LR 2.285550 Loss 7.846784, Accuracy 77.396%\n",
      "Epoch 9, Batch 701, LR 2.285721 Loss 7.847482, Accuracy 77.389%\n",
      "Epoch 9, Batch 702, LR 2.285891 Loss 7.847507, Accuracy 77.388%\n",
      "Epoch 9, Batch 703, LR 2.286061 Loss 7.847662, Accuracy 77.388%\n",
      "Epoch 9, Batch 704, LR 2.286231 Loss 7.847969, Accuracy 77.381%\n",
      "Epoch 9, Batch 705, LR 2.286401 Loss 7.847935, Accuracy 77.379%\n",
      "Epoch 9, Batch 706, LR 2.286571 Loss 7.847897, Accuracy 77.377%\n",
      "Epoch 9, Batch 707, LR 2.286741 Loss 7.847127, Accuracy 77.379%\n",
      "Epoch 9, Batch 708, LR 2.286910 Loss 7.846900, Accuracy 77.382%\n",
      "Epoch 9, Batch 709, LR 2.287080 Loss 7.847553, Accuracy 77.377%\n",
      "Epoch 9, Batch 710, LR 2.287250 Loss 7.847437, Accuracy 77.379%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 711, LR 2.287419 Loss 7.847201, Accuracy 77.373%\n",
      "Epoch 9, Batch 712, LR 2.287589 Loss 7.847370, Accuracy 77.369%\n",
      "Epoch 9, Batch 713, LR 2.287758 Loss 7.848288, Accuracy 77.361%\n",
      "Epoch 9, Batch 714, LR 2.287928 Loss 7.848871, Accuracy 77.351%\n",
      "Epoch 9, Batch 715, LR 2.288097 Loss 7.849569, Accuracy 77.345%\n",
      "Epoch 9, Batch 716, LR 2.288266 Loss 7.850644, Accuracy 77.334%\n",
      "Epoch 9, Batch 717, LR 2.288435 Loss 7.850930, Accuracy 77.332%\n",
      "Epoch 9, Batch 718, LR 2.288605 Loss 7.850640, Accuracy 77.335%\n",
      "Epoch 9, Batch 719, LR 2.288774 Loss 7.850634, Accuracy 77.339%\n",
      "Epoch 9, Batch 720, LR 2.288943 Loss 7.850520, Accuracy 77.339%\n",
      "Epoch 9, Batch 721, LR 2.289112 Loss 7.850867, Accuracy 77.339%\n",
      "Epoch 9, Batch 722, LR 2.289281 Loss 7.851100, Accuracy 77.335%\n",
      "Epoch 9, Batch 723, LR 2.289449 Loss 7.851494, Accuracy 77.333%\n",
      "Epoch 9, Batch 724, LR 2.289618 Loss 7.852756, Accuracy 77.329%\n",
      "Epoch 9, Batch 725, LR 2.289787 Loss 7.852522, Accuracy 77.334%\n",
      "Epoch 9, Batch 726, LR 2.289956 Loss 7.852214, Accuracy 77.344%\n",
      "Epoch 9, Batch 727, LR 2.290124 Loss 7.851259, Accuracy 77.352%\n",
      "Epoch 9, Batch 728, LR 2.290293 Loss 7.852426, Accuracy 77.344%\n",
      "Epoch 9, Batch 729, LR 2.290461 Loss 7.852092, Accuracy 77.349%\n",
      "Epoch 9, Batch 730, LR 2.290630 Loss 7.852130, Accuracy 77.352%\n",
      "Epoch 9, Batch 731, LR 2.290798 Loss 7.851951, Accuracy 77.354%\n",
      "Epoch 9, Batch 732, LR 2.290966 Loss 7.852273, Accuracy 77.350%\n",
      "Epoch 9, Batch 733, LR 2.291134 Loss 7.851631, Accuracy 77.355%\n",
      "Epoch 9, Batch 734, LR 2.291303 Loss 7.851909, Accuracy 77.353%\n",
      "Epoch 9, Batch 735, LR 2.291471 Loss 7.852225, Accuracy 77.350%\n",
      "Epoch 9, Batch 736, LR 2.291639 Loss 7.852639, Accuracy 77.342%\n",
      "Epoch 9, Batch 737, LR 2.291807 Loss 7.852285, Accuracy 77.344%\n",
      "Epoch 9, Batch 738, LR 2.291975 Loss 7.852993, Accuracy 77.336%\n",
      "Epoch 9, Batch 739, LR 2.292143 Loss 7.853533, Accuracy 77.335%\n",
      "Epoch 9, Batch 740, LR 2.292310 Loss 7.852658, Accuracy 77.338%\n",
      "Epoch 9, Batch 741, LR 2.292478 Loss 7.852763, Accuracy 77.334%\n",
      "Epoch 9, Batch 742, LR 2.292646 Loss 7.852433, Accuracy 77.338%\n",
      "Epoch 9, Batch 743, LR 2.292813 Loss 7.852229, Accuracy 77.338%\n",
      "Epoch 9, Batch 744, LR 2.292981 Loss 7.852742, Accuracy 77.335%\n",
      "Epoch 9, Batch 745, LR 2.293148 Loss 7.853166, Accuracy 77.333%\n",
      "Epoch 9, Batch 746, LR 2.293316 Loss 7.853355, Accuracy 77.336%\n",
      "Epoch 9, Batch 747, LR 2.293483 Loss 7.853447, Accuracy 77.342%\n",
      "Epoch 9, Batch 748, LR 2.293651 Loss 7.854032, Accuracy 77.336%\n",
      "Epoch 9, Batch 749, LR 2.293818 Loss 7.854283, Accuracy 77.335%\n",
      "Epoch 9, Batch 750, LR 2.293985 Loss 7.854925, Accuracy 77.331%\n",
      "Epoch 9, Batch 751, LR 2.294152 Loss 7.854522, Accuracy 77.333%\n",
      "Epoch 9, Batch 752, LR 2.294319 Loss 7.853567, Accuracy 77.338%\n",
      "Epoch 9, Batch 753, LR 2.294486 Loss 7.853651, Accuracy 77.340%\n",
      "Epoch 9, Batch 754, LR 2.294653 Loss 7.853669, Accuracy 77.338%\n",
      "Epoch 9, Batch 755, LR 2.294820 Loss 7.852906, Accuracy 77.341%\n",
      "Epoch 9, Batch 756, LR 2.294987 Loss 7.852696, Accuracy 77.344%\n",
      "Epoch 9, Batch 757, LR 2.295154 Loss 7.852368, Accuracy 77.343%\n",
      "Epoch 9, Batch 758, LR 2.295320 Loss 7.853174, Accuracy 77.337%\n",
      "Epoch 9, Batch 759, LR 2.295487 Loss 7.853439, Accuracy 77.333%\n",
      "Epoch 9, Batch 760, LR 2.295654 Loss 7.853523, Accuracy 77.334%\n",
      "Epoch 9, Batch 761, LR 2.295820 Loss 7.852068, Accuracy 77.341%\n",
      "Epoch 9, Batch 762, LR 2.295987 Loss 7.851891, Accuracy 77.347%\n",
      "Epoch 9, Batch 763, LR 2.296153 Loss 7.851570, Accuracy 77.350%\n",
      "Epoch 9, Batch 764, LR 2.296319 Loss 7.852079, Accuracy 77.348%\n",
      "Epoch 9, Batch 765, LR 2.296486 Loss 7.852786, Accuracy 77.345%\n",
      "Epoch 9, Batch 766, LR 2.296652 Loss 7.852336, Accuracy 77.347%\n",
      "Epoch 9, Batch 767, LR 2.296818 Loss 7.851761, Accuracy 77.348%\n",
      "Epoch 9, Batch 768, LR 2.296984 Loss 7.852304, Accuracy 77.345%\n",
      "Epoch 9, Batch 769, LR 2.297150 Loss 7.851820, Accuracy 77.354%\n",
      "Epoch 9, Batch 770, LR 2.297316 Loss 7.851707, Accuracy 77.354%\n",
      "Epoch 9, Batch 771, LR 2.297482 Loss 7.851248, Accuracy 77.353%\n",
      "Epoch 9, Batch 772, LR 2.297648 Loss 7.851485, Accuracy 77.352%\n",
      "Epoch 9, Batch 773, LR 2.297813 Loss 7.851761, Accuracy 77.344%\n",
      "Epoch 9, Batch 774, LR 2.297979 Loss 7.851192, Accuracy 77.339%\n",
      "Epoch 9, Batch 775, LR 2.298145 Loss 7.850332, Accuracy 77.346%\n",
      "Epoch 9, Batch 776, LR 2.298310 Loss 7.849315, Accuracy 77.350%\n",
      "Epoch 9, Batch 777, LR 2.298476 Loss 7.849615, Accuracy 77.348%\n",
      "Epoch 9, Batch 778, LR 2.298641 Loss 7.848823, Accuracy 77.357%\n",
      "Epoch 9, Batch 779, LR 2.298807 Loss 7.849767, Accuracy 77.348%\n",
      "Epoch 9, Batch 780, LR 2.298972 Loss 7.849637, Accuracy 77.342%\n",
      "Epoch 9, Batch 781, LR 2.299137 Loss 7.849065, Accuracy 77.348%\n",
      "Epoch 9, Batch 782, LR 2.299303 Loss 7.848944, Accuracy 77.346%\n",
      "Epoch 9, Batch 783, LR 2.299468 Loss 7.848486, Accuracy 77.351%\n",
      "Epoch 9, Batch 784, LR 2.299633 Loss 7.849411, Accuracy 77.339%\n",
      "Epoch 9, Batch 785, LR 2.299798 Loss 7.849641, Accuracy 77.340%\n",
      "Epoch 9, Batch 786, LR 2.299963 Loss 7.849012, Accuracy 77.346%\n",
      "Epoch 9, Batch 787, LR 2.300128 Loss 7.849040, Accuracy 77.340%\n",
      "Epoch 9, Batch 788, LR 2.300293 Loss 7.848906, Accuracy 77.345%\n",
      "Epoch 9, Batch 789, LR 2.300458 Loss 7.848823, Accuracy 77.343%\n",
      "Epoch 9, Batch 790, LR 2.300622 Loss 7.848384, Accuracy 77.346%\n",
      "Epoch 9, Batch 791, LR 2.300787 Loss 7.847480, Accuracy 77.348%\n",
      "Epoch 9, Batch 792, LR 2.300952 Loss 7.847853, Accuracy 77.347%\n",
      "Epoch 9, Batch 793, LR 2.301116 Loss 7.847873, Accuracy 77.350%\n",
      "Epoch 9, Batch 794, LR 2.301281 Loss 7.848098, Accuracy 77.348%\n",
      "Epoch 9, Batch 795, LR 2.301445 Loss 7.848020, Accuracy 77.349%\n",
      "Epoch 9, Batch 796, LR 2.301609 Loss 7.848475, Accuracy 77.349%\n",
      "Epoch 9, Batch 797, LR 2.301774 Loss 7.848265, Accuracy 77.347%\n",
      "Epoch 9, Batch 798, LR 2.301938 Loss 7.848182, Accuracy 77.345%\n",
      "Epoch 9, Batch 799, LR 2.302102 Loss 7.848575, Accuracy 77.342%\n",
      "Epoch 9, Batch 800, LR 2.302266 Loss 7.849346, Accuracy 77.332%\n",
      "Epoch 9, Batch 801, LR 2.302430 Loss 7.849904, Accuracy 77.328%\n",
      "Epoch 9, Batch 802, LR 2.302594 Loss 7.849089, Accuracy 77.338%\n",
      "Epoch 9, Batch 803, LR 2.302758 Loss 7.848750, Accuracy 77.341%\n",
      "Epoch 9, Batch 804, LR 2.302922 Loss 7.847892, Accuracy 77.350%\n",
      "Epoch 9, Batch 805, LR 2.303086 Loss 7.847536, Accuracy 77.350%\n",
      "Epoch 9, Batch 806, LR 2.303249 Loss 7.846646, Accuracy 77.352%\n",
      "Epoch 9, Batch 807, LR 2.303413 Loss 7.847321, Accuracy 77.343%\n",
      "Epoch 9, Batch 808, LR 2.303577 Loss 7.847118, Accuracy 77.345%\n",
      "Epoch 9, Batch 809, LR 2.303740 Loss 7.847423, Accuracy 77.346%\n",
      "Epoch 9, Batch 810, LR 2.303904 Loss 7.847287, Accuracy 77.346%\n",
      "Epoch 9, Batch 811, LR 2.304067 Loss 7.846513, Accuracy 77.348%\n",
      "Epoch 9, Batch 812, LR 2.304231 Loss 7.846137, Accuracy 77.350%\n",
      "Epoch 9, Batch 813, LR 2.304394 Loss 7.845117, Accuracy 77.358%\n",
      "Epoch 9, Batch 814, LR 2.304557 Loss 7.845325, Accuracy 77.355%\n",
      "Epoch 9, Batch 815, LR 2.304720 Loss 7.845728, Accuracy 77.353%\n",
      "Epoch 9, Batch 816, LR 2.304883 Loss 7.846453, Accuracy 77.346%\n",
      "Epoch 9, Batch 817, LR 2.305047 Loss 7.846740, Accuracy 77.342%\n",
      "Epoch 9, Batch 818, LR 2.305210 Loss 7.846003, Accuracy 77.350%\n",
      "Epoch 9, Batch 819, LR 2.305372 Loss 7.846673, Accuracy 77.338%\n",
      "Epoch 9, Batch 820, LR 2.305535 Loss 7.847057, Accuracy 77.334%\n",
      "Epoch 9, Batch 821, LR 2.305698 Loss 7.846602, Accuracy 77.339%\n",
      "Epoch 9, Batch 822, LR 2.305861 Loss 7.846506, Accuracy 77.337%\n",
      "Epoch 9, Batch 823, LR 2.306024 Loss 7.846864, Accuracy 77.327%\n",
      "Epoch 9, Batch 824, LR 2.306186 Loss 7.847104, Accuracy 77.324%\n",
      "Epoch 9, Batch 825, LR 2.306349 Loss 7.847083, Accuracy 77.323%\n",
      "Epoch 9, Batch 826, LR 2.306511 Loss 7.846850, Accuracy 77.328%\n",
      "Epoch 9, Batch 827, LR 2.306674 Loss 7.846465, Accuracy 77.326%\n",
      "Epoch 9, Batch 828, LR 2.306836 Loss 7.847220, Accuracy 77.326%\n",
      "Epoch 9, Batch 829, LR 2.306998 Loss 7.847442, Accuracy 77.323%\n",
      "Epoch 9, Batch 830, LR 2.307161 Loss 7.847587, Accuracy 77.316%\n",
      "Epoch 9, Batch 831, LR 2.307323 Loss 7.845966, Accuracy 77.330%\n",
      "Epoch 9, Batch 832, LR 2.307485 Loss 7.845932, Accuracy 77.328%\n",
      "Epoch 9, Batch 833, LR 2.307647 Loss 7.846510, Accuracy 77.323%\n",
      "Epoch 9, Batch 834, LR 2.307809 Loss 7.845937, Accuracy 77.328%\n",
      "Epoch 9, Batch 835, LR 2.307971 Loss 7.847159, Accuracy 77.313%\n",
      "Epoch 9, Batch 836, LR 2.308133 Loss 7.847276, Accuracy 77.315%\n",
      "Epoch 9, Batch 837, LR 2.308295 Loss 7.846631, Accuracy 77.319%\n",
      "Epoch 9, Batch 838, LR 2.308456 Loss 7.846264, Accuracy 77.316%\n",
      "Epoch 9, Batch 839, LR 2.308618 Loss 7.846839, Accuracy 77.312%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 840, LR 2.308780 Loss 7.847047, Accuracy 77.312%\n",
      "Epoch 9, Batch 841, LR 2.308941 Loss 7.847414, Accuracy 77.311%\n",
      "Epoch 9, Batch 842, LR 2.309103 Loss 7.847561, Accuracy 77.309%\n",
      "Epoch 9, Batch 843, LR 2.309264 Loss 7.847400, Accuracy 77.311%\n",
      "Epoch 9, Batch 844, LR 2.309426 Loss 7.847062, Accuracy 77.313%\n",
      "Epoch 9, Batch 845, LR 2.309587 Loss 7.847577, Accuracy 77.308%\n",
      "Epoch 9, Batch 846, LR 2.309748 Loss 7.847503, Accuracy 77.309%\n",
      "Epoch 9, Batch 847, LR 2.309909 Loss 7.847766, Accuracy 77.310%\n",
      "Epoch 9, Batch 848, LR 2.310070 Loss 7.847234, Accuracy 77.312%\n",
      "Epoch 9, Batch 849, LR 2.310231 Loss 7.847391, Accuracy 77.311%\n",
      "Epoch 9, Batch 850, LR 2.310392 Loss 7.847428, Accuracy 77.309%\n",
      "Epoch 9, Batch 851, LR 2.310553 Loss 7.847384, Accuracy 77.311%\n",
      "Epoch 9, Batch 852, LR 2.310714 Loss 7.846976, Accuracy 77.318%\n",
      "Epoch 9, Batch 853, LR 2.310875 Loss 7.846483, Accuracy 77.317%\n",
      "Epoch 9, Batch 854, LR 2.311036 Loss 7.845967, Accuracy 77.317%\n",
      "Epoch 9, Batch 855, LR 2.311197 Loss 7.846185, Accuracy 77.313%\n",
      "Epoch 9, Batch 856, LR 2.311357 Loss 7.846631, Accuracy 77.307%\n",
      "Epoch 9, Batch 857, LR 2.311518 Loss 7.846058, Accuracy 77.315%\n",
      "Epoch 9, Batch 858, LR 2.311678 Loss 7.845524, Accuracy 77.316%\n",
      "Epoch 9, Batch 859, LR 2.311839 Loss 7.845791, Accuracy 77.307%\n",
      "Epoch 9, Batch 860, LR 2.311999 Loss 7.845959, Accuracy 77.307%\n",
      "Epoch 9, Batch 861, LR 2.312159 Loss 7.845434, Accuracy 77.311%\n",
      "Epoch 9, Batch 862, LR 2.312320 Loss 7.845582, Accuracy 77.308%\n",
      "Epoch 9, Batch 863, LR 2.312480 Loss 7.846285, Accuracy 77.305%\n",
      "Epoch 9, Batch 864, LR 2.312640 Loss 7.846853, Accuracy 77.300%\n",
      "Epoch 9, Batch 865, LR 2.312800 Loss 7.846505, Accuracy 77.300%\n",
      "Epoch 9, Batch 866, LR 2.312960 Loss 7.846409, Accuracy 77.309%\n",
      "Epoch 9, Batch 867, LR 2.313120 Loss 7.846511, Accuracy 77.310%\n",
      "Epoch 9, Batch 868, LR 2.313280 Loss 7.846659, Accuracy 77.307%\n",
      "Epoch 9, Batch 869, LR 2.313440 Loss 7.846529, Accuracy 77.307%\n",
      "Epoch 9, Batch 870, LR 2.313599 Loss 7.845724, Accuracy 77.318%\n",
      "Epoch 9, Batch 871, LR 2.313759 Loss 7.845817, Accuracy 77.313%\n",
      "Epoch 9, Batch 872, LR 2.313919 Loss 7.844691, Accuracy 77.322%\n",
      "Epoch 9, Batch 873, LR 2.314078 Loss 7.844447, Accuracy 77.319%\n",
      "Epoch 9, Batch 874, LR 2.314238 Loss 7.844447, Accuracy 77.315%\n",
      "Epoch 9, Batch 875, LR 2.314397 Loss 7.843690, Accuracy 77.320%\n",
      "Epoch 9, Batch 876, LR 2.314557 Loss 7.843044, Accuracy 77.325%\n",
      "Epoch 9, Batch 877, LR 2.314716 Loss 7.842765, Accuracy 77.325%\n",
      "Epoch 9, Batch 878, LR 2.314875 Loss 7.843051, Accuracy 77.328%\n",
      "Epoch 9, Batch 879, LR 2.315034 Loss 7.842608, Accuracy 77.326%\n",
      "Epoch 9, Batch 880, LR 2.315194 Loss 7.842738, Accuracy 77.330%\n",
      "Epoch 9, Batch 881, LR 2.315353 Loss 7.842281, Accuracy 77.335%\n",
      "Epoch 9, Batch 882, LR 2.315512 Loss 7.843255, Accuracy 77.329%\n",
      "Epoch 9, Batch 883, LR 2.315671 Loss 7.843088, Accuracy 77.330%\n",
      "Epoch 9, Batch 884, LR 2.315829 Loss 7.842917, Accuracy 77.330%\n",
      "Epoch 9, Batch 885, LR 2.315988 Loss 7.842241, Accuracy 77.331%\n",
      "Epoch 9, Batch 886, LR 2.316147 Loss 7.842574, Accuracy 77.323%\n",
      "Epoch 9, Batch 887, LR 2.316306 Loss 7.842929, Accuracy 77.320%\n",
      "Epoch 9, Batch 888, LR 2.316464 Loss 7.843159, Accuracy 77.315%\n",
      "Epoch 9, Batch 889, LR 2.316623 Loss 7.843509, Accuracy 77.312%\n",
      "Epoch 9, Batch 890, LR 2.316781 Loss 7.843560, Accuracy 77.313%\n",
      "Epoch 9, Batch 891, LR 2.316940 Loss 7.842516, Accuracy 77.316%\n",
      "Epoch 9, Batch 892, LR 2.317098 Loss 7.841616, Accuracy 77.319%\n",
      "Epoch 9, Batch 893, LR 2.317256 Loss 7.841747, Accuracy 77.315%\n",
      "Epoch 9, Batch 894, LR 2.317415 Loss 7.841946, Accuracy 77.318%\n",
      "Epoch 9, Batch 895, LR 2.317573 Loss 7.841761, Accuracy 77.321%\n",
      "Epoch 9, Batch 896, LR 2.317731 Loss 7.841183, Accuracy 77.325%\n",
      "Epoch 9, Batch 897, LR 2.317889 Loss 7.841319, Accuracy 77.326%\n",
      "Epoch 9, Batch 898, LR 2.318047 Loss 7.841457, Accuracy 77.325%\n",
      "Epoch 9, Batch 899, LR 2.318205 Loss 7.841786, Accuracy 77.318%\n",
      "Epoch 9, Batch 900, LR 2.318363 Loss 7.841997, Accuracy 77.319%\n",
      "Epoch 9, Batch 901, LR 2.318521 Loss 7.842777, Accuracy 77.314%\n",
      "Epoch 9, Batch 902, LR 2.318678 Loss 7.842757, Accuracy 77.313%\n",
      "Epoch 9, Batch 903, LR 2.318836 Loss 7.843099, Accuracy 77.313%\n",
      "Epoch 9, Batch 904, LR 2.318994 Loss 7.842735, Accuracy 77.318%\n",
      "Epoch 9, Batch 905, LR 2.319151 Loss 7.842350, Accuracy 77.320%\n",
      "Epoch 9, Batch 906, LR 2.319309 Loss 7.842414, Accuracy 77.319%\n",
      "Epoch 9, Batch 907, LR 2.319466 Loss 7.841847, Accuracy 77.320%\n",
      "Epoch 9, Batch 908, LR 2.319624 Loss 7.841945, Accuracy 77.318%\n",
      "Epoch 9, Batch 909, LR 2.319781 Loss 7.841714, Accuracy 77.318%\n",
      "Epoch 9, Batch 910, LR 2.319938 Loss 7.841664, Accuracy 77.320%\n",
      "Epoch 9, Batch 911, LR 2.320095 Loss 7.842261, Accuracy 77.315%\n",
      "Epoch 9, Batch 912, LR 2.320252 Loss 7.842775, Accuracy 77.309%\n",
      "Epoch 9, Batch 913, LR 2.320409 Loss 7.843035, Accuracy 77.303%\n",
      "Epoch 9, Batch 914, LR 2.320566 Loss 7.843463, Accuracy 77.299%\n",
      "Epoch 9, Batch 915, LR 2.320723 Loss 7.842857, Accuracy 77.302%\n",
      "Epoch 9, Batch 916, LR 2.320880 Loss 7.843563, Accuracy 77.296%\n",
      "Epoch 9, Batch 917, LR 2.321037 Loss 7.843347, Accuracy 77.293%\n",
      "Epoch 9, Batch 918, LR 2.321194 Loss 7.842883, Accuracy 77.297%\n",
      "Epoch 9, Batch 919, LR 2.321350 Loss 7.842679, Accuracy 77.298%\n",
      "Epoch 9, Batch 920, LR 2.321507 Loss 7.841584, Accuracy 77.305%\n",
      "Epoch 9, Batch 921, LR 2.321664 Loss 7.841784, Accuracy 77.310%\n",
      "Epoch 9, Batch 922, LR 2.321820 Loss 7.841908, Accuracy 77.308%\n",
      "Epoch 9, Batch 923, LR 2.321976 Loss 7.841685, Accuracy 77.308%\n",
      "Epoch 9, Batch 924, LR 2.322133 Loss 7.840892, Accuracy 77.310%\n",
      "Epoch 9, Batch 925, LR 2.322289 Loss 7.841345, Accuracy 77.310%\n",
      "Epoch 9, Batch 926, LR 2.322445 Loss 7.840878, Accuracy 77.313%\n",
      "Epoch 9, Batch 927, LR 2.322602 Loss 7.841167, Accuracy 77.316%\n",
      "Epoch 9, Batch 928, LR 2.322758 Loss 7.841305, Accuracy 77.314%\n",
      "Epoch 9, Batch 929, LR 2.322914 Loss 7.840989, Accuracy 77.315%\n",
      "Epoch 9, Batch 930, LR 2.323070 Loss 7.841440, Accuracy 77.312%\n",
      "Epoch 9, Batch 931, LR 2.323226 Loss 7.841699, Accuracy 77.315%\n",
      "Epoch 9, Batch 932, LR 2.323381 Loss 7.841187, Accuracy 77.318%\n",
      "Epoch 9, Batch 933, LR 2.323537 Loss 7.840813, Accuracy 77.318%\n",
      "Epoch 9, Batch 934, LR 2.323693 Loss 7.840470, Accuracy 77.316%\n",
      "Epoch 9, Batch 935, LR 2.323849 Loss 7.840064, Accuracy 77.314%\n",
      "Epoch 9, Batch 936, LR 2.324004 Loss 7.840391, Accuracy 77.311%\n",
      "Epoch 9, Batch 937, LR 2.324160 Loss 7.841036, Accuracy 77.308%\n",
      "Epoch 9, Batch 938, LR 2.324315 Loss 7.840060, Accuracy 77.315%\n",
      "Epoch 9, Batch 939, LR 2.324471 Loss 7.840044, Accuracy 77.314%\n",
      "Epoch 9, Batch 940, LR 2.324626 Loss 7.839848, Accuracy 77.315%\n",
      "Epoch 9, Batch 941, LR 2.324781 Loss 7.839244, Accuracy 77.317%\n",
      "Epoch 9, Batch 942, LR 2.324936 Loss 7.838994, Accuracy 77.321%\n",
      "Epoch 9, Batch 943, LR 2.325092 Loss 7.839525, Accuracy 77.311%\n",
      "Epoch 9, Batch 944, LR 2.325247 Loss 7.839760, Accuracy 77.305%\n",
      "Epoch 9, Batch 945, LR 2.325402 Loss 7.838536, Accuracy 77.315%\n",
      "Epoch 9, Batch 946, LR 2.325557 Loss 7.838154, Accuracy 77.320%\n",
      "Epoch 9, Batch 947, LR 2.325712 Loss 7.838514, Accuracy 77.317%\n",
      "Epoch 9, Batch 948, LR 2.325866 Loss 7.838225, Accuracy 77.315%\n",
      "Epoch 9, Batch 949, LR 2.326021 Loss 7.838024, Accuracy 77.317%\n",
      "Epoch 9, Batch 950, LR 2.326176 Loss 7.838471, Accuracy 77.314%\n",
      "Epoch 9, Batch 951, LR 2.326331 Loss 7.838580, Accuracy 77.316%\n",
      "Epoch 9, Batch 952, LR 2.326485 Loss 7.838070, Accuracy 77.313%\n",
      "Epoch 9, Batch 953, LR 2.326640 Loss 7.838081, Accuracy 77.310%\n",
      "Epoch 9, Batch 954, LR 2.326794 Loss 7.838013, Accuracy 77.309%\n",
      "Epoch 9, Batch 955, LR 2.326949 Loss 7.837814, Accuracy 77.308%\n",
      "Epoch 9, Batch 956, LR 2.327103 Loss 7.837911, Accuracy 77.312%\n",
      "Epoch 9, Batch 957, LR 2.327257 Loss 7.838588, Accuracy 77.307%\n",
      "Epoch 9, Batch 958, LR 2.327411 Loss 7.838213, Accuracy 77.309%\n",
      "Epoch 9, Batch 959, LR 2.327566 Loss 7.838437, Accuracy 77.309%\n",
      "Epoch 9, Batch 960, LR 2.327720 Loss 7.838273, Accuracy 77.307%\n",
      "Epoch 9, Batch 961, LR 2.327874 Loss 7.838248, Accuracy 77.306%\n",
      "Epoch 9, Batch 962, LR 2.328028 Loss 7.837759, Accuracy 77.310%\n",
      "Epoch 9, Batch 963, LR 2.328181 Loss 7.837943, Accuracy 77.310%\n",
      "Epoch 9, Batch 964, LR 2.328335 Loss 7.837097, Accuracy 77.319%\n",
      "Epoch 9, Batch 965, LR 2.328489 Loss 7.837544, Accuracy 77.314%\n",
      "Epoch 9, Batch 966, LR 2.328643 Loss 7.837216, Accuracy 77.314%\n",
      "Epoch 9, Batch 967, LR 2.328796 Loss 7.837015, Accuracy 77.315%\n",
      "Epoch 9, Batch 968, LR 2.328950 Loss 7.836180, Accuracy 77.322%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 969, LR 2.329103 Loss 7.835895, Accuracy 77.328%\n",
      "Epoch 9, Batch 970, LR 2.329257 Loss 7.834912, Accuracy 77.336%\n",
      "Epoch 9, Batch 971, LR 2.329410 Loss 7.834522, Accuracy 77.335%\n",
      "Epoch 9, Batch 972, LR 2.329564 Loss 7.834160, Accuracy 77.341%\n",
      "Epoch 9, Batch 973, LR 2.329717 Loss 7.833935, Accuracy 77.340%\n",
      "Epoch 9, Batch 974, LR 2.329870 Loss 7.834285, Accuracy 77.338%\n",
      "Epoch 9, Batch 975, LR 2.330023 Loss 7.834011, Accuracy 77.341%\n",
      "Epoch 9, Batch 976, LR 2.330176 Loss 7.833774, Accuracy 77.342%\n",
      "Epoch 9, Batch 977, LR 2.330329 Loss 7.833990, Accuracy 77.344%\n",
      "Epoch 9, Batch 978, LR 2.330482 Loss 7.834425, Accuracy 77.341%\n",
      "Epoch 9, Batch 979, LR 2.330635 Loss 7.833573, Accuracy 77.342%\n",
      "Epoch 9, Batch 980, LR 2.330788 Loss 7.833718, Accuracy 77.343%\n",
      "Epoch 9, Batch 981, LR 2.330941 Loss 7.833368, Accuracy 77.349%\n",
      "Epoch 9, Batch 982, LR 2.331093 Loss 7.833117, Accuracy 77.353%\n",
      "Epoch 9, Batch 983, LR 2.331246 Loss 7.833081, Accuracy 77.352%\n",
      "Epoch 9, Batch 984, LR 2.331398 Loss 7.833028, Accuracy 77.353%\n",
      "Epoch 9, Batch 985, LR 2.331551 Loss 7.833273, Accuracy 77.355%\n",
      "Epoch 9, Batch 986, LR 2.331703 Loss 7.833159, Accuracy 77.360%\n",
      "Epoch 9, Batch 987, LR 2.331856 Loss 7.832874, Accuracy 77.361%\n",
      "Epoch 9, Batch 988, LR 2.332008 Loss 7.833041, Accuracy 77.360%\n",
      "Epoch 9, Batch 989, LR 2.332160 Loss 7.833125, Accuracy 77.361%\n",
      "Epoch 9, Batch 990, LR 2.332312 Loss 7.832740, Accuracy 77.363%\n",
      "Epoch 9, Batch 991, LR 2.332465 Loss 7.832501, Accuracy 77.363%\n",
      "Epoch 9, Batch 992, LR 2.332617 Loss 7.832681, Accuracy 77.361%\n",
      "Epoch 9, Batch 993, LR 2.332769 Loss 7.832576, Accuracy 77.361%\n",
      "Epoch 9, Batch 994, LR 2.332920 Loss 7.832804, Accuracy 77.359%\n",
      "Epoch 9, Batch 995, LR 2.333072 Loss 7.833037, Accuracy 77.361%\n",
      "Epoch 9, Batch 996, LR 2.333224 Loss 7.833017, Accuracy 77.363%\n",
      "Epoch 9, Batch 997, LR 2.333376 Loss 7.833435, Accuracy 77.359%\n",
      "Epoch 9, Batch 998, LR 2.333527 Loss 7.833119, Accuracy 77.366%\n",
      "Epoch 9, Batch 999, LR 2.333679 Loss 7.833323, Accuracy 77.368%\n",
      "Epoch 9, Batch 1000, LR 2.333831 Loss 7.833232, Accuracy 77.367%\n",
      "Epoch 9, Batch 1001, LR 2.333982 Loss 7.833318, Accuracy 77.366%\n",
      "Epoch 9, Batch 1002, LR 2.334133 Loss 7.832962, Accuracy 77.366%\n",
      "Epoch 9, Batch 1003, LR 2.334285 Loss 7.833665, Accuracy 77.363%\n",
      "Epoch 9, Batch 1004, LR 2.334436 Loss 7.833124, Accuracy 77.367%\n",
      "Epoch 9, Batch 1005, LR 2.334587 Loss 7.833190, Accuracy 77.366%\n",
      "Epoch 9, Batch 1006, LR 2.334738 Loss 7.833278, Accuracy 77.367%\n",
      "Epoch 9, Batch 1007, LR 2.334890 Loss 7.832966, Accuracy 77.372%\n",
      "Epoch 9, Batch 1008, LR 2.335041 Loss 7.832423, Accuracy 77.374%\n",
      "Epoch 9, Batch 1009, LR 2.335192 Loss 7.832270, Accuracy 77.377%\n",
      "Epoch 9, Batch 1010, LR 2.335342 Loss 7.833073, Accuracy 77.375%\n",
      "Epoch 9, Batch 1011, LR 2.335493 Loss 7.833313, Accuracy 77.369%\n",
      "Epoch 9, Batch 1012, LR 2.335644 Loss 7.832627, Accuracy 77.376%\n",
      "Epoch 9, Batch 1013, LR 2.335795 Loss 7.832536, Accuracy 77.373%\n",
      "Epoch 9, Batch 1014, LR 2.335945 Loss 7.832750, Accuracy 77.369%\n",
      "Epoch 9, Batch 1015, LR 2.336096 Loss 7.832535, Accuracy 77.368%\n",
      "Epoch 9, Batch 1016, LR 2.336246 Loss 7.832646, Accuracy 77.371%\n",
      "Epoch 9, Batch 1017, LR 2.336397 Loss 7.832549, Accuracy 77.370%\n",
      "Epoch 9, Batch 1018, LR 2.336547 Loss 7.832433, Accuracy 77.371%\n",
      "Epoch 9, Batch 1019, LR 2.336698 Loss 7.832179, Accuracy 77.373%\n",
      "Epoch 9, Batch 1020, LR 2.336848 Loss 7.832469, Accuracy 77.371%\n",
      "Epoch 9, Batch 1021, LR 2.336998 Loss 7.832990, Accuracy 77.370%\n",
      "Epoch 9, Batch 1022, LR 2.337148 Loss 7.832950, Accuracy 77.372%\n",
      "Epoch 9, Batch 1023, LR 2.337298 Loss 7.833208, Accuracy 77.372%\n",
      "Epoch 9, Batch 1024, LR 2.337448 Loss 7.832825, Accuracy 77.374%\n",
      "Epoch 9, Batch 1025, LR 2.337598 Loss 7.832095, Accuracy 77.382%\n",
      "Epoch 9, Batch 1026, LR 2.337748 Loss 7.831900, Accuracy 77.386%\n",
      "Epoch 9, Batch 1027, LR 2.337898 Loss 7.831835, Accuracy 77.389%\n",
      "Epoch 9, Batch 1028, LR 2.338048 Loss 7.832142, Accuracy 77.386%\n",
      "Epoch 9, Batch 1029, LR 2.338197 Loss 7.831779, Accuracy 77.384%\n",
      "Epoch 9, Batch 1030, LR 2.338347 Loss 7.832326, Accuracy 77.377%\n",
      "Epoch 9, Batch 1031, LR 2.338496 Loss 7.832621, Accuracy 77.378%\n",
      "Epoch 9, Batch 1032, LR 2.338646 Loss 7.831856, Accuracy 77.382%\n",
      "Epoch 9, Batch 1033, LR 2.338795 Loss 7.832100, Accuracy 77.383%\n",
      "Epoch 9, Batch 1034, LR 2.338945 Loss 7.831687, Accuracy 77.384%\n",
      "Epoch 9, Batch 1035, LR 2.339094 Loss 7.831916, Accuracy 77.384%\n",
      "Epoch 9, Batch 1036, LR 2.339243 Loss 7.832195, Accuracy 77.378%\n",
      "Epoch 9, Batch 1037, LR 2.339392 Loss 7.832649, Accuracy 77.374%\n",
      "Epoch 9, Batch 1038, LR 2.339541 Loss 7.832179, Accuracy 77.378%\n",
      "Epoch 9, Batch 1039, LR 2.339690 Loss 7.832206, Accuracy 77.375%\n",
      "Epoch 9, Batch 1040, LR 2.339839 Loss 7.831335, Accuracy 77.378%\n",
      "Epoch 9, Batch 1041, LR 2.339988 Loss 7.831865, Accuracy 77.375%\n",
      "Epoch 9, Batch 1042, LR 2.340137 Loss 7.831946, Accuracy 77.377%\n",
      "Epoch 9, Batch 1043, LR 2.340286 Loss 7.831637, Accuracy 77.380%\n",
      "Epoch 9, Batch 1044, LR 2.340435 Loss 7.831816, Accuracy 77.374%\n",
      "Epoch 9, Batch 1045, LR 2.340583 Loss 7.832315, Accuracy 77.371%\n",
      "Epoch 9, Batch 1046, LR 2.340732 Loss 7.832165, Accuracy 77.371%\n",
      "Epoch 9, Batch 1047, LR 2.340880 Loss 7.832275, Accuracy 77.367%\n",
      "Epoch 9, Loss (train set) 7.832275, Accuracy (train set) 77.367%\n",
      "Epoch 9, Accuracy (validation set) 33.470%\n",
      "Epoch 10, Batch 1, LR 2.341029 Loss 7.222009, Accuracy 83.594%\n",
      "Epoch 10, Batch 2, LR 2.341177 Loss 7.596695, Accuracy 81.641%\n",
      "Epoch 10, Batch 3, LR 2.341326 Loss 7.874311, Accuracy 80.729%\n",
      "Epoch 10, Batch 4, LR 2.341474 Loss 7.715190, Accuracy 81.445%\n",
      "Epoch 10, Batch 5, LR 2.341622 Loss 7.647086, Accuracy 81.406%\n",
      "Epoch 10, Batch 6, LR 2.341770 Loss 7.588755, Accuracy 80.990%\n",
      "Epoch 10, Batch 7, LR 2.341918 Loss 7.577731, Accuracy 80.469%\n",
      "Epoch 10, Batch 8, LR 2.342066 Loss 7.622696, Accuracy 79.590%\n",
      "Epoch 10, Batch 9, LR 2.342214 Loss 7.521883, Accuracy 79.774%\n",
      "Epoch 10, Batch 10, LR 2.342362 Loss 7.511099, Accuracy 79.766%\n",
      "Epoch 10, Batch 11, LR 2.342510 Loss 7.532777, Accuracy 79.830%\n",
      "Epoch 10, Batch 12, LR 2.342658 Loss 7.554332, Accuracy 79.948%\n",
      "Epoch 10, Batch 13, LR 2.342805 Loss 7.602269, Accuracy 79.627%\n",
      "Epoch 10, Batch 14, LR 2.342953 Loss 7.651394, Accuracy 79.688%\n",
      "Epoch 10, Batch 15, LR 2.343101 Loss 7.611297, Accuracy 79.896%\n",
      "Epoch 10, Batch 16, LR 2.343248 Loss 7.574631, Accuracy 80.029%\n",
      "Epoch 10, Batch 17, LR 2.343395 Loss 7.585653, Accuracy 79.688%\n",
      "Epoch 10, Batch 18, LR 2.343543 Loss 7.566728, Accuracy 79.818%\n",
      "Epoch 10, Batch 19, LR 2.343690 Loss 7.573373, Accuracy 79.729%\n",
      "Epoch 10, Batch 20, LR 2.343837 Loss 7.580562, Accuracy 79.531%\n",
      "Epoch 10, Batch 21, LR 2.343984 Loss 7.563904, Accuracy 79.650%\n",
      "Epoch 10, Batch 22, LR 2.344132 Loss 7.589348, Accuracy 79.439%\n",
      "Epoch 10, Batch 23, LR 2.344279 Loss 7.551802, Accuracy 79.620%\n",
      "Epoch 10, Batch 24, LR 2.344426 Loss 7.550167, Accuracy 79.818%\n",
      "Epoch 10, Batch 25, LR 2.344573 Loss 7.516869, Accuracy 79.938%\n",
      "Epoch 10, Batch 26, LR 2.344719 Loss 7.529289, Accuracy 79.688%\n",
      "Epoch 10, Batch 27, LR 2.344866 Loss 7.534046, Accuracy 79.456%\n",
      "Epoch 10, Batch 28, LR 2.345013 Loss 7.531256, Accuracy 79.464%\n",
      "Epoch 10, Batch 29, LR 2.345159 Loss 7.534079, Accuracy 79.553%\n",
      "Epoch 10, Batch 30, LR 2.345306 Loss 7.496777, Accuracy 79.792%\n",
      "Epoch 10, Batch 31, LR 2.345453 Loss 7.491977, Accuracy 79.839%\n",
      "Epoch 10, Batch 32, LR 2.345599 Loss 7.494788, Accuracy 79.810%\n",
      "Epoch 10, Batch 33, LR 2.345745 Loss 7.467307, Accuracy 79.948%\n",
      "Epoch 10, Batch 34, LR 2.345892 Loss 7.479305, Accuracy 79.917%\n",
      "Epoch 10, Batch 35, LR 2.346038 Loss 7.483841, Accuracy 79.844%\n",
      "Epoch 10, Batch 36, LR 2.346184 Loss 7.481364, Accuracy 79.926%\n",
      "Epoch 10, Batch 37, LR 2.346330 Loss 7.482909, Accuracy 79.920%\n",
      "Epoch 10, Batch 38, LR 2.346476 Loss 7.482959, Accuracy 79.893%\n",
      "Epoch 10, Batch 39, LR 2.346622 Loss 7.467560, Accuracy 79.948%\n",
      "Epoch 10, Batch 40, LR 2.346768 Loss 7.490995, Accuracy 79.785%\n",
      "Epoch 10, Batch 41, LR 2.346914 Loss 7.500757, Accuracy 79.726%\n",
      "Epoch 10, Batch 42, LR 2.347060 Loss 7.509538, Accuracy 79.781%\n",
      "Epoch 10, Batch 43, LR 2.347206 Loss 7.533651, Accuracy 79.542%\n",
      "Epoch 10, Batch 44, LR 2.347352 Loss 7.512812, Accuracy 79.599%\n",
      "Epoch 10, Batch 45, LR 2.347497 Loss 7.498426, Accuracy 79.653%\n",
      "Epoch 10, Batch 46, LR 2.347643 Loss 7.496841, Accuracy 79.654%\n",
      "Epoch 10, Batch 47, LR 2.347788 Loss 7.495616, Accuracy 79.704%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 48, LR 2.347934 Loss 7.494934, Accuracy 79.574%\n",
      "Epoch 10, Batch 49, LR 2.348079 Loss 7.484334, Accuracy 79.640%\n",
      "Epoch 10, Batch 50, LR 2.348224 Loss 7.497453, Accuracy 79.641%\n",
      "Epoch 10, Batch 51, LR 2.348369 Loss 7.497236, Accuracy 79.626%\n",
      "Epoch 10, Batch 52, LR 2.348515 Loss 7.499838, Accuracy 79.552%\n",
      "Epoch 10, Batch 53, LR 2.348660 Loss 7.495873, Accuracy 79.570%\n",
      "Epoch 10, Batch 54, LR 2.348805 Loss 7.493964, Accuracy 79.586%\n",
      "Epoch 10, Batch 55, LR 2.348950 Loss 7.490965, Accuracy 79.574%\n",
      "Epoch 10, Batch 56, LR 2.349095 Loss 7.481673, Accuracy 79.646%\n",
      "Epoch 10, Batch 57, LR 2.349239 Loss 7.477889, Accuracy 79.674%\n",
      "Epoch 10, Batch 58, LR 2.349384 Loss 7.480258, Accuracy 79.688%\n",
      "Epoch 10, Batch 59, LR 2.349529 Loss 7.480002, Accuracy 79.793%\n",
      "Epoch 10, Batch 60, LR 2.349674 Loss 7.460582, Accuracy 79.896%\n",
      "Epoch 10, Batch 61, LR 2.349818 Loss 7.460189, Accuracy 79.918%\n",
      "Epoch 10, Batch 62, LR 2.349963 Loss 7.458006, Accuracy 79.889%\n",
      "Epoch 10, Batch 63, LR 2.350107 Loss 7.461663, Accuracy 79.936%\n",
      "Epoch 10, Batch 64, LR 2.350252 Loss 7.477269, Accuracy 79.761%\n",
      "Epoch 10, Batch 65, LR 2.350396 Loss 7.487573, Accuracy 79.724%\n",
      "Epoch 10, Batch 66, LR 2.350540 Loss 7.485723, Accuracy 79.699%\n",
      "Epoch 10, Batch 67, LR 2.350684 Loss 7.485130, Accuracy 79.711%\n",
      "Epoch 10, Batch 68, LR 2.350828 Loss 7.492162, Accuracy 79.550%\n",
      "Epoch 10, Batch 69, LR 2.350973 Loss 7.483631, Accuracy 79.597%\n",
      "Epoch 10, Batch 70, LR 2.351117 Loss 7.473363, Accuracy 79.676%\n",
      "Epoch 10, Batch 71, LR 2.351260 Loss 7.466748, Accuracy 79.721%\n",
      "Epoch 10, Batch 72, LR 2.351404 Loss 7.453994, Accuracy 79.753%\n",
      "Epoch 10, Batch 73, LR 2.351548 Loss 7.450378, Accuracy 79.752%\n",
      "Epoch 10, Batch 74, LR 2.351692 Loss 7.442250, Accuracy 79.856%\n",
      "Epoch 10, Batch 75, LR 2.351836 Loss 7.443082, Accuracy 79.875%\n",
      "Epoch 10, Batch 76, LR 2.351979 Loss 7.452657, Accuracy 79.801%\n",
      "Epoch 10, Batch 77, LR 2.352123 Loss 7.448064, Accuracy 79.799%\n",
      "Epoch 10, Batch 78, LR 2.352266 Loss 7.450751, Accuracy 79.818%\n",
      "Epoch 10, Batch 79, LR 2.352410 Loss 7.452202, Accuracy 79.786%\n",
      "Epoch 10, Batch 80, LR 2.352553 Loss 7.448392, Accuracy 79.824%\n",
      "Epoch 10, Batch 81, LR 2.352696 Loss 7.449756, Accuracy 79.842%\n",
      "Epoch 10, Batch 82, LR 2.352839 Loss 7.437012, Accuracy 79.859%\n",
      "Epoch 10, Batch 83, LR 2.352983 Loss 7.437420, Accuracy 79.838%\n",
      "Epoch 10, Batch 84, LR 2.353126 Loss 7.442682, Accuracy 79.836%\n",
      "Epoch 10, Batch 85, LR 2.353269 Loss 7.444218, Accuracy 79.807%\n",
      "Epoch 10, Batch 86, LR 2.353412 Loss 7.448054, Accuracy 79.787%\n",
      "Epoch 10, Batch 87, LR 2.353554 Loss 7.449730, Accuracy 79.795%\n",
      "Epoch 10, Batch 88, LR 2.353697 Loss 7.448546, Accuracy 79.776%\n",
      "Epoch 10, Batch 89, LR 2.353840 Loss 7.442495, Accuracy 79.810%\n",
      "Epoch 10, Batch 90, LR 2.353983 Loss 7.440803, Accuracy 79.835%\n",
      "Epoch 10, Batch 91, LR 2.354125 Loss 7.433874, Accuracy 79.876%\n",
      "Epoch 10, Batch 92, LR 2.354268 Loss 7.434885, Accuracy 79.891%\n",
      "Epoch 10, Batch 93, LR 2.354410 Loss 7.439652, Accuracy 79.856%\n",
      "Epoch 10, Batch 94, LR 2.354553 Loss 7.437593, Accuracy 79.870%\n",
      "Epoch 10, Batch 95, LR 2.354695 Loss 7.442316, Accuracy 79.827%\n",
      "Epoch 10, Batch 96, LR 2.354838 Loss 7.438736, Accuracy 79.818%\n",
      "Epoch 10, Batch 97, LR 2.354980 Loss 7.440798, Accuracy 79.824%\n",
      "Epoch 10, Batch 98, LR 2.355122 Loss 7.437447, Accuracy 79.839%\n",
      "Epoch 10, Batch 99, LR 2.355264 Loss 7.437725, Accuracy 79.869%\n",
      "Epoch 10, Batch 100, LR 2.355406 Loss 7.438003, Accuracy 79.867%\n",
      "Epoch 10, Batch 101, LR 2.355548 Loss 7.445436, Accuracy 79.819%\n",
      "Epoch 10, Batch 102, LR 2.355690 Loss 7.450106, Accuracy 79.772%\n",
      "Epoch 10, Batch 103, LR 2.355832 Loss 7.444983, Accuracy 79.824%\n",
      "Epoch 10, Batch 104, LR 2.355974 Loss 7.439986, Accuracy 79.875%\n",
      "Epoch 10, Batch 105, LR 2.356115 Loss 7.440708, Accuracy 79.851%\n",
      "Epoch 10, Batch 106, LR 2.356257 Loss 7.444330, Accuracy 79.805%\n",
      "Epoch 10, Batch 107, LR 2.356399 Loss 7.446199, Accuracy 79.782%\n",
      "Epoch 10, Batch 108, LR 2.356540 Loss 7.442330, Accuracy 79.803%\n",
      "Epoch 10, Batch 109, LR 2.356682 Loss 7.444168, Accuracy 79.795%\n",
      "Epoch 10, Batch 110, LR 2.356823 Loss 7.438618, Accuracy 79.879%\n",
      "Epoch 10, Batch 111, LR 2.356964 Loss 7.444526, Accuracy 79.821%\n",
      "Epoch 10, Batch 112, LR 2.357106 Loss 7.446084, Accuracy 79.806%\n",
      "Epoch 10, Batch 113, LR 2.357247 Loss 7.445271, Accuracy 79.791%\n",
      "Epoch 10, Batch 114, LR 2.357388 Loss 7.444772, Accuracy 79.797%\n",
      "Epoch 10, Batch 115, LR 2.357529 Loss 7.440993, Accuracy 79.830%\n",
      "Epoch 10, Batch 116, LR 2.357670 Loss 7.439049, Accuracy 79.829%\n",
      "Epoch 10, Batch 117, LR 2.357811 Loss 7.437868, Accuracy 79.841%\n",
      "Epoch 10, Batch 118, LR 2.357952 Loss 7.435979, Accuracy 79.873%\n",
      "Epoch 10, Batch 119, LR 2.358093 Loss 7.436690, Accuracy 79.865%\n",
      "Epoch 10, Batch 120, LR 2.358233 Loss 7.440408, Accuracy 79.818%\n",
      "Epoch 10, Batch 121, LR 2.358374 Loss 7.431625, Accuracy 79.875%\n",
      "Epoch 10, Batch 122, LR 2.358515 Loss 7.437308, Accuracy 79.816%\n",
      "Epoch 10, Batch 123, LR 2.358655 Loss 7.436519, Accuracy 79.834%\n",
      "Epoch 10, Batch 124, LR 2.358796 Loss 7.437160, Accuracy 79.832%\n",
      "Epoch 10, Batch 125, LR 2.358936 Loss 7.427529, Accuracy 79.875%\n",
      "Epoch 10, Batch 126, LR 2.359076 Loss 7.425027, Accuracy 79.892%\n",
      "Epoch 10, Batch 127, LR 2.359217 Loss 7.428391, Accuracy 79.847%\n",
      "Epoch 10, Batch 128, LR 2.359357 Loss 7.425146, Accuracy 79.883%\n",
      "Epoch 10, Batch 129, LR 2.359497 Loss 7.424295, Accuracy 79.881%\n",
      "Epoch 10, Batch 130, LR 2.359637 Loss 7.427581, Accuracy 79.874%\n",
      "Epoch 10, Batch 131, LR 2.359777 Loss 7.429817, Accuracy 79.849%\n",
      "Epoch 10, Batch 132, LR 2.359917 Loss 7.428254, Accuracy 79.883%\n",
      "Epoch 10, Batch 133, LR 2.360057 Loss 7.423480, Accuracy 79.911%\n",
      "Epoch 10, Batch 134, LR 2.360197 Loss 7.423471, Accuracy 79.938%\n",
      "Epoch 10, Batch 135, LR 2.360337 Loss 7.432024, Accuracy 79.861%\n",
      "Epoch 10, Batch 136, LR 2.360476 Loss 7.430035, Accuracy 79.866%\n",
      "Epoch 10, Batch 137, LR 2.360616 Loss 7.431546, Accuracy 79.864%\n",
      "Epoch 10, Batch 138, LR 2.360755 Loss 7.438468, Accuracy 79.840%\n",
      "Epoch 10, Batch 139, LR 2.360895 Loss 7.439206, Accuracy 79.828%\n",
      "Epoch 10, Batch 140, LR 2.361034 Loss 7.445445, Accuracy 79.782%\n",
      "Epoch 10, Batch 141, LR 2.361174 Loss 7.442916, Accuracy 79.793%\n",
      "Epoch 10, Batch 142, LR 2.361313 Loss 7.437389, Accuracy 79.847%\n",
      "Epoch 10, Batch 143, LR 2.361452 Loss 7.433242, Accuracy 79.868%\n",
      "Epoch 10, Batch 144, LR 2.361591 Loss 7.431584, Accuracy 79.894%\n",
      "Epoch 10, Batch 145, LR 2.361731 Loss 7.432669, Accuracy 79.876%\n",
      "Epoch 10, Batch 146, LR 2.361870 Loss 7.430477, Accuracy 79.869%\n",
      "Epoch 10, Batch 147, LR 2.362009 Loss 7.430447, Accuracy 79.874%\n",
      "Epoch 10, Batch 148, LR 2.362147 Loss 7.430952, Accuracy 79.856%\n",
      "Epoch 10, Batch 149, LR 2.362286 Loss 7.427981, Accuracy 79.845%\n",
      "Epoch 10, Batch 150, LR 2.362425 Loss 7.423945, Accuracy 79.875%\n",
      "Epoch 10, Batch 151, LR 2.362564 Loss 7.421454, Accuracy 79.874%\n",
      "Epoch 10, Batch 152, LR 2.362702 Loss 7.423925, Accuracy 79.842%\n",
      "Epoch 10, Batch 153, LR 2.362841 Loss 7.426941, Accuracy 79.830%\n",
      "Epoch 10, Batch 154, LR 2.362979 Loss 7.433478, Accuracy 79.753%\n",
      "Epoch 10, Batch 155, LR 2.363118 Loss 7.439279, Accuracy 79.723%\n",
      "Epoch 10, Batch 156, LR 2.363256 Loss 7.440026, Accuracy 79.708%\n",
      "Epoch 10, Batch 157, LR 2.363395 Loss 7.441696, Accuracy 79.712%\n",
      "Epoch 10, Batch 158, LR 2.363533 Loss 7.441502, Accuracy 79.707%\n",
      "Epoch 10, Batch 159, LR 2.363671 Loss 7.437952, Accuracy 79.702%\n",
      "Epoch 10, Batch 160, LR 2.363809 Loss 7.439324, Accuracy 79.692%\n",
      "Epoch 10, Batch 161, LR 2.363947 Loss 7.441623, Accuracy 79.678%\n",
      "Epoch 10, Batch 162, LR 2.364085 Loss 7.442511, Accuracy 79.663%\n",
      "Epoch 10, Batch 163, LR 2.364223 Loss 7.440178, Accuracy 79.673%\n",
      "Epoch 10, Batch 164, LR 2.364361 Loss 7.439878, Accuracy 79.711%\n",
      "Epoch 10, Batch 165, LR 2.364499 Loss 7.442505, Accuracy 79.683%\n",
      "Epoch 10, Batch 166, LR 2.364636 Loss 7.439267, Accuracy 79.683%\n",
      "Epoch 10, Batch 167, LR 2.364774 Loss 7.442988, Accuracy 79.645%\n",
      "Epoch 10, Batch 168, LR 2.364912 Loss 7.445532, Accuracy 79.641%\n",
      "Epoch 10, Batch 169, LR 2.365049 Loss 7.447736, Accuracy 79.627%\n",
      "Epoch 10, Batch 170, LR 2.365187 Loss 7.448465, Accuracy 79.619%\n",
      "Epoch 10, Batch 171, LR 2.365324 Loss 7.450017, Accuracy 79.610%\n",
      "Epoch 10, Batch 172, LR 2.365461 Loss 7.455162, Accuracy 79.583%\n",
      "Epoch 10, Batch 173, LR 2.365599 Loss 7.456276, Accuracy 79.597%\n",
      "Epoch 10, Batch 174, LR 2.365736 Loss 7.454201, Accuracy 79.611%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 175, LR 2.365873 Loss 7.452003, Accuracy 79.643%\n",
      "Epoch 10, Batch 176, LR 2.366010 Loss 7.455349, Accuracy 79.599%\n",
      "Epoch 10, Batch 177, LR 2.366147 Loss 7.456501, Accuracy 79.582%\n",
      "Epoch 10, Batch 178, LR 2.366284 Loss 7.456287, Accuracy 79.578%\n",
      "Epoch 10, Batch 179, LR 2.366421 Loss 7.453426, Accuracy 79.574%\n",
      "Epoch 10, Batch 180, LR 2.366558 Loss 7.452558, Accuracy 79.583%\n",
      "Epoch 10, Batch 181, LR 2.366694 Loss 7.454164, Accuracy 79.575%\n",
      "Epoch 10, Batch 182, LR 2.366831 Loss 7.455852, Accuracy 79.580%\n",
      "Epoch 10, Batch 183, LR 2.366968 Loss 7.456091, Accuracy 79.585%\n",
      "Epoch 10, Batch 184, LR 2.367104 Loss 7.451764, Accuracy 79.607%\n",
      "Epoch 10, Batch 185, LR 2.367241 Loss 7.451989, Accuracy 79.616%\n",
      "Epoch 10, Batch 186, LR 2.367377 Loss 7.452629, Accuracy 79.591%\n",
      "Epoch 10, Batch 187, LR 2.367513 Loss 7.456522, Accuracy 79.575%\n",
      "Epoch 10, Batch 188, LR 2.367650 Loss 7.458917, Accuracy 79.546%\n",
      "Epoch 10, Batch 189, LR 2.367786 Loss 7.461412, Accuracy 79.551%\n",
      "Epoch 10, Batch 190, LR 2.367922 Loss 7.460694, Accuracy 79.564%\n",
      "Epoch 10, Batch 191, LR 2.368058 Loss 7.466298, Accuracy 79.524%\n",
      "Epoch 10, Batch 192, LR 2.368194 Loss 7.463553, Accuracy 79.529%\n",
      "Epoch 10, Batch 193, LR 2.368330 Loss 7.462165, Accuracy 79.522%\n",
      "Epoch 10, Batch 194, LR 2.368466 Loss 7.466340, Accuracy 79.474%\n",
      "Epoch 10, Batch 195, LR 2.368602 Loss 7.465861, Accuracy 79.475%\n",
      "Epoch 10, Batch 196, LR 2.368737 Loss 7.463941, Accuracy 79.484%\n",
      "Epoch 10, Batch 197, LR 2.368873 Loss 7.462335, Accuracy 79.505%\n",
      "Epoch 10, Batch 198, LR 2.369009 Loss 7.461276, Accuracy 79.530%\n",
      "Epoch 10, Batch 199, LR 2.369144 Loss 7.463619, Accuracy 79.515%\n",
      "Epoch 10, Batch 200, LR 2.369280 Loss 7.463805, Accuracy 79.527%\n",
      "Epoch 10, Batch 201, LR 2.369415 Loss 7.463335, Accuracy 79.548%\n",
      "Epoch 10, Batch 202, LR 2.369551 Loss 7.466015, Accuracy 79.498%\n",
      "Epoch 10, Batch 203, LR 2.369686 Loss 7.471585, Accuracy 79.464%\n",
      "Epoch 10, Batch 204, LR 2.369821 Loss 7.472042, Accuracy 79.458%\n",
      "Epoch 10, Batch 205, LR 2.369956 Loss 7.470112, Accuracy 79.470%\n",
      "Epoch 10, Batch 206, LR 2.370091 Loss 7.468110, Accuracy 79.475%\n",
      "Epoch 10, Batch 207, LR 2.370226 Loss 7.470840, Accuracy 79.476%\n",
      "Epoch 10, Batch 208, LR 2.370361 Loss 7.470364, Accuracy 79.473%\n",
      "Epoch 10, Batch 209, LR 2.370496 Loss 7.472795, Accuracy 79.452%\n",
      "Epoch 10, Batch 210, LR 2.370631 Loss 7.470826, Accuracy 79.464%\n",
      "Epoch 10, Batch 211, LR 2.370766 Loss 7.475429, Accuracy 79.425%\n",
      "Epoch 10, Batch 212, LR 2.370900 Loss 7.476229, Accuracy 79.430%\n",
      "Epoch 10, Batch 213, LR 2.371035 Loss 7.476870, Accuracy 79.438%\n",
      "Epoch 10, Batch 214, LR 2.371170 Loss 7.476727, Accuracy 79.417%\n",
      "Epoch 10, Batch 215, LR 2.371304 Loss 7.477648, Accuracy 79.426%\n",
      "Epoch 10, Batch 216, LR 2.371438 Loss 7.473936, Accuracy 79.438%\n",
      "Epoch 10, Batch 217, LR 2.371573 Loss 7.474257, Accuracy 79.435%\n",
      "Epoch 10, Batch 218, LR 2.371707 Loss 7.478991, Accuracy 79.401%\n",
      "Epoch 10, Batch 219, LR 2.371841 Loss 7.478535, Accuracy 79.395%\n",
      "Epoch 10, Batch 220, LR 2.371975 Loss 7.477535, Accuracy 79.403%\n",
      "Epoch 10, Batch 221, LR 2.372110 Loss 7.477867, Accuracy 79.405%\n",
      "Epoch 10, Batch 222, LR 2.372244 Loss 7.477861, Accuracy 79.399%\n",
      "Epoch 10, Batch 223, LR 2.372378 Loss 7.480034, Accuracy 79.393%\n",
      "Epoch 10, Batch 224, LR 2.372511 Loss 7.480119, Accuracy 79.384%\n",
      "Epoch 10, Batch 225, LR 2.372645 Loss 7.483318, Accuracy 79.365%\n",
      "Epoch 10, Batch 226, LR 2.372779 Loss 7.483647, Accuracy 79.359%\n",
      "Epoch 10, Batch 227, LR 2.372913 Loss 7.485481, Accuracy 79.330%\n",
      "Epoch 10, Batch 228, LR 2.373046 Loss 7.484940, Accuracy 79.321%\n",
      "Epoch 10, Batch 229, LR 2.373180 Loss 7.479876, Accuracy 79.357%\n",
      "Epoch 10, Batch 230, LR 2.373313 Loss 7.476777, Accuracy 79.361%\n",
      "Epoch 10, Batch 231, LR 2.373447 Loss 7.476696, Accuracy 79.370%\n",
      "Epoch 10, Batch 232, LR 2.373580 Loss 7.477494, Accuracy 79.361%\n",
      "Epoch 10, Batch 233, LR 2.373713 Loss 7.480704, Accuracy 79.325%\n",
      "Epoch 10, Batch 234, LR 2.373847 Loss 7.481018, Accuracy 79.330%\n",
      "Epoch 10, Batch 235, LR 2.373980 Loss 7.483959, Accuracy 79.315%\n",
      "Epoch 10, Batch 236, LR 2.374113 Loss 7.484054, Accuracy 79.333%\n",
      "Epoch 10, Batch 237, LR 2.374246 Loss 7.487150, Accuracy 79.322%\n",
      "Epoch 10, Batch 238, LR 2.374379 Loss 7.488270, Accuracy 79.317%\n",
      "Epoch 10, Batch 239, LR 2.374512 Loss 7.486944, Accuracy 79.318%\n",
      "Epoch 10, Batch 240, LR 2.374645 Loss 7.484323, Accuracy 79.333%\n",
      "Epoch 10, Batch 241, LR 2.374777 Loss 7.483708, Accuracy 79.321%\n",
      "Epoch 10, Batch 242, LR 2.374910 Loss 7.482733, Accuracy 79.319%\n",
      "Epoch 10, Batch 243, LR 2.375043 Loss 7.484276, Accuracy 79.302%\n",
      "Epoch 10, Batch 244, LR 2.375175 Loss 7.484715, Accuracy 79.306%\n",
      "Epoch 10, Batch 245, LR 2.375308 Loss 7.481740, Accuracy 79.314%\n",
      "Epoch 10, Batch 246, LR 2.375440 Loss 7.485185, Accuracy 79.278%\n",
      "Epoch 10, Batch 247, LR 2.375573 Loss 7.483427, Accuracy 79.276%\n",
      "Epoch 10, Batch 248, LR 2.375705 Loss 7.486915, Accuracy 79.262%\n",
      "Epoch 10, Batch 249, LR 2.375837 Loss 7.483352, Accuracy 79.280%\n",
      "Epoch 10, Batch 250, LR 2.375969 Loss 7.487052, Accuracy 79.272%\n",
      "Epoch 10, Batch 251, LR 2.376101 Loss 7.488512, Accuracy 79.264%\n",
      "Epoch 10, Batch 252, LR 2.376233 Loss 7.489616, Accuracy 79.235%\n",
      "Epoch 10, Batch 253, LR 2.376365 Loss 7.489926, Accuracy 79.234%\n",
      "Epoch 10, Batch 254, LR 2.376497 Loss 7.492301, Accuracy 79.223%\n",
      "Epoch 10, Batch 255, LR 2.376629 Loss 7.494115, Accuracy 79.210%\n",
      "Epoch 10, Batch 256, LR 2.376761 Loss 7.495032, Accuracy 79.211%\n",
      "Epoch 10, Batch 257, LR 2.376893 Loss 7.494761, Accuracy 79.219%\n",
      "Epoch 10, Batch 258, LR 2.377024 Loss 7.494184, Accuracy 79.227%\n",
      "Epoch 10, Batch 259, LR 2.377156 Loss 7.494032, Accuracy 79.226%\n",
      "Epoch 10, Batch 260, LR 2.377287 Loss 7.495094, Accuracy 79.207%\n",
      "Epoch 10, Batch 261, LR 2.377419 Loss 7.495866, Accuracy 79.200%\n",
      "Epoch 10, Batch 262, LR 2.377550 Loss 7.496369, Accuracy 79.184%\n",
      "Epoch 10, Batch 263, LR 2.377681 Loss 7.497226, Accuracy 79.171%\n",
      "Epoch 10, Batch 264, LR 2.377813 Loss 7.497388, Accuracy 79.167%\n",
      "Epoch 10, Batch 265, LR 2.377944 Loss 7.499706, Accuracy 79.148%\n",
      "Epoch 10, Batch 266, LR 2.378075 Loss 7.501267, Accuracy 79.144%\n",
      "Epoch 10, Batch 267, LR 2.378206 Loss 7.501730, Accuracy 79.132%\n",
      "Epoch 10, Batch 268, LR 2.378337 Loss 7.500195, Accuracy 79.142%\n",
      "Epoch 10, Batch 269, LR 2.378468 Loss 7.499867, Accuracy 79.147%\n",
      "Epoch 10, Batch 270, LR 2.378599 Loss 7.499827, Accuracy 79.146%\n",
      "Epoch 10, Batch 271, LR 2.378729 Loss 7.503963, Accuracy 79.108%\n",
      "Epoch 10, Batch 272, LR 2.378860 Loss 7.504088, Accuracy 79.110%\n",
      "Epoch 10, Batch 273, LR 2.378991 Loss 7.505898, Accuracy 79.101%\n",
      "Epoch 10, Batch 274, LR 2.379121 Loss 7.506554, Accuracy 79.106%\n",
      "Epoch 10, Batch 275, LR 2.379252 Loss 7.509564, Accuracy 79.082%\n",
      "Epoch 10, Batch 276, LR 2.379382 Loss 7.511242, Accuracy 79.073%\n",
      "Epoch 10, Batch 277, LR 2.379513 Loss 7.511410, Accuracy 79.087%\n",
      "Epoch 10, Batch 278, LR 2.379643 Loss 7.511962, Accuracy 79.075%\n",
      "Epoch 10, Batch 279, LR 2.379773 Loss 7.513342, Accuracy 79.069%\n",
      "Epoch 10, Batch 280, LR 2.379903 Loss 7.511615, Accuracy 79.099%\n",
      "Epoch 10, Batch 281, LR 2.380033 Loss 7.511422, Accuracy 79.109%\n",
      "Epoch 10, Batch 282, LR 2.380163 Loss 7.513448, Accuracy 79.081%\n",
      "Epoch 10, Batch 283, LR 2.380293 Loss 7.512422, Accuracy 79.094%\n",
      "Epoch 10, Batch 284, LR 2.380423 Loss 7.513381, Accuracy 79.096%\n",
      "Epoch 10, Batch 285, LR 2.380553 Loss 7.512970, Accuracy 79.101%\n",
      "Epoch 10, Batch 286, LR 2.380683 Loss 7.512049, Accuracy 79.106%\n",
      "Epoch 10, Batch 287, LR 2.380813 Loss 7.513237, Accuracy 79.089%\n",
      "Epoch 10, Batch 288, LR 2.380942 Loss 7.514530, Accuracy 79.096%\n",
      "Epoch 10, Batch 289, LR 2.381072 Loss 7.515616, Accuracy 79.095%\n",
      "Epoch 10, Batch 290, LR 2.381201 Loss 7.516342, Accuracy 79.084%\n",
      "Epoch 10, Batch 291, LR 2.381331 Loss 7.515404, Accuracy 79.086%\n",
      "Epoch 10, Batch 292, LR 2.381460 Loss 7.513295, Accuracy 79.110%\n",
      "Epoch 10, Batch 293, LR 2.381589 Loss 7.512583, Accuracy 79.117%\n",
      "Epoch 10, Batch 294, LR 2.381719 Loss 7.512296, Accuracy 79.116%\n",
      "Epoch 10, Batch 295, LR 2.381848 Loss 7.512098, Accuracy 79.113%\n",
      "Epoch 10, Batch 296, LR 2.381977 Loss 7.509611, Accuracy 79.123%\n",
      "Epoch 10, Batch 297, LR 2.382106 Loss 7.508462, Accuracy 79.138%\n",
      "Epoch 10, Batch 298, LR 2.382235 Loss 7.507689, Accuracy 79.137%\n",
      "Epoch 10, Batch 299, LR 2.382364 Loss 7.508792, Accuracy 79.121%\n",
      "Epoch 10, Batch 300, LR 2.382493 Loss 7.509254, Accuracy 79.112%\n",
      "Epoch 10, Batch 301, LR 2.382621 Loss 7.511157, Accuracy 79.098%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 302, LR 2.382750 Loss 7.510570, Accuracy 79.100%\n",
      "Epoch 10, Batch 303, LR 2.382879 Loss 7.511517, Accuracy 79.100%\n",
      "Epoch 10, Batch 304, LR 2.383007 Loss 7.514379, Accuracy 79.084%\n",
      "Epoch 10, Batch 305, LR 2.383136 Loss 7.515848, Accuracy 79.088%\n",
      "Epoch 10, Batch 306, LR 2.383264 Loss 7.517025, Accuracy 79.075%\n",
      "Epoch 10, Batch 307, LR 2.383392 Loss 7.517299, Accuracy 79.077%\n",
      "Epoch 10, Batch 308, LR 2.383521 Loss 7.516514, Accuracy 79.076%\n",
      "Epoch 10, Batch 309, LR 2.383649 Loss 7.516786, Accuracy 79.083%\n",
      "Epoch 10, Batch 310, LR 2.383777 Loss 7.516041, Accuracy 79.100%\n",
      "Epoch 10, Batch 311, LR 2.383905 Loss 7.514554, Accuracy 79.105%\n",
      "Epoch 10, Batch 312, LR 2.384033 Loss 7.512861, Accuracy 79.124%\n",
      "Epoch 10, Batch 313, LR 2.384161 Loss 7.512719, Accuracy 79.123%\n",
      "Epoch 10, Batch 314, LR 2.384289 Loss 7.514029, Accuracy 79.108%\n",
      "Epoch 10, Batch 315, LR 2.384417 Loss 7.515403, Accuracy 79.082%\n",
      "Epoch 10, Batch 316, LR 2.384545 Loss 7.515913, Accuracy 79.087%\n",
      "Epoch 10, Batch 317, LR 2.384672 Loss 7.517429, Accuracy 79.081%\n",
      "Epoch 10, Batch 318, LR 2.384800 Loss 7.519121, Accuracy 79.088%\n",
      "Epoch 10, Batch 319, LR 2.384928 Loss 7.518885, Accuracy 79.105%\n",
      "Epoch 10, Batch 320, LR 2.385055 Loss 7.519075, Accuracy 79.094%\n",
      "Epoch 10, Batch 321, LR 2.385182 Loss 7.519011, Accuracy 79.081%\n",
      "Epoch 10, Batch 322, LR 2.385310 Loss 7.518666, Accuracy 79.086%\n",
      "Epoch 10, Batch 323, LR 2.385437 Loss 7.517305, Accuracy 79.102%\n",
      "Epoch 10, Batch 324, LR 2.385564 Loss 7.516482, Accuracy 79.102%\n",
      "Epoch 10, Batch 325, LR 2.385692 Loss 7.517087, Accuracy 79.099%\n",
      "Epoch 10, Batch 326, LR 2.385819 Loss 7.517032, Accuracy 79.098%\n",
      "Epoch 10, Batch 327, LR 2.385946 Loss 7.518763, Accuracy 79.105%\n",
      "Epoch 10, Batch 328, LR 2.386073 Loss 7.518750, Accuracy 79.109%\n",
      "Epoch 10, Batch 329, LR 2.386199 Loss 7.518400, Accuracy 79.118%\n",
      "Epoch 10, Batch 330, LR 2.386326 Loss 7.520181, Accuracy 79.107%\n",
      "Epoch 10, Batch 331, LR 2.386453 Loss 7.519309, Accuracy 79.105%\n",
      "Epoch 10, Batch 332, LR 2.386580 Loss 7.521160, Accuracy 79.095%\n",
      "Epoch 10, Batch 333, LR 2.386706 Loss 7.521888, Accuracy 79.080%\n",
      "Epoch 10, Batch 334, LR 2.386833 Loss 7.522166, Accuracy 79.084%\n",
      "Epoch 10, Batch 335, LR 2.386959 Loss 7.522236, Accuracy 79.079%\n",
      "Epoch 10, Batch 336, LR 2.387086 Loss 7.522409, Accuracy 79.085%\n",
      "Epoch 10, Batch 337, LR 2.387212 Loss 7.520320, Accuracy 79.092%\n",
      "Epoch 10, Batch 338, LR 2.387338 Loss 7.519638, Accuracy 79.096%\n",
      "Epoch 10, Batch 339, LR 2.387465 Loss 7.518532, Accuracy 79.109%\n",
      "Epoch 10, Batch 340, LR 2.387591 Loss 7.518931, Accuracy 79.106%\n",
      "Epoch 10, Batch 341, LR 2.387717 Loss 7.519736, Accuracy 79.103%\n",
      "Epoch 10, Batch 342, LR 2.387843 Loss 7.518116, Accuracy 79.121%\n",
      "Epoch 10, Batch 343, LR 2.387969 Loss 7.519523, Accuracy 79.132%\n",
      "Epoch 10, Batch 344, LR 2.388095 Loss 7.519375, Accuracy 79.131%\n",
      "Epoch 10, Batch 345, LR 2.388220 Loss 7.517939, Accuracy 79.135%\n",
      "Epoch 10, Batch 346, LR 2.388346 Loss 7.517641, Accuracy 79.137%\n",
      "Epoch 10, Batch 347, LR 2.388472 Loss 7.516403, Accuracy 79.143%\n",
      "Epoch 10, Batch 348, LR 2.388598 Loss 7.516892, Accuracy 79.140%\n",
      "Epoch 10, Batch 349, LR 2.388723 Loss 7.517022, Accuracy 79.137%\n",
      "Epoch 10, Batch 350, LR 2.388849 Loss 7.516258, Accuracy 79.132%\n",
      "Epoch 10, Batch 351, LR 2.388974 Loss 7.516009, Accuracy 79.147%\n",
      "Epoch 10, Batch 352, LR 2.389099 Loss 7.516052, Accuracy 79.148%\n",
      "Epoch 10, Batch 353, LR 2.389225 Loss 7.516093, Accuracy 79.145%\n",
      "Epoch 10, Batch 354, LR 2.389350 Loss 7.516296, Accuracy 79.149%\n",
      "Epoch 10, Batch 355, LR 2.389475 Loss 7.514444, Accuracy 79.159%\n",
      "Epoch 10, Batch 356, LR 2.389600 Loss 7.513584, Accuracy 79.170%\n",
      "Epoch 10, Batch 357, LR 2.389725 Loss 7.514959, Accuracy 79.171%\n",
      "Epoch 10, Batch 358, LR 2.389850 Loss 7.514802, Accuracy 79.168%\n",
      "Epoch 10, Batch 359, LR 2.389975 Loss 7.516159, Accuracy 79.152%\n",
      "Epoch 10, Batch 360, LR 2.390100 Loss 7.517656, Accuracy 79.138%\n",
      "Epoch 10, Batch 361, LR 2.390224 Loss 7.517226, Accuracy 79.133%\n",
      "Epoch 10, Batch 362, LR 2.390349 Loss 7.518288, Accuracy 79.137%\n",
      "Epoch 10, Batch 363, LR 2.390474 Loss 7.518117, Accuracy 79.130%\n",
      "Epoch 10, Batch 364, LR 2.390598 Loss 7.517155, Accuracy 79.136%\n",
      "Epoch 10, Batch 365, LR 2.390723 Loss 7.516259, Accuracy 79.142%\n",
      "Epoch 10, Batch 366, LR 2.390847 Loss 7.516745, Accuracy 79.130%\n",
      "Epoch 10, Batch 367, LR 2.390971 Loss 7.515519, Accuracy 79.145%\n",
      "Epoch 10, Batch 368, LR 2.391096 Loss 7.514977, Accuracy 79.138%\n",
      "Epoch 10, Batch 369, LR 2.391220 Loss 7.517464, Accuracy 79.124%\n",
      "Epoch 10, Batch 370, LR 2.391344 Loss 7.517493, Accuracy 79.120%\n",
      "Epoch 10, Batch 371, LR 2.391468 Loss 7.515432, Accuracy 79.125%\n",
      "Epoch 10, Batch 372, LR 2.391592 Loss 7.516206, Accuracy 79.123%\n",
      "Epoch 10, Batch 373, LR 2.391716 Loss 7.516687, Accuracy 79.118%\n",
      "Epoch 10, Batch 374, LR 2.391840 Loss 7.516853, Accuracy 79.107%\n",
      "Epoch 10, Batch 375, LR 2.391963 Loss 7.515667, Accuracy 79.096%\n",
      "Epoch 10, Batch 376, LR 2.392087 Loss 7.515931, Accuracy 79.093%\n",
      "Epoch 10, Batch 377, LR 2.392211 Loss 7.515657, Accuracy 79.093%\n",
      "Epoch 10, Batch 378, LR 2.392334 Loss 7.514117, Accuracy 79.113%\n",
      "Epoch 10, Batch 379, LR 2.392458 Loss 7.513572, Accuracy 79.125%\n",
      "Epoch 10, Batch 380, LR 2.392581 Loss 7.513327, Accuracy 79.122%\n",
      "Epoch 10, Batch 381, LR 2.392705 Loss 7.511959, Accuracy 79.132%\n",
      "Epoch 10, Batch 382, LR 2.392828 Loss 7.512047, Accuracy 79.129%\n",
      "Epoch 10, Batch 383, LR 2.392951 Loss 7.510665, Accuracy 79.143%\n",
      "Epoch 10, Batch 384, LR 2.393075 Loss 7.512569, Accuracy 79.130%\n",
      "Epoch 10, Batch 385, LR 2.393198 Loss 7.512474, Accuracy 79.127%\n",
      "Epoch 10, Batch 386, LR 2.393321 Loss 7.514106, Accuracy 79.125%\n",
      "Epoch 10, Batch 387, LR 2.393444 Loss 7.514628, Accuracy 79.122%\n",
      "Epoch 10, Batch 388, LR 2.393567 Loss 7.516169, Accuracy 79.124%\n",
      "Epoch 10, Batch 389, LR 2.393689 Loss 7.514781, Accuracy 79.127%\n",
      "Epoch 10, Batch 390, LR 2.393812 Loss 7.513085, Accuracy 79.145%\n",
      "Epoch 10, Batch 391, LR 2.393935 Loss 7.512413, Accuracy 79.154%\n",
      "Epoch 10, Batch 392, LR 2.394058 Loss 7.512213, Accuracy 79.151%\n",
      "Epoch 10, Batch 393, LR 2.394180 Loss 7.512696, Accuracy 79.149%\n",
      "Epoch 10, Batch 394, LR 2.394303 Loss 7.515007, Accuracy 79.142%\n",
      "Epoch 10, Batch 395, LR 2.394425 Loss 7.514810, Accuracy 79.140%\n",
      "Epoch 10, Batch 396, LR 2.394547 Loss 7.515629, Accuracy 79.127%\n",
      "Epoch 10, Batch 397, LR 2.394670 Loss 7.514448, Accuracy 79.135%\n",
      "Epoch 10, Batch 398, LR 2.394792 Loss 7.511588, Accuracy 79.152%\n",
      "Epoch 10, Batch 399, LR 2.394914 Loss 7.511453, Accuracy 79.155%\n",
      "Epoch 10, Batch 400, LR 2.395036 Loss 7.512179, Accuracy 79.150%\n",
      "Epoch 10, Batch 401, LR 2.395158 Loss 7.510583, Accuracy 79.154%\n",
      "Epoch 10, Batch 402, LR 2.395280 Loss 7.510345, Accuracy 79.161%\n",
      "Epoch 10, Batch 403, LR 2.395402 Loss 7.511036, Accuracy 79.145%\n",
      "Epoch 10, Batch 404, LR 2.395524 Loss 7.511422, Accuracy 79.150%\n",
      "Epoch 10, Batch 405, LR 2.395646 Loss 7.513325, Accuracy 79.144%\n",
      "Epoch 10, Batch 406, LR 2.395767 Loss 7.511989, Accuracy 79.158%\n",
      "Epoch 10, Batch 407, LR 2.395889 Loss 7.512729, Accuracy 79.156%\n",
      "Epoch 10, Batch 408, LR 2.396011 Loss 7.512628, Accuracy 79.165%\n",
      "Epoch 10, Batch 409, LR 2.396132 Loss 7.513841, Accuracy 79.151%\n",
      "Epoch 10, Batch 410, LR 2.396254 Loss 7.514686, Accuracy 79.146%\n",
      "Epoch 10, Batch 411, LR 2.396375 Loss 7.515109, Accuracy 79.131%\n",
      "Epoch 10, Batch 412, LR 2.396496 Loss 7.515514, Accuracy 79.126%\n",
      "Epoch 10, Batch 413, LR 2.396617 Loss 7.514653, Accuracy 79.129%\n",
      "Epoch 10, Batch 414, LR 2.396739 Loss 7.513814, Accuracy 79.138%\n",
      "Epoch 10, Batch 415, LR 2.396860 Loss 7.512589, Accuracy 79.155%\n",
      "Epoch 10, Batch 416, LR 2.396981 Loss 7.513239, Accuracy 79.145%\n",
      "Epoch 10, Batch 417, LR 2.397102 Loss 7.513864, Accuracy 79.140%\n",
      "Epoch 10, Batch 418, LR 2.397222 Loss 7.513161, Accuracy 79.144%\n",
      "Epoch 10, Batch 419, LR 2.397343 Loss 7.511307, Accuracy 79.151%\n",
      "Epoch 10, Batch 420, LR 2.397464 Loss 7.511117, Accuracy 79.154%\n",
      "Epoch 10, Batch 421, LR 2.397585 Loss 7.510313, Accuracy 79.160%\n",
      "Epoch 10, Batch 422, LR 2.397705 Loss 7.511287, Accuracy 79.165%\n",
      "Epoch 10, Batch 423, LR 2.397826 Loss 7.509809, Accuracy 79.178%\n",
      "Epoch 10, Batch 424, LR 2.397946 Loss 7.509612, Accuracy 79.181%\n",
      "Epoch 10, Batch 425, LR 2.398067 Loss 7.508872, Accuracy 79.191%\n",
      "Epoch 10, Batch 426, LR 2.398187 Loss 7.508866, Accuracy 79.198%\n",
      "Epoch 10, Batch 427, LR 2.398307 Loss 7.508433, Accuracy 79.199%\n",
      "Epoch 10, Batch 428, LR 2.398428 Loss 7.507897, Accuracy 79.204%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 429, LR 2.398548 Loss 7.508038, Accuracy 79.209%\n",
      "Epoch 10, Batch 430, LR 2.398668 Loss 7.507790, Accuracy 79.208%\n",
      "Epoch 10, Batch 431, LR 2.398788 Loss 7.507557, Accuracy 79.205%\n",
      "Epoch 10, Batch 432, LR 2.398908 Loss 7.508466, Accuracy 79.205%\n",
      "Epoch 10, Batch 433, LR 2.399028 Loss 7.508926, Accuracy 79.202%\n",
      "Epoch 10, Batch 434, LR 2.399147 Loss 7.509443, Accuracy 79.200%\n",
      "Epoch 10, Batch 435, LR 2.399267 Loss 7.509388, Accuracy 79.206%\n",
      "Epoch 10, Batch 436, LR 2.399387 Loss 7.509119, Accuracy 79.200%\n",
      "Epoch 10, Batch 437, LR 2.399506 Loss 7.508620, Accuracy 79.194%\n",
      "Epoch 10, Batch 438, LR 2.399626 Loss 7.508589, Accuracy 79.190%\n",
      "Epoch 10, Batch 439, LR 2.399745 Loss 7.508620, Accuracy 79.193%\n",
      "Epoch 10, Batch 440, LR 2.399865 Loss 7.508065, Accuracy 79.185%\n",
      "Epoch 10, Batch 441, LR 2.399984 Loss 7.507594, Accuracy 79.190%\n",
      "Epoch 10, Batch 442, LR 2.400103 Loss 7.506005, Accuracy 79.191%\n",
      "Epoch 10, Batch 443, LR 2.400222 Loss 7.505697, Accuracy 79.203%\n",
      "Epoch 10, Batch 444, LR 2.400342 Loss 7.504492, Accuracy 79.211%\n",
      "Epoch 10, Batch 445, LR 2.400461 Loss 7.504135, Accuracy 79.212%\n",
      "Epoch 10, Batch 446, LR 2.400580 Loss 7.502738, Accuracy 79.216%\n",
      "Epoch 10, Batch 447, LR 2.400698 Loss 7.502690, Accuracy 79.223%\n",
      "Epoch 10, Batch 448, LR 2.400817 Loss 7.501908, Accuracy 79.224%\n",
      "Epoch 10, Batch 449, LR 2.400936 Loss 7.501502, Accuracy 79.221%\n",
      "Epoch 10, Batch 450, LR 2.401055 Loss 7.500707, Accuracy 79.226%\n",
      "Epoch 10, Batch 451, LR 2.401173 Loss 7.500562, Accuracy 79.220%\n",
      "Epoch 10, Batch 452, LR 2.401292 Loss 7.500126, Accuracy 79.223%\n",
      "Epoch 10, Batch 453, LR 2.401410 Loss 7.500559, Accuracy 79.229%\n",
      "Epoch 10, Batch 454, LR 2.401529 Loss 7.501360, Accuracy 79.226%\n",
      "Epoch 10, Batch 455, LR 2.401647 Loss 7.502545, Accuracy 79.219%\n",
      "Epoch 10, Batch 456, LR 2.401766 Loss 7.501263, Accuracy 79.223%\n",
      "Epoch 10, Batch 457, LR 2.401884 Loss 7.500176, Accuracy 79.236%\n",
      "Epoch 10, Batch 458, LR 2.402002 Loss 7.500884, Accuracy 79.229%\n",
      "Epoch 10, Batch 459, LR 2.402120 Loss 7.500599, Accuracy 79.225%\n",
      "Epoch 10, Batch 460, LR 2.402238 Loss 7.500718, Accuracy 79.215%\n",
      "Epoch 10, Batch 461, LR 2.402356 Loss 7.501026, Accuracy 79.221%\n",
      "Epoch 10, Batch 462, LR 2.402474 Loss 7.500741, Accuracy 79.226%\n",
      "Epoch 10, Batch 463, LR 2.402592 Loss 7.501259, Accuracy 79.227%\n",
      "Epoch 10, Batch 464, LR 2.402709 Loss 7.500342, Accuracy 79.233%\n",
      "Epoch 10, Batch 465, LR 2.402827 Loss 7.500177, Accuracy 79.236%\n",
      "Epoch 10, Batch 466, LR 2.402945 Loss 7.499601, Accuracy 79.242%\n",
      "Epoch 10, Batch 467, LR 2.403062 Loss 7.499319, Accuracy 79.236%\n",
      "Epoch 10, Batch 468, LR 2.403180 Loss 7.498625, Accuracy 79.242%\n",
      "Epoch 10, Batch 469, LR 2.403297 Loss 7.499352, Accuracy 79.239%\n",
      "Epoch 10, Batch 470, LR 2.403415 Loss 7.499803, Accuracy 79.234%\n",
      "Epoch 10, Batch 471, LR 2.403532 Loss 7.500049, Accuracy 79.236%\n",
      "Epoch 10, Batch 472, LR 2.403649 Loss 7.501774, Accuracy 79.219%\n",
      "Epoch 10, Batch 473, LR 2.403766 Loss 7.502353, Accuracy 79.220%\n",
      "Epoch 10, Batch 474, LR 2.403883 Loss 7.503068, Accuracy 79.213%\n",
      "Epoch 10, Batch 475, LR 2.404000 Loss 7.503780, Accuracy 79.204%\n",
      "Epoch 10, Batch 476, LR 2.404117 Loss 7.504503, Accuracy 79.195%\n",
      "Epoch 10, Batch 477, LR 2.404234 Loss 7.503263, Accuracy 79.199%\n",
      "Epoch 10, Batch 478, LR 2.404351 Loss 7.503532, Accuracy 79.199%\n",
      "Epoch 10, Batch 479, LR 2.404468 Loss 7.504942, Accuracy 79.192%\n",
      "Epoch 10, Batch 480, LR 2.404584 Loss 7.506963, Accuracy 79.176%\n",
      "Epoch 10, Batch 481, LR 2.404701 Loss 7.507310, Accuracy 79.181%\n",
      "Epoch 10, Batch 482, LR 2.404817 Loss 7.506953, Accuracy 79.182%\n",
      "Epoch 10, Batch 483, LR 2.404934 Loss 7.507732, Accuracy 79.176%\n",
      "Epoch 10, Batch 484, LR 2.405050 Loss 7.508465, Accuracy 79.169%\n",
      "Epoch 10, Batch 485, LR 2.405167 Loss 7.508217, Accuracy 79.170%\n",
      "Epoch 10, Batch 486, LR 2.405283 Loss 7.509607, Accuracy 79.159%\n",
      "Epoch 10, Batch 487, LR 2.405399 Loss 7.510717, Accuracy 79.150%\n",
      "Epoch 10, Batch 488, LR 2.405515 Loss 7.511385, Accuracy 79.148%\n",
      "Epoch 10, Batch 489, LR 2.405631 Loss 7.511887, Accuracy 79.149%\n",
      "Epoch 10, Batch 490, LR 2.405747 Loss 7.511451, Accuracy 79.157%\n",
      "Epoch 10, Batch 491, LR 2.405863 Loss 7.512355, Accuracy 79.153%\n",
      "Epoch 10, Batch 492, LR 2.405979 Loss 7.511608, Accuracy 79.165%\n",
      "Epoch 10, Batch 493, LR 2.406095 Loss 7.510540, Accuracy 79.166%\n",
      "Epoch 10, Batch 494, LR 2.406210 Loss 7.511084, Accuracy 79.162%\n",
      "Epoch 10, Batch 495, LR 2.406326 Loss 7.509791, Accuracy 79.165%\n",
      "Epoch 10, Batch 496, LR 2.406441 Loss 7.509792, Accuracy 79.160%\n",
      "Epoch 10, Batch 497, LR 2.406557 Loss 7.509055, Accuracy 79.156%\n",
      "Epoch 10, Batch 498, LR 2.406672 Loss 7.508583, Accuracy 79.154%\n",
      "Epoch 10, Batch 499, LR 2.406788 Loss 7.509504, Accuracy 79.150%\n",
      "Epoch 10, Batch 500, LR 2.406903 Loss 7.509479, Accuracy 79.158%\n",
      "Epoch 10, Batch 501, LR 2.407018 Loss 7.509260, Accuracy 79.160%\n",
      "Epoch 10, Batch 502, LR 2.407133 Loss 7.508794, Accuracy 79.171%\n",
      "Epoch 10, Batch 503, LR 2.407249 Loss 7.508569, Accuracy 79.177%\n",
      "Epoch 10, Batch 504, LR 2.407364 Loss 7.509644, Accuracy 79.178%\n",
      "Epoch 10, Batch 505, LR 2.407479 Loss 7.509737, Accuracy 79.179%\n",
      "Epoch 10, Batch 506, LR 2.407593 Loss 7.510445, Accuracy 79.173%\n",
      "Epoch 10, Batch 507, LR 2.407708 Loss 7.511362, Accuracy 79.170%\n",
      "Epoch 10, Batch 508, LR 2.407823 Loss 7.509478, Accuracy 79.185%\n",
      "Epoch 10, Batch 509, LR 2.407938 Loss 7.509113, Accuracy 79.186%\n",
      "Epoch 10, Batch 510, LR 2.408052 Loss 7.510151, Accuracy 79.182%\n",
      "Epoch 10, Batch 511, LR 2.408167 Loss 7.510452, Accuracy 79.177%\n",
      "Epoch 10, Batch 512, LR 2.408281 Loss 7.511539, Accuracy 79.167%\n",
      "Epoch 10, Batch 513, LR 2.408396 Loss 7.511620, Accuracy 79.165%\n",
      "Epoch 10, Batch 514, LR 2.408510 Loss 7.512295, Accuracy 79.163%\n",
      "Epoch 10, Batch 515, LR 2.408624 Loss 7.511302, Accuracy 79.175%\n",
      "Epoch 10, Batch 516, LR 2.408738 Loss 7.512837, Accuracy 79.171%\n",
      "Epoch 10, Batch 517, LR 2.408853 Loss 7.512935, Accuracy 79.163%\n",
      "Epoch 10, Batch 518, LR 2.408967 Loss 7.512371, Accuracy 79.167%\n",
      "Epoch 10, Batch 519, LR 2.409081 Loss 7.510864, Accuracy 79.174%\n",
      "Epoch 10, Batch 520, LR 2.409195 Loss 7.511513, Accuracy 79.169%\n",
      "Epoch 10, Batch 521, LR 2.409308 Loss 7.511534, Accuracy 79.170%\n",
      "Epoch 10, Batch 522, LR 2.409422 Loss 7.511694, Accuracy 79.176%\n",
      "Epoch 10, Batch 523, LR 2.409536 Loss 7.512910, Accuracy 79.169%\n",
      "Epoch 10, Batch 524, LR 2.409650 Loss 7.512614, Accuracy 79.173%\n",
      "Epoch 10, Batch 525, LR 2.409763 Loss 7.511908, Accuracy 79.182%\n",
      "Epoch 10, Batch 526, LR 2.409877 Loss 7.512876, Accuracy 79.162%\n",
      "Epoch 10, Batch 527, LR 2.409990 Loss 7.514819, Accuracy 79.155%\n",
      "Epoch 10, Batch 528, LR 2.410103 Loss 7.515143, Accuracy 79.153%\n",
      "Epoch 10, Batch 529, LR 2.410217 Loss 7.514904, Accuracy 79.154%\n",
      "Epoch 10, Batch 530, LR 2.410330 Loss 7.514691, Accuracy 79.154%\n",
      "Epoch 10, Batch 531, LR 2.410443 Loss 7.513264, Accuracy 79.165%\n",
      "Epoch 10, Batch 532, LR 2.410556 Loss 7.513812, Accuracy 79.156%\n",
      "Epoch 10, Batch 533, LR 2.410669 Loss 7.514507, Accuracy 79.142%\n",
      "Epoch 10, Batch 534, LR 2.410782 Loss 7.514925, Accuracy 79.145%\n",
      "Epoch 10, Batch 535, LR 2.410895 Loss 7.516152, Accuracy 79.141%\n",
      "Epoch 10, Batch 536, LR 2.411008 Loss 7.517482, Accuracy 79.126%\n",
      "Epoch 10, Batch 537, LR 2.411121 Loss 7.516562, Accuracy 79.135%\n",
      "Epoch 10, Batch 538, LR 2.411233 Loss 7.516275, Accuracy 79.141%\n",
      "Epoch 10, Batch 539, LR 2.411346 Loss 7.517127, Accuracy 79.137%\n",
      "Epoch 10, Batch 540, LR 2.411459 Loss 7.518786, Accuracy 79.130%\n",
      "Epoch 10, Batch 541, LR 2.411571 Loss 7.519771, Accuracy 79.123%\n",
      "Epoch 10, Batch 542, LR 2.411683 Loss 7.520674, Accuracy 79.118%\n",
      "Epoch 10, Batch 543, LR 2.411796 Loss 7.520840, Accuracy 79.109%\n",
      "Epoch 10, Batch 544, LR 2.411908 Loss 7.521506, Accuracy 79.102%\n",
      "Epoch 10, Batch 545, LR 2.412020 Loss 7.522466, Accuracy 79.087%\n",
      "Epoch 10, Batch 546, LR 2.412132 Loss 7.522358, Accuracy 79.085%\n",
      "Epoch 10, Batch 547, LR 2.412245 Loss 7.523196, Accuracy 79.073%\n",
      "Epoch 10, Batch 548, LR 2.412357 Loss 7.523486, Accuracy 79.076%\n",
      "Epoch 10, Batch 549, LR 2.412468 Loss 7.522651, Accuracy 79.083%\n",
      "Epoch 10, Batch 550, LR 2.412580 Loss 7.521541, Accuracy 79.091%\n",
      "Epoch 10, Batch 551, LR 2.412692 Loss 7.520870, Accuracy 79.091%\n",
      "Epoch 10, Batch 552, LR 2.412804 Loss 7.521075, Accuracy 79.087%\n",
      "Epoch 10, Batch 553, LR 2.412915 Loss 7.521298, Accuracy 79.087%\n",
      "Epoch 10, Batch 554, LR 2.413027 Loss 7.520696, Accuracy 79.094%\n",
      "Epoch 10, Batch 555, LR 2.413139 Loss 7.519197, Accuracy 79.101%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 556, LR 2.413250 Loss 7.519427, Accuracy 79.095%\n",
      "Epoch 10, Batch 557, LR 2.413361 Loss 7.519756, Accuracy 79.094%\n",
      "Epoch 10, Batch 558, LR 2.413473 Loss 7.519732, Accuracy 79.094%\n",
      "Epoch 10, Batch 559, LR 2.413584 Loss 7.520857, Accuracy 79.085%\n",
      "Epoch 10, Batch 560, LR 2.413695 Loss 7.521657, Accuracy 79.072%\n",
      "Epoch 10, Batch 561, LR 2.413806 Loss 7.522071, Accuracy 79.066%\n",
      "Epoch 10, Batch 562, LR 2.413917 Loss 7.522385, Accuracy 79.065%\n",
      "Epoch 10, Batch 563, LR 2.414028 Loss 7.522734, Accuracy 79.066%\n",
      "Epoch 10, Batch 564, LR 2.414139 Loss 7.522315, Accuracy 79.071%\n",
      "Epoch 10, Batch 565, LR 2.414250 Loss 7.521716, Accuracy 79.076%\n",
      "Epoch 10, Batch 566, LR 2.414361 Loss 7.521460, Accuracy 79.076%\n",
      "Epoch 10, Batch 567, LR 2.414471 Loss 7.522142, Accuracy 79.067%\n",
      "Epoch 10, Batch 568, LR 2.414582 Loss 7.521300, Accuracy 79.067%\n",
      "Epoch 10, Batch 569, LR 2.414693 Loss 7.521541, Accuracy 79.072%\n",
      "Epoch 10, Batch 570, LR 2.414803 Loss 7.520422, Accuracy 79.082%\n",
      "Epoch 10, Batch 571, LR 2.414914 Loss 7.520221, Accuracy 79.084%\n",
      "Epoch 10, Batch 572, LR 2.415024 Loss 7.520402, Accuracy 79.085%\n",
      "Epoch 10, Batch 573, LR 2.415134 Loss 7.519303, Accuracy 79.090%\n",
      "Epoch 10, Batch 574, LR 2.415244 Loss 7.521017, Accuracy 79.079%\n",
      "Epoch 10, Batch 575, LR 2.415354 Loss 7.520622, Accuracy 79.087%\n",
      "Epoch 10, Batch 576, LR 2.415465 Loss 7.520674, Accuracy 79.089%\n",
      "Epoch 10, Batch 577, LR 2.415575 Loss 7.521645, Accuracy 79.085%\n",
      "Epoch 10, Batch 578, LR 2.415684 Loss 7.522870, Accuracy 79.078%\n",
      "Epoch 10, Batch 579, LR 2.415794 Loss 7.522635, Accuracy 79.078%\n",
      "Epoch 10, Batch 580, LR 2.415904 Loss 7.521640, Accuracy 79.075%\n",
      "Epoch 10, Batch 581, LR 2.416014 Loss 7.521929, Accuracy 79.076%\n",
      "Epoch 10, Batch 582, LR 2.416124 Loss 7.522594, Accuracy 79.073%\n",
      "Epoch 10, Batch 583, LR 2.416233 Loss 7.524233, Accuracy 79.064%\n",
      "Epoch 10, Batch 584, LR 2.416343 Loss 7.523692, Accuracy 79.057%\n",
      "Epoch 10, Batch 585, LR 2.416452 Loss 7.524001, Accuracy 79.064%\n",
      "Epoch 10, Batch 586, LR 2.416561 Loss 7.524808, Accuracy 79.062%\n",
      "Epoch 10, Batch 587, LR 2.416671 Loss 7.524542, Accuracy 79.067%\n",
      "Epoch 10, Batch 588, LR 2.416780 Loss 7.523866, Accuracy 79.071%\n",
      "Epoch 10, Batch 589, LR 2.416889 Loss 7.524688, Accuracy 79.068%\n",
      "Epoch 10, Batch 590, LR 2.416998 Loss 7.525352, Accuracy 79.057%\n",
      "Epoch 10, Batch 591, LR 2.417107 Loss 7.525114, Accuracy 79.058%\n",
      "Epoch 10, Batch 592, LR 2.417216 Loss 7.524201, Accuracy 79.062%\n",
      "Epoch 10, Batch 593, LR 2.417325 Loss 7.525522, Accuracy 79.046%\n",
      "Epoch 10, Batch 594, LR 2.417434 Loss 7.525082, Accuracy 79.054%\n",
      "Epoch 10, Batch 595, LR 2.417543 Loss 7.524515, Accuracy 79.052%\n",
      "Epoch 10, Batch 596, LR 2.417651 Loss 7.523738, Accuracy 79.057%\n",
      "Epoch 10, Batch 597, LR 2.417760 Loss 7.523720, Accuracy 79.046%\n",
      "Epoch 10, Batch 598, LR 2.417869 Loss 7.524185, Accuracy 79.042%\n",
      "Epoch 10, Batch 599, LR 2.417977 Loss 7.525090, Accuracy 79.042%\n",
      "Epoch 10, Batch 600, LR 2.418086 Loss 7.524826, Accuracy 79.042%\n",
      "Epoch 10, Batch 601, LR 2.418194 Loss 7.524195, Accuracy 79.040%\n",
      "Epoch 10, Batch 602, LR 2.418302 Loss 7.524741, Accuracy 79.039%\n",
      "Epoch 10, Batch 603, LR 2.418410 Loss 7.524030, Accuracy 79.044%\n",
      "Epoch 10, Batch 604, LR 2.418519 Loss 7.524188, Accuracy 79.036%\n",
      "Epoch 10, Batch 605, LR 2.418627 Loss 7.522559, Accuracy 79.055%\n",
      "Epoch 10, Batch 606, LR 2.418735 Loss 7.523018, Accuracy 79.053%\n",
      "Epoch 10, Batch 607, LR 2.418843 Loss 7.523492, Accuracy 79.054%\n",
      "Epoch 10, Batch 608, LR 2.418950 Loss 7.524371, Accuracy 79.045%\n",
      "Epoch 10, Batch 609, LR 2.419058 Loss 7.523588, Accuracy 79.055%\n",
      "Epoch 10, Batch 610, LR 2.419166 Loss 7.524547, Accuracy 79.047%\n",
      "Epoch 10, Batch 611, LR 2.419274 Loss 7.524513, Accuracy 79.044%\n",
      "Epoch 10, Batch 612, LR 2.419381 Loss 7.523488, Accuracy 79.047%\n",
      "Epoch 10, Batch 613, LR 2.419489 Loss 7.522604, Accuracy 79.048%\n",
      "Epoch 10, Batch 614, LR 2.419596 Loss 7.522757, Accuracy 79.046%\n",
      "Epoch 10, Batch 615, LR 2.419703 Loss 7.522141, Accuracy 79.047%\n",
      "Epoch 10, Batch 616, LR 2.419811 Loss 7.522290, Accuracy 79.044%\n",
      "Epoch 10, Batch 617, LR 2.419918 Loss 7.522714, Accuracy 79.040%\n",
      "Epoch 10, Batch 618, LR 2.420025 Loss 7.522182, Accuracy 79.035%\n",
      "Epoch 10, Batch 619, LR 2.420132 Loss 7.522612, Accuracy 79.040%\n",
      "Epoch 10, Batch 620, LR 2.420239 Loss 7.522370, Accuracy 79.040%\n",
      "Epoch 10, Batch 621, LR 2.420346 Loss 7.522503, Accuracy 79.037%\n",
      "Epoch 10, Batch 622, LR 2.420453 Loss 7.522889, Accuracy 79.041%\n",
      "Epoch 10, Batch 623, LR 2.420560 Loss 7.523177, Accuracy 79.037%\n",
      "Epoch 10, Batch 624, LR 2.420667 Loss 7.524171, Accuracy 79.025%\n",
      "Epoch 10, Batch 625, LR 2.420773 Loss 7.524845, Accuracy 79.024%\n",
      "Epoch 10, Batch 626, LR 2.420880 Loss 7.524459, Accuracy 79.029%\n",
      "Epoch 10, Batch 627, LR 2.420987 Loss 7.524790, Accuracy 79.026%\n",
      "Epoch 10, Batch 628, LR 2.421093 Loss 7.525084, Accuracy 79.028%\n",
      "Epoch 10, Batch 629, LR 2.421199 Loss 7.525308, Accuracy 79.030%\n",
      "Epoch 10, Batch 630, LR 2.421306 Loss 7.525514, Accuracy 79.023%\n",
      "Epoch 10, Batch 631, LR 2.421412 Loss 7.526624, Accuracy 79.018%\n",
      "Epoch 10, Batch 632, LR 2.421518 Loss 7.526116, Accuracy 79.020%\n",
      "Epoch 10, Batch 633, LR 2.421624 Loss 7.526800, Accuracy 79.023%\n",
      "Epoch 10, Batch 634, LR 2.421730 Loss 7.527600, Accuracy 79.018%\n",
      "Epoch 10, Batch 635, LR 2.421836 Loss 7.526718, Accuracy 79.027%\n",
      "Epoch 10, Batch 636, LR 2.421942 Loss 7.526256, Accuracy 79.025%\n",
      "Epoch 10, Batch 637, LR 2.422048 Loss 7.526290, Accuracy 79.024%\n",
      "Epoch 10, Batch 638, LR 2.422154 Loss 7.527374, Accuracy 79.013%\n",
      "Epoch 10, Batch 639, LR 2.422260 Loss 7.527211, Accuracy 79.019%\n",
      "Epoch 10, Batch 640, LR 2.422365 Loss 7.527172, Accuracy 79.022%\n",
      "Epoch 10, Batch 641, LR 2.422471 Loss 7.526672, Accuracy 79.024%\n",
      "Epoch 10, Batch 642, LR 2.422576 Loss 7.527099, Accuracy 79.030%\n",
      "Epoch 10, Batch 643, LR 2.422682 Loss 7.527537, Accuracy 79.030%\n",
      "Epoch 10, Batch 644, LR 2.422787 Loss 7.526938, Accuracy 79.032%\n",
      "Epoch 10, Batch 645, LR 2.422893 Loss 7.528557, Accuracy 79.015%\n",
      "Epoch 10, Batch 646, LR 2.422998 Loss 7.528570, Accuracy 79.021%\n",
      "Epoch 10, Batch 647, LR 2.423103 Loss 7.528198, Accuracy 79.028%\n",
      "Epoch 10, Batch 648, LR 2.423208 Loss 7.528414, Accuracy 79.028%\n",
      "Epoch 10, Batch 649, LR 2.423313 Loss 7.529103, Accuracy 79.023%\n",
      "Epoch 10, Batch 650, LR 2.423418 Loss 7.528184, Accuracy 79.029%\n",
      "Epoch 10, Batch 651, LR 2.423523 Loss 7.528934, Accuracy 79.020%\n",
      "Epoch 10, Batch 652, LR 2.423628 Loss 7.528757, Accuracy 79.024%\n",
      "Epoch 10, Batch 653, LR 2.423732 Loss 7.529133, Accuracy 79.021%\n",
      "Epoch 10, Batch 654, LR 2.423837 Loss 7.529781, Accuracy 79.017%\n",
      "Epoch 10, Batch 655, LR 2.423942 Loss 7.530618, Accuracy 79.015%\n",
      "Epoch 10, Batch 656, LR 2.424046 Loss 7.530186, Accuracy 79.015%\n",
      "Epoch 10, Batch 657, LR 2.424151 Loss 7.530409, Accuracy 79.016%\n",
      "Epoch 10, Batch 658, LR 2.424255 Loss 7.530174, Accuracy 79.017%\n",
      "Epoch 10, Batch 659, LR 2.424359 Loss 7.528838, Accuracy 79.025%\n",
      "Epoch 10, Batch 660, LR 2.424464 Loss 7.529561, Accuracy 79.023%\n",
      "Epoch 10, Batch 661, LR 2.424568 Loss 7.529344, Accuracy 79.029%\n",
      "Epoch 10, Batch 662, LR 2.424672 Loss 7.530444, Accuracy 79.022%\n",
      "Epoch 10, Batch 663, LR 2.424776 Loss 7.530908, Accuracy 79.012%\n",
      "Epoch 10, Batch 664, LR 2.424880 Loss 7.530243, Accuracy 79.014%\n",
      "Epoch 10, Batch 665, LR 2.424984 Loss 7.530386, Accuracy 79.016%\n",
      "Epoch 10, Batch 666, LR 2.425088 Loss 7.531934, Accuracy 79.001%\n",
      "Epoch 10, Batch 667, LR 2.425191 Loss 7.531980, Accuracy 78.999%\n",
      "Epoch 10, Batch 668, LR 2.425295 Loss 7.532150, Accuracy 79.000%\n",
      "Epoch 10, Batch 669, LR 2.425399 Loss 7.532295, Accuracy 79.006%\n",
      "Epoch 10, Batch 670, LR 2.425502 Loss 7.531263, Accuracy 79.014%\n",
      "Epoch 10, Batch 671, LR 2.425606 Loss 7.531293, Accuracy 79.012%\n",
      "Epoch 10, Batch 672, LR 2.425709 Loss 7.530940, Accuracy 79.011%\n",
      "Epoch 10, Batch 673, LR 2.425813 Loss 7.530018, Accuracy 79.012%\n",
      "Epoch 10, Batch 674, LR 2.425916 Loss 7.529782, Accuracy 79.011%\n",
      "Epoch 10, Batch 675, LR 2.426019 Loss 7.530252, Accuracy 79.008%\n",
      "Epoch 10, Batch 676, LR 2.426122 Loss 7.529999, Accuracy 78.998%\n",
      "Epoch 10, Batch 677, LR 2.426225 Loss 7.531133, Accuracy 78.997%\n",
      "Epoch 10, Batch 678, LR 2.426328 Loss 7.531183, Accuracy 79.001%\n",
      "Epoch 10, Batch 679, LR 2.426431 Loss 7.531701, Accuracy 78.999%\n",
      "Epoch 10, Batch 680, LR 2.426534 Loss 7.531763, Accuracy 78.997%\n",
      "Epoch 10, Batch 681, LR 2.426637 Loss 7.531971, Accuracy 78.993%\n",
      "Epoch 10, Batch 682, LR 2.426740 Loss 7.532617, Accuracy 78.990%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 683, LR 2.426842 Loss 7.533559, Accuracy 78.984%\n",
      "Epoch 10, Batch 684, LR 2.426945 Loss 7.533020, Accuracy 78.988%\n",
      "Epoch 10, Batch 685, LR 2.427047 Loss 7.531534, Accuracy 78.995%\n",
      "Epoch 10, Batch 686, LR 2.427150 Loss 7.531557, Accuracy 78.991%\n",
      "Epoch 10, Batch 687, LR 2.427252 Loss 7.532155, Accuracy 78.994%\n",
      "Epoch 10, Batch 688, LR 2.427354 Loss 7.531856, Accuracy 78.996%\n",
      "Epoch 10, Batch 689, LR 2.427457 Loss 7.532679, Accuracy 78.991%\n",
      "Epoch 10, Batch 690, LR 2.427559 Loss 7.533467, Accuracy 78.987%\n",
      "Epoch 10, Batch 691, LR 2.427661 Loss 7.532878, Accuracy 78.994%\n",
      "Epoch 10, Batch 692, LR 2.427763 Loss 7.531664, Accuracy 79.003%\n",
      "Epoch 10, Batch 693, LR 2.427865 Loss 7.532038, Accuracy 79.005%\n",
      "Epoch 10, Batch 694, LR 2.427967 Loss 7.531308, Accuracy 79.008%\n",
      "Epoch 10, Batch 695, LR 2.428069 Loss 7.530998, Accuracy 79.011%\n",
      "Epoch 10, Batch 696, LR 2.428170 Loss 7.530779, Accuracy 79.011%\n",
      "Epoch 10, Batch 697, LR 2.428272 Loss 7.530672, Accuracy 79.009%\n",
      "Epoch 10, Batch 698, LR 2.428374 Loss 7.530467, Accuracy 79.010%\n",
      "Epoch 10, Batch 699, LR 2.428475 Loss 7.530296, Accuracy 79.009%\n",
      "Epoch 10, Batch 700, LR 2.428577 Loss 7.529895, Accuracy 79.013%\n",
      "Epoch 10, Batch 701, LR 2.428678 Loss 7.530325, Accuracy 79.014%\n",
      "Epoch 10, Batch 702, LR 2.428779 Loss 7.529604, Accuracy 79.016%\n",
      "Epoch 10, Batch 703, LR 2.428881 Loss 7.530995, Accuracy 79.010%\n",
      "Epoch 10, Batch 704, LR 2.428982 Loss 7.530562, Accuracy 79.013%\n",
      "Epoch 10, Batch 705, LR 2.429083 Loss 7.530825, Accuracy 79.009%\n",
      "Epoch 10, Batch 706, LR 2.429184 Loss 7.530571, Accuracy 79.014%\n",
      "Epoch 10, Batch 707, LR 2.429285 Loss 7.529532, Accuracy 79.016%\n",
      "Epoch 10, Batch 708, LR 2.429386 Loss 7.528593, Accuracy 79.021%\n",
      "Epoch 10, Batch 709, LR 2.429487 Loss 7.527874, Accuracy 79.019%\n",
      "Epoch 10, Batch 710, LR 2.429588 Loss 7.528256, Accuracy 79.016%\n",
      "Epoch 10, Batch 711, LR 2.429688 Loss 7.528438, Accuracy 79.018%\n",
      "Epoch 10, Batch 712, LR 2.429789 Loss 7.529507, Accuracy 79.010%\n",
      "Epoch 10, Batch 713, LR 2.429889 Loss 7.528926, Accuracy 79.011%\n",
      "Epoch 10, Batch 714, LR 2.429990 Loss 7.528828, Accuracy 79.019%\n",
      "Epoch 10, Batch 715, LR 2.430090 Loss 7.528270, Accuracy 79.025%\n",
      "Epoch 10, Batch 716, LR 2.430191 Loss 7.527661, Accuracy 79.035%\n",
      "Epoch 10, Batch 717, LR 2.430291 Loss 7.527955, Accuracy 79.040%\n",
      "Epoch 10, Batch 718, LR 2.430391 Loss 7.528385, Accuracy 79.037%\n",
      "Epoch 10, Batch 719, LR 2.430491 Loss 7.529336, Accuracy 79.029%\n",
      "Epoch 10, Batch 720, LR 2.430591 Loss 7.529204, Accuracy 79.028%\n",
      "Epoch 10, Batch 721, LR 2.430691 Loss 7.529630, Accuracy 79.023%\n",
      "Epoch 10, Batch 722, LR 2.430791 Loss 7.529656, Accuracy 79.027%\n",
      "Epoch 10, Batch 723, LR 2.430891 Loss 7.529255, Accuracy 79.036%\n",
      "Epoch 10, Batch 724, LR 2.430991 Loss 7.528577, Accuracy 79.039%\n",
      "Epoch 10, Batch 725, LR 2.431091 Loss 7.529779, Accuracy 79.031%\n",
      "Epoch 10, Batch 726, LR 2.431190 Loss 7.529860, Accuracy 79.028%\n",
      "Epoch 10, Batch 727, LR 2.431290 Loss 7.529525, Accuracy 79.028%\n",
      "Epoch 10, Batch 728, LR 2.431389 Loss 7.529633, Accuracy 79.025%\n",
      "Epoch 10, Batch 729, LR 2.431489 Loss 7.529200, Accuracy 79.025%\n",
      "Epoch 10, Batch 730, LR 2.431588 Loss 7.530246, Accuracy 79.018%\n",
      "Epoch 10, Batch 731, LR 2.431688 Loss 7.529845, Accuracy 79.021%\n",
      "Epoch 10, Batch 732, LR 2.431787 Loss 7.529741, Accuracy 79.017%\n",
      "Epoch 10, Batch 733, LR 2.431886 Loss 7.529198, Accuracy 79.018%\n",
      "Epoch 10, Batch 734, LR 2.431985 Loss 7.529868, Accuracy 79.008%\n",
      "Epoch 10, Batch 735, LR 2.432084 Loss 7.530420, Accuracy 79.007%\n",
      "Epoch 10, Batch 736, LR 2.432183 Loss 7.530638, Accuracy 79.006%\n",
      "Epoch 10, Batch 737, LR 2.432282 Loss 7.531180, Accuracy 79.007%\n",
      "Epoch 10, Batch 738, LR 2.432381 Loss 7.531306, Accuracy 79.006%\n",
      "Epoch 10, Batch 739, LR 2.432479 Loss 7.531490, Accuracy 79.001%\n",
      "Epoch 10, Batch 740, LR 2.432578 Loss 7.530513, Accuracy 79.008%\n",
      "Epoch 10, Batch 741, LR 2.432677 Loss 7.531552, Accuracy 79.001%\n",
      "Epoch 10, Batch 742, LR 2.432775 Loss 7.531340, Accuracy 79.005%\n",
      "Epoch 10, Batch 743, LR 2.432874 Loss 7.531975, Accuracy 79.002%\n",
      "Epoch 10, Batch 744, LR 2.432972 Loss 7.532043, Accuracy 78.999%\n",
      "Epoch 10, Batch 745, LR 2.433070 Loss 7.531568, Accuracy 79.003%\n",
      "Epoch 10, Batch 746, LR 2.433169 Loss 7.531775, Accuracy 78.998%\n",
      "Epoch 10, Batch 747, LR 2.433267 Loss 7.531417, Accuracy 78.996%\n",
      "Epoch 10, Batch 748, LR 2.433365 Loss 7.531189, Accuracy 78.996%\n",
      "Epoch 10, Batch 749, LR 2.433463 Loss 7.531456, Accuracy 78.996%\n",
      "Epoch 10, Batch 750, LR 2.433561 Loss 7.532244, Accuracy 78.990%\n",
      "Epoch 10, Batch 751, LR 2.433659 Loss 7.532598, Accuracy 78.992%\n",
      "Epoch 10, Batch 752, LR 2.433757 Loss 7.532216, Accuracy 78.995%\n",
      "Epoch 10, Batch 753, LR 2.433854 Loss 7.531875, Accuracy 78.994%\n",
      "Epoch 10, Batch 754, LR 2.433952 Loss 7.532105, Accuracy 78.997%\n",
      "Epoch 10, Batch 755, LR 2.434050 Loss 7.533045, Accuracy 78.996%\n",
      "Epoch 10, Batch 756, LR 2.434147 Loss 7.532386, Accuracy 78.999%\n",
      "Epoch 10, Batch 757, LR 2.434245 Loss 7.532790, Accuracy 78.997%\n",
      "Epoch 10, Batch 758, LR 2.434342 Loss 7.532684, Accuracy 78.995%\n",
      "Epoch 10, Batch 759, LR 2.434440 Loss 7.532752, Accuracy 79.001%\n",
      "Epoch 10, Batch 760, LR 2.434537 Loss 7.533217, Accuracy 79.003%\n",
      "Epoch 10, Batch 761, LR 2.434634 Loss 7.533134, Accuracy 79.004%\n",
      "Epoch 10, Batch 762, LR 2.434731 Loss 7.534565, Accuracy 78.987%\n",
      "Epoch 10, Batch 763, LR 2.434828 Loss 7.534684, Accuracy 78.984%\n",
      "Epoch 10, Batch 764, LR 2.434925 Loss 7.533693, Accuracy 78.983%\n",
      "Epoch 10, Batch 765, LR 2.435022 Loss 7.533487, Accuracy 78.982%\n",
      "Epoch 10, Batch 766, LR 2.435119 Loss 7.532220, Accuracy 78.991%\n",
      "Epoch 10, Batch 767, LR 2.435216 Loss 7.532484, Accuracy 78.991%\n",
      "Epoch 10, Batch 768, LR 2.435312 Loss 7.531714, Accuracy 78.990%\n",
      "Epoch 10, Batch 769, LR 2.435409 Loss 7.531565, Accuracy 78.987%\n",
      "Epoch 10, Batch 770, LR 2.435506 Loss 7.531300, Accuracy 78.987%\n",
      "Epoch 10, Batch 771, LR 2.435602 Loss 7.531571, Accuracy 78.991%\n",
      "Epoch 10, Batch 772, LR 2.435699 Loss 7.531115, Accuracy 78.996%\n",
      "Epoch 10, Batch 773, LR 2.435795 Loss 7.530948, Accuracy 78.995%\n",
      "Epoch 10, Batch 774, LR 2.435891 Loss 7.530972, Accuracy 78.995%\n",
      "Epoch 10, Batch 775, LR 2.435987 Loss 7.530437, Accuracy 78.995%\n",
      "Epoch 10, Batch 776, LR 2.436084 Loss 7.529377, Accuracy 78.999%\n",
      "Epoch 10, Batch 777, LR 2.436180 Loss 7.529646, Accuracy 78.995%\n",
      "Epoch 10, Batch 778, LR 2.436276 Loss 7.530041, Accuracy 78.989%\n",
      "Epoch 10, Batch 779, LR 2.436372 Loss 7.530593, Accuracy 78.981%\n",
      "Epoch 10, Batch 780, LR 2.436467 Loss 7.529831, Accuracy 78.986%\n",
      "Epoch 10, Batch 781, LR 2.436563 Loss 7.530618, Accuracy 78.979%\n",
      "Epoch 10, Batch 782, LR 2.436659 Loss 7.530993, Accuracy 78.982%\n",
      "Epoch 10, Batch 783, LR 2.436755 Loss 7.531128, Accuracy 78.984%\n",
      "Epoch 10, Batch 784, LR 2.436850 Loss 7.530624, Accuracy 78.987%\n",
      "Epoch 10, Batch 785, LR 2.436946 Loss 7.530588, Accuracy 78.991%\n",
      "Epoch 10, Batch 786, LR 2.437041 Loss 7.529981, Accuracy 78.993%\n",
      "Epoch 10, Batch 787, LR 2.437137 Loss 7.530607, Accuracy 78.985%\n",
      "Epoch 10, Batch 788, LR 2.437232 Loss 7.530789, Accuracy 78.985%\n",
      "Epoch 10, Batch 789, LR 2.437327 Loss 7.530674, Accuracy 78.985%\n",
      "Epoch 10, Batch 790, LR 2.437422 Loss 7.530344, Accuracy 78.993%\n",
      "Epoch 10, Batch 791, LR 2.437517 Loss 7.530383, Accuracy 78.995%\n",
      "Epoch 10, Batch 792, LR 2.437612 Loss 7.530355, Accuracy 78.996%\n",
      "Epoch 10, Batch 793, LR 2.437707 Loss 7.530980, Accuracy 78.996%\n",
      "Epoch 10, Batch 794, LR 2.437802 Loss 7.530309, Accuracy 78.998%\n",
      "Epoch 10, Batch 795, LR 2.437897 Loss 7.529492, Accuracy 78.996%\n",
      "Epoch 10, Batch 796, LR 2.437992 Loss 7.530286, Accuracy 78.998%\n",
      "Epoch 10, Batch 797, LR 2.438086 Loss 7.531376, Accuracy 78.993%\n",
      "Epoch 10, Batch 798, LR 2.438181 Loss 7.532029, Accuracy 78.986%\n",
      "Epoch 10, Batch 799, LR 2.438276 Loss 7.532206, Accuracy 78.985%\n",
      "Epoch 10, Batch 800, LR 2.438370 Loss 7.532554, Accuracy 78.981%\n",
      "Epoch 10, Batch 801, LR 2.438464 Loss 7.532291, Accuracy 78.983%\n",
      "Epoch 10, Batch 802, LR 2.438559 Loss 7.531596, Accuracy 78.987%\n",
      "Epoch 10, Batch 803, LR 2.438653 Loss 7.531416, Accuracy 78.990%\n",
      "Epoch 10, Batch 804, LR 2.438747 Loss 7.531312, Accuracy 78.994%\n",
      "Epoch 10, Batch 805, LR 2.438841 Loss 7.531899, Accuracy 78.987%\n",
      "Epoch 10, Batch 806, LR 2.438935 Loss 7.532018, Accuracy 78.987%\n",
      "Epoch 10, Batch 807, LR 2.439029 Loss 7.531773, Accuracy 78.991%\n",
      "Epoch 10, Batch 808, LR 2.439123 Loss 7.531877, Accuracy 78.988%\n",
      "Epoch 10, Batch 809, LR 2.439217 Loss 7.531768, Accuracy 78.995%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 810, LR 2.439311 Loss 7.530812, Accuracy 79.003%\n",
      "Epoch 10, Batch 811, LR 2.439404 Loss 7.530293, Accuracy 79.005%\n",
      "Epoch 10, Batch 812, LR 2.439498 Loss 7.528658, Accuracy 79.014%\n",
      "Epoch 10, Batch 813, LR 2.439592 Loss 7.528762, Accuracy 79.016%\n",
      "Epoch 10, Batch 814, LR 2.439685 Loss 7.527850, Accuracy 79.020%\n",
      "Epoch 10, Batch 815, LR 2.439778 Loss 7.527117, Accuracy 79.027%\n",
      "Epoch 10, Batch 816, LR 2.439872 Loss 7.526814, Accuracy 79.029%\n",
      "Epoch 10, Batch 817, LR 2.439965 Loss 7.527723, Accuracy 79.016%\n",
      "Epoch 10, Batch 818, LR 2.440058 Loss 7.526632, Accuracy 79.025%\n",
      "Epoch 10, Batch 819, LR 2.440151 Loss 7.527147, Accuracy 79.019%\n",
      "Epoch 10, Batch 820, LR 2.440244 Loss 7.527474, Accuracy 79.018%\n",
      "Epoch 10, Batch 821, LR 2.440337 Loss 7.527212, Accuracy 79.022%\n",
      "Epoch 10, Batch 822, LR 2.440430 Loss 7.526418, Accuracy 79.028%\n",
      "Epoch 10, Batch 823, LR 2.440523 Loss 7.526184, Accuracy 79.031%\n",
      "Epoch 10, Batch 824, LR 2.440616 Loss 7.525297, Accuracy 79.040%\n",
      "Epoch 10, Batch 825, LR 2.440709 Loss 7.524919, Accuracy 79.043%\n",
      "Epoch 10, Batch 826, LR 2.440801 Loss 7.524617, Accuracy 79.050%\n",
      "Epoch 10, Batch 827, LR 2.440894 Loss 7.524886, Accuracy 79.047%\n",
      "Epoch 10, Batch 828, LR 2.440986 Loss 7.524748, Accuracy 79.050%\n",
      "Epoch 10, Batch 829, LR 2.441079 Loss 7.525482, Accuracy 79.041%\n",
      "Epoch 10, Batch 830, LR 2.441171 Loss 7.525888, Accuracy 79.037%\n",
      "Epoch 10, Batch 831, LR 2.441263 Loss 7.526399, Accuracy 79.035%\n",
      "Epoch 10, Batch 832, LR 2.441355 Loss 7.526503, Accuracy 79.033%\n",
      "Epoch 10, Batch 833, LR 2.441448 Loss 7.527296, Accuracy 79.031%\n",
      "Epoch 10, Batch 834, LR 2.441540 Loss 7.527345, Accuracy 79.025%\n",
      "Epoch 10, Batch 835, LR 2.441632 Loss 7.526994, Accuracy 79.024%\n",
      "Epoch 10, Batch 836, LR 2.441724 Loss 7.526725, Accuracy 79.023%\n",
      "Epoch 10, Batch 837, LR 2.441815 Loss 7.526360, Accuracy 79.019%\n",
      "Epoch 10, Batch 838, LR 2.441907 Loss 7.526183, Accuracy 79.022%\n",
      "Epoch 10, Batch 839, LR 2.441999 Loss 7.525838, Accuracy 79.028%\n",
      "Epoch 10, Batch 840, LR 2.442090 Loss 7.526857, Accuracy 79.022%\n",
      "Epoch 10, Batch 841, LR 2.442182 Loss 7.527128, Accuracy 79.020%\n",
      "Epoch 10, Batch 842, LR 2.442274 Loss 7.526433, Accuracy 79.029%\n",
      "Epoch 10, Batch 843, LR 2.442365 Loss 7.527242, Accuracy 79.027%\n",
      "Epoch 10, Batch 844, LR 2.442456 Loss 7.527074, Accuracy 79.029%\n",
      "Epoch 10, Batch 845, LR 2.442548 Loss 7.527975, Accuracy 79.024%\n",
      "Epoch 10, Batch 846, LR 2.442639 Loss 7.528437, Accuracy 79.021%\n",
      "Epoch 10, Batch 847, LR 2.442730 Loss 7.528656, Accuracy 79.015%\n",
      "Epoch 10, Batch 848, LR 2.442821 Loss 7.528621, Accuracy 79.021%\n",
      "Epoch 10, Batch 849, LR 2.442912 Loss 7.529222, Accuracy 79.015%\n",
      "Epoch 10, Batch 850, LR 2.443003 Loss 7.529438, Accuracy 79.017%\n",
      "Epoch 10, Batch 851, LR 2.443094 Loss 7.528519, Accuracy 79.018%\n",
      "Epoch 10, Batch 852, LR 2.443185 Loss 7.528806, Accuracy 79.013%\n",
      "Epoch 10, Batch 853, LR 2.443275 Loss 7.528169, Accuracy 79.019%\n",
      "Epoch 10, Batch 854, LR 2.443366 Loss 7.528857, Accuracy 79.013%\n",
      "Epoch 10, Batch 855, LR 2.443457 Loss 7.529031, Accuracy 79.012%\n",
      "Epoch 10, Batch 856, LR 2.443547 Loss 7.528095, Accuracy 79.019%\n",
      "Epoch 10, Batch 857, LR 2.443637 Loss 7.528418, Accuracy 79.017%\n",
      "Epoch 10, Batch 858, LR 2.443728 Loss 7.527686, Accuracy 79.018%\n",
      "Epoch 10, Batch 859, LR 2.443818 Loss 7.528279, Accuracy 79.013%\n",
      "Epoch 10, Batch 860, LR 2.443908 Loss 7.527485, Accuracy 79.023%\n",
      "Epoch 10, Batch 861, LR 2.443999 Loss 7.526946, Accuracy 79.029%\n",
      "Epoch 10, Batch 862, LR 2.444089 Loss 7.527246, Accuracy 79.029%\n",
      "Epoch 10, Batch 863, LR 2.444179 Loss 7.526833, Accuracy 79.032%\n",
      "Epoch 10, Batch 864, LR 2.444269 Loss 7.526372, Accuracy 79.039%\n",
      "Epoch 10, Batch 865, LR 2.444358 Loss 7.526039, Accuracy 79.043%\n",
      "Epoch 10, Batch 866, LR 2.444448 Loss 7.525797, Accuracy 79.040%\n",
      "Epoch 10, Batch 867, LR 2.444538 Loss 7.525493, Accuracy 79.039%\n",
      "Epoch 10, Batch 868, LR 2.444628 Loss 7.525216, Accuracy 79.042%\n",
      "Epoch 10, Batch 869, LR 2.444717 Loss 7.524659, Accuracy 79.049%\n",
      "Epoch 10, Batch 870, LR 2.444807 Loss 7.525752, Accuracy 79.045%\n",
      "Epoch 10, Batch 871, LR 2.444896 Loss 7.526396, Accuracy 79.041%\n",
      "Epoch 10, Batch 872, LR 2.444986 Loss 7.527509, Accuracy 79.029%\n",
      "Epoch 10, Batch 873, LR 2.445075 Loss 7.526522, Accuracy 79.038%\n",
      "Epoch 10, Batch 874, LR 2.445164 Loss 7.527258, Accuracy 79.039%\n",
      "Epoch 10, Batch 875, LR 2.445253 Loss 7.527019, Accuracy 79.041%\n",
      "Epoch 10, Batch 876, LR 2.445342 Loss 7.526750, Accuracy 79.042%\n",
      "Epoch 10, Batch 877, LR 2.445431 Loss 7.526903, Accuracy 79.042%\n",
      "Epoch 10, Batch 878, LR 2.445520 Loss 7.526671, Accuracy 79.041%\n",
      "Epoch 10, Batch 879, LR 2.445609 Loss 7.526309, Accuracy 79.048%\n",
      "Epoch 10, Batch 880, LR 2.445698 Loss 7.525783, Accuracy 79.053%\n",
      "Epoch 10, Batch 881, LR 2.445787 Loss 7.526025, Accuracy 79.047%\n",
      "Epoch 10, Batch 882, LR 2.445875 Loss 7.526076, Accuracy 79.045%\n",
      "Epoch 10, Batch 883, LR 2.445964 Loss 7.525705, Accuracy 79.048%\n",
      "Epoch 10, Batch 884, LR 2.446052 Loss 7.525597, Accuracy 79.050%\n",
      "Epoch 10, Batch 885, LR 2.446141 Loss 7.525861, Accuracy 79.051%\n",
      "Epoch 10, Batch 886, LR 2.446229 Loss 7.525701, Accuracy 79.052%\n",
      "Epoch 10, Batch 887, LR 2.446318 Loss 7.525947, Accuracy 79.052%\n",
      "Epoch 10, Batch 888, LR 2.446406 Loss 7.525881, Accuracy 79.053%\n",
      "Epoch 10, Batch 889, LR 2.446494 Loss 7.525912, Accuracy 79.049%\n",
      "Epoch 10, Batch 890, LR 2.446582 Loss 7.525601, Accuracy 79.055%\n",
      "Epoch 10, Batch 891, LR 2.446670 Loss 7.525413, Accuracy 79.057%\n",
      "Epoch 10, Batch 892, LR 2.446758 Loss 7.525481, Accuracy 79.060%\n",
      "Epoch 10, Batch 893, LR 2.446846 Loss 7.525678, Accuracy 79.062%\n",
      "Epoch 10, Batch 894, LR 2.446934 Loss 7.524861, Accuracy 79.063%\n",
      "Epoch 10, Batch 895, LR 2.447022 Loss 7.525302, Accuracy 79.063%\n",
      "Epoch 10, Batch 896, LR 2.447109 Loss 7.525003, Accuracy 79.066%\n",
      "Epoch 10, Batch 897, LR 2.447197 Loss 7.524640, Accuracy 79.067%\n",
      "Epoch 10, Batch 898, LR 2.447284 Loss 7.524701, Accuracy 79.066%\n",
      "Epoch 10, Batch 899, LR 2.447372 Loss 7.524808, Accuracy 79.064%\n",
      "Epoch 10, Batch 900, LR 2.447459 Loss 7.524400, Accuracy 79.069%\n",
      "Epoch 10, Batch 901, LR 2.447547 Loss 7.523709, Accuracy 79.074%\n",
      "Epoch 10, Batch 902, LR 2.447634 Loss 7.523294, Accuracy 79.077%\n",
      "Epoch 10, Batch 903, LR 2.447721 Loss 7.522838, Accuracy 79.081%\n",
      "Epoch 10, Batch 904, LR 2.447808 Loss 7.523253, Accuracy 79.077%\n",
      "Epoch 10, Batch 905, LR 2.447895 Loss 7.523554, Accuracy 79.076%\n",
      "Epoch 10, Batch 906, LR 2.447982 Loss 7.524315, Accuracy 79.072%\n",
      "Epoch 10, Batch 907, LR 2.448069 Loss 7.523404, Accuracy 79.079%\n",
      "Epoch 10, Batch 908, LR 2.448156 Loss 7.523393, Accuracy 79.085%\n",
      "Epoch 10, Batch 909, LR 2.448243 Loss 7.523620, Accuracy 79.084%\n",
      "Epoch 10, Batch 910, LR 2.448329 Loss 7.523549, Accuracy 79.087%\n",
      "Epoch 10, Batch 911, LR 2.448416 Loss 7.523085, Accuracy 79.090%\n",
      "Epoch 10, Batch 912, LR 2.448502 Loss 7.523369, Accuracy 79.085%\n",
      "Epoch 10, Batch 913, LR 2.448589 Loss 7.523372, Accuracy 79.083%\n",
      "Epoch 10, Batch 914, LR 2.448675 Loss 7.522588, Accuracy 79.088%\n",
      "Epoch 10, Batch 915, LR 2.448762 Loss 7.522347, Accuracy 79.085%\n",
      "Epoch 10, Batch 916, LR 2.448848 Loss 7.521823, Accuracy 79.085%\n",
      "Epoch 10, Batch 917, LR 2.448934 Loss 7.521959, Accuracy 79.085%\n",
      "Epoch 10, Batch 918, LR 2.449020 Loss 7.522853, Accuracy 79.078%\n",
      "Epoch 10, Batch 919, LR 2.449106 Loss 7.522807, Accuracy 79.079%\n",
      "Epoch 10, Batch 920, LR 2.449192 Loss 7.522838, Accuracy 79.075%\n",
      "Epoch 10, Batch 921, LR 2.449278 Loss 7.522554, Accuracy 79.074%\n",
      "Epoch 10, Batch 922, LR 2.449364 Loss 7.522161, Accuracy 79.077%\n",
      "Epoch 10, Batch 923, LR 2.449450 Loss 7.521853, Accuracy 79.080%\n",
      "Epoch 10, Batch 924, LR 2.449535 Loss 7.521868, Accuracy 79.081%\n",
      "Epoch 10, Batch 925, LR 2.449621 Loss 7.521622, Accuracy 79.084%\n",
      "Epoch 10, Batch 926, LR 2.449706 Loss 7.521500, Accuracy 79.084%\n",
      "Epoch 10, Batch 927, LR 2.449792 Loss 7.521148, Accuracy 79.082%\n",
      "Epoch 10, Batch 928, LR 2.449877 Loss 7.520682, Accuracy 79.087%\n",
      "Epoch 10, Batch 929, LR 2.449963 Loss 7.521819, Accuracy 79.080%\n",
      "Epoch 10, Batch 930, LR 2.450048 Loss 7.521817, Accuracy 79.077%\n",
      "Epoch 10, Batch 931, LR 2.450133 Loss 7.521728, Accuracy 79.077%\n",
      "Epoch 10, Batch 932, LR 2.450218 Loss 7.521749, Accuracy 79.080%\n",
      "Epoch 10, Batch 933, LR 2.450303 Loss 7.521997, Accuracy 79.081%\n",
      "Epoch 10, Batch 934, LR 2.450388 Loss 7.522391, Accuracy 79.079%\n",
      "Epoch 10, Batch 935, LR 2.450473 Loss 7.522147, Accuracy 79.081%\n",
      "Epoch 10, Batch 936, LR 2.450558 Loss 7.521730, Accuracy 79.082%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 937, LR 2.450643 Loss 7.521541, Accuracy 79.081%\n",
      "Epoch 10, Batch 938, LR 2.450727 Loss 7.520646, Accuracy 79.087%\n",
      "Epoch 10, Batch 939, LR 2.450812 Loss 7.521151, Accuracy 79.081%\n",
      "Epoch 10, Batch 940, LR 2.450897 Loss 7.521034, Accuracy 79.078%\n",
      "Epoch 10, Batch 941, LR 2.450981 Loss 7.521503, Accuracy 79.076%\n",
      "Epoch 10, Batch 942, LR 2.451065 Loss 7.520611, Accuracy 79.080%\n",
      "Epoch 10, Batch 943, LR 2.451150 Loss 7.519703, Accuracy 79.082%\n",
      "Epoch 10, Batch 944, LR 2.451234 Loss 7.519500, Accuracy 79.083%\n",
      "Epoch 10, Batch 945, LR 2.451318 Loss 7.519360, Accuracy 79.085%\n",
      "Epoch 10, Batch 946, LR 2.451402 Loss 7.519899, Accuracy 79.082%\n",
      "Epoch 10, Batch 947, LR 2.451486 Loss 7.520247, Accuracy 79.083%\n",
      "Epoch 10, Batch 948, LR 2.451570 Loss 7.520105, Accuracy 79.087%\n",
      "Epoch 10, Batch 949, LR 2.451654 Loss 7.520009, Accuracy 79.086%\n",
      "Epoch 10, Batch 950, LR 2.451738 Loss 7.519456, Accuracy 79.090%\n",
      "Epoch 10, Batch 951, LR 2.451822 Loss 7.520069, Accuracy 79.089%\n",
      "Epoch 10, Batch 952, LR 2.451906 Loss 7.519527, Accuracy 79.092%\n",
      "Epoch 10, Batch 953, LR 2.451989 Loss 7.521031, Accuracy 79.082%\n",
      "Epoch 10, Batch 954, LR 2.452073 Loss 7.521109, Accuracy 79.081%\n",
      "Epoch 10, Batch 955, LR 2.452156 Loss 7.521820, Accuracy 79.077%\n",
      "Epoch 10, Batch 956, LR 2.452240 Loss 7.521349, Accuracy 79.076%\n",
      "Epoch 10, Batch 957, LR 2.452323 Loss 7.521153, Accuracy 79.074%\n",
      "Epoch 10, Batch 958, LR 2.452406 Loss 7.521174, Accuracy 79.073%\n",
      "Epoch 10, Batch 959, LR 2.452489 Loss 7.520407, Accuracy 79.078%\n",
      "Epoch 10, Batch 960, LR 2.452572 Loss 7.521130, Accuracy 79.076%\n",
      "Epoch 10, Batch 961, LR 2.452656 Loss 7.520482, Accuracy 79.079%\n",
      "Epoch 10, Batch 962, LR 2.452739 Loss 7.519700, Accuracy 79.086%\n",
      "Epoch 10, Batch 963, LR 2.452821 Loss 7.519442, Accuracy 79.087%\n",
      "Epoch 10, Batch 964, LR 2.452904 Loss 7.519302, Accuracy 79.091%\n",
      "Epoch 10, Batch 965, LR 2.452987 Loss 7.519756, Accuracy 79.080%\n",
      "Epoch 10, Batch 966, LR 2.453070 Loss 7.519306, Accuracy 79.079%\n",
      "Epoch 10, Batch 967, LR 2.453152 Loss 7.519169, Accuracy 79.079%\n",
      "Epoch 10, Batch 968, LR 2.453235 Loss 7.518781, Accuracy 79.081%\n",
      "Epoch 10, Batch 969, LR 2.453317 Loss 7.518713, Accuracy 79.085%\n",
      "Epoch 10, Batch 970, LR 2.453400 Loss 7.518955, Accuracy 79.083%\n",
      "Epoch 10, Batch 971, LR 2.453482 Loss 7.518487, Accuracy 79.090%\n",
      "Epoch 10, Batch 972, LR 2.453564 Loss 7.518715, Accuracy 79.087%\n",
      "Epoch 10, Batch 973, LR 2.453647 Loss 7.519707, Accuracy 79.075%\n",
      "Epoch 10, Batch 974, LR 2.453729 Loss 7.519684, Accuracy 79.076%\n",
      "Epoch 10, Batch 975, LR 2.453811 Loss 7.520385, Accuracy 79.070%\n",
      "Epoch 10, Batch 976, LR 2.453893 Loss 7.520467, Accuracy 79.066%\n",
      "Epoch 10, Batch 977, LR 2.453975 Loss 7.520792, Accuracy 79.067%\n",
      "Epoch 10, Batch 978, LR 2.454056 Loss 7.520416, Accuracy 79.070%\n",
      "Epoch 10, Batch 979, LR 2.454138 Loss 7.521122, Accuracy 79.066%\n",
      "Epoch 10, Batch 980, LR 2.454220 Loss 7.520745, Accuracy 79.067%\n",
      "Epoch 10, Batch 981, LR 2.454302 Loss 7.520069, Accuracy 79.073%\n",
      "Epoch 10, Batch 982, LR 2.454383 Loss 7.519454, Accuracy 79.078%\n",
      "Epoch 10, Batch 983, LR 2.454465 Loss 7.519542, Accuracy 79.081%\n",
      "Epoch 10, Batch 984, LR 2.454546 Loss 7.518965, Accuracy 79.087%\n",
      "Epoch 10, Batch 985, LR 2.454627 Loss 7.517977, Accuracy 79.091%\n",
      "Epoch 10, Batch 986, LR 2.454709 Loss 7.518337, Accuracy 79.090%\n",
      "Epoch 10, Batch 987, LR 2.454790 Loss 7.518641, Accuracy 79.090%\n",
      "Epoch 10, Batch 988, LR 2.454871 Loss 7.518246, Accuracy 79.092%\n",
      "Epoch 10, Batch 989, LR 2.454952 Loss 7.518855, Accuracy 79.087%\n",
      "Epoch 10, Batch 990, LR 2.455033 Loss 7.518962, Accuracy 79.087%\n",
      "Epoch 10, Batch 991, LR 2.455114 Loss 7.518814, Accuracy 79.086%\n",
      "Epoch 10, Batch 992, LR 2.455195 Loss 7.519209, Accuracy 79.087%\n",
      "Epoch 10, Batch 993, LR 2.455275 Loss 7.518998, Accuracy 79.089%\n",
      "Epoch 10, Batch 994, LR 2.455356 Loss 7.519516, Accuracy 79.090%\n",
      "Epoch 10, Batch 995, LR 2.455437 Loss 7.520166, Accuracy 79.086%\n",
      "Epoch 10, Batch 996, LR 2.455517 Loss 7.520685, Accuracy 79.081%\n",
      "Epoch 10, Batch 997, LR 2.455598 Loss 7.520354, Accuracy 79.080%\n",
      "Epoch 10, Batch 998, LR 2.455678 Loss 7.520890, Accuracy 79.076%\n",
      "Epoch 10, Batch 999, LR 2.455759 Loss 7.521284, Accuracy 79.074%\n",
      "Epoch 10, Batch 1000, LR 2.455839 Loss 7.522032, Accuracy 79.073%\n",
      "Epoch 10, Batch 1001, LR 2.455919 Loss 7.521993, Accuracy 79.068%\n",
      "Epoch 10, Batch 1002, LR 2.455999 Loss 7.521726, Accuracy 79.071%\n",
      "Epoch 10, Batch 1003, LR 2.456079 Loss 7.522460, Accuracy 79.068%\n",
      "Epoch 10, Batch 1004, LR 2.456159 Loss 7.522817, Accuracy 79.070%\n",
      "Epoch 10, Batch 1005, LR 2.456239 Loss 7.522560, Accuracy 79.071%\n",
      "Epoch 10, Batch 1006, LR 2.456319 Loss 7.522047, Accuracy 79.072%\n",
      "Epoch 10, Batch 1007, LR 2.456399 Loss 7.521827, Accuracy 79.072%\n",
      "Epoch 10, Batch 1008, LR 2.456478 Loss 7.522118, Accuracy 79.071%\n",
      "Epoch 10, Batch 1009, LR 2.456558 Loss 7.522374, Accuracy 79.067%\n",
      "Epoch 10, Batch 1010, LR 2.456638 Loss 7.521731, Accuracy 79.075%\n",
      "Epoch 10, Batch 1011, LR 2.456717 Loss 7.521201, Accuracy 79.079%\n",
      "Epoch 10, Batch 1012, LR 2.456796 Loss 7.521493, Accuracy 79.078%\n",
      "Epoch 10, Batch 1013, LR 2.456876 Loss 7.520618, Accuracy 79.084%\n",
      "Epoch 10, Batch 1014, LR 2.456955 Loss 7.520557, Accuracy 79.086%\n",
      "Epoch 10, Batch 1015, LR 2.457034 Loss 7.520349, Accuracy 79.087%\n",
      "Epoch 10, Batch 1016, LR 2.457113 Loss 7.520284, Accuracy 79.088%\n",
      "Epoch 10, Batch 1017, LR 2.457192 Loss 7.521002, Accuracy 79.084%\n",
      "Epoch 10, Batch 1018, LR 2.457271 Loss 7.520769, Accuracy 79.089%\n",
      "Epoch 10, Batch 1019, LR 2.457350 Loss 7.519811, Accuracy 79.096%\n",
      "Epoch 10, Batch 1020, LR 2.457429 Loss 7.519764, Accuracy 79.097%\n",
      "Epoch 10, Batch 1021, LR 2.457508 Loss 7.518965, Accuracy 79.099%\n",
      "Epoch 10, Batch 1022, LR 2.457587 Loss 7.519047, Accuracy 79.098%\n",
      "Epoch 10, Batch 1023, LR 2.457665 Loss 7.519991, Accuracy 79.094%\n",
      "Epoch 10, Batch 1024, LR 2.457744 Loss 7.519589, Accuracy 79.097%\n",
      "Epoch 10, Batch 1025, LR 2.457822 Loss 7.519244, Accuracy 79.101%\n",
      "Epoch 10, Batch 1026, LR 2.457901 Loss 7.519229, Accuracy 79.101%\n",
      "Epoch 10, Batch 1027, LR 2.457979 Loss 7.518742, Accuracy 79.105%\n",
      "Epoch 10, Batch 1028, LR 2.458057 Loss 7.518048, Accuracy 79.111%\n",
      "Epoch 10, Batch 1029, LR 2.458135 Loss 7.518090, Accuracy 79.111%\n",
      "Epoch 10, Batch 1030, LR 2.458213 Loss 7.518167, Accuracy 79.115%\n",
      "Epoch 10, Batch 1031, LR 2.458292 Loss 7.518394, Accuracy 79.112%\n",
      "Epoch 10, Batch 1032, LR 2.458369 Loss 7.518218, Accuracy 79.112%\n",
      "Epoch 10, Batch 1033, LR 2.458447 Loss 7.518064, Accuracy 79.114%\n",
      "Epoch 10, Batch 1034, LR 2.458525 Loss 7.517920, Accuracy 79.114%\n",
      "Epoch 10, Batch 1035, LR 2.458603 Loss 7.518202, Accuracy 79.114%\n",
      "Epoch 10, Batch 1036, LR 2.458681 Loss 7.518218, Accuracy 79.111%\n",
      "Epoch 10, Batch 1037, LR 2.458758 Loss 7.517848, Accuracy 79.113%\n",
      "Epoch 10, Batch 1038, LR 2.458836 Loss 7.517714, Accuracy 79.118%\n",
      "Epoch 10, Batch 1039, LR 2.458913 Loss 7.517185, Accuracy 79.121%\n",
      "Epoch 10, Batch 1040, LR 2.458991 Loss 7.517534, Accuracy 79.122%\n",
      "Epoch 10, Batch 1041, LR 2.459068 Loss 7.517716, Accuracy 79.116%\n",
      "Epoch 10, Batch 1042, LR 2.459145 Loss 7.517845, Accuracy 79.117%\n",
      "Epoch 10, Batch 1043, LR 2.459222 Loss 7.518237, Accuracy 79.116%\n",
      "Epoch 10, Batch 1044, LR 2.459300 Loss 7.518764, Accuracy 79.112%\n",
      "Epoch 10, Batch 1045, LR 2.459377 Loss 7.518945, Accuracy 79.112%\n",
      "Epoch 10, Batch 1046, LR 2.459454 Loss 7.518995, Accuracy 79.107%\n",
      "Epoch 10, Batch 1047, LR 2.459531 Loss 7.518945, Accuracy 79.104%\n",
      "Epoch 10, Loss (train set) 7.518945, Accuracy (train set) 79.104%\n",
      "Epoch 11, Batch 1, LR 2.459607 Loss 7.537717, Accuracy 79.688%\n",
      "Epoch 11, Batch 2, LR 2.459684 Loss 7.363865, Accuracy 78.516%\n",
      "Epoch 11, Batch 3, LR 2.459761 Loss 7.313739, Accuracy 80.469%\n",
      "Epoch 11, Batch 4, LR 2.459837 Loss 7.443394, Accuracy 78.320%\n",
      "Epoch 11, Batch 5, LR 2.459914 Loss 7.472271, Accuracy 78.594%\n",
      "Epoch 11, Batch 6, LR 2.459990 Loss 7.417276, Accuracy 79.167%\n",
      "Epoch 11, Batch 7, LR 2.460067 Loss 7.419802, Accuracy 79.241%\n",
      "Epoch 11, Batch 8, LR 2.460143 Loss 7.527585, Accuracy 78.906%\n",
      "Epoch 11, Batch 9, LR 2.460219 Loss 7.468197, Accuracy 79.601%\n",
      "Epoch 11, Batch 10, LR 2.460296 Loss 7.448150, Accuracy 79.688%\n",
      "Epoch 11, Batch 11, LR 2.460372 Loss 7.404729, Accuracy 80.043%\n",
      "Epoch 11, Batch 12, LR 2.460448 Loss 7.319656, Accuracy 80.404%\n",
      "Epoch 11, Batch 13, LR 2.460524 Loss 7.338013, Accuracy 80.469%\n",
      "Epoch 11, Batch 14, LR 2.460600 Loss 7.341641, Accuracy 80.525%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 15, LR 2.460675 Loss 7.378583, Accuracy 80.000%\n",
      "Epoch 11, Batch 16, LR 2.460751 Loss 7.365662, Accuracy 80.029%\n",
      "Epoch 11, Batch 17, LR 2.460827 Loss 7.374769, Accuracy 79.871%\n",
      "Epoch 11, Batch 18, LR 2.460902 Loss 7.376226, Accuracy 79.948%\n",
      "Epoch 11, Batch 19, LR 2.460978 Loss 7.346114, Accuracy 80.058%\n",
      "Epoch 11, Batch 20, LR 2.461053 Loss 7.335717, Accuracy 80.234%\n",
      "Epoch 11, Batch 21, LR 2.461129 Loss 7.365412, Accuracy 79.948%\n",
      "Epoch 11, Batch 22, LR 2.461204 Loss 7.343644, Accuracy 79.723%\n",
      "Epoch 11, Batch 23, LR 2.461279 Loss 7.337804, Accuracy 79.721%\n",
      "Epoch 11, Batch 24, LR 2.461355 Loss 7.329991, Accuracy 79.753%\n",
      "Epoch 11, Batch 25, LR 2.461430 Loss 7.313208, Accuracy 79.812%\n",
      "Epoch 11, Batch 26, LR 2.461505 Loss 7.326048, Accuracy 79.597%\n",
      "Epoch 11, Batch 27, LR 2.461580 Loss 7.321092, Accuracy 79.485%\n",
      "Epoch 11, Batch 28, LR 2.461655 Loss 7.316493, Accuracy 79.464%\n",
      "Epoch 11, Batch 29, LR 2.461729 Loss 7.303837, Accuracy 79.634%\n",
      "Epoch 11, Batch 30, LR 2.461804 Loss 7.312644, Accuracy 79.635%\n",
      "Epoch 11, Batch 31, LR 2.461879 Loss 7.297418, Accuracy 79.738%\n",
      "Epoch 11, Batch 32, LR 2.461953 Loss 7.297027, Accuracy 79.736%\n",
      "Epoch 11, Batch 33, LR 2.462028 Loss 7.316821, Accuracy 79.735%\n",
      "Epoch 11, Batch 34, LR 2.462102 Loss 7.311042, Accuracy 79.825%\n",
      "Epoch 11, Batch 35, LR 2.462177 Loss 7.317950, Accuracy 79.754%\n",
      "Epoch 11, Batch 36, LR 2.462251 Loss 7.313697, Accuracy 79.774%\n",
      "Epoch 11, Batch 37, LR 2.462325 Loss 7.298944, Accuracy 79.878%\n",
      "Epoch 11, Batch 38, LR 2.462399 Loss 7.283130, Accuracy 79.955%\n",
      "Epoch 11, Batch 39, LR 2.462474 Loss 7.275981, Accuracy 79.988%\n",
      "Epoch 11, Batch 40, LR 2.462548 Loss 7.272402, Accuracy 79.941%\n",
      "Epoch 11, Batch 41, LR 2.462622 Loss 7.272871, Accuracy 79.992%\n",
      "Epoch 11, Batch 42, LR 2.462695 Loss 7.276889, Accuracy 79.929%\n",
      "Epoch 11, Batch 43, LR 2.462769 Loss 7.284010, Accuracy 79.797%\n",
      "Epoch 11, Batch 44, LR 2.462843 Loss 7.276410, Accuracy 79.918%\n",
      "Epoch 11, Batch 45, LR 2.462917 Loss 7.262537, Accuracy 80.069%\n",
      "Epoch 11, Batch 46, LR 2.462990 Loss 7.248655, Accuracy 80.214%\n",
      "Epoch 11, Batch 47, LR 2.463064 Loss 7.241646, Accuracy 80.336%\n",
      "Epoch 11, Batch 48, LR 2.463137 Loss 7.242962, Accuracy 80.420%\n",
      "Epoch 11, Batch 49, LR 2.463211 Loss 7.253112, Accuracy 80.405%\n",
      "Epoch 11, Batch 50, LR 2.463284 Loss 7.265472, Accuracy 80.312%\n",
      "Epoch 11, Batch 51, LR 2.463357 Loss 7.276455, Accuracy 80.162%\n",
      "Epoch 11, Batch 52, LR 2.463430 Loss 7.280206, Accuracy 80.243%\n",
      "Epoch 11, Batch 53, LR 2.463503 Loss 7.277995, Accuracy 80.262%\n",
      "Epoch 11, Batch 54, LR 2.463576 Loss 7.292093, Accuracy 80.194%\n",
      "Epoch 11, Batch 55, LR 2.463649 Loss 7.301924, Accuracy 80.014%\n",
      "Epoch 11, Batch 56, LR 2.463722 Loss 7.291912, Accuracy 80.106%\n",
      "Epoch 11, Batch 57, LR 2.463795 Loss 7.286832, Accuracy 80.085%\n",
      "Epoch 11, Batch 58, LR 2.463868 Loss 7.285789, Accuracy 80.092%\n",
      "Epoch 11, Batch 59, LR 2.463940 Loss 7.282898, Accuracy 80.177%\n",
      "Epoch 11, Batch 60, LR 2.464013 Loss 7.286367, Accuracy 80.182%\n",
      "Epoch 11, Batch 61, LR 2.464086 Loss 7.276670, Accuracy 80.289%\n",
      "Epoch 11, Batch 62, LR 2.464158 Loss 7.263216, Accuracy 80.343%\n",
      "Epoch 11, Batch 63, LR 2.464230 Loss 7.262755, Accuracy 80.357%\n",
      "Epoch 11, Batch 64, LR 2.464303 Loss 7.255693, Accuracy 80.408%\n",
      "Epoch 11, Batch 65, LR 2.464375 Loss 7.252493, Accuracy 80.445%\n",
      "Epoch 11, Batch 66, LR 2.464447 Loss 7.257016, Accuracy 80.457%\n",
      "Epoch 11, Batch 67, LR 2.464519 Loss 7.258565, Accuracy 80.422%\n",
      "Epoch 11, Batch 68, LR 2.464591 Loss 7.249658, Accuracy 80.434%\n",
      "Epoch 11, Batch 69, LR 2.464663 Loss 7.255870, Accuracy 80.367%\n",
      "Epoch 11, Batch 70, LR 2.464735 Loss 7.265328, Accuracy 80.290%\n",
      "Epoch 11, Batch 71, LR 2.464807 Loss 7.253903, Accuracy 80.326%\n",
      "Epoch 11, Batch 72, LR 2.464878 Loss 7.239722, Accuracy 80.414%\n",
      "Epoch 11, Batch 73, LR 2.464950 Loss 7.242559, Accuracy 80.437%\n",
      "Epoch 11, Batch 74, LR 2.465022 Loss 7.246043, Accuracy 80.437%\n",
      "Epoch 11, Batch 75, LR 2.465093 Loss 7.255835, Accuracy 80.396%\n",
      "Epoch 11, Batch 76, LR 2.465165 Loss 7.267346, Accuracy 80.345%\n",
      "Epoch 11, Batch 77, LR 2.465236 Loss 7.280279, Accuracy 80.235%\n",
      "Epoch 11, Batch 78, LR 2.465307 Loss 7.281746, Accuracy 80.238%\n",
      "Epoch 11, Batch 79, LR 2.465379 Loss 7.281686, Accuracy 80.291%\n",
      "Epoch 11, Batch 80, LR 2.465450 Loss 7.285436, Accuracy 80.293%\n",
      "Epoch 11, Batch 81, LR 2.465521 Loss 7.281643, Accuracy 80.295%\n",
      "Epoch 11, Batch 82, LR 2.465592 Loss 7.284416, Accuracy 80.259%\n",
      "Epoch 11, Batch 83, LR 2.465663 Loss 7.285982, Accuracy 80.252%\n",
      "Epoch 11, Batch 84, LR 2.465734 Loss 7.280435, Accuracy 80.301%\n",
      "Epoch 11, Batch 85, LR 2.465804 Loss 7.273519, Accuracy 80.331%\n",
      "Epoch 11, Batch 86, LR 2.465875 Loss 7.280950, Accuracy 80.242%\n",
      "Epoch 11, Batch 87, LR 2.465946 Loss 7.284125, Accuracy 80.262%\n",
      "Epoch 11, Batch 88, LR 2.466016 Loss 7.281899, Accuracy 80.300%\n",
      "Epoch 11, Batch 89, LR 2.466087 Loss 7.287813, Accuracy 80.258%\n",
      "Epoch 11, Batch 90, LR 2.466157 Loss 7.286530, Accuracy 80.295%\n",
      "Epoch 11, Batch 91, LR 2.466228 Loss 7.284622, Accuracy 80.263%\n",
      "Epoch 11, Batch 92, LR 2.466298 Loss 7.284804, Accuracy 80.239%\n",
      "Epoch 11, Batch 93, LR 2.466368 Loss 7.278706, Accuracy 80.242%\n",
      "Epoch 11, Batch 94, LR 2.466438 Loss 7.285317, Accuracy 80.211%\n",
      "Epoch 11, Batch 95, LR 2.466508 Loss 7.293868, Accuracy 80.230%\n",
      "Epoch 11, Batch 96, LR 2.466578 Loss 7.291549, Accuracy 80.257%\n",
      "Epoch 11, Batch 97, LR 2.466648 Loss 7.294173, Accuracy 80.227%\n",
      "Epoch 11, Batch 98, LR 2.466718 Loss 7.295782, Accuracy 80.206%\n",
      "Epoch 11, Batch 99, LR 2.466788 Loss 7.296646, Accuracy 80.216%\n",
      "Epoch 11, Batch 100, LR 2.466857 Loss 7.299033, Accuracy 80.219%\n",
      "Epoch 11, Batch 101, LR 2.466927 Loss 7.301341, Accuracy 80.221%\n",
      "Epoch 11, Batch 102, LR 2.466997 Loss 7.303686, Accuracy 80.208%\n",
      "Epoch 11, Batch 103, LR 2.467066 Loss 7.306182, Accuracy 80.218%\n",
      "Epoch 11, Batch 104, LR 2.467136 Loss 7.301553, Accuracy 80.236%\n",
      "Epoch 11, Batch 105, LR 2.467205 Loss 7.302433, Accuracy 80.231%\n",
      "Epoch 11, Batch 106, LR 2.467274 Loss 7.304938, Accuracy 80.240%\n",
      "Epoch 11, Batch 107, LR 2.467343 Loss 7.302310, Accuracy 80.264%\n",
      "Epoch 11, Batch 108, LR 2.467412 Loss 7.306713, Accuracy 80.252%\n",
      "Epoch 11, Batch 109, LR 2.467482 Loss 7.305694, Accuracy 80.239%\n",
      "Epoch 11, Batch 110, LR 2.467551 Loss 7.306243, Accuracy 80.220%\n",
      "Epoch 11, Batch 111, LR 2.467619 Loss 7.305254, Accuracy 80.208%\n",
      "Epoch 11, Batch 112, LR 2.467688 Loss 7.298075, Accuracy 80.218%\n",
      "Epoch 11, Batch 113, LR 2.467757 Loss 7.301684, Accuracy 80.199%\n",
      "Epoch 11, Batch 114, LR 2.467826 Loss 7.304816, Accuracy 80.167%\n",
      "Epoch 11, Batch 115, LR 2.467894 Loss 7.310415, Accuracy 80.143%\n",
      "Epoch 11, Batch 116, LR 2.467963 Loss 7.312272, Accuracy 80.145%\n",
      "Epoch 11, Batch 117, LR 2.468031 Loss 7.313497, Accuracy 80.135%\n",
      "Epoch 11, Batch 118, LR 2.468100 Loss 7.314353, Accuracy 80.171%\n",
      "Epoch 11, Batch 119, LR 2.468168 Loss 7.314033, Accuracy 80.186%\n",
      "Epoch 11, Batch 120, LR 2.468236 Loss 7.308248, Accuracy 80.247%\n",
      "Epoch 11, Batch 121, LR 2.468305 Loss 7.306640, Accuracy 80.269%\n",
      "Epoch 11, Batch 122, LR 2.468373 Loss 7.299467, Accuracy 80.302%\n",
      "Epoch 11, Batch 123, LR 2.468441 Loss 7.299369, Accuracy 80.278%\n",
      "Epoch 11, Batch 124, LR 2.468509 Loss 7.297306, Accuracy 80.292%\n",
      "Epoch 11, Batch 125, LR 2.468577 Loss 7.295812, Accuracy 80.306%\n",
      "Epoch 11, Batch 126, LR 2.468644 Loss 7.293223, Accuracy 80.339%\n",
      "Epoch 11, Batch 127, LR 2.468712 Loss 7.294655, Accuracy 80.358%\n",
      "Epoch 11, Batch 128, LR 2.468780 Loss 7.297486, Accuracy 80.322%\n",
      "Epoch 11, Batch 129, LR 2.468847 Loss 7.292519, Accuracy 80.366%\n",
      "Epoch 11, Batch 130, LR 2.468915 Loss 7.293583, Accuracy 80.355%\n",
      "Epoch 11, Batch 131, LR 2.468982 Loss 7.296601, Accuracy 80.320%\n",
      "Epoch 11, Batch 132, LR 2.469050 Loss 7.300001, Accuracy 80.268%\n",
      "Epoch 11, Batch 133, LR 2.469117 Loss 7.299886, Accuracy 80.263%\n",
      "Epoch 11, Batch 134, LR 2.469184 Loss 7.300200, Accuracy 80.271%\n",
      "Epoch 11, Batch 135, LR 2.469252 Loss 7.301732, Accuracy 80.272%\n",
      "Epoch 11, Batch 136, LR 2.469319 Loss 7.297783, Accuracy 80.302%\n",
      "Epoch 11, Batch 137, LR 2.469386 Loss 7.294403, Accuracy 80.320%\n",
      "Epoch 11, Batch 138, LR 2.469453 Loss 7.288739, Accuracy 80.367%\n",
      "Epoch 11, Batch 139, LR 2.469520 Loss 7.286692, Accuracy 80.379%\n",
      "Epoch 11, Batch 140, LR 2.469586 Loss 7.284811, Accuracy 80.419%\n",
      "Epoch 11, Batch 141, LR 2.469653 Loss 7.283672, Accuracy 80.447%\n",
      "Epoch 11, Batch 142, LR 2.469720 Loss 7.290137, Accuracy 80.414%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 143, LR 2.469786 Loss 7.294724, Accuracy 80.387%\n",
      "Epoch 11, Batch 144, LR 2.469853 Loss 7.295711, Accuracy 80.366%\n",
      "Epoch 11, Batch 145, LR 2.469919 Loss 7.297595, Accuracy 80.356%\n",
      "Epoch 11, Batch 146, LR 2.469986 Loss 7.301984, Accuracy 80.314%\n",
      "Epoch 11, Batch 147, LR 2.470052 Loss 7.301223, Accuracy 80.309%\n",
      "Epoch 11, Batch 148, LR 2.470118 Loss 7.301962, Accuracy 80.316%\n",
      "Epoch 11, Batch 149, LR 2.470184 Loss 7.303647, Accuracy 80.306%\n",
      "Epoch 11, Batch 150, LR 2.470250 Loss 7.307546, Accuracy 80.276%\n",
      "Epoch 11, Batch 151, LR 2.470317 Loss 7.303424, Accuracy 80.303%\n",
      "Epoch 11, Batch 152, LR 2.470382 Loss 7.302362, Accuracy 80.279%\n",
      "Epoch 11, Batch 153, LR 2.470448 Loss 7.308352, Accuracy 80.244%\n",
      "Epoch 11, Batch 154, LR 2.470514 Loss 7.316405, Accuracy 80.180%\n",
      "Epoch 11, Batch 155, LR 2.470580 Loss 7.315815, Accuracy 80.181%\n",
      "Epoch 11, Batch 156, LR 2.470646 Loss 7.314233, Accuracy 80.178%\n",
      "Epoch 11, Batch 157, LR 2.470711 Loss 7.309866, Accuracy 80.220%\n",
      "Epoch 11, Batch 158, LR 2.470777 Loss 7.311553, Accuracy 80.197%\n",
      "Epoch 11, Batch 159, LR 2.470842 Loss 7.313646, Accuracy 80.199%\n",
      "Epoch 11, Batch 160, LR 2.470907 Loss 7.314506, Accuracy 80.205%\n",
      "Epoch 11, Batch 161, LR 2.470973 Loss 7.319156, Accuracy 80.178%\n",
      "Epoch 11, Batch 162, LR 2.471038 Loss 7.317886, Accuracy 80.179%\n",
      "Epoch 11, Batch 163, LR 2.471103 Loss 7.314897, Accuracy 80.224%\n",
      "Epoch 11, Batch 164, LR 2.471168 Loss 7.316586, Accuracy 80.197%\n",
      "Epoch 11, Batch 165, LR 2.471233 Loss 7.315149, Accuracy 80.213%\n",
      "Epoch 11, Batch 166, LR 2.471298 Loss 7.313390, Accuracy 80.238%\n",
      "Epoch 11, Batch 167, LR 2.471363 Loss 7.312784, Accuracy 80.221%\n",
      "Epoch 11, Batch 168, LR 2.471428 Loss 7.307917, Accuracy 80.241%\n",
      "Epoch 11, Batch 169, LR 2.471492 Loss 7.304626, Accuracy 80.256%\n",
      "Epoch 11, Batch 170, LR 2.471557 Loss 7.299763, Accuracy 80.294%\n",
      "Epoch 11, Batch 171, LR 2.471622 Loss 7.303037, Accuracy 80.272%\n",
      "Epoch 11, Batch 172, LR 2.471686 Loss 7.306576, Accuracy 80.242%\n",
      "Epoch 11, Batch 173, LR 2.471751 Loss 7.307662, Accuracy 80.243%\n",
      "Epoch 11, Batch 174, LR 2.471815 Loss 7.314349, Accuracy 80.195%\n",
      "Epoch 11, Batch 175, LR 2.471879 Loss 7.313854, Accuracy 80.196%\n",
      "Epoch 11, Batch 176, LR 2.471943 Loss 7.315937, Accuracy 80.198%\n",
      "Epoch 11, Batch 177, LR 2.472008 Loss 7.316061, Accuracy 80.204%\n",
      "Epoch 11, Batch 178, LR 2.472072 Loss 7.311533, Accuracy 80.236%\n",
      "Epoch 11, Batch 179, LR 2.472136 Loss 7.312602, Accuracy 80.216%\n",
      "Epoch 11, Batch 180, LR 2.472199 Loss 7.307160, Accuracy 80.243%\n",
      "Epoch 11, Batch 181, LR 2.472263 Loss 7.301067, Accuracy 80.279%\n",
      "Epoch 11, Batch 182, LR 2.472327 Loss 7.301058, Accuracy 80.263%\n",
      "Epoch 11, Batch 183, LR 2.472391 Loss 7.301402, Accuracy 80.247%\n",
      "Epoch 11, Batch 184, LR 2.472454 Loss 7.299863, Accuracy 80.269%\n",
      "Epoch 11, Batch 185, LR 2.472518 Loss 7.297028, Accuracy 80.291%\n",
      "Epoch 11, Batch 186, LR 2.472581 Loss 7.295457, Accuracy 80.288%\n",
      "Epoch 11, Batch 187, LR 2.472645 Loss 7.293772, Accuracy 80.293%\n",
      "Epoch 11, Batch 188, LR 2.472708 Loss 7.290445, Accuracy 80.294%\n",
      "Epoch 11, Batch 189, LR 2.472771 Loss 7.290376, Accuracy 80.308%\n",
      "Epoch 11, Batch 190, LR 2.472835 Loss 7.292808, Accuracy 80.296%\n",
      "Epoch 11, Batch 191, LR 2.472898 Loss 7.290163, Accuracy 80.334%\n",
      "Epoch 11, Batch 192, LR 2.472961 Loss 7.286984, Accuracy 80.363%\n",
      "Epoch 11, Batch 193, LR 2.473024 Loss 7.291059, Accuracy 80.351%\n",
      "Epoch 11, Batch 194, LR 2.473087 Loss 7.292826, Accuracy 80.316%\n",
      "Epoch 11, Batch 195, LR 2.473150 Loss 7.294573, Accuracy 80.325%\n",
      "Epoch 11, Batch 196, LR 2.473212 Loss 7.293366, Accuracy 80.333%\n",
      "Epoch 11, Batch 197, LR 2.473275 Loss 7.289828, Accuracy 80.354%\n",
      "Epoch 11, Batch 198, LR 2.473338 Loss 7.289975, Accuracy 80.342%\n",
      "Epoch 11, Batch 199, LR 2.473400 Loss 7.289976, Accuracy 80.343%\n",
      "Epoch 11, Batch 200, LR 2.473463 Loss 7.288848, Accuracy 80.344%\n",
      "Epoch 11, Batch 201, LR 2.473525 Loss 7.288732, Accuracy 80.325%\n",
      "Epoch 11, Batch 202, LR 2.473587 Loss 7.285985, Accuracy 80.349%\n",
      "Epoch 11, Batch 203, LR 2.473650 Loss 7.286498, Accuracy 80.373%\n",
      "Epoch 11, Batch 204, LR 2.473712 Loss 7.285150, Accuracy 80.388%\n",
      "Epoch 11, Batch 205, LR 2.473774 Loss 7.286534, Accuracy 80.389%\n",
      "Epoch 11, Batch 206, LR 2.473836 Loss 7.283069, Accuracy 80.400%\n",
      "Epoch 11, Batch 207, LR 2.473898 Loss 7.284959, Accuracy 80.382%\n",
      "Epoch 11, Batch 208, LR 2.473960 Loss 7.285464, Accuracy 80.390%\n",
      "Epoch 11, Batch 209, LR 2.474021 Loss 7.286165, Accuracy 80.379%\n",
      "Epoch 11, Batch 210, LR 2.474083 Loss 7.286430, Accuracy 80.379%\n",
      "Epoch 11, Batch 211, LR 2.474145 Loss 7.282153, Accuracy 80.424%\n",
      "Epoch 11, Batch 212, LR 2.474206 Loss 7.277627, Accuracy 80.447%\n",
      "Epoch 11, Batch 213, LR 2.474268 Loss 7.277127, Accuracy 80.454%\n",
      "Epoch 11, Batch 214, LR 2.474329 Loss 7.276178, Accuracy 80.461%\n",
      "Epoch 11, Batch 215, LR 2.474391 Loss 7.277211, Accuracy 80.472%\n",
      "Epoch 11, Batch 216, LR 2.474452 Loss 7.273315, Accuracy 80.490%\n",
      "Epoch 11, Batch 217, LR 2.474513 Loss 7.276160, Accuracy 80.462%\n",
      "Epoch 11, Batch 218, LR 2.474574 Loss 7.277969, Accuracy 80.454%\n",
      "Epoch 11, Batch 219, LR 2.474636 Loss 7.278507, Accuracy 80.451%\n",
      "Epoch 11, Batch 220, LR 2.474697 Loss 7.280128, Accuracy 80.430%\n",
      "Epoch 11, Batch 221, LR 2.474758 Loss 7.279605, Accuracy 80.426%\n",
      "Epoch 11, Batch 222, LR 2.474818 Loss 7.278756, Accuracy 80.455%\n",
      "Epoch 11, Batch 223, LR 2.474879 Loss 7.275072, Accuracy 80.472%\n",
      "Epoch 11, Batch 224, LR 2.474940 Loss 7.275233, Accuracy 80.462%\n",
      "Epoch 11, Batch 225, LR 2.475001 Loss 7.275862, Accuracy 80.455%\n",
      "Epoch 11, Batch 226, LR 2.475061 Loss 7.275941, Accuracy 80.455%\n",
      "Epoch 11, Batch 227, LR 2.475122 Loss 7.275399, Accuracy 80.448%\n",
      "Epoch 11, Batch 228, LR 2.475182 Loss 7.277753, Accuracy 80.441%\n",
      "Epoch 11, Batch 229, LR 2.475242 Loss 7.276876, Accuracy 80.445%\n",
      "Epoch 11, Batch 230, LR 2.475303 Loss 7.274316, Accuracy 80.462%\n",
      "Epoch 11, Batch 231, LR 2.475363 Loss 7.274416, Accuracy 80.479%\n",
      "Epoch 11, Batch 232, LR 2.475423 Loss 7.274914, Accuracy 80.475%\n",
      "Epoch 11, Batch 233, LR 2.475483 Loss 7.274122, Accuracy 80.499%\n",
      "Epoch 11, Batch 234, LR 2.475543 Loss 7.272542, Accuracy 80.495%\n",
      "Epoch 11, Batch 235, LR 2.475603 Loss 7.272379, Accuracy 80.492%\n",
      "Epoch 11, Batch 236, LR 2.475663 Loss 7.274760, Accuracy 80.475%\n",
      "Epoch 11, Batch 237, LR 2.475723 Loss 7.274249, Accuracy 80.475%\n",
      "Epoch 11, Batch 238, LR 2.475782 Loss 7.270176, Accuracy 80.482%\n",
      "Epoch 11, Batch 239, LR 2.475842 Loss 7.268457, Accuracy 80.469%\n",
      "Epoch 11, Batch 240, LR 2.475902 Loss 7.265500, Accuracy 80.495%\n",
      "Epoch 11, Batch 241, LR 2.475961 Loss 7.264072, Accuracy 80.498%\n",
      "Epoch 11, Batch 242, LR 2.476020 Loss 7.263663, Accuracy 80.511%\n",
      "Epoch 11, Batch 243, LR 2.476080 Loss 7.262869, Accuracy 80.523%\n",
      "Epoch 11, Batch 244, LR 2.476139 Loss 7.264186, Accuracy 80.514%\n",
      "Epoch 11, Batch 245, LR 2.476198 Loss 7.264753, Accuracy 80.501%\n",
      "Epoch 11, Batch 246, LR 2.476257 Loss 7.263625, Accuracy 80.510%\n",
      "Epoch 11, Batch 247, LR 2.476316 Loss 7.263067, Accuracy 80.513%\n",
      "Epoch 11, Batch 248, LR 2.476375 Loss 7.263730, Accuracy 80.503%\n",
      "Epoch 11, Batch 249, LR 2.476434 Loss 7.260153, Accuracy 80.532%\n",
      "Epoch 11, Batch 250, LR 2.476493 Loss 7.259897, Accuracy 80.513%\n",
      "Epoch 11, Batch 251, LR 2.476552 Loss 7.256861, Accuracy 80.515%\n",
      "Epoch 11, Batch 252, LR 2.476611 Loss 7.257058, Accuracy 80.506%\n",
      "Epoch 11, Batch 253, LR 2.476669 Loss 7.256917, Accuracy 80.506%\n",
      "Epoch 11, Batch 254, LR 2.476728 Loss 7.253596, Accuracy 80.515%\n",
      "Epoch 11, Batch 255, LR 2.476786 Loss 7.251596, Accuracy 80.542%\n",
      "Epoch 11, Batch 256, LR 2.476845 Loss 7.250202, Accuracy 80.551%\n",
      "Epoch 11, Batch 257, LR 2.476903 Loss 7.251281, Accuracy 80.539%\n",
      "Epoch 11, Batch 258, LR 2.476961 Loss 7.251258, Accuracy 80.532%\n",
      "Epoch 11, Batch 259, LR 2.477019 Loss 7.251182, Accuracy 80.535%\n",
      "Epoch 11, Batch 260, LR 2.477077 Loss 7.251578, Accuracy 80.535%\n",
      "Epoch 11, Batch 261, LR 2.477135 Loss 7.254456, Accuracy 80.523%\n",
      "Epoch 11, Batch 262, LR 2.477193 Loss 7.253176, Accuracy 80.540%\n",
      "Epoch 11, Batch 263, LR 2.477251 Loss 7.253303, Accuracy 80.537%\n",
      "Epoch 11, Batch 264, LR 2.477309 Loss 7.250794, Accuracy 80.552%\n",
      "Epoch 11, Batch 265, LR 2.477367 Loss 7.251170, Accuracy 80.557%\n",
      "Epoch 11, Batch 266, LR 2.477424 Loss 7.249880, Accuracy 80.563%\n",
      "Epoch 11, Batch 267, LR 2.477482 Loss 7.252107, Accuracy 80.554%\n",
      "Epoch 11, Batch 268, LR 2.477540 Loss 7.252110, Accuracy 80.565%\n",
      "Epoch 11, Batch 269, LR 2.477597 Loss 7.252309, Accuracy 80.547%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 270, LR 2.477654 Loss 7.252605, Accuracy 80.567%\n",
      "Epoch 11, Batch 271, LR 2.477712 Loss 7.253997, Accuracy 80.544%\n",
      "Epoch 11, Batch 272, LR 2.477769 Loss 7.252948, Accuracy 80.552%\n",
      "Epoch 11, Batch 273, LR 2.477826 Loss 7.251010, Accuracy 80.546%\n",
      "Epoch 11, Batch 274, LR 2.477883 Loss 7.251091, Accuracy 80.543%\n",
      "Epoch 11, Batch 275, LR 2.477940 Loss 7.250077, Accuracy 80.554%\n",
      "Epoch 11, Batch 276, LR 2.477997 Loss 7.250476, Accuracy 80.545%\n",
      "Epoch 11, Batch 277, LR 2.478054 Loss 7.248275, Accuracy 80.579%\n",
      "Epoch 11, Batch 278, LR 2.478111 Loss 7.248254, Accuracy 80.578%\n",
      "Epoch 11, Batch 279, LR 2.478167 Loss 7.248254, Accuracy 80.578%\n",
      "Epoch 11, Batch 280, LR 2.478224 Loss 7.247197, Accuracy 80.594%\n",
      "Epoch 11, Batch 281, LR 2.478281 Loss 7.247056, Accuracy 80.599%\n",
      "Epoch 11, Batch 282, LR 2.478337 Loss 7.246987, Accuracy 80.580%\n",
      "Epoch 11, Batch 283, LR 2.478394 Loss 7.247759, Accuracy 80.568%\n",
      "Epoch 11, Batch 284, LR 2.478450 Loss 7.246334, Accuracy 80.573%\n",
      "Epoch 11, Batch 285, LR 2.478506 Loss 7.246287, Accuracy 80.576%\n",
      "Epoch 11, Batch 286, LR 2.478562 Loss 7.245305, Accuracy 80.575%\n",
      "Epoch 11, Batch 287, LR 2.478619 Loss 7.245179, Accuracy 80.578%\n",
      "Epoch 11, Batch 288, LR 2.478675 Loss 7.243488, Accuracy 80.591%\n",
      "Epoch 11, Batch 289, LR 2.478731 Loss 7.244158, Accuracy 80.580%\n",
      "Epoch 11, Batch 290, LR 2.478787 Loss 7.245416, Accuracy 80.582%\n",
      "Epoch 11, Batch 291, LR 2.478842 Loss 7.246922, Accuracy 80.573%\n",
      "Epoch 11, Batch 292, LR 2.478898 Loss 7.248766, Accuracy 80.554%\n",
      "Epoch 11, Batch 293, LR 2.478954 Loss 7.248613, Accuracy 80.554%\n",
      "Epoch 11, Batch 294, LR 2.479009 Loss 7.247059, Accuracy 80.556%\n",
      "Epoch 11, Batch 295, LR 2.479065 Loss 7.249164, Accuracy 80.556%\n",
      "Epoch 11, Batch 296, LR 2.479121 Loss 7.250240, Accuracy 80.548%\n",
      "Epoch 11, Batch 297, LR 2.479176 Loss 7.247527, Accuracy 80.571%\n",
      "Epoch 11, Batch 298, LR 2.479231 Loss 7.246033, Accuracy 80.581%\n",
      "Epoch 11, Batch 299, LR 2.479287 Loss 7.246693, Accuracy 80.589%\n",
      "Epoch 11, Batch 300, LR 2.479342 Loss 7.245539, Accuracy 80.591%\n",
      "Epoch 11, Batch 301, LR 2.479397 Loss 7.249481, Accuracy 80.567%\n",
      "Epoch 11, Batch 302, LR 2.479452 Loss 7.252249, Accuracy 80.549%\n",
      "Epoch 11, Batch 303, LR 2.479507 Loss 7.250090, Accuracy 80.554%\n",
      "Epoch 11, Batch 304, LR 2.479562 Loss 7.252046, Accuracy 80.536%\n",
      "Epoch 11, Batch 305, LR 2.479617 Loss 7.250905, Accuracy 80.540%\n",
      "Epoch 11, Batch 306, LR 2.479671 Loss 7.250731, Accuracy 80.533%\n",
      "Epoch 11, Batch 307, LR 2.479726 Loss 7.248976, Accuracy 80.553%\n",
      "Epoch 11, Batch 308, LR 2.479781 Loss 7.247976, Accuracy 80.558%\n",
      "Epoch 11, Batch 309, LR 2.479835 Loss 7.248800, Accuracy 80.540%\n",
      "Epoch 11, Batch 310, LR 2.479890 Loss 7.248919, Accuracy 80.539%\n",
      "Epoch 11, Batch 311, LR 2.479944 Loss 7.247807, Accuracy 80.544%\n",
      "Epoch 11, Batch 312, LR 2.479998 Loss 7.246478, Accuracy 80.544%\n",
      "Epoch 11, Batch 313, LR 2.480052 Loss 7.246813, Accuracy 80.549%\n",
      "Epoch 11, Batch 314, LR 2.480107 Loss 7.246208, Accuracy 80.563%\n",
      "Epoch 11, Batch 315, LR 2.480161 Loss 7.249889, Accuracy 80.543%\n",
      "Epoch 11, Batch 316, LR 2.480215 Loss 7.248019, Accuracy 80.555%\n",
      "Epoch 11, Batch 317, LR 2.480269 Loss 7.248706, Accuracy 80.555%\n",
      "Epoch 11, Batch 318, LR 2.480323 Loss 7.246769, Accuracy 80.567%\n",
      "Epoch 11, Batch 319, LR 2.480376 Loss 7.248211, Accuracy 80.554%\n",
      "Epoch 11, Batch 320, LR 2.480430 Loss 7.249420, Accuracy 80.554%\n",
      "Epoch 11, Batch 321, LR 2.480484 Loss 7.248134, Accuracy 80.556%\n",
      "Epoch 11, Batch 322, LR 2.480537 Loss 7.248851, Accuracy 80.561%\n",
      "Epoch 11, Batch 323, LR 2.480591 Loss 7.249553, Accuracy 80.558%\n",
      "Epoch 11, Batch 324, LR 2.480644 Loss 7.248661, Accuracy 80.558%\n",
      "Epoch 11, Batch 325, LR 2.480698 Loss 7.249074, Accuracy 80.548%\n",
      "Epoch 11, Batch 326, LR 2.480751 Loss 7.248098, Accuracy 80.550%\n",
      "Epoch 11, Batch 327, LR 2.480804 Loss 7.248367, Accuracy 80.552%\n",
      "Epoch 11, Batch 328, LR 2.480857 Loss 7.247609, Accuracy 80.554%\n",
      "Epoch 11, Batch 329, LR 2.480910 Loss 7.247794, Accuracy 80.557%\n",
      "Epoch 11, Batch 330, LR 2.480963 Loss 7.248148, Accuracy 80.559%\n",
      "Epoch 11, Batch 331, LR 2.481016 Loss 7.248508, Accuracy 80.551%\n",
      "Epoch 11, Batch 332, LR 2.481069 Loss 7.247763, Accuracy 80.575%\n",
      "Epoch 11, Batch 333, LR 2.481122 Loss 7.247131, Accuracy 80.586%\n",
      "Epoch 11, Batch 334, LR 2.481175 Loss 7.248605, Accuracy 80.583%\n",
      "Epoch 11, Batch 335, LR 2.481227 Loss 7.250604, Accuracy 80.574%\n",
      "Epoch 11, Batch 336, LR 2.481280 Loss 7.249928, Accuracy 80.573%\n",
      "Epoch 11, Batch 337, LR 2.481332 Loss 7.249472, Accuracy 80.589%\n",
      "Epoch 11, Batch 338, LR 2.481385 Loss 7.249964, Accuracy 80.587%\n",
      "Epoch 11, Batch 339, LR 2.481437 Loss 7.248768, Accuracy 80.596%\n",
      "Epoch 11, Batch 340, LR 2.481489 Loss 7.248565, Accuracy 80.602%\n",
      "Epoch 11, Batch 341, LR 2.481541 Loss 7.250414, Accuracy 80.592%\n",
      "Epoch 11, Batch 342, LR 2.481594 Loss 7.251107, Accuracy 80.576%\n",
      "Epoch 11, Batch 343, LR 2.481646 Loss 7.251218, Accuracy 80.569%\n",
      "Epoch 11, Batch 344, LR 2.481698 Loss 7.252317, Accuracy 80.571%\n",
      "Epoch 11, Batch 345, LR 2.481749 Loss 7.254378, Accuracy 80.559%\n",
      "Epoch 11, Batch 346, LR 2.481801 Loss 7.255870, Accuracy 80.555%\n",
      "Epoch 11, Batch 347, LR 2.481853 Loss 7.256462, Accuracy 80.550%\n",
      "Epoch 11, Batch 348, LR 2.481905 Loss 7.256353, Accuracy 80.547%\n",
      "Epoch 11, Batch 349, LR 2.481956 Loss 7.258230, Accuracy 80.534%\n",
      "Epoch 11, Batch 350, LR 2.482008 Loss 7.257851, Accuracy 80.538%\n",
      "Epoch 11, Batch 351, LR 2.482059 Loss 7.258242, Accuracy 80.524%\n",
      "Epoch 11, Batch 352, LR 2.482111 Loss 7.258146, Accuracy 80.518%\n",
      "Epoch 11, Batch 353, LR 2.482162 Loss 7.257683, Accuracy 80.522%\n",
      "Epoch 11, Batch 354, LR 2.482213 Loss 7.260143, Accuracy 80.520%\n",
      "Epoch 11, Batch 355, LR 2.482264 Loss 7.261267, Accuracy 80.528%\n",
      "Epoch 11, Batch 356, LR 2.482316 Loss 7.261937, Accuracy 80.530%\n",
      "Epoch 11, Batch 357, LR 2.482367 Loss 7.263389, Accuracy 80.513%\n",
      "Epoch 11, Batch 358, LR 2.482418 Loss 7.264147, Accuracy 80.519%\n",
      "Epoch 11, Batch 359, LR 2.482468 Loss 7.262785, Accuracy 80.523%\n",
      "Epoch 11, Batch 360, LR 2.482519 Loss 7.262176, Accuracy 80.516%\n",
      "Epoch 11, Batch 361, LR 2.482570 Loss 7.265920, Accuracy 80.484%\n",
      "Epoch 11, Batch 362, LR 2.482621 Loss 7.265634, Accuracy 80.488%\n",
      "Epoch 11, Batch 363, LR 2.482671 Loss 7.267968, Accuracy 80.473%\n",
      "Epoch 11, Batch 364, LR 2.482722 Loss 7.266762, Accuracy 80.482%\n",
      "Epoch 11, Batch 365, LR 2.482772 Loss 7.266584, Accuracy 80.486%\n",
      "Epoch 11, Batch 366, LR 2.482822 Loss 7.267908, Accuracy 80.479%\n",
      "Epoch 11, Batch 367, LR 2.482873 Loss 7.268546, Accuracy 80.475%\n",
      "Epoch 11, Batch 368, LR 2.482923 Loss 7.268437, Accuracy 80.477%\n",
      "Epoch 11, Batch 369, LR 2.482973 Loss 7.267174, Accuracy 80.475%\n",
      "Epoch 11, Batch 370, LR 2.483023 Loss 7.267598, Accuracy 80.473%\n",
      "Epoch 11, Batch 371, LR 2.483073 Loss 7.267614, Accuracy 80.469%\n",
      "Epoch 11, Batch 372, LR 2.483123 Loss 7.267608, Accuracy 80.486%\n",
      "Epoch 11, Batch 373, LR 2.483173 Loss 7.267628, Accuracy 80.481%\n",
      "Epoch 11, Batch 374, LR 2.483223 Loss 7.266017, Accuracy 80.481%\n",
      "Epoch 11, Batch 375, LR 2.483272 Loss 7.267691, Accuracy 80.477%\n",
      "Epoch 11, Batch 376, LR 2.483322 Loss 7.266923, Accuracy 80.479%\n",
      "Epoch 11, Batch 377, LR 2.483372 Loss 7.268237, Accuracy 80.473%\n",
      "Epoch 11, Batch 378, LR 2.483421 Loss 7.268242, Accuracy 80.477%\n",
      "Epoch 11, Batch 379, LR 2.483471 Loss 7.268470, Accuracy 80.487%\n",
      "Epoch 11, Batch 380, LR 2.483520 Loss 7.270561, Accuracy 80.479%\n",
      "Epoch 11, Batch 381, LR 2.483569 Loss 7.270672, Accuracy 80.479%\n",
      "Epoch 11, Batch 382, LR 2.483618 Loss 7.271397, Accuracy 80.475%\n",
      "Epoch 11, Batch 383, LR 2.483667 Loss 7.272202, Accuracy 80.475%\n",
      "Epoch 11, Batch 384, LR 2.483716 Loss 7.270193, Accuracy 80.493%\n",
      "Epoch 11, Batch 385, LR 2.483765 Loss 7.270623, Accuracy 80.481%\n",
      "Epoch 11, Batch 386, LR 2.483814 Loss 7.269440, Accuracy 80.485%\n",
      "Epoch 11, Batch 387, LR 2.483863 Loss 7.269390, Accuracy 80.483%\n",
      "Epoch 11, Batch 388, LR 2.483912 Loss 7.268881, Accuracy 80.489%\n",
      "Epoch 11, Batch 389, LR 2.483961 Loss 7.267375, Accuracy 80.513%\n",
      "Epoch 11, Batch 390, LR 2.484009 Loss 7.267197, Accuracy 80.515%\n",
      "Epoch 11, Batch 391, LR 2.484058 Loss 7.268189, Accuracy 80.503%\n",
      "Epoch 11, Batch 392, LR 2.484106 Loss 7.269415, Accuracy 80.481%\n",
      "Epoch 11, Batch 393, LR 2.484155 Loss 7.267648, Accuracy 80.495%\n",
      "Epoch 11, Batch 394, LR 2.484203 Loss 7.266942, Accuracy 80.502%\n",
      "Epoch 11, Batch 395, LR 2.484251 Loss 7.266052, Accuracy 80.504%\n",
      "Epoch 11, Batch 396, LR 2.484299 Loss 7.265451, Accuracy 80.508%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 397, LR 2.484347 Loss 7.265882, Accuracy 80.502%\n",
      "Epoch 11, Batch 398, LR 2.484395 Loss 7.267230, Accuracy 80.482%\n",
      "Epoch 11, Batch 399, LR 2.484443 Loss 7.265618, Accuracy 80.496%\n",
      "Epoch 11, Batch 400, LR 2.484491 Loss 7.265639, Accuracy 80.492%\n",
      "Epoch 11, Batch 401, LR 2.484539 Loss 7.266281, Accuracy 80.480%\n",
      "Epoch 11, Batch 402, LR 2.484587 Loss 7.266620, Accuracy 80.475%\n",
      "Epoch 11, Batch 403, LR 2.484634 Loss 7.268548, Accuracy 80.471%\n",
      "Epoch 11, Batch 404, LR 2.484682 Loss 7.268714, Accuracy 80.467%\n",
      "Epoch 11, Batch 405, LR 2.484729 Loss 7.269214, Accuracy 80.465%\n",
      "Epoch 11, Batch 406, LR 2.484777 Loss 7.268510, Accuracy 80.475%\n",
      "Epoch 11, Batch 407, LR 2.484824 Loss 7.270149, Accuracy 80.461%\n",
      "Epoch 11, Batch 408, LR 2.484872 Loss 7.271377, Accuracy 80.459%\n",
      "Epoch 11, Batch 409, LR 2.484919 Loss 7.270455, Accuracy 80.469%\n",
      "Epoch 11, Batch 410, LR 2.484966 Loss 7.271518, Accuracy 80.461%\n",
      "Epoch 11, Batch 411, LR 2.485013 Loss 7.271574, Accuracy 80.452%\n",
      "Epoch 11, Batch 412, LR 2.485060 Loss 7.272418, Accuracy 80.442%\n",
      "Epoch 11, Batch 413, LR 2.485107 Loss 7.272710, Accuracy 80.435%\n",
      "Epoch 11, Batch 414, LR 2.485154 Loss 7.273338, Accuracy 80.433%\n",
      "Epoch 11, Batch 415, LR 2.485201 Loss 7.274126, Accuracy 80.422%\n",
      "Epoch 11, Batch 416, LR 2.485247 Loss 7.274797, Accuracy 80.412%\n",
      "Epoch 11, Batch 417, LR 2.485294 Loss 7.274510, Accuracy 80.418%\n",
      "Epoch 11, Batch 418, LR 2.485340 Loss 7.273450, Accuracy 80.426%\n",
      "Epoch 11, Batch 419, LR 2.485387 Loss 7.272521, Accuracy 80.430%\n",
      "Epoch 11, Batch 420, LR 2.485433 Loss 7.273657, Accuracy 80.411%\n",
      "Epoch 11, Batch 421, LR 2.485480 Loss 7.274249, Accuracy 80.409%\n",
      "Epoch 11, Batch 422, LR 2.485526 Loss 7.273625, Accuracy 80.406%\n",
      "Epoch 11, Batch 423, LR 2.485572 Loss 7.273268, Accuracy 80.404%\n",
      "Epoch 11, Batch 424, LR 2.485618 Loss 7.273390, Accuracy 80.390%\n",
      "Epoch 11, Batch 425, LR 2.485664 Loss 7.272247, Accuracy 80.397%\n",
      "Epoch 11, Batch 426, LR 2.485710 Loss 7.272983, Accuracy 80.397%\n",
      "Epoch 11, Batch 427, LR 2.485756 Loss 7.272237, Accuracy 80.392%\n",
      "Epoch 11, Batch 428, LR 2.485802 Loss 7.273106, Accuracy 80.374%\n",
      "Epoch 11, Batch 429, LR 2.485848 Loss 7.273746, Accuracy 80.356%\n",
      "Epoch 11, Batch 430, LR 2.485893 Loss 7.274687, Accuracy 80.351%\n",
      "Epoch 11, Batch 431, LR 2.485939 Loss 7.274395, Accuracy 80.353%\n",
      "Epoch 11, Batch 432, LR 2.485984 Loss 7.273658, Accuracy 80.351%\n",
      "Epoch 11, Batch 433, LR 2.486030 Loss 7.273802, Accuracy 80.353%\n",
      "Epoch 11, Batch 434, LR 2.486075 Loss 7.273817, Accuracy 80.357%\n",
      "Epoch 11, Batch 435, LR 2.486121 Loss 7.273139, Accuracy 80.359%\n",
      "Epoch 11, Batch 436, LR 2.486166 Loss 7.272857, Accuracy 80.359%\n",
      "Epoch 11, Batch 437, LR 2.486211 Loss 7.273192, Accuracy 80.351%\n",
      "Epoch 11, Batch 438, LR 2.486256 Loss 7.273165, Accuracy 80.349%\n",
      "Epoch 11, Batch 439, LR 2.486301 Loss 7.273394, Accuracy 80.346%\n",
      "Epoch 11, Batch 440, LR 2.486346 Loss 7.272394, Accuracy 80.348%\n",
      "Epoch 11, Batch 441, LR 2.486391 Loss 7.271180, Accuracy 80.373%\n",
      "Epoch 11, Batch 442, LR 2.486436 Loss 7.270881, Accuracy 80.370%\n",
      "Epoch 11, Batch 443, LR 2.486480 Loss 7.270805, Accuracy 80.379%\n",
      "Epoch 11, Batch 444, LR 2.486525 Loss 7.272433, Accuracy 80.379%\n",
      "Epoch 11, Batch 445, LR 2.486570 Loss 7.272292, Accuracy 80.379%\n",
      "Epoch 11, Batch 446, LR 2.486614 Loss 7.273143, Accuracy 80.381%\n",
      "Epoch 11, Batch 447, LR 2.486659 Loss 7.273094, Accuracy 80.380%\n",
      "Epoch 11, Batch 448, LR 2.486703 Loss 7.272403, Accuracy 80.382%\n",
      "Epoch 11, Batch 449, LR 2.486747 Loss 7.272494, Accuracy 80.382%\n",
      "Epoch 11, Batch 450, LR 2.486791 Loss 7.272412, Accuracy 80.391%\n",
      "Epoch 11, Batch 451, LR 2.486835 Loss 7.274226, Accuracy 80.386%\n",
      "Epoch 11, Batch 452, LR 2.486880 Loss 7.274907, Accuracy 80.382%\n",
      "Epoch 11, Batch 453, LR 2.486924 Loss 7.274370, Accuracy 80.391%\n",
      "Epoch 11, Batch 454, LR 2.486967 Loss 7.274506, Accuracy 80.402%\n",
      "Epoch 11, Batch 455, LR 2.487011 Loss 7.275593, Accuracy 80.395%\n",
      "Epoch 11, Batch 456, LR 2.487055 Loss 7.276248, Accuracy 80.390%\n",
      "Epoch 11, Batch 457, LR 2.487099 Loss 7.275479, Accuracy 80.400%\n",
      "Epoch 11, Batch 458, LR 2.487142 Loss 7.273120, Accuracy 80.409%\n",
      "Epoch 11, Batch 459, LR 2.487186 Loss 7.273145, Accuracy 80.413%\n",
      "Epoch 11, Batch 460, LR 2.487229 Loss 7.272595, Accuracy 80.423%\n",
      "Epoch 11, Batch 461, LR 2.487273 Loss 7.274263, Accuracy 80.416%\n",
      "Epoch 11, Batch 462, LR 2.487316 Loss 7.273552, Accuracy 80.420%\n",
      "Epoch 11, Batch 463, LR 2.487359 Loss 7.275039, Accuracy 80.413%\n",
      "Epoch 11, Batch 464, LR 2.487402 Loss 7.274974, Accuracy 80.412%\n",
      "Epoch 11, Batch 465, LR 2.487446 Loss 7.276024, Accuracy 80.402%\n",
      "Epoch 11, Batch 466, LR 2.487489 Loss 7.274148, Accuracy 80.412%\n",
      "Epoch 11, Batch 467, LR 2.487532 Loss 7.274539, Accuracy 80.409%\n",
      "Epoch 11, Batch 468, LR 2.487574 Loss 7.274164, Accuracy 80.417%\n",
      "Epoch 11, Batch 469, LR 2.487617 Loss 7.273756, Accuracy 80.419%\n",
      "Epoch 11, Batch 470, LR 2.487660 Loss 7.272802, Accuracy 80.421%\n",
      "Epoch 11, Batch 471, LR 2.487703 Loss 7.271676, Accuracy 80.432%\n",
      "Epoch 11, Batch 472, LR 2.487745 Loss 7.270709, Accuracy 80.431%\n",
      "Epoch 11, Batch 473, LR 2.487788 Loss 7.269130, Accuracy 80.439%\n",
      "Epoch 11, Batch 474, LR 2.487830 Loss 7.270012, Accuracy 80.437%\n",
      "Epoch 11, Batch 475, LR 2.487873 Loss 7.270307, Accuracy 80.433%\n",
      "Epoch 11, Batch 476, LR 2.487915 Loss 7.272110, Accuracy 80.410%\n",
      "Epoch 11, Batch 477, LR 2.487957 Loss 7.272582, Accuracy 80.400%\n",
      "Epoch 11, Batch 478, LR 2.487999 Loss 7.272130, Accuracy 80.407%\n",
      "Epoch 11, Batch 479, LR 2.488041 Loss 7.270566, Accuracy 80.418%\n",
      "Epoch 11, Batch 480, LR 2.488083 Loss 7.270207, Accuracy 80.426%\n",
      "Epoch 11, Batch 481, LR 2.488125 Loss 7.270976, Accuracy 80.433%\n",
      "Epoch 11, Batch 482, LR 2.488167 Loss 7.270008, Accuracy 80.433%\n",
      "Epoch 11, Batch 483, LR 2.488209 Loss 7.269114, Accuracy 80.446%\n",
      "Epoch 11, Batch 484, LR 2.488251 Loss 7.268997, Accuracy 80.446%\n",
      "Epoch 11, Batch 485, LR 2.488292 Loss 7.268311, Accuracy 80.453%\n",
      "Epoch 11, Batch 486, LR 2.488334 Loss 7.268959, Accuracy 80.454%\n",
      "Epoch 11, Batch 487, LR 2.488375 Loss 7.269469, Accuracy 80.454%\n",
      "Epoch 11, Batch 488, LR 2.488417 Loss 7.270566, Accuracy 80.442%\n",
      "Epoch 11, Batch 489, LR 2.488458 Loss 7.269761, Accuracy 80.456%\n",
      "Epoch 11, Batch 490, LR 2.488499 Loss 7.269648, Accuracy 80.462%\n",
      "Epoch 11, Batch 491, LR 2.488540 Loss 7.270039, Accuracy 80.456%\n",
      "Epoch 11, Batch 492, LR 2.488582 Loss 7.269093, Accuracy 80.466%\n",
      "Epoch 11, Batch 493, LR 2.488623 Loss 7.271242, Accuracy 80.456%\n",
      "Epoch 11, Batch 494, LR 2.488664 Loss 7.272130, Accuracy 80.450%\n",
      "Epoch 11, Batch 495, LR 2.488705 Loss 7.271894, Accuracy 80.450%\n",
      "Epoch 11, Batch 496, LR 2.488745 Loss 7.271630, Accuracy 80.451%\n",
      "Epoch 11, Batch 497, LR 2.488786 Loss 7.270584, Accuracy 80.455%\n",
      "Epoch 11, Batch 498, LR 2.488827 Loss 7.271300, Accuracy 80.448%\n",
      "Epoch 11, Batch 499, LR 2.488867 Loss 7.271741, Accuracy 80.447%\n",
      "Epoch 11, Batch 500, LR 2.488908 Loss 7.270611, Accuracy 80.464%\n",
      "Epoch 11, Batch 501, LR 2.488948 Loss 7.270556, Accuracy 80.472%\n",
      "Epoch 11, Batch 502, LR 2.488989 Loss 7.270364, Accuracy 80.469%\n",
      "Epoch 11, Batch 503, LR 2.489029 Loss 7.270149, Accuracy 80.472%\n",
      "Epoch 11, Batch 504, LR 2.489069 Loss 7.270316, Accuracy 80.464%\n",
      "Epoch 11, Batch 505, LR 2.489109 Loss 7.270578, Accuracy 80.469%\n",
      "Epoch 11, Batch 506, LR 2.489150 Loss 7.273226, Accuracy 80.447%\n",
      "Epoch 11, Batch 507, LR 2.489190 Loss 7.273589, Accuracy 80.452%\n",
      "Epoch 11, Batch 508, LR 2.489229 Loss 7.274181, Accuracy 80.443%\n",
      "Epoch 11, Batch 509, LR 2.489269 Loss 7.274165, Accuracy 80.440%\n",
      "Epoch 11, Batch 510, LR 2.489309 Loss 7.274825, Accuracy 80.429%\n",
      "Epoch 11, Batch 511, LR 2.489349 Loss 7.274911, Accuracy 80.423%\n",
      "Epoch 11, Batch 512, LR 2.489389 Loss 7.275335, Accuracy 80.423%\n",
      "Epoch 11, Batch 513, LR 2.489428 Loss 7.276211, Accuracy 80.420%\n",
      "Epoch 11, Batch 514, LR 2.489468 Loss 7.276080, Accuracy 80.425%\n",
      "Epoch 11, Batch 515, LR 2.489507 Loss 7.275770, Accuracy 80.420%\n",
      "Epoch 11, Batch 516, LR 2.489546 Loss 7.275780, Accuracy 80.426%\n",
      "Epoch 11, Batch 517, LR 2.489586 Loss 7.275153, Accuracy 80.436%\n",
      "Epoch 11, Batch 518, LR 2.489625 Loss 7.274568, Accuracy 80.431%\n",
      "Epoch 11, Batch 519, LR 2.489664 Loss 7.273565, Accuracy 80.437%\n",
      "Epoch 11, Batch 520, LR 2.489703 Loss 7.272837, Accuracy 80.445%\n",
      "Epoch 11, Batch 521, LR 2.489742 Loss 7.273620, Accuracy 80.436%\n",
      "Epoch 11, Batch 522, LR 2.489781 Loss 7.273968, Accuracy 80.440%\n",
      "Epoch 11, Batch 523, LR 2.489820 Loss 7.273820, Accuracy 80.445%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 524, LR 2.489859 Loss 7.275573, Accuracy 80.427%\n",
      "Epoch 11, Batch 525, LR 2.489897 Loss 7.276598, Accuracy 80.426%\n",
      "Epoch 11, Batch 526, LR 2.489936 Loss 7.278196, Accuracy 80.420%\n",
      "Epoch 11, Batch 527, LR 2.489974 Loss 7.277652, Accuracy 80.427%\n",
      "Epoch 11, Batch 528, LR 2.490013 Loss 7.277563, Accuracy 80.436%\n",
      "Epoch 11, Batch 529, LR 2.490051 Loss 7.277350, Accuracy 80.439%\n",
      "Epoch 11, Batch 530, LR 2.490090 Loss 7.277322, Accuracy 80.442%\n",
      "Epoch 11, Batch 531, LR 2.490128 Loss 7.277684, Accuracy 80.442%\n",
      "Epoch 11, Batch 532, LR 2.490166 Loss 7.276478, Accuracy 80.448%\n",
      "Epoch 11, Batch 533, LR 2.490204 Loss 7.277459, Accuracy 80.439%\n",
      "Epoch 11, Batch 534, LR 2.490242 Loss 7.277775, Accuracy 80.442%\n",
      "Epoch 11, Batch 535, LR 2.490280 Loss 7.278040, Accuracy 80.444%\n",
      "Epoch 11, Batch 536, LR 2.490318 Loss 7.278314, Accuracy 80.438%\n",
      "Epoch 11, Batch 537, LR 2.490356 Loss 7.277885, Accuracy 80.432%\n",
      "Epoch 11, Batch 538, LR 2.490393 Loss 7.279229, Accuracy 80.432%\n",
      "Epoch 11, Batch 539, LR 2.490431 Loss 7.278526, Accuracy 80.434%\n",
      "Epoch 11, Batch 540, LR 2.490469 Loss 7.277817, Accuracy 80.435%\n",
      "Epoch 11, Batch 541, LR 2.490506 Loss 7.279145, Accuracy 80.423%\n",
      "Epoch 11, Batch 542, LR 2.490544 Loss 7.278943, Accuracy 80.418%\n",
      "Epoch 11, Batch 543, LR 2.490581 Loss 7.279259, Accuracy 80.420%\n",
      "Epoch 11, Batch 544, LR 2.490618 Loss 7.279012, Accuracy 80.421%\n",
      "Epoch 11, Batch 545, LR 2.490656 Loss 7.278076, Accuracy 80.421%\n",
      "Epoch 11, Batch 546, LR 2.490693 Loss 7.277912, Accuracy 80.414%\n",
      "Epoch 11, Batch 547, LR 2.490730 Loss 7.278429, Accuracy 80.417%\n",
      "Epoch 11, Batch 548, LR 2.490767 Loss 7.278686, Accuracy 80.410%\n",
      "Epoch 11, Batch 549, LR 2.490804 Loss 7.277750, Accuracy 80.423%\n",
      "Epoch 11, Batch 550, LR 2.490841 Loss 7.277712, Accuracy 80.420%\n",
      "Epoch 11, Batch 551, LR 2.490877 Loss 7.278123, Accuracy 80.416%\n",
      "Epoch 11, Batch 552, LR 2.490914 Loss 7.278788, Accuracy 80.412%\n",
      "Epoch 11, Batch 553, LR 2.490951 Loss 7.277708, Accuracy 80.414%\n",
      "Epoch 11, Batch 554, LR 2.490987 Loss 7.278199, Accuracy 80.417%\n",
      "Epoch 11, Batch 555, LR 2.491024 Loss 7.279616, Accuracy 80.407%\n",
      "Epoch 11, Batch 556, LR 2.491060 Loss 7.280582, Accuracy 80.408%\n",
      "Epoch 11, Batch 557, LR 2.491096 Loss 7.279291, Accuracy 80.418%\n",
      "Epoch 11, Batch 558, LR 2.491133 Loss 7.279365, Accuracy 80.423%\n",
      "Epoch 11, Batch 559, LR 2.491169 Loss 7.278951, Accuracy 80.432%\n",
      "Epoch 11, Batch 560, LR 2.491205 Loss 7.278263, Accuracy 80.438%\n",
      "Epoch 11, Batch 561, LR 2.491241 Loss 7.278751, Accuracy 80.438%\n",
      "Epoch 11, Batch 562, LR 2.491277 Loss 7.278397, Accuracy 80.440%\n",
      "Epoch 11, Batch 563, LR 2.491313 Loss 7.277770, Accuracy 80.441%\n",
      "Epoch 11, Batch 564, LR 2.491349 Loss 7.277786, Accuracy 80.449%\n",
      "Epoch 11, Batch 565, LR 2.491384 Loss 7.277074, Accuracy 80.454%\n",
      "Epoch 11, Batch 566, LR 2.491420 Loss 7.276685, Accuracy 80.458%\n",
      "Epoch 11, Batch 567, LR 2.491456 Loss 7.277956, Accuracy 80.452%\n",
      "Epoch 11, Batch 568, LR 2.491491 Loss 7.276875, Accuracy 80.462%\n",
      "Epoch 11, Batch 569, LR 2.491527 Loss 7.277426, Accuracy 80.456%\n",
      "Epoch 11, Batch 570, LR 2.491562 Loss 7.278594, Accuracy 80.448%\n",
      "Epoch 11, Batch 571, LR 2.491597 Loss 7.278422, Accuracy 80.445%\n",
      "Epoch 11, Batch 572, LR 2.491633 Loss 7.278050, Accuracy 80.446%\n",
      "Epoch 11, Batch 573, LR 2.491668 Loss 7.277355, Accuracy 80.450%\n",
      "Epoch 11, Batch 574, LR 2.491703 Loss 7.276300, Accuracy 80.458%\n",
      "Epoch 11, Batch 575, LR 2.491738 Loss 7.277008, Accuracy 80.461%\n",
      "Epoch 11, Batch 576, LR 2.491773 Loss 7.275621, Accuracy 80.469%\n",
      "Epoch 11, Batch 577, LR 2.491808 Loss 7.274823, Accuracy 80.474%\n",
      "Epoch 11, Batch 578, LR 2.491842 Loss 7.276137, Accuracy 80.463%\n",
      "Epoch 11, Batch 579, LR 2.491877 Loss 7.276045, Accuracy 80.463%\n",
      "Epoch 11, Batch 580, LR 2.491912 Loss 7.276135, Accuracy 80.466%\n",
      "Epoch 11, Batch 581, LR 2.491946 Loss 7.276414, Accuracy 80.462%\n",
      "Epoch 11, Batch 582, LR 2.491981 Loss 7.277079, Accuracy 80.453%\n",
      "Epoch 11, Batch 583, LR 2.492015 Loss 7.276874, Accuracy 80.451%\n",
      "Epoch 11, Batch 584, LR 2.492050 Loss 7.279047, Accuracy 80.442%\n",
      "Epoch 11, Batch 585, LR 2.492084 Loss 7.279185, Accuracy 80.433%\n",
      "Epoch 11, Batch 586, LR 2.492118 Loss 7.278804, Accuracy 80.430%\n",
      "Epoch 11, Batch 587, LR 2.492152 Loss 7.279587, Accuracy 80.421%\n",
      "Epoch 11, Batch 588, LR 2.492186 Loss 7.278489, Accuracy 80.428%\n",
      "Epoch 11, Batch 589, LR 2.492220 Loss 7.278738, Accuracy 80.420%\n",
      "Epoch 11, Batch 590, LR 2.492254 Loss 7.280399, Accuracy 80.405%\n",
      "Epoch 11, Batch 591, LR 2.492288 Loss 7.280180, Accuracy 80.405%\n",
      "Epoch 11, Batch 592, LR 2.492322 Loss 7.280287, Accuracy 80.403%\n",
      "Epoch 11, Batch 593, LR 2.492355 Loss 7.279334, Accuracy 80.402%\n",
      "Epoch 11, Batch 594, LR 2.492389 Loss 7.278185, Accuracy 80.403%\n",
      "Epoch 11, Batch 595, LR 2.492422 Loss 7.279994, Accuracy 80.382%\n",
      "Epoch 11, Batch 596, LR 2.492456 Loss 7.280480, Accuracy 80.378%\n",
      "Epoch 11, Batch 597, LR 2.492489 Loss 7.278813, Accuracy 80.388%\n",
      "Epoch 11, Batch 598, LR 2.492523 Loss 7.280448, Accuracy 80.377%\n",
      "Epoch 11, Batch 599, LR 2.492556 Loss 7.280374, Accuracy 80.371%\n",
      "Epoch 11, Batch 600, LR 2.492589 Loss 7.281120, Accuracy 80.362%\n",
      "Epoch 11, Batch 601, LR 2.492622 Loss 7.281027, Accuracy 80.354%\n",
      "Epoch 11, Batch 602, LR 2.492655 Loss 7.281135, Accuracy 80.356%\n",
      "Epoch 11, Batch 603, LR 2.492688 Loss 7.281375, Accuracy 80.348%\n",
      "Epoch 11, Batch 604, LR 2.492721 Loss 7.280736, Accuracy 80.355%\n",
      "Epoch 11, Batch 605, LR 2.492754 Loss 7.281288, Accuracy 80.359%\n",
      "Epoch 11, Batch 606, LR 2.492786 Loss 7.280954, Accuracy 80.362%\n",
      "Epoch 11, Batch 607, LR 2.492819 Loss 7.280735, Accuracy 80.357%\n",
      "Epoch 11, Batch 608, LR 2.492852 Loss 7.280539, Accuracy 80.361%\n",
      "Epoch 11, Batch 609, LR 2.492884 Loss 7.279581, Accuracy 80.369%\n",
      "Epoch 11, Batch 610, LR 2.492917 Loss 7.279827, Accuracy 80.365%\n",
      "Epoch 11, Batch 611, LR 2.492949 Loss 7.278788, Accuracy 80.373%\n",
      "Epoch 11, Batch 612, LR 2.492981 Loss 7.279073, Accuracy 80.370%\n",
      "Epoch 11, Batch 613, LR 2.493013 Loss 7.279701, Accuracy 80.367%\n",
      "Epoch 11, Batch 614, LR 2.493045 Loss 7.278909, Accuracy 80.371%\n",
      "Epoch 11, Batch 615, LR 2.493078 Loss 7.278283, Accuracy 80.377%\n",
      "Epoch 11, Batch 616, LR 2.493109 Loss 7.277160, Accuracy 80.381%\n",
      "Epoch 11, Batch 617, LR 2.493141 Loss 7.277719, Accuracy 80.383%\n",
      "Epoch 11, Batch 618, LR 2.493173 Loss 7.277345, Accuracy 80.393%\n",
      "Epoch 11, Batch 619, LR 2.493205 Loss 7.278239, Accuracy 80.394%\n",
      "Epoch 11, Batch 620, LR 2.493237 Loss 7.277649, Accuracy 80.402%\n",
      "Epoch 11, Batch 621, LR 2.493268 Loss 7.277550, Accuracy 80.401%\n",
      "Epoch 11, Batch 622, LR 2.493300 Loss 7.279071, Accuracy 80.390%\n",
      "Epoch 11, Batch 623, LR 2.493331 Loss 7.278401, Accuracy 80.396%\n",
      "Epoch 11, Batch 624, LR 2.493363 Loss 7.279376, Accuracy 80.392%\n",
      "Epoch 11, Batch 625, LR 2.493394 Loss 7.279453, Accuracy 80.391%\n",
      "Epoch 11, Batch 626, LR 2.493425 Loss 7.279606, Accuracy 80.386%\n",
      "Epoch 11, Batch 627, LR 2.493456 Loss 7.279327, Accuracy 80.390%\n",
      "Epoch 11, Batch 628, LR 2.493488 Loss 7.279446, Accuracy 80.385%\n",
      "Epoch 11, Batch 629, LR 2.493519 Loss 7.280289, Accuracy 80.379%\n",
      "Epoch 11, Batch 630, LR 2.493549 Loss 7.280461, Accuracy 80.373%\n",
      "Epoch 11, Batch 631, LR 2.493580 Loss 7.280328, Accuracy 80.368%\n",
      "Epoch 11, Batch 632, LR 2.493611 Loss 7.280158, Accuracy 80.369%\n",
      "Epoch 11, Batch 633, LR 2.493642 Loss 7.280442, Accuracy 80.368%\n",
      "Epoch 11, Batch 634, LR 2.493673 Loss 7.280081, Accuracy 80.370%\n",
      "Epoch 11, Batch 635, LR 2.493703 Loss 7.279473, Accuracy 80.365%\n",
      "Epoch 11, Batch 636, LR 2.493734 Loss 7.278840, Accuracy 80.370%\n",
      "Epoch 11, Batch 637, LR 2.493764 Loss 7.278544, Accuracy 80.378%\n",
      "Epoch 11, Batch 638, LR 2.493794 Loss 7.277873, Accuracy 80.383%\n",
      "Epoch 11, Batch 639, LR 2.493825 Loss 7.277644, Accuracy 80.382%\n",
      "Epoch 11, Batch 640, LR 2.493855 Loss 7.277595, Accuracy 80.380%\n",
      "Epoch 11, Batch 641, LR 2.493885 Loss 7.278292, Accuracy 80.375%\n",
      "Epoch 11, Batch 642, LR 2.493915 Loss 7.278419, Accuracy 80.368%\n",
      "Epoch 11, Batch 643, LR 2.493945 Loss 7.278253, Accuracy 80.370%\n",
      "Epoch 11, Batch 644, LR 2.493975 Loss 7.277212, Accuracy 80.372%\n",
      "Epoch 11, Batch 645, LR 2.494005 Loss 7.278222, Accuracy 80.363%\n",
      "Epoch 11, Batch 646, LR 2.494035 Loss 7.277477, Accuracy 80.368%\n",
      "Epoch 11, Batch 647, LR 2.494064 Loss 7.278227, Accuracy 80.362%\n",
      "Epoch 11, Batch 648, LR 2.494094 Loss 7.278084, Accuracy 80.363%\n",
      "Epoch 11, Batch 649, LR 2.494123 Loss 7.277541, Accuracy 80.369%\n",
      "Epoch 11, Batch 650, LR 2.494153 Loss 7.278692, Accuracy 80.362%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 651, LR 2.494182 Loss 7.279254, Accuracy 80.357%\n",
      "Epoch 11, Batch 652, LR 2.494212 Loss 7.278368, Accuracy 80.355%\n",
      "Epoch 11, Batch 653, LR 2.494241 Loss 7.277412, Accuracy 80.355%\n",
      "Epoch 11, Batch 654, LR 2.494270 Loss 7.276819, Accuracy 80.360%\n",
      "Epoch 11, Batch 655, LR 2.494299 Loss 7.275575, Accuracy 80.363%\n",
      "Epoch 11, Batch 656, LR 2.494328 Loss 7.274686, Accuracy 80.364%\n",
      "Epoch 11, Batch 657, LR 2.494357 Loss 7.275237, Accuracy 80.361%\n",
      "Epoch 11, Batch 658, LR 2.494386 Loss 7.275118, Accuracy 80.350%\n",
      "Epoch 11, Batch 659, LR 2.494415 Loss 7.275511, Accuracy 80.350%\n",
      "Epoch 11, Batch 660, LR 2.494444 Loss 7.275971, Accuracy 80.348%\n",
      "Epoch 11, Batch 661, LR 2.494472 Loss 7.274928, Accuracy 80.351%\n",
      "Epoch 11, Batch 662, LR 2.494501 Loss 7.273770, Accuracy 80.360%\n",
      "Epoch 11, Batch 663, LR 2.494529 Loss 7.273389, Accuracy 80.359%\n",
      "Epoch 11, Batch 664, LR 2.494558 Loss 7.273042, Accuracy 80.359%\n",
      "Epoch 11, Batch 665, LR 2.494586 Loss 7.273735, Accuracy 80.351%\n",
      "Epoch 11, Batch 666, LR 2.494614 Loss 7.274282, Accuracy 80.344%\n",
      "Epoch 11, Batch 667, LR 2.494643 Loss 7.274729, Accuracy 80.345%\n",
      "Epoch 11, Batch 668, LR 2.494671 Loss 7.274251, Accuracy 80.351%\n",
      "Epoch 11, Batch 669, LR 2.494699 Loss 7.275005, Accuracy 80.344%\n",
      "Epoch 11, Batch 670, LR 2.494727 Loss 7.273910, Accuracy 80.351%\n",
      "Epoch 11, Batch 671, LR 2.494755 Loss 7.273544, Accuracy 80.355%\n",
      "Epoch 11, Batch 672, LR 2.494783 Loss 7.273752, Accuracy 80.354%\n",
      "Epoch 11, Batch 673, LR 2.494810 Loss 7.272870, Accuracy 80.357%\n",
      "Epoch 11, Batch 674, LR 2.494838 Loss 7.273191, Accuracy 80.355%\n",
      "Epoch 11, Batch 675, LR 2.494866 Loss 7.273539, Accuracy 80.353%\n",
      "Epoch 11, Batch 676, LR 2.494893 Loss 7.274403, Accuracy 80.350%\n",
      "Epoch 11, Batch 677, LR 2.494921 Loss 7.274663, Accuracy 80.340%\n",
      "Epoch 11, Batch 678, LR 2.494948 Loss 7.275096, Accuracy 80.339%\n",
      "Epoch 11, Batch 679, LR 2.494975 Loss 7.274879, Accuracy 80.338%\n",
      "Epoch 11, Batch 680, LR 2.495003 Loss 7.273986, Accuracy 80.342%\n",
      "Epoch 11, Batch 681, LR 2.495030 Loss 7.273174, Accuracy 80.341%\n",
      "Epoch 11, Batch 682, LR 2.495057 Loss 7.272473, Accuracy 80.348%\n",
      "Epoch 11, Batch 683, LR 2.495084 Loss 7.272196, Accuracy 80.352%\n",
      "Epoch 11, Batch 684, LR 2.495111 Loss 7.271930, Accuracy 80.356%\n",
      "Epoch 11, Batch 685, LR 2.495138 Loss 7.272254, Accuracy 80.355%\n",
      "Epoch 11, Batch 686, LR 2.495165 Loss 7.272412, Accuracy 80.358%\n",
      "Epoch 11, Batch 687, LR 2.495191 Loss 7.272727, Accuracy 80.356%\n",
      "Epoch 11, Batch 688, LR 2.495218 Loss 7.273573, Accuracy 80.346%\n",
      "Epoch 11, Batch 689, LR 2.495245 Loss 7.272559, Accuracy 80.349%\n",
      "Epoch 11, Batch 690, LR 2.495271 Loss 7.272257, Accuracy 80.351%\n",
      "Epoch 11, Batch 691, LR 2.495297 Loss 7.271628, Accuracy 80.356%\n",
      "Epoch 11, Batch 692, LR 2.495324 Loss 7.271447, Accuracy 80.354%\n",
      "Epoch 11, Batch 693, LR 2.495350 Loss 7.270707, Accuracy 80.357%\n",
      "Epoch 11, Batch 694, LR 2.495376 Loss 7.270483, Accuracy 80.362%\n",
      "Epoch 11, Batch 695, LR 2.495403 Loss 7.269497, Accuracy 80.369%\n",
      "Epoch 11, Batch 696, LR 2.495429 Loss 7.269854, Accuracy 80.363%\n",
      "Epoch 11, Batch 697, LR 2.495455 Loss 7.270225, Accuracy 80.362%\n",
      "Epoch 11, Batch 698, LR 2.495480 Loss 7.269870, Accuracy 80.365%\n",
      "Epoch 11, Batch 699, LR 2.495506 Loss 7.269574, Accuracy 80.364%\n",
      "Epoch 11, Batch 700, LR 2.495532 Loss 7.269332, Accuracy 80.364%\n",
      "Epoch 11, Batch 701, LR 2.495558 Loss 7.269079, Accuracy 80.363%\n",
      "Epoch 11, Batch 702, LR 2.495583 Loss 7.268569, Accuracy 80.362%\n",
      "Epoch 11, Batch 703, LR 2.495609 Loss 7.268944, Accuracy 80.361%\n",
      "Epoch 11, Batch 704, LR 2.495634 Loss 7.269301, Accuracy 80.357%\n",
      "Epoch 11, Batch 705, LR 2.495660 Loss 7.270332, Accuracy 80.347%\n",
      "Epoch 11, Batch 706, LR 2.495685 Loss 7.269661, Accuracy 80.359%\n",
      "Epoch 11, Batch 707, LR 2.495710 Loss 7.270607, Accuracy 80.355%\n",
      "Epoch 11, Batch 708, LR 2.495736 Loss 7.270101, Accuracy 80.357%\n",
      "Epoch 11, Batch 709, LR 2.495761 Loss 7.269799, Accuracy 80.356%\n",
      "Epoch 11, Batch 710, LR 2.495786 Loss 7.269484, Accuracy 80.354%\n",
      "Epoch 11, Batch 711, LR 2.495811 Loss 7.269114, Accuracy 80.357%\n",
      "Epoch 11, Batch 712, LR 2.495836 Loss 7.268609, Accuracy 80.358%\n",
      "Epoch 11, Batch 713, LR 2.495860 Loss 7.268540, Accuracy 80.364%\n",
      "Epoch 11, Batch 714, LR 2.495885 Loss 7.268515, Accuracy 80.366%\n",
      "Epoch 11, Batch 715, LR 2.495910 Loss 7.268453, Accuracy 80.369%\n",
      "Epoch 11, Batch 716, LR 2.495934 Loss 7.268513, Accuracy 80.372%\n",
      "Epoch 11, Batch 717, LR 2.495959 Loss 7.268567, Accuracy 80.369%\n",
      "Epoch 11, Batch 718, LR 2.495983 Loss 7.268136, Accuracy 80.368%\n",
      "Epoch 11, Batch 719, LR 2.496008 Loss 7.268030, Accuracy 80.370%\n",
      "Epoch 11, Batch 720, LR 2.496032 Loss 7.268094, Accuracy 80.362%\n",
      "Epoch 11, Batch 721, LR 2.496056 Loss 7.268016, Accuracy 80.369%\n",
      "Epoch 11, Batch 722, LR 2.496080 Loss 7.267131, Accuracy 80.378%\n",
      "Epoch 11, Batch 723, LR 2.496104 Loss 7.266769, Accuracy 80.387%\n",
      "Epoch 11, Batch 724, LR 2.496128 Loss 7.266095, Accuracy 80.386%\n",
      "Epoch 11, Batch 725, LR 2.496152 Loss 7.265753, Accuracy 80.389%\n",
      "Epoch 11, Batch 726, LR 2.496176 Loss 7.265658, Accuracy 80.390%\n",
      "Epoch 11, Batch 727, LR 2.496200 Loss 7.265240, Accuracy 80.394%\n",
      "Epoch 11, Batch 728, LR 2.496224 Loss 7.265209, Accuracy 80.389%\n",
      "Epoch 11, Batch 729, LR 2.496247 Loss 7.265510, Accuracy 80.394%\n",
      "Epoch 11, Batch 730, LR 2.496271 Loss 7.266123, Accuracy 80.388%\n",
      "Epoch 11, Batch 731, LR 2.496294 Loss 7.266278, Accuracy 80.391%\n",
      "Epoch 11, Batch 732, LR 2.496318 Loss 7.266136, Accuracy 80.392%\n",
      "Epoch 11, Batch 733, LR 2.496341 Loss 7.266458, Accuracy 80.389%\n",
      "Epoch 11, Batch 734, LR 2.496364 Loss 7.266514, Accuracy 80.388%\n",
      "Epoch 11, Batch 735, LR 2.496388 Loss 7.265241, Accuracy 80.394%\n",
      "Epoch 11, Batch 736, LR 2.496411 Loss 7.264633, Accuracy 80.403%\n",
      "Epoch 11, Batch 737, LR 2.496434 Loss 7.264468, Accuracy 80.404%\n",
      "Epoch 11, Batch 738, LR 2.496457 Loss 7.265449, Accuracy 80.388%\n",
      "Epoch 11, Batch 739, LR 2.496480 Loss 7.266598, Accuracy 80.378%\n",
      "Epoch 11, Batch 740, LR 2.496502 Loss 7.267497, Accuracy 80.376%\n",
      "Epoch 11, Batch 741, LR 2.496525 Loss 7.267251, Accuracy 80.373%\n",
      "Epoch 11, Batch 742, LR 2.496548 Loss 7.266788, Accuracy 80.376%\n",
      "Epoch 11, Batch 743, LR 2.496570 Loss 7.268069, Accuracy 80.366%\n",
      "Epoch 11, Batch 744, LR 2.496593 Loss 7.268253, Accuracy 80.367%\n",
      "Epoch 11, Batch 745, LR 2.496615 Loss 7.269546, Accuracy 80.360%\n",
      "Epoch 11, Batch 746, LR 2.496638 Loss 7.269140, Accuracy 80.358%\n",
      "Epoch 11, Batch 747, LR 2.496660 Loss 7.269588, Accuracy 80.354%\n",
      "Epoch 11, Batch 748, LR 2.496682 Loss 7.270412, Accuracy 80.351%\n",
      "Epoch 11, Batch 749, LR 2.496704 Loss 7.270342, Accuracy 80.349%\n",
      "Epoch 11, Batch 750, LR 2.496726 Loss 7.271594, Accuracy 80.343%\n",
      "Epoch 11, Batch 751, LR 2.496748 Loss 7.272330, Accuracy 80.342%\n",
      "Epoch 11, Batch 752, LR 2.496770 Loss 7.272290, Accuracy 80.341%\n",
      "Epoch 11, Batch 753, LR 2.496792 Loss 7.273217, Accuracy 80.336%\n",
      "Epoch 11, Batch 754, LR 2.496814 Loss 7.273527, Accuracy 80.335%\n",
      "Epoch 11, Batch 755, LR 2.496836 Loss 7.273534, Accuracy 80.330%\n",
      "Epoch 11, Batch 756, LR 2.496857 Loss 7.272548, Accuracy 80.332%\n",
      "Epoch 11, Batch 757, LR 2.496879 Loss 7.273017, Accuracy 80.328%\n",
      "Epoch 11, Batch 758, LR 2.496900 Loss 7.273143, Accuracy 80.325%\n",
      "Epoch 11, Batch 759, LR 2.496922 Loss 7.273148, Accuracy 80.327%\n",
      "Epoch 11, Batch 760, LR 2.496943 Loss 7.272994, Accuracy 80.330%\n",
      "Epoch 11, Batch 761, LR 2.496964 Loss 7.273537, Accuracy 80.327%\n",
      "Epoch 11, Batch 762, LR 2.496985 Loss 7.273331, Accuracy 80.328%\n",
      "Epoch 11, Batch 763, LR 2.497007 Loss 7.273731, Accuracy 80.321%\n",
      "Epoch 11, Batch 764, LR 2.497028 Loss 7.273253, Accuracy 80.320%\n",
      "Epoch 11, Batch 765, LR 2.497049 Loss 7.272746, Accuracy 80.327%\n",
      "Epoch 11, Batch 766, LR 2.497069 Loss 7.272529, Accuracy 80.323%\n",
      "Epoch 11, Batch 767, LR 2.497090 Loss 7.272212, Accuracy 80.326%\n",
      "Epoch 11, Batch 768, LR 2.497111 Loss 7.272941, Accuracy 80.321%\n",
      "Epoch 11, Batch 769, LR 2.497132 Loss 7.274253, Accuracy 80.311%\n",
      "Epoch 11, Batch 770, LR 2.497152 Loss 7.274869, Accuracy 80.309%\n",
      "Epoch 11, Batch 771, LR 2.497173 Loss 7.274177, Accuracy 80.313%\n",
      "Epoch 11, Batch 772, LR 2.497193 Loss 7.273919, Accuracy 80.315%\n",
      "Epoch 11, Batch 773, LR 2.497214 Loss 7.272231, Accuracy 80.326%\n",
      "Epoch 11, Batch 774, LR 2.497234 Loss 7.271568, Accuracy 80.330%\n",
      "Epoch 11, Batch 775, LR 2.497254 Loss 7.270831, Accuracy 80.329%\n",
      "Epoch 11, Batch 776, LR 2.497274 Loss 7.270742, Accuracy 80.328%\n",
      "Epoch 11, Batch 777, LR 2.497294 Loss 7.271328, Accuracy 80.324%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 778, LR 2.497314 Loss 7.270945, Accuracy 80.327%\n",
      "Epoch 11, Batch 779, LR 2.497334 Loss 7.271978, Accuracy 80.320%\n",
      "Epoch 11, Batch 780, LR 2.497354 Loss 7.271089, Accuracy 80.321%\n",
      "Epoch 11, Batch 781, LR 2.497374 Loss 7.271204, Accuracy 80.319%\n",
      "Epoch 11, Batch 782, LR 2.497394 Loss 7.272200, Accuracy 80.318%\n",
      "Epoch 11, Batch 783, LR 2.497413 Loss 7.272236, Accuracy 80.314%\n",
      "Epoch 11, Batch 784, LR 2.497433 Loss 7.272196, Accuracy 80.311%\n",
      "Epoch 11, Batch 785, LR 2.497452 Loss 7.271316, Accuracy 80.311%\n",
      "Epoch 11, Batch 786, LR 2.497472 Loss 7.271243, Accuracy 80.309%\n",
      "Epoch 11, Batch 787, LR 2.497491 Loss 7.270149, Accuracy 80.314%\n",
      "Epoch 11, Batch 788, LR 2.497510 Loss 7.270111, Accuracy 80.311%\n",
      "Epoch 11, Batch 789, LR 2.497529 Loss 7.269480, Accuracy 80.318%\n",
      "Epoch 11, Batch 790, LR 2.497548 Loss 7.268648, Accuracy 80.319%\n",
      "Epoch 11, Batch 791, LR 2.497568 Loss 7.268809, Accuracy 80.317%\n",
      "Epoch 11, Batch 792, LR 2.497586 Loss 7.269372, Accuracy 80.310%\n",
      "Epoch 11, Batch 793, LR 2.497605 Loss 7.270000, Accuracy 80.305%\n",
      "Epoch 11, Batch 794, LR 2.497624 Loss 7.269910, Accuracy 80.303%\n",
      "Epoch 11, Batch 795, LR 2.497643 Loss 7.269686, Accuracy 80.301%\n",
      "Epoch 11, Batch 796, LR 2.497662 Loss 7.269369, Accuracy 80.303%\n",
      "Epoch 11, Batch 797, LR 2.497680 Loss 7.269975, Accuracy 80.301%\n",
      "Epoch 11, Batch 798, LR 2.497699 Loss 7.270361, Accuracy 80.294%\n",
      "Epoch 11, Batch 799, LR 2.497717 Loss 7.270305, Accuracy 80.298%\n",
      "Epoch 11, Batch 800, LR 2.497735 Loss 7.270209, Accuracy 80.298%\n",
      "Epoch 11, Batch 801, LR 2.497754 Loss 7.269644, Accuracy 80.306%\n",
      "Epoch 11, Batch 802, LR 2.497772 Loss 7.268998, Accuracy 80.311%\n",
      "Epoch 11, Batch 803, LR 2.497790 Loss 7.269335, Accuracy 80.311%\n",
      "Epoch 11, Batch 804, LR 2.497808 Loss 7.268674, Accuracy 80.317%\n",
      "Epoch 11, Batch 805, LR 2.497826 Loss 7.268789, Accuracy 80.315%\n",
      "Epoch 11, Batch 806, LR 2.497844 Loss 7.268355, Accuracy 80.320%\n",
      "Epoch 11, Batch 807, LR 2.497862 Loss 7.267697, Accuracy 80.326%\n",
      "Epoch 11, Batch 808, LR 2.497880 Loss 7.267959, Accuracy 80.322%\n",
      "Epoch 11, Batch 809, LR 2.497897 Loss 7.267846, Accuracy 80.323%\n",
      "Epoch 11, Batch 810, LR 2.497915 Loss 7.268110, Accuracy 80.323%\n",
      "Epoch 11, Batch 811, LR 2.497933 Loss 7.267734, Accuracy 80.321%\n",
      "Epoch 11, Batch 812, LR 2.497950 Loss 7.267167, Accuracy 80.323%\n",
      "Epoch 11, Batch 813, LR 2.497968 Loss 7.266616, Accuracy 80.327%\n",
      "Epoch 11, Batch 814, LR 2.497985 Loss 7.265881, Accuracy 80.333%\n",
      "Epoch 11, Batch 815, LR 2.498002 Loss 7.265972, Accuracy 80.334%\n",
      "Epoch 11, Batch 816, LR 2.498019 Loss 7.265167, Accuracy 80.339%\n",
      "Epoch 11, Batch 817, LR 2.498036 Loss 7.264546, Accuracy 80.346%\n",
      "Epoch 11, Batch 818, LR 2.498053 Loss 7.264339, Accuracy 80.350%\n",
      "Epoch 11, Batch 819, LR 2.498070 Loss 7.263883, Accuracy 80.355%\n",
      "Epoch 11, Batch 820, LR 2.498087 Loss 7.264131, Accuracy 80.353%\n",
      "Epoch 11, Batch 821, LR 2.498104 Loss 7.264295, Accuracy 80.355%\n",
      "Epoch 11, Batch 822, LR 2.498121 Loss 7.264910, Accuracy 80.353%\n",
      "Epoch 11, Batch 823, LR 2.498137 Loss 7.264393, Accuracy 80.360%\n",
      "Epoch 11, Batch 824, LR 2.498154 Loss 7.263943, Accuracy 80.359%\n",
      "Epoch 11, Batch 825, LR 2.498171 Loss 7.264360, Accuracy 80.353%\n",
      "Epoch 11, Batch 826, LR 2.498187 Loss 7.264482, Accuracy 80.355%\n",
      "Epoch 11, Batch 827, LR 2.498203 Loss 7.266034, Accuracy 80.342%\n",
      "Epoch 11, Batch 828, LR 2.498220 Loss 7.264808, Accuracy 80.347%\n",
      "Epoch 11, Batch 829, LR 2.498236 Loss 7.264744, Accuracy 80.350%\n",
      "Epoch 11, Batch 830, LR 2.498252 Loss 7.265584, Accuracy 80.344%\n",
      "Epoch 11, Batch 831, LR 2.498268 Loss 7.264705, Accuracy 80.351%\n",
      "Epoch 11, Batch 832, LR 2.498284 Loss 7.265047, Accuracy 80.348%\n",
      "Epoch 11, Batch 833, LR 2.498300 Loss 7.265533, Accuracy 80.343%\n",
      "Epoch 11, Batch 834, LR 2.498316 Loss 7.264665, Accuracy 80.348%\n",
      "Epoch 11, Batch 835, LR 2.498332 Loss 7.263663, Accuracy 80.356%\n",
      "Epoch 11, Batch 836, LR 2.498347 Loss 7.263167, Accuracy 80.358%\n",
      "Epoch 11, Batch 837, LR 2.498363 Loss 7.263894, Accuracy 80.349%\n",
      "Epoch 11, Batch 838, LR 2.498379 Loss 7.264184, Accuracy 80.353%\n",
      "Epoch 11, Batch 839, LR 2.498394 Loss 7.264229, Accuracy 80.357%\n",
      "Epoch 11, Batch 840, LR 2.498409 Loss 7.264080, Accuracy 80.361%\n",
      "Epoch 11, Batch 841, LR 2.498425 Loss 7.263937, Accuracy 80.359%\n",
      "Epoch 11, Batch 842, LR 2.498440 Loss 7.265083, Accuracy 80.345%\n",
      "Epoch 11, Batch 843, LR 2.498455 Loss 7.265872, Accuracy 80.344%\n",
      "Epoch 11, Batch 844, LR 2.498470 Loss 7.265845, Accuracy 80.344%\n",
      "Epoch 11, Batch 845, LR 2.498485 Loss 7.265902, Accuracy 80.346%\n",
      "Epoch 11, Batch 846, LR 2.498500 Loss 7.266091, Accuracy 80.352%\n",
      "Epoch 11, Batch 847, LR 2.498515 Loss 7.265686, Accuracy 80.353%\n",
      "Epoch 11, Batch 848, LR 2.498530 Loss 7.266194, Accuracy 80.348%\n",
      "Epoch 11, Batch 849, LR 2.498545 Loss 7.266109, Accuracy 80.351%\n",
      "Epoch 11, Batch 850, LR 2.498559 Loss 7.265983, Accuracy 80.352%\n",
      "Epoch 11, Batch 851, LR 2.498574 Loss 7.265726, Accuracy 80.355%\n",
      "Epoch 11, Batch 852, LR 2.498588 Loss 7.264986, Accuracy 80.355%\n",
      "Epoch 11, Batch 853, LR 2.498603 Loss 7.264784, Accuracy 80.353%\n",
      "Epoch 11, Batch 854, LR 2.498617 Loss 7.264086, Accuracy 80.358%\n",
      "Epoch 11, Batch 855, LR 2.498632 Loss 7.264150, Accuracy 80.353%\n",
      "Epoch 11, Batch 856, LR 2.498646 Loss 7.264916, Accuracy 80.348%\n",
      "Epoch 11, Batch 857, LR 2.498660 Loss 7.264717, Accuracy 80.349%\n",
      "Epoch 11, Batch 858, LR 2.498674 Loss 7.264899, Accuracy 80.349%\n",
      "Epoch 11, Batch 859, LR 2.498688 Loss 7.264599, Accuracy 80.353%\n",
      "Epoch 11, Batch 860, LR 2.498702 Loss 7.263949, Accuracy 80.357%\n",
      "Epoch 11, Batch 861, LR 2.498716 Loss 7.264104, Accuracy 80.357%\n",
      "Epoch 11, Batch 862, LR 2.498729 Loss 7.263945, Accuracy 80.361%\n",
      "Epoch 11, Batch 863, LR 2.498743 Loss 7.263300, Accuracy 80.366%\n",
      "Epoch 11, Batch 864, LR 2.498757 Loss 7.263336, Accuracy 80.366%\n",
      "Epoch 11, Batch 865, LR 2.498770 Loss 7.263429, Accuracy 80.369%\n",
      "Epoch 11, Batch 866, LR 2.498784 Loss 7.263619, Accuracy 80.363%\n",
      "Epoch 11, Batch 867, LR 2.498797 Loss 7.263409, Accuracy 80.365%\n",
      "Epoch 11, Batch 868, LR 2.498811 Loss 7.263539, Accuracy 80.366%\n",
      "Epoch 11, Batch 869, LR 2.498824 Loss 7.263142, Accuracy 80.369%\n",
      "Epoch 11, Batch 870, LR 2.498837 Loss 7.262687, Accuracy 80.369%\n",
      "Epoch 11, Batch 871, LR 2.498850 Loss 7.263098, Accuracy 80.361%\n",
      "Epoch 11, Batch 872, LR 2.498863 Loss 7.262671, Accuracy 80.359%\n",
      "Epoch 11, Batch 873, LR 2.498876 Loss 7.262501, Accuracy 80.358%\n",
      "Epoch 11, Batch 874, LR 2.498889 Loss 7.262757, Accuracy 80.357%\n",
      "Epoch 11, Batch 875, LR 2.498902 Loss 7.262406, Accuracy 80.354%\n",
      "Epoch 11, Batch 876, LR 2.498914 Loss 7.262064, Accuracy 80.355%\n",
      "Epoch 11, Batch 877, LR 2.498927 Loss 7.261572, Accuracy 80.357%\n",
      "Epoch 11, Batch 878, LR 2.498940 Loss 7.260953, Accuracy 80.361%\n",
      "Epoch 11, Batch 879, LR 2.498952 Loss 7.261950, Accuracy 80.354%\n",
      "Epoch 11, Batch 880, LR 2.498965 Loss 7.262043, Accuracy 80.351%\n",
      "Epoch 11, Batch 881, LR 2.498977 Loss 7.261779, Accuracy 80.350%\n",
      "Epoch 11, Batch 882, LR 2.498989 Loss 7.262772, Accuracy 80.347%\n",
      "Epoch 11, Batch 883, LR 2.499002 Loss 7.263044, Accuracy 80.347%\n",
      "Epoch 11, Batch 884, LR 2.499014 Loss 7.263117, Accuracy 80.348%\n",
      "Epoch 11, Batch 885, LR 2.499026 Loss 7.263071, Accuracy 80.350%\n",
      "Epoch 11, Batch 886, LR 2.499038 Loss 7.263478, Accuracy 80.342%\n",
      "Epoch 11, Batch 887, LR 2.499050 Loss 7.262927, Accuracy 80.345%\n",
      "Epoch 11, Batch 888, LR 2.499061 Loss 7.263354, Accuracy 80.337%\n",
      "Epoch 11, Batch 889, LR 2.499073 Loss 7.263904, Accuracy 80.331%\n",
      "Epoch 11, Batch 890, LR 2.499085 Loss 7.264166, Accuracy 80.328%\n",
      "Epoch 11, Batch 891, LR 2.499097 Loss 7.263761, Accuracy 80.331%\n",
      "Epoch 11, Batch 892, LR 2.499108 Loss 7.264247, Accuracy 80.329%\n",
      "Epoch 11, Batch 893, LR 2.499120 Loss 7.264135, Accuracy 80.331%\n",
      "Epoch 11, Batch 894, LR 2.499131 Loss 7.263865, Accuracy 80.333%\n",
      "Epoch 11, Batch 895, LR 2.499142 Loss 7.263465, Accuracy 80.337%\n",
      "Epoch 11, Batch 896, LR 2.499154 Loss 7.263946, Accuracy 80.334%\n",
      "Epoch 11, Batch 897, LR 2.499165 Loss 7.263507, Accuracy 80.336%\n",
      "Epoch 11, Batch 898, LR 2.499176 Loss 7.262850, Accuracy 80.340%\n",
      "Epoch 11, Batch 899, LR 2.499187 Loss 7.261826, Accuracy 80.348%\n",
      "Epoch 11, Batch 900, LR 2.499198 Loss 7.262306, Accuracy 80.345%\n",
      "Epoch 11, Batch 901, LR 2.499209 Loss 7.261944, Accuracy 80.343%\n",
      "Epoch 11, Batch 902, LR 2.499219 Loss 7.261771, Accuracy 80.341%\n",
      "Epoch 11, Batch 903, LR 2.499230 Loss 7.261462, Accuracy 80.343%\n",
      "Epoch 11, Batch 904, LR 2.499241 Loss 7.261227, Accuracy 80.346%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 905, LR 2.499251 Loss 7.261065, Accuracy 80.344%\n",
      "Epoch 11, Batch 906, LR 2.499262 Loss 7.261146, Accuracy 80.347%\n",
      "Epoch 11, Batch 907, LR 2.499272 Loss 7.260563, Accuracy 80.352%\n",
      "Epoch 11, Batch 908, LR 2.499283 Loss 7.259850, Accuracy 80.356%\n",
      "Epoch 11, Batch 909, LR 2.499293 Loss 7.259552, Accuracy 80.360%\n",
      "Epoch 11, Batch 910, LR 2.499303 Loss 7.259460, Accuracy 80.361%\n",
      "Epoch 11, Batch 911, LR 2.499313 Loss 7.260583, Accuracy 80.354%\n",
      "Epoch 11, Batch 912, LR 2.499323 Loss 7.260173, Accuracy 80.357%\n",
      "Epoch 11, Batch 913, LR 2.499333 Loss 7.260867, Accuracy 80.352%\n",
      "Epoch 11, Batch 914, LR 2.499343 Loss 7.261246, Accuracy 80.349%\n",
      "Epoch 11, Batch 915, LR 2.499353 Loss 7.261240, Accuracy 80.353%\n",
      "Epoch 11, Batch 916, LR 2.499363 Loss 7.261120, Accuracy 80.353%\n",
      "Epoch 11, Batch 917, LR 2.499373 Loss 7.261332, Accuracy 80.352%\n",
      "Epoch 11, Batch 918, LR 2.499382 Loss 7.261547, Accuracy 80.350%\n",
      "Epoch 11, Batch 919, LR 2.499392 Loss 7.262048, Accuracy 80.346%\n",
      "Epoch 11, Batch 920, LR 2.499401 Loss 7.262357, Accuracy 80.349%\n",
      "Epoch 11, Batch 921, LR 2.499411 Loss 7.262442, Accuracy 80.346%\n",
      "Epoch 11, Batch 922, LR 2.499420 Loss 7.262018, Accuracy 80.350%\n",
      "Epoch 11, Batch 923, LR 2.499429 Loss 7.261913, Accuracy 80.349%\n",
      "Epoch 11, Batch 924, LR 2.499438 Loss 7.261912, Accuracy 80.348%\n",
      "Epoch 11, Batch 925, LR 2.499447 Loss 7.261889, Accuracy 80.345%\n",
      "Epoch 11, Batch 926, LR 2.499456 Loss 7.262102, Accuracy 80.342%\n",
      "Epoch 11, Batch 927, LR 2.499465 Loss 7.262590, Accuracy 80.340%\n",
      "Epoch 11, Batch 928, LR 2.499474 Loss 7.262363, Accuracy 80.337%\n",
      "Epoch 11, Batch 929, LR 2.499483 Loss 7.261882, Accuracy 80.339%\n",
      "Epoch 11, Batch 930, LR 2.499492 Loss 7.262694, Accuracy 80.334%\n",
      "Epoch 11, Batch 931, LR 2.499500 Loss 7.261970, Accuracy 80.329%\n",
      "Epoch 11, Batch 932, LR 2.499509 Loss 7.262114, Accuracy 80.330%\n",
      "Epoch 11, Batch 933, LR 2.499518 Loss 7.262674, Accuracy 80.323%\n",
      "Epoch 11, Batch 934, LR 2.499526 Loss 7.261498, Accuracy 80.330%\n",
      "Epoch 11, Batch 935, LR 2.499534 Loss 7.261231, Accuracy 80.332%\n",
      "Epoch 11, Batch 936, LR 2.499543 Loss 7.259966, Accuracy 80.337%\n",
      "Epoch 11, Batch 937, LR 2.499551 Loss 7.260682, Accuracy 80.330%\n",
      "Epoch 11, Batch 938, LR 2.499559 Loss 7.260349, Accuracy 80.331%\n",
      "Epoch 11, Batch 939, LR 2.499567 Loss 7.259803, Accuracy 80.336%\n",
      "Epoch 11, Batch 940, LR 2.499575 Loss 7.260310, Accuracy 80.334%\n",
      "Epoch 11, Batch 941, LR 2.499583 Loss 7.259732, Accuracy 80.340%\n",
      "Epoch 11, Batch 942, LR 2.499591 Loss 7.259304, Accuracy 80.340%\n",
      "Epoch 11, Batch 943, LR 2.499598 Loss 7.258629, Accuracy 80.348%\n",
      "Epoch 11, Batch 944, LR 2.499606 Loss 7.258334, Accuracy 80.348%\n",
      "Epoch 11, Batch 945, LR 2.499614 Loss 7.257830, Accuracy 80.348%\n",
      "Epoch 11, Batch 946, LR 2.499621 Loss 7.257921, Accuracy 80.345%\n",
      "Epoch 11, Batch 947, LR 2.499629 Loss 7.257917, Accuracy 80.342%\n",
      "Epoch 11, Batch 948, LR 2.499636 Loss 7.258278, Accuracy 80.340%\n",
      "Epoch 11, Batch 949, LR 2.499643 Loss 7.258937, Accuracy 80.336%\n",
      "Epoch 11, Batch 950, LR 2.499651 Loss 7.258550, Accuracy 80.339%\n",
      "Epoch 11, Batch 951, LR 2.499658 Loss 7.258507, Accuracy 80.339%\n",
      "Epoch 11, Batch 952, LR 2.499665 Loss 7.258365, Accuracy 80.338%\n",
      "Epoch 11, Batch 953, LR 2.499672 Loss 7.258481, Accuracy 80.338%\n",
      "Epoch 11, Batch 954, LR 2.499679 Loss 7.259101, Accuracy 80.334%\n",
      "Epoch 11, Batch 955, LR 2.499686 Loss 7.258430, Accuracy 80.337%\n",
      "Epoch 11, Batch 956, LR 2.499693 Loss 7.258570, Accuracy 80.340%\n",
      "Epoch 11, Batch 957, LR 2.499699 Loss 7.258165, Accuracy 80.345%\n",
      "Epoch 11, Batch 958, LR 2.499706 Loss 7.258138, Accuracy 80.340%\n",
      "Epoch 11, Batch 959, LR 2.499712 Loss 7.257531, Accuracy 80.344%\n",
      "Epoch 11, Batch 960, LR 2.499719 Loss 7.256959, Accuracy 80.347%\n",
      "Epoch 11, Batch 961, LR 2.499725 Loss 7.256950, Accuracy 80.348%\n",
      "Epoch 11, Batch 962, LR 2.499732 Loss 7.256924, Accuracy 80.351%\n",
      "Epoch 11, Batch 963, LR 2.499738 Loss 7.256361, Accuracy 80.358%\n",
      "Epoch 11, Batch 964, LR 2.499744 Loss 7.256592, Accuracy 80.356%\n",
      "Epoch 11, Batch 965, LR 2.499750 Loss 7.256316, Accuracy 80.352%\n",
      "Epoch 11, Batch 966, LR 2.499756 Loss 7.256750, Accuracy 80.346%\n",
      "Epoch 11, Batch 967, LR 2.499762 Loss 7.256296, Accuracy 80.348%\n",
      "Epoch 11, Batch 968, LR 2.499768 Loss 7.256327, Accuracy 80.345%\n",
      "Epoch 11, Batch 969, LR 2.499774 Loss 7.255637, Accuracy 80.349%\n",
      "Epoch 11, Batch 970, LR 2.499780 Loss 7.255985, Accuracy 80.347%\n",
      "Epoch 11, Batch 971, LR 2.499786 Loss 7.255539, Accuracy 80.350%\n",
      "Epoch 11, Batch 972, LR 2.499791 Loss 7.255515, Accuracy 80.349%\n",
      "Epoch 11, Batch 973, LR 2.499797 Loss 7.255856, Accuracy 80.353%\n",
      "Epoch 11, Batch 974, LR 2.499802 Loss 7.255478, Accuracy 80.355%\n",
      "Epoch 11, Batch 975, LR 2.499808 Loss 7.254745, Accuracy 80.357%\n",
      "Epoch 11, Batch 976, LR 2.499813 Loss 7.255497, Accuracy 80.353%\n",
      "Epoch 11, Batch 977, LR 2.499818 Loss 7.255536, Accuracy 80.347%\n",
      "Epoch 11, Batch 978, LR 2.499823 Loss 7.255174, Accuracy 80.351%\n",
      "Epoch 11, Batch 979, LR 2.499828 Loss 7.254619, Accuracy 80.357%\n",
      "Epoch 11, Batch 980, LR 2.499833 Loss 7.254080, Accuracy 80.360%\n",
      "Epoch 11, Batch 981, LR 2.499838 Loss 7.254325, Accuracy 80.359%\n",
      "Epoch 11, Batch 982, LR 2.499843 Loss 7.254637, Accuracy 80.355%\n",
      "Epoch 11, Batch 983, LR 2.499848 Loss 7.254873, Accuracy 80.352%\n",
      "Epoch 11, Batch 984, LR 2.499853 Loss 7.254874, Accuracy 80.349%\n",
      "Epoch 11, Batch 985, LR 2.499857 Loss 7.254940, Accuracy 80.347%\n",
      "Epoch 11, Batch 986, LR 2.499862 Loss 7.254309, Accuracy 80.353%\n",
      "Epoch 11, Batch 987, LR 2.499866 Loss 7.254067, Accuracy 80.351%\n",
      "Epoch 11, Batch 988, LR 2.499871 Loss 7.254741, Accuracy 80.343%\n",
      "Epoch 11, Batch 989, LR 2.499875 Loss 7.254467, Accuracy 80.347%\n",
      "Epoch 11, Batch 990, LR 2.499879 Loss 7.254162, Accuracy 80.352%\n",
      "Epoch 11, Batch 991, LR 2.499884 Loss 7.254535, Accuracy 80.354%\n",
      "Epoch 11, Batch 992, LR 2.499888 Loss 7.254076, Accuracy 80.356%\n",
      "Epoch 11, Batch 993, LR 2.499892 Loss 7.254188, Accuracy 80.355%\n",
      "Epoch 11, Batch 994, LR 2.499896 Loss 7.254332, Accuracy 80.351%\n",
      "Epoch 11, Batch 995, LR 2.499900 Loss 7.254358, Accuracy 80.350%\n",
      "Epoch 11, Batch 996, LR 2.499903 Loss 7.254291, Accuracy 80.353%\n",
      "Epoch 11, Batch 997, LR 2.499907 Loss 7.253896, Accuracy 80.359%\n",
      "Epoch 11, Batch 998, LR 2.499911 Loss 7.253043, Accuracy 80.367%\n",
      "Epoch 11, Batch 999, LR 2.499914 Loss 7.253600, Accuracy 80.361%\n",
      "Epoch 11, Batch 1000, LR 2.499918 Loss 7.253421, Accuracy 80.361%\n",
      "Epoch 11, Batch 1001, LR 2.499921 Loss 7.253836, Accuracy 80.356%\n",
      "Epoch 11, Batch 1002, LR 2.499925 Loss 7.254327, Accuracy 80.351%\n",
      "Epoch 11, Batch 1003, LR 2.499928 Loss 7.253482, Accuracy 80.356%\n",
      "Epoch 11, Batch 1004, LR 2.499931 Loss 7.252600, Accuracy 80.359%\n",
      "Epoch 11, Batch 1005, LR 2.499935 Loss 7.252383, Accuracy 80.362%\n",
      "Epoch 11, Batch 1006, LR 2.499938 Loss 7.251999, Accuracy 80.364%\n",
      "Epoch 11, Batch 1007, LR 2.499941 Loss 7.251819, Accuracy 80.364%\n",
      "Epoch 11, Batch 1008, LR 2.499944 Loss 7.252285, Accuracy 80.359%\n",
      "Epoch 11, Batch 1009, LR 2.499946 Loss 7.251166, Accuracy 80.361%\n",
      "Epoch 11, Batch 1010, LR 2.499949 Loss 7.250440, Accuracy 80.366%\n",
      "Epoch 11, Batch 1011, LR 2.499952 Loss 7.250527, Accuracy 80.364%\n",
      "Epoch 11, Batch 1012, LR 2.499955 Loss 7.251644, Accuracy 80.355%\n",
      "Epoch 11, Batch 1013, LR 2.499957 Loss 7.251809, Accuracy 80.353%\n",
      "Epoch 11, Batch 1014, LR 2.499960 Loss 7.252691, Accuracy 80.343%\n",
      "Epoch 11, Batch 1015, LR 2.499962 Loss 7.253122, Accuracy 80.337%\n",
      "Epoch 11, Batch 1016, LR 2.499964 Loss 7.252671, Accuracy 80.339%\n",
      "Epoch 11, Batch 1017, LR 2.499967 Loss 7.252193, Accuracy 80.340%\n",
      "Epoch 11, Batch 1018, LR 2.499969 Loss 7.251531, Accuracy 80.344%\n",
      "Epoch 11, Batch 1019, LR 2.499971 Loss 7.251410, Accuracy 80.348%\n",
      "Epoch 11, Batch 1020, LR 2.499973 Loss 7.251642, Accuracy 80.346%\n",
      "Epoch 11, Batch 1021, LR 2.499975 Loss 7.251292, Accuracy 80.348%\n",
      "Epoch 11, Batch 1022, LR 2.499977 Loss 7.250751, Accuracy 80.354%\n",
      "Epoch 11, Batch 1023, LR 2.499979 Loss 7.250438, Accuracy 80.359%\n",
      "Epoch 11, Batch 1024, LR 2.499980 Loss 7.250101, Accuracy 80.364%\n",
      "Epoch 11, Batch 1025, LR 2.499982 Loss 7.250383, Accuracy 80.359%\n",
      "Epoch 11, Batch 1026, LR 2.499984 Loss 7.251237, Accuracy 80.356%\n",
      "Epoch 11, Batch 1027, LR 2.499985 Loss 7.251474, Accuracy 80.352%\n",
      "Epoch 11, Batch 1028, LR 2.499987 Loss 7.251070, Accuracy 80.355%\n",
      "Epoch 11, Batch 1029, LR 2.499988 Loss 7.250646, Accuracy 80.359%\n",
      "Epoch 11, Batch 1030, LR 2.499989 Loss 7.251012, Accuracy 80.357%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 1031, LR 2.499990 Loss 7.251053, Accuracy 80.357%\n",
      "Epoch 11, Batch 1032, LR 2.499992 Loss 7.250927, Accuracy 80.354%\n",
      "Epoch 11, Batch 1033, LR 2.499993 Loss 7.250672, Accuracy 80.356%\n",
      "Epoch 11, Batch 1034, LR 2.499994 Loss 7.250364, Accuracy 80.359%\n",
      "Epoch 11, Batch 1035, LR 2.499995 Loss 7.250529, Accuracy 80.361%\n",
      "Epoch 11, Batch 1036, LR 2.499996 Loss 7.250505, Accuracy 80.357%\n",
      "Epoch 11, Batch 1037, LR 2.499996 Loss 7.249983, Accuracy 80.360%\n",
      "Epoch 11, Batch 1038, LR 2.499997 Loss 7.249604, Accuracy 80.362%\n",
      "Epoch 11, Batch 1039, LR 2.499998 Loss 7.249629, Accuracy 80.363%\n",
      "Epoch 11, Batch 1040, LR 2.499998 Loss 7.249726, Accuracy 80.361%\n",
      "Epoch 11, Batch 1041, LR 2.499999 Loss 7.249835, Accuracy 80.361%\n",
      "Epoch 11, Batch 1042, LR 2.499999 Loss 7.249785, Accuracy 80.362%\n",
      "Epoch 11, Batch 1043, LR 2.499999 Loss 7.250188, Accuracy 80.359%\n",
      "Epoch 11, Batch 1044, LR 2.500000 Loss 7.250003, Accuracy 80.358%\n",
      "Epoch 11, Batch 1045, LR 2.500000 Loss 7.250132, Accuracy 80.356%\n",
      "Epoch 11, Batch 1046, LR 2.500000 Loss 7.249804, Accuracy 80.357%\n",
      "Epoch 11, Batch 1047, LR 2.500000 Loss 7.249717, Accuracy 80.359%\n",
      "Epoch 11, Loss (train set) 7.249717, Accuracy (train set) 80.359%\n",
      "Epoch 12, Batch 1, LR 2.500000 Loss 6.776727, Accuracy 83.594%\n",
      "Epoch 12, Batch 2, LR 2.500000 Loss 6.752089, Accuracy 83.203%\n",
      "Epoch 12, Batch 3, LR 2.500000 Loss 6.865148, Accuracy 83.073%\n",
      "Epoch 12, Batch 4, LR 2.500000 Loss 6.823176, Accuracy 83.008%\n",
      "Epoch 12, Batch 5, LR 2.500000 Loss 6.834314, Accuracy 82.812%\n",
      "Epoch 12, Batch 6, LR 2.500000 Loss 6.780391, Accuracy 83.464%\n",
      "Epoch 12, Batch 7, LR 2.500000 Loss 6.802061, Accuracy 83.594%\n",
      "Epoch 12, Batch 8, LR 2.500000 Loss 6.794775, Accuracy 83.398%\n",
      "Epoch 12, Batch 9, LR 2.499999 Loss 6.756593, Accuracy 83.594%\n",
      "Epoch 12, Batch 10, LR 2.499999 Loss 6.750458, Accuracy 84.141%\n",
      "Epoch 12, Batch 11, LR 2.499999 Loss 6.800873, Accuracy 83.807%\n",
      "Epoch 12, Batch 12, LR 2.499999 Loss 6.864297, Accuracy 83.724%\n",
      "Epoch 12, Batch 13, LR 2.499999 Loss 6.827587, Accuracy 83.714%\n",
      "Epoch 12, Batch 14, LR 2.499999 Loss 6.851592, Accuracy 83.092%\n",
      "Epoch 12, Batch 15, LR 2.499998 Loss 6.878102, Accuracy 82.708%\n",
      "Epoch 12, Batch 16, LR 2.499998 Loss 6.915312, Accuracy 82.471%\n",
      "Epoch 12, Batch 17, LR 2.499998 Loss 6.945635, Accuracy 82.261%\n",
      "Epoch 12, Batch 18, LR 2.499998 Loss 7.004598, Accuracy 82.031%\n",
      "Epoch 12, Batch 19, LR 2.499997 Loss 7.000375, Accuracy 81.908%\n",
      "Epoch 12, Batch 20, LR 2.499997 Loss 6.997860, Accuracy 81.719%\n",
      "Epoch 12, Batch 21, LR 2.499997 Loss 6.970351, Accuracy 81.659%\n",
      "Epoch 12, Batch 22, LR 2.499997 Loss 6.938784, Accuracy 81.889%\n",
      "Epoch 12, Batch 23, LR 2.499996 Loss 6.916775, Accuracy 81.997%\n",
      "Epoch 12, Batch 24, LR 2.499996 Loss 6.906702, Accuracy 82.227%\n",
      "Epoch 12, Batch 25, LR 2.499996 Loss 6.903418, Accuracy 82.281%\n",
      "Epoch 12, Batch 26, LR 2.499995 Loss 6.869957, Accuracy 82.482%\n",
      "Epoch 12, Batch 27, LR 2.499995 Loss 6.881972, Accuracy 82.263%\n",
      "Epoch 12, Batch 28, LR 2.499994 Loss 6.861740, Accuracy 82.254%\n",
      "Epoch 12, Batch 29, LR 2.499994 Loss 6.865401, Accuracy 82.220%\n",
      "Epoch 12, Batch 30, LR 2.499994 Loss 6.848787, Accuracy 82.318%\n",
      "Epoch 12, Batch 31, LR 2.499993 Loss 6.827294, Accuracy 82.434%\n",
      "Epoch 12, Batch 32, LR 2.499993 Loss 6.846469, Accuracy 82.324%\n",
      "Epoch 12, Batch 33, LR 2.499992 Loss 6.846671, Accuracy 82.386%\n",
      "Epoch 12, Batch 34, LR 2.499992 Loss 6.853316, Accuracy 82.238%\n",
      "Epoch 12, Batch 35, LR 2.499991 Loss 6.868024, Accuracy 82.210%\n",
      "Epoch 12, Batch 36, LR 2.499991 Loss 6.868359, Accuracy 82.335%\n",
      "Epoch 12, Batch 37, LR 2.499990 Loss 6.874148, Accuracy 82.327%\n",
      "Epoch 12, Batch 38, LR 2.499990 Loss 6.894028, Accuracy 82.175%\n",
      "Epoch 12, Batch 39, LR 2.499989 Loss 6.899906, Accuracy 82.131%\n",
      "Epoch 12, Batch 40, LR 2.499989 Loss 6.939471, Accuracy 81.914%\n",
      "Epoch 12, Batch 41, LR 2.499988 Loss 6.943041, Accuracy 81.955%\n",
      "Epoch 12, Batch 42, LR 2.499987 Loss 6.936664, Accuracy 82.106%\n",
      "Epoch 12, Batch 43, LR 2.499987 Loss 6.925805, Accuracy 82.086%\n",
      "Epoch 12, Batch 44, LR 2.499986 Loss 6.919699, Accuracy 82.120%\n",
      "Epoch 12, Batch 45, LR 2.499985 Loss 6.900351, Accuracy 82.170%\n",
      "Epoch 12, Batch 46, LR 2.499985 Loss 6.899385, Accuracy 82.252%\n",
      "Epoch 12, Batch 47, LR 2.499984 Loss 6.911736, Accuracy 82.164%\n",
      "Epoch 12, Batch 48, LR 2.499983 Loss 6.922400, Accuracy 82.096%\n",
      "Epoch 12, Batch 49, LR 2.499983 Loss 6.907979, Accuracy 82.223%\n",
      "Epoch 12, Batch 50, LR 2.499982 Loss 6.903682, Accuracy 82.297%\n",
      "Epoch 12, Batch 51, LR 2.499981 Loss 6.894964, Accuracy 82.399%\n",
      "Epoch 12, Batch 52, LR 2.499981 Loss 6.888147, Accuracy 82.497%\n",
      "Epoch 12, Batch 53, LR 2.499980 Loss 6.898671, Accuracy 82.415%\n",
      "Epoch 12, Batch 54, LR 2.499979 Loss 6.905676, Accuracy 82.378%\n",
      "Epoch 12, Batch 55, LR 2.499978 Loss 6.904833, Accuracy 82.401%\n",
      "Epoch 12, Batch 56, LR 2.499977 Loss 6.898322, Accuracy 82.408%\n",
      "Epoch 12, Batch 57, LR 2.499977 Loss 6.890260, Accuracy 82.415%\n",
      "Epoch 12, Batch 58, LR 2.499976 Loss 6.888172, Accuracy 82.381%\n",
      "Epoch 12, Batch 59, LR 2.499975 Loss 6.876627, Accuracy 82.428%\n",
      "Epoch 12, Batch 60, LR 2.499974 Loss 6.886506, Accuracy 82.357%\n",
      "Epoch 12, Batch 61, LR 2.499973 Loss 6.886248, Accuracy 82.415%\n",
      "Epoch 12, Batch 62, LR 2.499972 Loss 6.895682, Accuracy 82.422%\n",
      "Epoch 12, Batch 63, LR 2.499972 Loss 6.890761, Accuracy 82.428%\n",
      "Epoch 12, Batch 64, LR 2.499971 Loss 6.892302, Accuracy 82.422%\n",
      "Epoch 12, Batch 65, LR 2.499970 Loss 6.888927, Accuracy 82.452%\n",
      "Epoch 12, Batch 66, LR 2.499969 Loss 6.898917, Accuracy 82.363%\n",
      "Epoch 12, Batch 67, LR 2.499968 Loss 6.907383, Accuracy 82.334%\n",
      "Epoch 12, Batch 68, LR 2.499967 Loss 6.907175, Accuracy 82.399%\n",
      "Epoch 12, Batch 69, LR 2.499966 Loss 6.915496, Accuracy 82.416%\n",
      "Epoch 12, Batch 70, LR 2.499965 Loss 6.928307, Accuracy 82.266%\n",
      "Epoch 12, Batch 71, LR 2.499964 Loss 6.927705, Accuracy 82.262%\n",
      "Epoch 12, Batch 72, LR 2.499963 Loss 6.938061, Accuracy 82.237%\n",
      "Epoch 12, Batch 73, LR 2.499962 Loss 6.944045, Accuracy 82.267%\n",
      "Epoch 12, Batch 74, LR 2.499961 Loss 6.944501, Accuracy 82.264%\n",
      "Epoch 12, Batch 75, LR 2.499960 Loss 6.940630, Accuracy 82.271%\n",
      "Epoch 12, Batch 76, LR 2.499959 Loss 6.936840, Accuracy 82.299%\n",
      "Epoch 12, Batch 77, LR 2.499957 Loss 6.941176, Accuracy 82.305%\n",
      "Epoch 12, Batch 78, LR 2.499956 Loss 6.943638, Accuracy 82.252%\n",
      "Epoch 12, Batch 79, LR 2.499955 Loss 6.945037, Accuracy 82.239%\n",
      "Epoch 12, Batch 80, LR 2.499954 Loss 6.943649, Accuracy 82.246%\n",
      "Epoch 12, Batch 81, LR 2.499953 Loss 6.951156, Accuracy 82.166%\n",
      "Epoch 12, Batch 82, LR 2.499952 Loss 6.959249, Accuracy 82.107%\n",
      "Epoch 12, Batch 83, LR 2.499951 Loss 6.959593, Accuracy 82.116%\n",
      "Epoch 12, Batch 84, LR 2.499949 Loss 6.970543, Accuracy 82.050%\n",
      "Epoch 12, Batch 85, LR 2.499948 Loss 6.969172, Accuracy 82.031%\n",
      "Epoch 12, Batch 86, LR 2.499947 Loss 6.960532, Accuracy 82.086%\n",
      "Epoch 12, Batch 87, LR 2.499946 Loss 6.966567, Accuracy 82.040%\n",
      "Epoch 12, Batch 88, LR 2.499944 Loss 6.965987, Accuracy 82.058%\n",
      "Epoch 12, Batch 89, LR 2.499943 Loss 6.960757, Accuracy 82.128%\n",
      "Epoch 12, Batch 90, LR 2.499942 Loss 6.971434, Accuracy 82.092%\n",
      "Epoch 12, Batch 91, LR 2.499941 Loss 6.985220, Accuracy 81.988%\n",
      "Epoch 12, Batch 92, LR 2.499939 Loss 6.986120, Accuracy 81.989%\n",
      "Epoch 12, Batch 93, LR 2.499938 Loss 6.987703, Accuracy 81.964%\n",
      "Epoch 12, Batch 94, LR 2.499937 Loss 6.987743, Accuracy 81.965%\n",
      "Epoch 12, Batch 95, LR 2.499935 Loss 6.990097, Accuracy 81.933%\n",
      "Epoch 12, Batch 96, LR 2.499934 Loss 6.982263, Accuracy 82.007%\n",
      "Epoch 12, Batch 97, LR 2.499932 Loss 6.988091, Accuracy 82.007%\n",
      "Epoch 12, Batch 98, LR 2.499931 Loss 6.993061, Accuracy 81.975%\n",
      "Epoch 12, Batch 99, LR 2.499930 Loss 6.990956, Accuracy 81.944%\n",
      "Epoch 12, Batch 100, LR 2.499928 Loss 6.980536, Accuracy 82.008%\n",
      "Epoch 12, Batch 101, LR 2.499927 Loss 6.978736, Accuracy 82.016%\n",
      "Epoch 12, Batch 102, LR 2.499925 Loss 6.980861, Accuracy 82.008%\n",
      "Epoch 12, Batch 103, LR 2.499924 Loss 6.981191, Accuracy 82.001%\n",
      "Epoch 12, Batch 104, LR 2.499922 Loss 6.978294, Accuracy 82.031%\n",
      "Epoch 12, Batch 105, LR 2.499921 Loss 6.975723, Accuracy 82.024%\n",
      "Epoch 12, Batch 106, LR 2.499919 Loss 6.972550, Accuracy 82.039%\n",
      "Epoch 12, Batch 107, LR 2.499918 Loss 6.973623, Accuracy 82.002%\n",
      "Epoch 12, Batch 108, LR 2.499916 Loss 6.976766, Accuracy 82.031%\n",
      "Epoch 12, Batch 109, LR 2.499915 Loss 6.973154, Accuracy 82.053%\n",
      "Epoch 12, Batch 110, LR 2.499913 Loss 6.986784, Accuracy 81.932%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 111, LR 2.499912 Loss 6.984492, Accuracy 81.975%\n",
      "Epoch 12, Batch 112, LR 2.499910 Loss 6.985683, Accuracy 81.955%\n",
      "Epoch 12, Batch 113, LR 2.499908 Loss 6.982907, Accuracy 81.962%\n",
      "Epoch 12, Batch 114, LR 2.499907 Loss 6.986033, Accuracy 81.963%\n",
      "Epoch 12, Batch 115, LR 2.499905 Loss 6.990687, Accuracy 81.929%\n",
      "Epoch 12, Batch 116, LR 2.499903 Loss 6.988734, Accuracy 81.917%\n",
      "Epoch 12, Batch 117, LR 2.499902 Loss 6.991342, Accuracy 81.898%\n",
      "Epoch 12, Batch 118, LR 2.499900 Loss 6.993759, Accuracy 81.866%\n",
      "Epoch 12, Batch 119, LR 2.499898 Loss 6.996875, Accuracy 81.861%\n",
      "Epoch 12, Batch 120, LR 2.499897 Loss 7.000326, Accuracy 81.849%\n",
      "Epoch 12, Batch 121, LR 2.499895 Loss 6.995410, Accuracy 81.863%\n",
      "Epoch 12, Batch 122, LR 2.499893 Loss 6.991706, Accuracy 81.878%\n",
      "Epoch 12, Batch 123, LR 2.499891 Loss 6.988099, Accuracy 81.885%\n",
      "Epoch 12, Batch 124, LR 2.499890 Loss 6.991674, Accuracy 81.867%\n",
      "Epoch 12, Batch 125, LR 2.499888 Loss 6.992195, Accuracy 81.881%\n",
      "Epoch 12, Batch 126, LR 2.499886 Loss 6.994550, Accuracy 81.851%\n",
      "Epoch 12, Batch 127, LR 2.499884 Loss 6.991979, Accuracy 81.871%\n",
      "Epoch 12, Batch 128, LR 2.499882 Loss 6.999031, Accuracy 81.805%\n",
      "Epoch 12, Batch 129, LR 2.499881 Loss 7.002397, Accuracy 81.789%\n",
      "Epoch 12, Batch 130, LR 2.499879 Loss 6.998125, Accuracy 81.815%\n",
      "Epoch 12, Batch 131, LR 2.499877 Loss 6.997540, Accuracy 81.817%\n",
      "Epoch 12, Batch 132, LR 2.499875 Loss 6.994675, Accuracy 81.848%\n",
      "Epoch 12, Batch 133, LR 2.499873 Loss 6.997486, Accuracy 81.820%\n",
      "Epoch 12, Batch 134, LR 2.499871 Loss 6.995414, Accuracy 81.856%\n",
      "Epoch 12, Batch 135, LR 2.499869 Loss 6.992396, Accuracy 81.904%\n",
      "Epoch 12, Batch 136, LR 2.499867 Loss 6.990755, Accuracy 81.922%\n",
      "Epoch 12, Batch 137, LR 2.499865 Loss 6.990368, Accuracy 81.923%\n",
      "Epoch 12, Batch 138, LR 2.499863 Loss 6.990139, Accuracy 81.924%\n",
      "Epoch 12, Batch 139, LR 2.499861 Loss 6.986931, Accuracy 81.930%\n",
      "Epoch 12, Batch 140, LR 2.499859 Loss 6.989829, Accuracy 81.892%\n",
      "Epoch 12, Batch 141, LR 2.499857 Loss 6.987845, Accuracy 81.909%\n",
      "Epoch 12, Batch 142, LR 2.499855 Loss 6.988940, Accuracy 81.910%\n",
      "Epoch 12, Batch 143, LR 2.499853 Loss 6.989370, Accuracy 81.906%\n",
      "Epoch 12, Batch 144, LR 2.499851 Loss 6.985240, Accuracy 81.939%\n",
      "Epoch 12, Batch 145, LR 2.499849 Loss 6.990002, Accuracy 81.913%\n",
      "Epoch 12, Batch 146, LR 2.499847 Loss 6.989153, Accuracy 81.903%\n",
      "Epoch 12, Batch 147, LR 2.499845 Loss 6.994597, Accuracy 81.851%\n",
      "Epoch 12, Batch 148, LR 2.499843 Loss 6.993377, Accuracy 81.883%\n",
      "Epoch 12, Batch 149, LR 2.499841 Loss 6.991368, Accuracy 81.900%\n",
      "Epoch 12, Batch 150, LR 2.499839 Loss 6.992046, Accuracy 81.896%\n",
      "Epoch 12, Batch 151, LR 2.499836 Loss 6.997476, Accuracy 81.866%\n",
      "Epoch 12, Batch 152, LR 2.499834 Loss 6.993291, Accuracy 81.892%\n",
      "Epoch 12, Batch 153, LR 2.499832 Loss 6.992441, Accuracy 81.893%\n",
      "Epoch 12, Batch 154, LR 2.499830 Loss 6.988666, Accuracy 81.935%\n",
      "Epoch 12, Batch 155, LR 2.499828 Loss 6.988012, Accuracy 81.941%\n",
      "Epoch 12, Batch 156, LR 2.499825 Loss 6.984733, Accuracy 81.961%\n",
      "Epoch 12, Batch 157, LR 2.499823 Loss 6.985115, Accuracy 81.957%\n",
      "Epoch 12, Batch 158, LR 2.499821 Loss 6.984831, Accuracy 81.947%\n",
      "Epoch 12, Batch 159, LR 2.499819 Loss 6.987153, Accuracy 81.908%\n",
      "Epoch 12, Batch 160, LR 2.499816 Loss 6.986002, Accuracy 81.909%\n",
      "Epoch 12, Batch 161, LR 2.499814 Loss 6.987332, Accuracy 81.900%\n",
      "Epoch 12, Batch 162, LR 2.499812 Loss 6.987200, Accuracy 81.930%\n",
      "Epoch 12, Batch 163, LR 2.499809 Loss 6.986683, Accuracy 81.926%\n",
      "Epoch 12, Batch 164, LR 2.499807 Loss 6.987611, Accuracy 81.926%\n",
      "Epoch 12, Batch 165, LR 2.499805 Loss 6.989095, Accuracy 81.927%\n",
      "Epoch 12, Batch 166, LR 2.499802 Loss 6.991319, Accuracy 81.909%\n",
      "Epoch 12, Batch 167, LR 2.499800 Loss 6.986644, Accuracy 81.942%\n",
      "Epoch 12, Batch 168, LR 2.499797 Loss 6.987398, Accuracy 81.938%\n",
      "Epoch 12, Batch 169, LR 2.499795 Loss 6.986345, Accuracy 81.920%\n",
      "Epoch 12, Batch 170, LR 2.499793 Loss 6.987229, Accuracy 81.907%\n",
      "Epoch 12, Batch 171, LR 2.499790 Loss 6.986144, Accuracy 81.903%\n",
      "Epoch 12, Batch 172, LR 2.499788 Loss 6.988568, Accuracy 81.886%\n",
      "Epoch 12, Batch 173, LR 2.499785 Loss 6.988024, Accuracy 81.882%\n",
      "Epoch 12, Batch 174, LR 2.499783 Loss 6.988051, Accuracy 81.892%\n",
      "Epoch 12, Batch 175, LR 2.499780 Loss 6.986686, Accuracy 81.897%\n",
      "Epoch 12, Batch 176, LR 2.499778 Loss 6.986731, Accuracy 81.907%\n",
      "Epoch 12, Batch 177, LR 2.499775 Loss 6.989776, Accuracy 81.899%\n",
      "Epoch 12, Batch 178, LR 2.499773 Loss 6.986452, Accuracy 81.922%\n",
      "Epoch 12, Batch 179, LR 2.499770 Loss 6.989935, Accuracy 81.909%\n",
      "Epoch 12, Batch 180, LR 2.499767 Loss 6.991036, Accuracy 81.910%\n",
      "Epoch 12, Batch 181, LR 2.499765 Loss 6.986394, Accuracy 81.923%\n",
      "Epoch 12, Batch 182, LR 2.499762 Loss 6.983333, Accuracy 81.958%\n",
      "Epoch 12, Batch 183, LR 2.499760 Loss 6.981142, Accuracy 81.954%\n",
      "Epoch 12, Batch 184, LR 2.499757 Loss 6.982242, Accuracy 81.942%\n",
      "Epoch 12, Batch 185, LR 2.499754 Loss 6.983558, Accuracy 81.900%\n",
      "Epoch 12, Batch 186, LR 2.499752 Loss 6.985360, Accuracy 81.893%\n",
      "Epoch 12, Batch 187, LR 2.499749 Loss 6.986432, Accuracy 81.885%\n",
      "Epoch 12, Batch 188, LR 2.499746 Loss 6.989373, Accuracy 81.861%\n",
      "Epoch 12, Batch 189, LR 2.499744 Loss 6.991329, Accuracy 81.845%\n",
      "Epoch 12, Batch 190, LR 2.499741 Loss 6.988273, Accuracy 81.854%\n",
      "Epoch 12, Batch 191, LR 2.499738 Loss 6.988137, Accuracy 81.843%\n",
      "Epoch 12, Batch 192, LR 2.499735 Loss 6.987820, Accuracy 81.848%\n",
      "Epoch 12, Batch 193, LR 2.499733 Loss 6.990106, Accuracy 81.833%\n",
      "Epoch 12, Batch 194, LR 2.499730 Loss 6.989628, Accuracy 81.822%\n",
      "Epoch 12, Batch 195, LR 2.499727 Loss 6.991173, Accuracy 81.815%\n",
      "Epoch 12, Batch 196, LR 2.499724 Loss 6.989957, Accuracy 81.804%\n",
      "Epoch 12, Batch 197, LR 2.499721 Loss 6.990431, Accuracy 81.817%\n",
      "Epoch 12, Batch 198, LR 2.499719 Loss 6.993470, Accuracy 81.806%\n",
      "Epoch 12, Batch 199, LR 2.499716 Loss 6.992019, Accuracy 81.819%\n",
      "Epoch 12, Batch 200, LR 2.499713 Loss 6.993801, Accuracy 81.809%\n",
      "Epoch 12, Batch 201, LR 2.499710 Loss 6.990814, Accuracy 81.833%\n",
      "Epoch 12, Batch 202, LR 2.499707 Loss 6.988271, Accuracy 81.834%\n",
      "Epoch 12, Batch 203, LR 2.499704 Loss 6.986303, Accuracy 81.843%\n",
      "Epoch 12, Batch 204, LR 2.499701 Loss 6.985610, Accuracy 81.840%\n",
      "Epoch 12, Batch 205, LR 2.499698 Loss 6.983500, Accuracy 81.845%\n",
      "Epoch 12, Batch 206, LR 2.499695 Loss 6.982837, Accuracy 81.842%\n",
      "Epoch 12, Batch 207, LR 2.499692 Loss 6.982228, Accuracy 81.843%\n",
      "Epoch 12, Batch 208, LR 2.499689 Loss 6.981022, Accuracy 81.858%\n",
      "Epoch 12, Batch 209, LR 2.499686 Loss 6.979585, Accuracy 81.878%\n",
      "Epoch 12, Batch 210, LR 2.499683 Loss 6.980363, Accuracy 81.882%\n",
      "Epoch 12, Batch 211, LR 2.499680 Loss 6.978518, Accuracy 81.887%\n",
      "Epoch 12, Batch 212, LR 2.499677 Loss 6.979948, Accuracy 81.869%\n",
      "Epoch 12, Batch 213, LR 2.499674 Loss 6.981015, Accuracy 81.852%\n",
      "Epoch 12, Batch 214, LR 2.499671 Loss 6.983227, Accuracy 81.823%\n",
      "Epoch 12, Batch 215, LR 2.499668 Loss 6.985103, Accuracy 81.817%\n",
      "Epoch 12, Batch 216, LR 2.499665 Loss 6.984711, Accuracy 81.821%\n",
      "Epoch 12, Batch 217, LR 2.499662 Loss 6.982781, Accuracy 81.833%\n",
      "Epoch 12, Batch 218, LR 2.499659 Loss 6.979911, Accuracy 81.834%\n",
      "Epoch 12, Batch 219, LR 2.499656 Loss 6.982472, Accuracy 81.831%\n",
      "Epoch 12, Batch 220, LR 2.499653 Loss 6.981362, Accuracy 81.825%\n",
      "Epoch 12, Batch 221, LR 2.499649 Loss 6.982821, Accuracy 81.830%\n",
      "Epoch 12, Batch 222, LR 2.499646 Loss 6.983305, Accuracy 81.852%\n",
      "Epoch 12, Batch 223, LR 2.499643 Loss 6.981612, Accuracy 81.867%\n",
      "Epoch 12, Batch 224, LR 2.499640 Loss 6.983323, Accuracy 81.843%\n",
      "Epoch 12, Batch 225, LR 2.499637 Loss 6.982683, Accuracy 81.844%\n",
      "Epoch 12, Batch 226, LR 2.499633 Loss 6.980456, Accuracy 81.848%\n",
      "Epoch 12, Batch 227, LR 2.499630 Loss 6.981167, Accuracy 81.839%\n",
      "Epoch 12, Batch 228, LR 2.499627 Loss 6.980201, Accuracy 81.836%\n",
      "Epoch 12, Batch 229, LR 2.499624 Loss 6.982711, Accuracy 81.833%\n",
      "Epoch 12, Batch 230, LR 2.499620 Loss 6.979628, Accuracy 81.851%\n",
      "Epoch 12, Batch 231, LR 2.499617 Loss 6.979118, Accuracy 81.845%\n",
      "Epoch 12, Batch 232, LR 2.499614 Loss 6.979606, Accuracy 81.833%\n",
      "Epoch 12, Batch 233, LR 2.499610 Loss 6.980426, Accuracy 81.817%\n",
      "Epoch 12, Batch 234, LR 2.499607 Loss 6.978085, Accuracy 81.824%\n",
      "Epoch 12, Batch 235, LR 2.499604 Loss 6.976518, Accuracy 81.828%\n",
      "Epoch 12, Batch 236, LR 2.499600 Loss 6.978302, Accuracy 81.833%\n",
      "Epoch 12, Batch 237, LR 2.499597 Loss 6.977671, Accuracy 81.840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 238, LR 2.499593 Loss 6.978679, Accuracy 81.851%\n",
      "Epoch 12, Batch 239, LR 2.499590 Loss 6.979605, Accuracy 81.832%\n",
      "Epoch 12, Batch 240, LR 2.499587 Loss 6.980013, Accuracy 81.829%\n",
      "Epoch 12, Batch 241, LR 2.499583 Loss 6.980393, Accuracy 81.811%\n",
      "Epoch 12, Batch 242, LR 2.499580 Loss 6.977662, Accuracy 81.825%\n",
      "Epoch 12, Batch 243, LR 2.499576 Loss 6.979708, Accuracy 81.809%\n",
      "Epoch 12, Batch 244, LR 2.499573 Loss 6.978156, Accuracy 81.820%\n",
      "Epoch 12, Batch 245, LR 2.499569 Loss 6.977283, Accuracy 81.834%\n",
      "Epoch 12, Batch 246, LR 2.499566 Loss 6.977128, Accuracy 81.825%\n",
      "Epoch 12, Batch 247, LR 2.499562 Loss 6.977081, Accuracy 81.829%\n",
      "Epoch 12, Batch 248, LR 2.499559 Loss 6.975784, Accuracy 81.833%\n",
      "Epoch 12, Batch 249, LR 2.499555 Loss 6.978870, Accuracy 81.821%\n",
      "Epoch 12, Batch 250, LR 2.499551 Loss 6.980128, Accuracy 81.812%\n",
      "Epoch 12, Batch 251, LR 2.499548 Loss 6.978551, Accuracy 81.820%\n",
      "Epoch 12, Batch 252, LR 2.499544 Loss 6.979071, Accuracy 81.814%\n",
      "Epoch 12, Batch 253, LR 2.499541 Loss 6.979957, Accuracy 81.815%\n",
      "Epoch 12, Batch 254, LR 2.499537 Loss 6.979089, Accuracy 81.825%\n",
      "Epoch 12, Batch 255, LR 2.499533 Loss 6.982010, Accuracy 81.814%\n",
      "Epoch 12, Batch 256, LR 2.499530 Loss 6.982025, Accuracy 81.818%\n",
      "Epoch 12, Batch 257, LR 2.499526 Loss 6.980450, Accuracy 81.815%\n",
      "Epoch 12, Batch 258, LR 2.499522 Loss 6.981028, Accuracy 81.813%\n",
      "Epoch 12, Batch 259, LR 2.499519 Loss 6.981049, Accuracy 81.817%\n",
      "Epoch 12, Batch 260, LR 2.499515 Loss 6.979279, Accuracy 81.827%\n",
      "Epoch 12, Batch 261, LR 2.499511 Loss 6.977994, Accuracy 81.846%\n",
      "Epoch 12, Batch 262, LR 2.499507 Loss 6.976528, Accuracy 81.849%\n",
      "Epoch 12, Batch 263, LR 2.499504 Loss 6.975749, Accuracy 81.850%\n",
      "Epoch 12, Batch 264, LR 2.499500 Loss 6.975285, Accuracy 81.842%\n",
      "Epoch 12, Batch 265, LR 2.499496 Loss 6.978031, Accuracy 81.813%\n",
      "Epoch 12, Batch 266, LR 2.499492 Loss 6.976404, Accuracy 81.829%\n",
      "Epoch 12, Batch 267, LR 2.499488 Loss 6.976361, Accuracy 81.844%\n",
      "Epoch 12, Batch 268, LR 2.499485 Loss 6.974736, Accuracy 81.848%\n",
      "Epoch 12, Batch 269, LR 2.499481 Loss 6.975723, Accuracy 81.822%\n",
      "Epoch 12, Batch 270, LR 2.499477 Loss 6.974003, Accuracy 81.829%\n",
      "Epoch 12, Batch 271, LR 2.499473 Loss 6.975416, Accuracy 81.806%\n",
      "Epoch 12, Batch 272, LR 2.499469 Loss 6.973782, Accuracy 81.813%\n",
      "Epoch 12, Batch 273, LR 2.499465 Loss 6.972898, Accuracy 81.834%\n",
      "Epoch 12, Batch 274, LR 2.499461 Loss 6.973514, Accuracy 81.823%\n",
      "Epoch 12, Batch 275, LR 2.499457 Loss 6.971537, Accuracy 81.832%\n",
      "Epoch 12, Batch 276, LR 2.499453 Loss 6.973775, Accuracy 81.808%\n",
      "Epoch 12, Batch 277, LR 2.499449 Loss 6.975363, Accuracy 81.808%\n",
      "Epoch 12, Batch 278, LR 2.499445 Loss 6.975623, Accuracy 81.818%\n",
      "Epoch 12, Batch 279, LR 2.499441 Loss 6.974946, Accuracy 81.832%\n",
      "Epoch 12, Batch 280, LR 2.499437 Loss 6.975528, Accuracy 81.825%\n",
      "Epoch 12, Batch 281, LR 2.499433 Loss 6.976954, Accuracy 81.814%\n",
      "Epoch 12, Batch 282, LR 2.499429 Loss 6.976295, Accuracy 81.832%\n",
      "Epoch 12, Batch 283, LR 2.499425 Loss 6.974501, Accuracy 81.857%\n",
      "Epoch 12, Batch 284, LR 2.499421 Loss 6.975692, Accuracy 81.844%\n",
      "Epoch 12, Batch 285, LR 2.499417 Loss 6.978750, Accuracy 81.826%\n",
      "Epoch 12, Batch 286, LR 2.499413 Loss 6.977892, Accuracy 81.837%\n",
      "Epoch 12, Batch 287, LR 2.499409 Loss 6.976209, Accuracy 81.860%\n",
      "Epoch 12, Batch 288, LR 2.499405 Loss 6.975173, Accuracy 81.874%\n",
      "Epoch 12, Batch 289, LR 2.499401 Loss 6.974089, Accuracy 81.883%\n",
      "Epoch 12, Batch 290, LR 2.499396 Loss 6.975303, Accuracy 81.870%\n",
      "Epoch 12, Batch 291, LR 2.499392 Loss 6.974667, Accuracy 81.873%\n",
      "Epoch 12, Batch 292, LR 2.499388 Loss 6.977377, Accuracy 81.868%\n",
      "Epoch 12, Batch 293, LR 2.499384 Loss 6.978887, Accuracy 81.863%\n",
      "Epoch 12, Batch 294, LR 2.499380 Loss 6.977856, Accuracy 81.864%\n",
      "Epoch 12, Batch 295, LR 2.499375 Loss 6.977570, Accuracy 81.870%\n",
      "Epoch 12, Batch 296, LR 2.499371 Loss 6.977938, Accuracy 81.870%\n",
      "Epoch 12, Batch 297, LR 2.499367 Loss 6.981028, Accuracy 81.852%\n",
      "Epoch 12, Batch 298, LR 2.499363 Loss 6.982616, Accuracy 81.837%\n",
      "Epoch 12, Batch 299, LR 2.499358 Loss 6.982014, Accuracy 81.848%\n",
      "Epoch 12, Batch 300, LR 2.499354 Loss 6.980386, Accuracy 81.859%\n",
      "Epoch 12, Batch 301, LR 2.499350 Loss 6.982937, Accuracy 81.842%\n",
      "Epoch 12, Batch 302, LR 2.499345 Loss 6.980387, Accuracy 81.850%\n",
      "Epoch 12, Batch 303, LR 2.499341 Loss 6.979859, Accuracy 81.848%\n",
      "Epoch 12, Batch 304, LR 2.499337 Loss 6.978302, Accuracy 81.867%\n",
      "Epoch 12, Batch 305, LR 2.499332 Loss 6.977342, Accuracy 81.875%\n",
      "Epoch 12, Batch 306, LR 2.499328 Loss 6.977845, Accuracy 81.881%\n",
      "Epoch 12, Batch 307, LR 2.499324 Loss 6.978802, Accuracy 81.876%\n",
      "Epoch 12, Batch 308, LR 2.499319 Loss 6.980568, Accuracy 81.859%\n",
      "Epoch 12, Batch 309, LR 2.499315 Loss 6.981151, Accuracy 81.852%\n",
      "Epoch 12, Batch 310, LR 2.499310 Loss 6.981280, Accuracy 81.860%\n",
      "Epoch 12, Batch 311, LR 2.499306 Loss 6.979972, Accuracy 81.863%\n",
      "Epoch 12, Batch 312, LR 2.499301 Loss 6.980405, Accuracy 81.866%\n",
      "Epoch 12, Batch 313, LR 2.499297 Loss 6.982188, Accuracy 81.854%\n",
      "Epoch 12, Batch 314, LR 2.499292 Loss 6.982485, Accuracy 81.842%\n",
      "Epoch 12, Batch 315, LR 2.499288 Loss 6.983488, Accuracy 81.823%\n",
      "Epoch 12, Batch 316, LR 2.499283 Loss 6.982680, Accuracy 81.831%\n",
      "Epoch 12, Batch 317, LR 2.499279 Loss 6.982678, Accuracy 81.822%\n",
      "Epoch 12, Batch 318, LR 2.499274 Loss 6.983245, Accuracy 81.822%\n",
      "Epoch 12, Batch 319, LR 2.499270 Loss 6.983636, Accuracy 81.821%\n",
      "Epoch 12, Batch 320, LR 2.499265 Loss 6.985513, Accuracy 81.819%\n",
      "Epoch 12, Batch 321, LR 2.499261 Loss 6.985127, Accuracy 81.832%\n",
      "Epoch 12, Batch 322, LR 2.499256 Loss 6.986653, Accuracy 81.815%\n",
      "Epoch 12, Batch 323, LR 2.499251 Loss 6.988400, Accuracy 81.806%\n",
      "Epoch 12, Batch 324, LR 2.499247 Loss 6.988181, Accuracy 81.814%\n",
      "Epoch 12, Batch 325, LR 2.499242 Loss 6.985988, Accuracy 81.817%\n",
      "Epoch 12, Batch 326, LR 2.499237 Loss 6.986686, Accuracy 81.818%\n",
      "Epoch 12, Batch 327, LR 2.499233 Loss 6.987388, Accuracy 81.811%\n",
      "Epoch 12, Batch 328, LR 2.499228 Loss 6.987324, Accuracy 81.822%\n",
      "Epoch 12, Batch 329, LR 2.499223 Loss 6.987195, Accuracy 81.834%\n",
      "Epoch 12, Batch 330, LR 2.499218 Loss 6.988403, Accuracy 81.828%\n",
      "Epoch 12, Batch 331, LR 2.499214 Loss 6.988158, Accuracy 81.833%\n",
      "Epoch 12, Batch 332, LR 2.499209 Loss 6.986646, Accuracy 81.834%\n",
      "Epoch 12, Batch 333, LR 2.499204 Loss 6.985342, Accuracy 81.844%\n",
      "Epoch 12, Batch 334, LR 2.499199 Loss 6.984993, Accuracy 81.849%\n",
      "Epoch 12, Batch 335, LR 2.499195 Loss 6.985375, Accuracy 81.849%\n",
      "Epoch 12, Batch 336, LR 2.499190 Loss 6.984522, Accuracy 81.852%\n",
      "Epoch 12, Batch 337, LR 2.499185 Loss 6.984413, Accuracy 81.841%\n",
      "Epoch 12, Batch 338, LR 2.499180 Loss 6.983000, Accuracy 81.851%\n",
      "Epoch 12, Batch 339, LR 2.499175 Loss 6.981940, Accuracy 81.863%\n",
      "Epoch 12, Batch 340, LR 2.499170 Loss 6.982681, Accuracy 81.847%\n",
      "Epoch 12, Batch 341, LR 2.499165 Loss 6.982496, Accuracy 81.848%\n",
      "Epoch 12, Batch 342, LR 2.499161 Loss 6.982330, Accuracy 81.853%\n",
      "Epoch 12, Batch 343, LR 2.499156 Loss 6.979757, Accuracy 81.858%\n",
      "Epoch 12, Batch 344, LR 2.499151 Loss 6.982178, Accuracy 81.854%\n",
      "Epoch 12, Batch 345, LR 2.499146 Loss 6.983596, Accuracy 81.855%\n",
      "Epoch 12, Batch 346, LR 2.499141 Loss 6.982763, Accuracy 81.857%\n",
      "Epoch 12, Batch 347, LR 2.499136 Loss 6.979686, Accuracy 81.876%\n",
      "Epoch 12, Batch 348, LR 2.499131 Loss 6.977875, Accuracy 81.885%\n",
      "Epoch 12, Batch 349, LR 2.499126 Loss 6.978901, Accuracy 81.870%\n",
      "Epoch 12, Batch 350, LR 2.499121 Loss 6.977390, Accuracy 81.877%\n",
      "Epoch 12, Batch 351, LR 2.499116 Loss 6.977362, Accuracy 81.869%\n",
      "Epoch 12, Batch 352, LR 2.499111 Loss 6.977990, Accuracy 81.860%\n",
      "Epoch 12, Batch 353, LR 2.499106 Loss 6.977092, Accuracy 81.865%\n",
      "Epoch 12, Batch 354, LR 2.499101 Loss 6.976057, Accuracy 81.866%\n",
      "Epoch 12, Batch 355, LR 2.499096 Loss 6.974423, Accuracy 81.879%\n",
      "Epoch 12, Batch 356, LR 2.499090 Loss 6.973733, Accuracy 81.880%\n",
      "Epoch 12, Batch 357, LR 2.499085 Loss 6.973337, Accuracy 81.878%\n",
      "Epoch 12, Batch 358, LR 2.499080 Loss 6.974423, Accuracy 81.874%\n",
      "Epoch 12, Batch 359, LR 2.499075 Loss 6.974770, Accuracy 81.866%\n",
      "Epoch 12, Batch 360, LR 2.499070 Loss 6.975162, Accuracy 81.866%\n",
      "Epoch 12, Batch 361, LR 2.499065 Loss 6.974151, Accuracy 81.869%\n",
      "Epoch 12, Batch 362, LR 2.499060 Loss 6.974131, Accuracy 81.880%\n",
      "Epoch 12, Batch 363, LR 2.499054 Loss 6.973248, Accuracy 81.887%\n",
      "Epoch 12, Batch 364, LR 2.499049 Loss 6.974071, Accuracy 81.879%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 365, LR 2.499044 Loss 6.975271, Accuracy 81.871%\n",
      "Epoch 12, Batch 366, LR 2.499039 Loss 6.973489, Accuracy 81.886%\n",
      "Epoch 12, Batch 367, LR 2.499033 Loss 6.972549, Accuracy 81.895%\n",
      "Epoch 12, Batch 368, LR 2.499028 Loss 6.972039, Accuracy 81.910%\n",
      "Epoch 12, Batch 369, LR 2.499023 Loss 6.971756, Accuracy 81.906%\n",
      "Epoch 12, Batch 370, LR 2.499018 Loss 6.972593, Accuracy 81.898%\n",
      "Epoch 12, Batch 371, LR 2.499012 Loss 6.970380, Accuracy 81.901%\n",
      "Epoch 12, Batch 372, LR 2.499007 Loss 6.970474, Accuracy 81.895%\n",
      "Epoch 12, Batch 373, LR 2.499002 Loss 6.969619, Accuracy 81.893%\n",
      "Epoch 12, Batch 374, LR 2.498996 Loss 6.969429, Accuracy 81.902%\n",
      "Epoch 12, Batch 375, LR 2.498991 Loss 6.969938, Accuracy 81.885%\n",
      "Epoch 12, Batch 376, LR 2.498985 Loss 6.970580, Accuracy 81.871%\n",
      "Epoch 12, Batch 377, LR 2.498980 Loss 6.972001, Accuracy 81.870%\n",
      "Epoch 12, Batch 378, LR 2.498975 Loss 6.970759, Accuracy 81.882%\n",
      "Epoch 12, Batch 379, LR 2.498969 Loss 6.972239, Accuracy 81.887%\n",
      "Epoch 12, Batch 380, LR 2.498964 Loss 6.973245, Accuracy 81.889%\n",
      "Epoch 12, Batch 381, LR 2.498958 Loss 6.974442, Accuracy 81.888%\n",
      "Epoch 12, Batch 382, LR 2.498953 Loss 6.975391, Accuracy 81.898%\n",
      "Epoch 12, Batch 383, LR 2.498947 Loss 6.975943, Accuracy 81.899%\n",
      "Epoch 12, Batch 384, LR 2.498942 Loss 6.976764, Accuracy 81.897%\n",
      "Epoch 12, Batch 385, LR 2.498936 Loss 6.977435, Accuracy 81.893%\n",
      "Epoch 12, Batch 386, LR 2.498931 Loss 6.978312, Accuracy 81.879%\n",
      "Epoch 12, Batch 387, LR 2.498925 Loss 6.977404, Accuracy 81.896%\n",
      "Epoch 12, Batch 388, LR 2.498920 Loss 6.978560, Accuracy 81.896%\n",
      "Epoch 12, Batch 389, LR 2.498914 Loss 6.978728, Accuracy 81.905%\n",
      "Epoch 12, Batch 390, LR 2.498908 Loss 6.979460, Accuracy 81.903%\n",
      "Epoch 12, Batch 391, LR 2.498903 Loss 6.978517, Accuracy 81.903%\n",
      "Epoch 12, Batch 392, LR 2.498897 Loss 6.976632, Accuracy 81.910%\n",
      "Epoch 12, Batch 393, LR 2.498892 Loss 6.976321, Accuracy 81.912%\n",
      "Epoch 12, Batch 394, LR 2.498886 Loss 6.975263, Accuracy 81.912%\n",
      "Epoch 12, Batch 395, LR 2.498880 Loss 6.976588, Accuracy 81.901%\n",
      "Epoch 12, Batch 396, LR 2.498875 Loss 6.977038, Accuracy 81.897%\n",
      "Epoch 12, Batch 397, LR 2.498869 Loss 6.978193, Accuracy 81.895%\n",
      "Epoch 12, Batch 398, LR 2.498863 Loss 6.977936, Accuracy 81.908%\n",
      "Epoch 12, Batch 399, LR 2.498858 Loss 6.978307, Accuracy 81.898%\n",
      "Epoch 12, Batch 400, LR 2.498852 Loss 6.980283, Accuracy 81.885%\n",
      "Epoch 12, Batch 401, LR 2.498846 Loss 6.981330, Accuracy 81.883%\n",
      "Epoch 12, Batch 402, LR 2.498840 Loss 6.981539, Accuracy 81.880%\n",
      "Epoch 12, Batch 403, LR 2.498835 Loss 6.980068, Accuracy 81.884%\n",
      "Epoch 12, Batch 404, LR 2.498829 Loss 6.980641, Accuracy 81.890%\n",
      "Epoch 12, Batch 405, LR 2.498823 Loss 6.981051, Accuracy 81.887%\n",
      "Epoch 12, Batch 406, LR 2.498817 Loss 6.979677, Accuracy 81.887%\n",
      "Epoch 12, Batch 407, LR 2.498811 Loss 6.979229, Accuracy 81.893%\n",
      "Epoch 12, Batch 408, LR 2.498805 Loss 6.978532, Accuracy 81.890%\n",
      "Epoch 12, Batch 409, LR 2.498800 Loss 6.978253, Accuracy 81.892%\n",
      "Epoch 12, Batch 410, LR 2.498794 Loss 6.978116, Accuracy 81.885%\n",
      "Epoch 12, Batch 411, LR 2.498788 Loss 6.979027, Accuracy 81.879%\n",
      "Epoch 12, Batch 412, LR 2.498782 Loss 6.980296, Accuracy 81.876%\n",
      "Epoch 12, Batch 413, LR 2.498776 Loss 6.981465, Accuracy 81.872%\n",
      "Epoch 12, Batch 414, LR 2.498770 Loss 6.981160, Accuracy 81.873%\n",
      "Epoch 12, Batch 415, LR 2.498764 Loss 6.979280, Accuracy 81.892%\n",
      "Epoch 12, Batch 416, LR 2.498758 Loss 6.980296, Accuracy 81.894%\n",
      "Epoch 12, Batch 417, LR 2.498752 Loss 6.981269, Accuracy 81.891%\n",
      "Epoch 12, Batch 418, LR 2.498746 Loss 6.981573, Accuracy 81.887%\n",
      "Epoch 12, Batch 419, LR 2.498740 Loss 6.981855, Accuracy 81.890%\n",
      "Epoch 12, Batch 420, LR 2.498734 Loss 6.981251, Accuracy 81.890%\n",
      "Epoch 12, Batch 421, LR 2.498728 Loss 6.983134, Accuracy 81.875%\n",
      "Epoch 12, Batch 422, LR 2.498722 Loss 6.983566, Accuracy 81.870%\n",
      "Epoch 12, Batch 423, LR 2.498716 Loss 6.982035, Accuracy 81.883%\n",
      "Epoch 12, Batch 424, LR 2.498710 Loss 6.982939, Accuracy 81.884%\n",
      "Epoch 12, Batch 425, LR 2.498704 Loss 6.983842, Accuracy 81.871%\n",
      "Epoch 12, Batch 426, LR 2.498698 Loss 6.984439, Accuracy 81.863%\n",
      "Epoch 12, Batch 427, LR 2.498692 Loss 6.985003, Accuracy 81.863%\n",
      "Epoch 12, Batch 428, LR 2.498685 Loss 6.986028, Accuracy 81.861%\n",
      "Epoch 12, Batch 429, LR 2.498679 Loss 6.988450, Accuracy 81.849%\n",
      "Epoch 12, Batch 430, LR 2.498673 Loss 6.988234, Accuracy 81.857%\n",
      "Epoch 12, Batch 431, LR 2.498667 Loss 6.987881, Accuracy 81.857%\n",
      "Epoch 12, Batch 432, LR 2.498661 Loss 6.987149, Accuracy 81.868%\n",
      "Epoch 12, Batch 433, LR 2.498655 Loss 6.988652, Accuracy 81.856%\n",
      "Epoch 12, Batch 434, LR 2.498648 Loss 6.989865, Accuracy 81.853%\n",
      "Epoch 12, Batch 435, LR 2.498642 Loss 6.990765, Accuracy 81.848%\n",
      "Epoch 12, Batch 436, LR 2.498636 Loss 6.990383, Accuracy 81.847%\n",
      "Epoch 12, Batch 437, LR 2.498630 Loss 6.990782, Accuracy 81.840%\n",
      "Epoch 12, Batch 438, LR 2.498623 Loss 6.991730, Accuracy 81.835%\n",
      "Epoch 12, Batch 439, LR 2.498617 Loss 6.990810, Accuracy 81.846%\n",
      "Epoch 12, Batch 440, LR 2.498611 Loss 6.990694, Accuracy 81.850%\n",
      "Epoch 12, Batch 441, LR 2.498604 Loss 6.990521, Accuracy 81.851%\n",
      "Epoch 12, Batch 442, LR 2.498598 Loss 6.990640, Accuracy 81.846%\n",
      "Epoch 12, Batch 443, LR 2.498592 Loss 6.991309, Accuracy 81.846%\n",
      "Epoch 12, Batch 444, LR 2.498585 Loss 6.992412, Accuracy 81.841%\n",
      "Epoch 12, Batch 445, LR 2.498579 Loss 6.990388, Accuracy 81.842%\n",
      "Epoch 12, Batch 446, LR 2.498573 Loss 6.990046, Accuracy 81.847%\n",
      "Epoch 12, Batch 447, LR 2.498566 Loss 6.989616, Accuracy 81.853%\n",
      "Epoch 12, Batch 448, LR 2.498560 Loss 6.989117, Accuracy 81.859%\n",
      "Epoch 12, Batch 449, LR 2.498553 Loss 6.988989, Accuracy 81.866%\n",
      "Epoch 12, Batch 450, LR 2.498547 Loss 6.988848, Accuracy 81.858%\n",
      "Epoch 12, Batch 451, LR 2.498540 Loss 6.989759, Accuracy 81.853%\n",
      "Epoch 12, Batch 452, LR 2.498534 Loss 6.988286, Accuracy 81.860%\n",
      "Epoch 12, Batch 453, LR 2.498527 Loss 6.987151, Accuracy 81.873%\n",
      "Epoch 12, Batch 454, LR 2.498521 Loss 6.987893, Accuracy 81.871%\n",
      "Epoch 12, Batch 455, LR 2.498514 Loss 6.987117, Accuracy 81.877%\n",
      "Epoch 12, Batch 456, LR 2.498508 Loss 6.986706, Accuracy 81.884%\n",
      "Epoch 12, Batch 457, LR 2.498501 Loss 6.986044, Accuracy 81.884%\n",
      "Epoch 12, Batch 458, LR 2.498495 Loss 6.986930, Accuracy 81.878%\n",
      "Epoch 12, Batch 459, LR 2.498488 Loss 6.985852, Accuracy 81.883%\n",
      "Epoch 12, Batch 460, LR 2.498482 Loss 6.984657, Accuracy 81.885%\n",
      "Epoch 12, Batch 461, LR 2.498475 Loss 6.984565, Accuracy 81.886%\n",
      "Epoch 12, Batch 462, LR 2.498468 Loss 6.984060, Accuracy 81.884%\n",
      "Epoch 12, Batch 463, LR 2.498462 Loss 6.983598, Accuracy 81.888%\n",
      "Epoch 12, Batch 464, LR 2.498455 Loss 6.984760, Accuracy 81.883%\n",
      "Epoch 12, Batch 465, LR 2.498448 Loss 6.983401, Accuracy 81.885%\n",
      "Epoch 12, Batch 466, LR 2.498442 Loss 6.983122, Accuracy 81.884%\n",
      "Epoch 12, Batch 467, LR 2.498435 Loss 6.983224, Accuracy 81.882%\n",
      "Epoch 12, Batch 468, LR 2.498428 Loss 6.983759, Accuracy 81.879%\n",
      "Epoch 12, Batch 469, LR 2.498422 Loss 6.983531, Accuracy 81.870%\n",
      "Epoch 12, Batch 470, LR 2.498415 Loss 6.982255, Accuracy 81.878%\n",
      "Epoch 12, Batch 471, LR 2.498408 Loss 6.984858, Accuracy 81.860%\n",
      "Epoch 12, Batch 472, LR 2.498401 Loss 6.984794, Accuracy 81.866%\n",
      "Epoch 12, Batch 473, LR 2.498395 Loss 6.984468, Accuracy 81.859%\n",
      "Epoch 12, Batch 474, LR 2.498388 Loss 6.985012, Accuracy 81.861%\n",
      "Epoch 12, Batch 475, LR 2.498381 Loss 6.984612, Accuracy 81.867%\n",
      "Epoch 12, Batch 476, LR 2.498374 Loss 6.986293, Accuracy 81.851%\n",
      "Epoch 12, Batch 477, LR 2.498367 Loss 6.987184, Accuracy 81.845%\n",
      "Epoch 12, Batch 478, LR 2.498360 Loss 6.986391, Accuracy 81.845%\n",
      "Epoch 12, Batch 479, LR 2.498354 Loss 6.986533, Accuracy 81.839%\n",
      "Epoch 12, Batch 480, LR 2.498347 Loss 6.987115, Accuracy 81.833%\n",
      "Epoch 12, Batch 481, LR 2.498340 Loss 6.987520, Accuracy 81.831%\n",
      "Epoch 12, Batch 482, LR 2.498333 Loss 6.987648, Accuracy 81.829%\n",
      "Epoch 12, Batch 483, LR 2.498326 Loss 6.987917, Accuracy 81.827%\n",
      "Epoch 12, Batch 484, LR 2.498319 Loss 6.988277, Accuracy 81.826%\n",
      "Epoch 12, Batch 485, LR 2.498312 Loss 6.988099, Accuracy 81.828%\n",
      "Epoch 12, Batch 486, LR 2.498305 Loss 6.988238, Accuracy 81.824%\n",
      "Epoch 12, Batch 487, LR 2.498298 Loss 6.988144, Accuracy 81.828%\n",
      "Epoch 12, Batch 488, LR 2.498291 Loss 6.988629, Accuracy 81.831%\n",
      "Epoch 12, Batch 489, LR 2.498284 Loss 6.989670, Accuracy 81.819%\n",
      "Epoch 12, Batch 490, LR 2.498277 Loss 6.989752, Accuracy 81.813%\n",
      "Epoch 12, Batch 491, LR 2.498270 Loss 6.989831, Accuracy 81.816%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 492, LR 2.498263 Loss 6.990847, Accuracy 81.809%\n",
      "Epoch 12, Batch 493, LR 2.498256 Loss 6.990581, Accuracy 81.809%\n",
      "Epoch 12, Batch 494, LR 2.498249 Loss 6.990395, Accuracy 81.815%\n",
      "Epoch 12, Batch 495, LR 2.498242 Loss 6.991314, Accuracy 81.802%\n",
      "Epoch 12, Batch 496, LR 2.498235 Loss 6.991251, Accuracy 81.804%\n",
      "Epoch 12, Batch 497, LR 2.498228 Loss 6.990855, Accuracy 81.810%\n",
      "Epoch 12, Batch 498, LR 2.498220 Loss 6.990253, Accuracy 81.810%\n",
      "Epoch 12, Batch 499, LR 2.498213 Loss 6.990646, Accuracy 81.812%\n",
      "Epoch 12, Batch 500, LR 2.498206 Loss 6.991088, Accuracy 81.811%\n",
      "Epoch 12, Batch 501, LR 2.498199 Loss 6.992016, Accuracy 81.807%\n",
      "Epoch 12, Batch 502, LR 2.498192 Loss 6.991563, Accuracy 81.795%\n",
      "Epoch 12, Batch 503, LR 2.498184 Loss 6.991531, Accuracy 81.791%\n",
      "Epoch 12, Batch 504, LR 2.498177 Loss 6.991904, Accuracy 81.799%\n",
      "Epoch 12, Batch 505, LR 2.498170 Loss 6.991834, Accuracy 81.798%\n",
      "Epoch 12, Batch 506, LR 2.498163 Loss 6.992313, Accuracy 81.783%\n",
      "Epoch 12, Batch 507, LR 2.498156 Loss 6.993335, Accuracy 81.771%\n",
      "Epoch 12, Batch 508, LR 2.498148 Loss 6.993420, Accuracy 81.770%\n",
      "Epoch 12, Batch 509, LR 2.498141 Loss 6.993804, Accuracy 81.772%\n",
      "Epoch 12, Batch 510, LR 2.498134 Loss 6.993537, Accuracy 81.777%\n",
      "Epoch 12, Batch 511, LR 2.498126 Loss 6.993666, Accuracy 81.774%\n",
      "Epoch 12, Batch 512, LR 2.498119 Loss 6.995030, Accuracy 81.758%\n",
      "Epoch 12, Batch 513, LR 2.498112 Loss 6.994475, Accuracy 81.760%\n",
      "Epoch 12, Batch 514, LR 2.498104 Loss 6.993197, Accuracy 81.768%\n",
      "Epoch 12, Batch 515, LR 2.498097 Loss 6.994210, Accuracy 81.763%\n",
      "Epoch 12, Batch 516, LR 2.498089 Loss 6.993302, Accuracy 81.775%\n",
      "Epoch 12, Batch 517, LR 2.498082 Loss 6.993660, Accuracy 81.768%\n",
      "Epoch 12, Batch 518, LR 2.498075 Loss 6.994358, Accuracy 81.770%\n",
      "Epoch 12, Batch 519, LR 2.498067 Loss 6.993028, Accuracy 81.781%\n",
      "Epoch 12, Batch 520, LR 2.498060 Loss 6.993082, Accuracy 81.782%\n",
      "Epoch 12, Batch 521, LR 2.498052 Loss 6.993520, Accuracy 81.781%\n",
      "Epoch 12, Batch 522, LR 2.498045 Loss 6.994537, Accuracy 81.778%\n",
      "Epoch 12, Batch 523, LR 2.498037 Loss 6.994276, Accuracy 81.780%\n",
      "Epoch 12, Batch 524, LR 2.498030 Loss 6.993649, Accuracy 81.782%\n",
      "Epoch 12, Batch 525, LR 2.498022 Loss 6.991962, Accuracy 81.792%\n",
      "Epoch 12, Batch 526, LR 2.498015 Loss 6.992526, Accuracy 81.788%\n",
      "Epoch 12, Batch 527, LR 2.498007 Loss 6.992871, Accuracy 81.784%\n",
      "Epoch 12, Batch 528, LR 2.498000 Loss 6.992109, Accuracy 81.777%\n",
      "Epoch 12, Batch 529, LR 2.497992 Loss 6.992411, Accuracy 81.777%\n",
      "Epoch 12, Batch 530, LR 2.497984 Loss 6.992392, Accuracy 81.776%\n",
      "Epoch 12, Batch 531, LR 2.497977 Loss 6.992845, Accuracy 81.775%\n",
      "Epoch 12, Batch 532, LR 2.497969 Loss 6.993507, Accuracy 81.768%\n",
      "Epoch 12, Batch 533, LR 2.497962 Loss 6.994514, Accuracy 81.763%\n",
      "Epoch 12, Batch 534, LR 2.497954 Loss 6.995087, Accuracy 81.759%\n",
      "Epoch 12, Batch 535, LR 2.497946 Loss 6.994720, Accuracy 81.754%\n",
      "Epoch 12, Batch 536, LR 2.497939 Loss 6.995220, Accuracy 81.753%\n",
      "Epoch 12, Batch 537, LR 2.497931 Loss 6.995271, Accuracy 81.758%\n",
      "Epoch 12, Batch 538, LR 2.497923 Loss 6.995534, Accuracy 81.757%\n",
      "Epoch 12, Batch 539, LR 2.497915 Loss 6.995329, Accuracy 81.757%\n",
      "Epoch 12, Batch 540, LR 2.497908 Loss 6.994438, Accuracy 81.771%\n",
      "Epoch 12, Batch 541, LR 2.497900 Loss 6.995327, Accuracy 81.768%\n",
      "Epoch 12, Batch 542, LR 2.497892 Loss 6.995241, Accuracy 81.772%\n",
      "Epoch 12, Batch 543, LR 2.497884 Loss 6.993599, Accuracy 81.782%\n",
      "Epoch 12, Batch 544, LR 2.497877 Loss 6.994502, Accuracy 81.771%\n",
      "Epoch 12, Batch 545, LR 2.497869 Loss 6.995762, Accuracy 81.755%\n",
      "Epoch 12, Batch 546, LR 2.497861 Loss 6.994257, Accuracy 81.758%\n",
      "Epoch 12, Batch 547, LR 2.497853 Loss 6.995251, Accuracy 81.750%\n",
      "Epoch 12, Batch 548, LR 2.497845 Loss 6.995005, Accuracy 81.746%\n",
      "Epoch 12, Batch 549, LR 2.497837 Loss 6.993942, Accuracy 81.745%\n",
      "Epoch 12, Batch 550, LR 2.497829 Loss 6.995189, Accuracy 81.739%\n",
      "Epoch 12, Batch 551, LR 2.497822 Loss 6.994351, Accuracy 81.748%\n",
      "Epoch 12, Batch 552, LR 2.497814 Loss 6.994184, Accuracy 81.747%\n",
      "Epoch 12, Batch 553, LR 2.497806 Loss 6.994399, Accuracy 81.746%\n",
      "Epoch 12, Batch 554, LR 2.497798 Loss 6.996754, Accuracy 81.727%\n",
      "Epoch 12, Batch 555, LR 2.497790 Loss 6.996310, Accuracy 81.727%\n",
      "Epoch 12, Batch 556, LR 2.497782 Loss 6.996746, Accuracy 81.728%\n",
      "Epoch 12, Batch 557, LR 2.497774 Loss 6.996634, Accuracy 81.728%\n",
      "Epoch 12, Batch 558, LR 2.497766 Loss 6.996953, Accuracy 81.727%\n",
      "Epoch 12, Batch 559, LR 2.497758 Loss 6.996920, Accuracy 81.732%\n",
      "Epoch 12, Batch 560, LR 2.497750 Loss 6.996962, Accuracy 81.731%\n",
      "Epoch 12, Batch 561, LR 2.497742 Loss 6.996538, Accuracy 81.735%\n",
      "Epoch 12, Batch 562, LR 2.497734 Loss 6.997237, Accuracy 81.737%\n",
      "Epoch 12, Batch 563, LR 2.497726 Loss 6.996129, Accuracy 81.752%\n",
      "Epoch 12, Batch 564, LR 2.497718 Loss 6.996903, Accuracy 81.747%\n",
      "Epoch 12, Batch 565, LR 2.497709 Loss 6.996234, Accuracy 81.755%\n",
      "Epoch 12, Batch 566, LR 2.497701 Loss 6.996175, Accuracy 81.748%\n",
      "Epoch 12, Batch 567, LR 2.497693 Loss 6.995246, Accuracy 81.754%\n",
      "Epoch 12, Batch 568, LR 2.497685 Loss 6.994600, Accuracy 81.758%\n",
      "Epoch 12, Batch 569, LR 2.497677 Loss 6.995011, Accuracy 81.754%\n",
      "Epoch 12, Batch 570, LR 2.497669 Loss 6.996443, Accuracy 81.746%\n",
      "Epoch 12, Batch 571, LR 2.497661 Loss 6.996853, Accuracy 81.743%\n",
      "Epoch 12, Batch 572, LR 2.497652 Loss 6.996130, Accuracy 81.750%\n",
      "Epoch 12, Batch 573, LR 2.497644 Loss 6.995398, Accuracy 81.757%\n",
      "Epoch 12, Batch 574, LR 2.497636 Loss 6.995040, Accuracy 81.758%\n",
      "Epoch 12, Batch 575, LR 2.497628 Loss 6.995160, Accuracy 81.757%\n",
      "Epoch 12, Batch 576, LR 2.497619 Loss 6.996132, Accuracy 81.746%\n",
      "Epoch 12, Batch 577, LR 2.497611 Loss 6.997032, Accuracy 81.732%\n",
      "Epoch 12, Batch 578, LR 2.497603 Loss 6.995438, Accuracy 81.739%\n",
      "Epoch 12, Batch 579, LR 2.497595 Loss 6.994508, Accuracy 81.736%\n",
      "Epoch 12, Batch 580, LR 2.497586 Loss 6.994440, Accuracy 81.736%\n",
      "Epoch 12, Batch 581, LR 2.497578 Loss 6.994130, Accuracy 81.734%\n",
      "Epoch 12, Batch 582, LR 2.497570 Loss 6.993717, Accuracy 81.739%\n",
      "Epoch 12, Batch 583, LR 2.497561 Loss 6.993646, Accuracy 81.740%\n",
      "Epoch 12, Batch 584, LR 2.497553 Loss 6.992869, Accuracy 81.740%\n",
      "Epoch 12, Batch 585, LR 2.497545 Loss 6.992212, Accuracy 81.737%\n",
      "Epoch 12, Batch 586, LR 2.497536 Loss 6.992576, Accuracy 81.745%\n",
      "Epoch 12, Batch 587, LR 2.497528 Loss 6.993174, Accuracy 81.741%\n",
      "Epoch 12, Batch 588, LR 2.497519 Loss 6.993581, Accuracy 81.743%\n",
      "Epoch 12, Batch 589, LR 2.497511 Loss 6.992873, Accuracy 81.750%\n",
      "Epoch 12, Batch 590, LR 2.497502 Loss 6.993519, Accuracy 81.748%\n",
      "Epoch 12, Batch 591, LR 2.497494 Loss 6.994553, Accuracy 81.742%\n",
      "Epoch 12, Batch 592, LR 2.497485 Loss 6.992596, Accuracy 81.754%\n",
      "Epoch 12, Batch 593, LR 2.497477 Loss 6.993167, Accuracy 81.752%\n",
      "Epoch 12, Batch 594, LR 2.497468 Loss 6.992958, Accuracy 81.748%\n",
      "Epoch 12, Batch 595, LR 2.497460 Loss 6.992721, Accuracy 81.749%\n",
      "Epoch 12, Batch 596, LR 2.497451 Loss 6.993872, Accuracy 81.744%\n",
      "Epoch 12, Batch 597, LR 2.497443 Loss 6.992926, Accuracy 81.749%\n",
      "Epoch 12, Batch 598, LR 2.497434 Loss 6.993928, Accuracy 81.745%\n",
      "Epoch 12, Batch 599, LR 2.497426 Loss 6.992954, Accuracy 81.747%\n",
      "Epoch 12, Batch 600, LR 2.497417 Loss 6.992262, Accuracy 81.746%\n",
      "Epoch 12, Batch 601, LR 2.497408 Loss 6.992266, Accuracy 81.748%\n",
      "Epoch 12, Batch 602, LR 2.497400 Loss 6.992390, Accuracy 81.744%\n",
      "Epoch 12, Batch 603, LR 2.497391 Loss 6.992677, Accuracy 81.742%\n",
      "Epoch 12, Batch 604, LR 2.497382 Loss 6.992395, Accuracy 81.742%\n",
      "Epoch 12, Batch 605, LR 2.497374 Loss 6.991758, Accuracy 81.748%\n",
      "Epoch 12, Batch 606, LR 2.497365 Loss 6.991054, Accuracy 81.751%\n",
      "Epoch 12, Batch 607, LR 2.497356 Loss 6.992209, Accuracy 81.744%\n",
      "Epoch 12, Batch 608, LR 2.497348 Loss 6.992553, Accuracy 81.745%\n",
      "Epoch 12, Batch 609, LR 2.497339 Loss 6.992377, Accuracy 81.752%\n",
      "Epoch 12, Batch 610, LR 2.497330 Loss 6.993090, Accuracy 81.742%\n",
      "Epoch 12, Batch 611, LR 2.497321 Loss 6.992758, Accuracy 81.742%\n",
      "Epoch 12, Batch 612, LR 2.497313 Loss 6.992635, Accuracy 81.740%\n",
      "Epoch 12, Batch 613, LR 2.497304 Loss 6.992470, Accuracy 81.743%\n",
      "Epoch 12, Batch 614, LR 2.497295 Loss 6.992356, Accuracy 81.740%\n",
      "Epoch 12, Batch 615, LR 2.497286 Loss 6.992534, Accuracy 81.733%\n",
      "Epoch 12, Batch 616, LR 2.497277 Loss 6.991701, Accuracy 81.733%\n",
      "Epoch 12, Batch 617, LR 2.497269 Loss 6.991960, Accuracy 81.731%\n",
      "Epoch 12, Batch 618, LR 2.497260 Loss 6.991678, Accuracy 81.735%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 619, LR 2.497251 Loss 6.992477, Accuracy 81.730%\n",
      "Epoch 12, Batch 620, LR 2.497242 Loss 6.992870, Accuracy 81.725%\n",
      "Epoch 12, Batch 621, LR 2.497233 Loss 6.993493, Accuracy 81.723%\n",
      "Epoch 12, Batch 622, LR 2.497224 Loss 6.993410, Accuracy 81.724%\n",
      "Epoch 12, Batch 623, LR 2.497215 Loss 6.993325, Accuracy 81.718%\n",
      "Epoch 12, Batch 624, LR 2.497206 Loss 6.992563, Accuracy 81.731%\n",
      "Epoch 12, Batch 625, LR 2.497197 Loss 6.992791, Accuracy 81.721%\n",
      "Epoch 12, Batch 626, LR 2.497188 Loss 6.992714, Accuracy 81.719%\n",
      "Epoch 12, Batch 627, LR 2.497179 Loss 6.991858, Accuracy 81.723%\n",
      "Epoch 12, Batch 628, LR 2.497170 Loss 6.992466, Accuracy 81.723%\n",
      "Epoch 12, Batch 629, LR 2.497161 Loss 6.992914, Accuracy 81.718%\n",
      "Epoch 12, Batch 630, LR 2.497152 Loss 6.992799, Accuracy 81.715%\n",
      "Epoch 12, Batch 631, LR 2.497143 Loss 6.991567, Accuracy 81.723%\n",
      "Epoch 12, Batch 632, LR 2.497134 Loss 6.992333, Accuracy 81.723%\n",
      "Epoch 12, Batch 633, LR 2.497125 Loss 6.992144, Accuracy 81.720%\n",
      "Epoch 12, Batch 634, LR 2.497116 Loss 6.992335, Accuracy 81.719%\n",
      "Epoch 12, Batch 635, LR 2.497107 Loss 6.992299, Accuracy 81.720%\n",
      "Epoch 12, Batch 636, LR 2.497098 Loss 6.992634, Accuracy 81.716%\n",
      "Epoch 12, Batch 637, LR 2.497089 Loss 6.991737, Accuracy 81.719%\n",
      "Epoch 12, Batch 638, LR 2.497080 Loss 6.991043, Accuracy 81.729%\n",
      "Epoch 12, Batch 639, LR 2.497070 Loss 6.991812, Accuracy 81.723%\n",
      "Epoch 12, Batch 640, LR 2.497061 Loss 6.992682, Accuracy 81.713%\n",
      "Epoch 12, Batch 641, LR 2.497052 Loss 6.993507, Accuracy 81.711%\n",
      "Epoch 12, Batch 642, LR 2.497043 Loss 6.993784, Accuracy 81.710%\n",
      "Epoch 12, Batch 643, LR 2.497034 Loss 6.993952, Accuracy 81.714%\n",
      "Epoch 12, Batch 644, LR 2.497024 Loss 6.994013, Accuracy 81.717%\n",
      "Epoch 12, Batch 645, LR 2.497015 Loss 6.993505, Accuracy 81.725%\n",
      "Epoch 12, Batch 646, LR 2.497006 Loss 6.993346, Accuracy 81.724%\n",
      "Epoch 12, Batch 647, LR 2.496997 Loss 6.993246, Accuracy 81.723%\n",
      "Epoch 12, Batch 648, LR 2.496987 Loss 6.993235, Accuracy 81.721%\n",
      "Epoch 12, Batch 649, LR 2.496978 Loss 6.993490, Accuracy 81.727%\n",
      "Epoch 12, Batch 650, LR 2.496969 Loss 6.993553, Accuracy 81.724%\n",
      "Epoch 12, Batch 651, LR 2.496959 Loss 6.993663, Accuracy 81.718%\n",
      "Epoch 12, Batch 652, LR 2.496950 Loss 6.994381, Accuracy 81.715%\n",
      "Epoch 12, Batch 653, LR 2.496941 Loss 6.994557, Accuracy 81.709%\n",
      "Epoch 12, Batch 654, LR 2.496931 Loss 6.994959, Accuracy 81.702%\n",
      "Epoch 12, Batch 655, LR 2.496922 Loss 6.995113, Accuracy 81.698%\n",
      "Epoch 12, Batch 656, LR 2.496913 Loss 6.994266, Accuracy 81.699%\n",
      "Epoch 12, Batch 657, LR 2.496903 Loss 6.994118, Accuracy 81.701%\n",
      "Epoch 12, Batch 658, LR 2.496894 Loss 6.994616, Accuracy 81.695%\n",
      "Epoch 12, Batch 659, LR 2.496884 Loss 6.994866, Accuracy 81.691%\n",
      "Epoch 12, Batch 660, LR 2.496875 Loss 6.995171, Accuracy 81.687%\n",
      "Epoch 12, Batch 661, LR 2.496865 Loss 6.994906, Accuracy 81.690%\n",
      "Epoch 12, Batch 662, LR 2.496856 Loss 6.995363, Accuracy 81.691%\n",
      "Epoch 12, Batch 663, LR 2.496846 Loss 6.994739, Accuracy 81.694%\n",
      "Epoch 12, Batch 664, LR 2.496837 Loss 6.994782, Accuracy 81.696%\n",
      "Epoch 12, Batch 665, LR 2.496827 Loss 6.994431, Accuracy 81.692%\n",
      "Epoch 12, Batch 666, LR 2.496818 Loss 6.993734, Accuracy 81.697%\n",
      "Epoch 12, Batch 667, LR 2.496808 Loss 6.992779, Accuracy 81.695%\n",
      "Epoch 12, Batch 668, LR 2.496799 Loss 6.992925, Accuracy 81.698%\n",
      "Epoch 12, Batch 669, LR 2.496789 Loss 6.992322, Accuracy 81.707%\n",
      "Epoch 12, Batch 670, LR 2.496779 Loss 6.991454, Accuracy 81.707%\n",
      "Epoch 12, Batch 671, LR 2.496770 Loss 6.990813, Accuracy 81.712%\n",
      "Epoch 12, Batch 672, LR 2.496760 Loss 6.992179, Accuracy 81.707%\n",
      "Epoch 12, Batch 673, LR 2.496751 Loss 6.990800, Accuracy 81.712%\n",
      "Epoch 12, Batch 674, LR 2.496741 Loss 6.990560, Accuracy 81.712%\n",
      "Epoch 12, Batch 675, LR 2.496731 Loss 6.988873, Accuracy 81.720%\n",
      "Epoch 12, Batch 676, LR 2.496722 Loss 6.988726, Accuracy 81.718%\n",
      "Epoch 12, Batch 677, LR 2.496712 Loss 6.988514, Accuracy 81.717%\n",
      "Epoch 12, Batch 678, LR 2.496702 Loss 6.988999, Accuracy 81.718%\n",
      "Epoch 12, Batch 679, LR 2.496692 Loss 6.988094, Accuracy 81.722%\n",
      "Epoch 12, Batch 680, LR 2.496683 Loss 6.988268, Accuracy 81.720%\n",
      "Epoch 12, Batch 681, LR 2.496673 Loss 6.988516, Accuracy 81.711%\n",
      "Epoch 12, Batch 682, LR 2.496663 Loss 6.988029, Accuracy 81.714%\n",
      "Epoch 12, Batch 683, LR 2.496653 Loss 6.988323, Accuracy 81.713%\n",
      "Epoch 12, Batch 684, LR 2.496644 Loss 6.989037, Accuracy 81.705%\n",
      "Epoch 12, Batch 685, LR 2.496634 Loss 6.989599, Accuracy 81.698%\n",
      "Epoch 12, Batch 686, LR 2.496624 Loss 6.990573, Accuracy 81.690%\n",
      "Epoch 12, Batch 687, LR 2.496614 Loss 6.991420, Accuracy 81.684%\n",
      "Epoch 12, Batch 688, LR 2.496604 Loss 6.991883, Accuracy 81.684%\n",
      "Epoch 12, Batch 689, LR 2.496594 Loss 6.991988, Accuracy 81.684%\n",
      "Epoch 12, Batch 690, LR 2.496584 Loss 6.990613, Accuracy 81.686%\n",
      "Epoch 12, Batch 691, LR 2.496574 Loss 6.990533, Accuracy 81.689%\n",
      "Epoch 12, Batch 692, LR 2.496565 Loss 6.989930, Accuracy 81.694%\n",
      "Epoch 12, Batch 693, LR 2.496555 Loss 6.989756, Accuracy 81.698%\n",
      "Epoch 12, Batch 694, LR 2.496545 Loss 6.989028, Accuracy 81.707%\n",
      "Epoch 12, Batch 695, LR 2.496535 Loss 6.989218, Accuracy 81.697%\n",
      "Epoch 12, Batch 696, LR 2.496525 Loss 6.988094, Accuracy 81.706%\n",
      "Epoch 12, Batch 697, LR 2.496515 Loss 6.988238, Accuracy 81.704%\n",
      "Epoch 12, Batch 698, LR 2.496505 Loss 6.989793, Accuracy 81.694%\n",
      "Epoch 12, Batch 699, LR 2.496495 Loss 6.990496, Accuracy 81.689%\n",
      "Epoch 12, Batch 700, LR 2.496485 Loss 6.989040, Accuracy 81.698%\n",
      "Epoch 12, Batch 701, LR 2.496475 Loss 6.989301, Accuracy 81.696%\n",
      "Epoch 12, Batch 702, LR 2.496465 Loss 6.989416, Accuracy 81.701%\n",
      "Epoch 12, Batch 703, LR 2.496455 Loss 6.989374, Accuracy 81.705%\n",
      "Epoch 12, Batch 704, LR 2.496444 Loss 6.988917, Accuracy 81.707%\n",
      "Epoch 12, Batch 705, LR 2.496434 Loss 6.989339, Accuracy 81.703%\n",
      "Epoch 12, Batch 706, LR 2.496424 Loss 6.988208, Accuracy 81.708%\n",
      "Epoch 12, Batch 707, LR 2.496414 Loss 6.988948, Accuracy 81.702%\n",
      "Epoch 12, Batch 708, LR 2.496404 Loss 6.988948, Accuracy 81.699%\n",
      "Epoch 12, Batch 709, LR 2.496394 Loss 6.988583, Accuracy 81.702%\n",
      "Epoch 12, Batch 710, LR 2.496384 Loss 6.987771, Accuracy 81.704%\n",
      "Epoch 12, Batch 711, LR 2.496373 Loss 6.988100, Accuracy 81.708%\n",
      "Epoch 12, Batch 712, LR 2.496363 Loss 6.988372, Accuracy 81.713%\n",
      "Epoch 12, Batch 713, LR 2.496353 Loss 6.988537, Accuracy 81.712%\n",
      "Epoch 12, Batch 714, LR 2.496343 Loss 6.988011, Accuracy 81.710%\n",
      "Epoch 12, Batch 715, LR 2.496333 Loss 6.987172, Accuracy 81.717%\n",
      "Epoch 12, Batch 716, LR 2.496322 Loss 6.987356, Accuracy 81.717%\n",
      "Epoch 12, Batch 717, LR 2.496312 Loss 6.985910, Accuracy 81.725%\n",
      "Epoch 12, Batch 718, LR 2.496302 Loss 6.986150, Accuracy 81.719%\n",
      "Epoch 12, Batch 719, LR 2.496291 Loss 6.986088, Accuracy 81.719%\n",
      "Epoch 12, Batch 720, LR 2.496281 Loss 6.986291, Accuracy 81.714%\n",
      "Epoch 12, Batch 721, LR 2.496271 Loss 6.986145, Accuracy 81.713%\n",
      "Epoch 12, Batch 722, LR 2.496260 Loss 6.986371, Accuracy 81.709%\n",
      "Epoch 12, Batch 723, LR 2.496250 Loss 6.985134, Accuracy 81.720%\n",
      "Epoch 12, Batch 724, LR 2.496240 Loss 6.985991, Accuracy 81.715%\n",
      "Epoch 12, Batch 725, LR 2.496229 Loss 6.986738, Accuracy 81.709%\n",
      "Epoch 12, Batch 726, LR 2.496219 Loss 6.987087, Accuracy 81.714%\n",
      "Epoch 12, Batch 727, LR 2.496208 Loss 6.987563, Accuracy 81.712%\n",
      "Epoch 12, Batch 728, LR 2.496198 Loss 6.987276, Accuracy 81.711%\n",
      "Epoch 12, Batch 729, LR 2.496188 Loss 6.987436, Accuracy 81.712%\n",
      "Epoch 12, Batch 730, LR 2.496177 Loss 6.988145, Accuracy 81.707%\n",
      "Epoch 12, Batch 731, LR 2.496167 Loss 6.988264, Accuracy 81.712%\n",
      "Epoch 12, Batch 732, LR 2.496156 Loss 6.987566, Accuracy 81.719%\n",
      "Epoch 12, Batch 733, LR 2.496146 Loss 6.987821, Accuracy 81.713%\n",
      "Epoch 12, Batch 734, LR 2.496135 Loss 6.988430, Accuracy 81.712%\n",
      "Epoch 12, Batch 735, LR 2.496125 Loss 6.988025, Accuracy 81.711%\n",
      "Epoch 12, Batch 736, LR 2.496114 Loss 6.987762, Accuracy 81.709%\n",
      "Epoch 12, Batch 737, LR 2.496103 Loss 6.987378, Accuracy 81.707%\n",
      "Epoch 12, Batch 738, LR 2.496093 Loss 6.987373, Accuracy 81.702%\n",
      "Epoch 12, Batch 739, LR 2.496082 Loss 6.987468, Accuracy 81.701%\n",
      "Epoch 12, Batch 740, LR 2.496072 Loss 6.987293, Accuracy 81.701%\n",
      "Epoch 12, Batch 741, LR 2.496061 Loss 6.987150, Accuracy 81.704%\n",
      "Epoch 12, Batch 742, LR 2.496050 Loss 6.986744, Accuracy 81.705%\n",
      "Epoch 12, Batch 743, LR 2.496040 Loss 6.985906, Accuracy 81.709%\n",
      "Epoch 12, Batch 744, LR 2.496029 Loss 6.985655, Accuracy 81.710%\n",
      "Epoch 12, Batch 745, LR 2.496018 Loss 6.986180, Accuracy 81.708%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 746, LR 2.496008 Loss 6.987485, Accuracy 81.701%\n",
      "Epoch 12, Batch 747, LR 2.495997 Loss 6.987308, Accuracy 81.703%\n",
      "Epoch 12, Batch 748, LR 2.495986 Loss 6.987106, Accuracy 81.702%\n",
      "Epoch 12, Batch 749, LR 2.495976 Loss 6.987501, Accuracy 81.697%\n",
      "Epoch 12, Batch 750, LR 2.495965 Loss 6.987156, Accuracy 81.697%\n",
      "Epoch 12, Batch 751, LR 2.495954 Loss 6.986982, Accuracy 81.699%\n",
      "Epoch 12, Batch 752, LR 2.495943 Loss 6.986371, Accuracy 81.703%\n",
      "Epoch 12, Batch 753, LR 2.495933 Loss 6.986254, Accuracy 81.703%\n",
      "Epoch 12, Batch 754, LR 2.495922 Loss 6.986407, Accuracy 81.701%\n",
      "Epoch 12, Batch 755, LR 2.495911 Loss 6.986746, Accuracy 81.699%\n",
      "Epoch 12, Batch 756, LR 2.495900 Loss 6.986182, Accuracy 81.705%\n",
      "Epoch 12, Batch 757, LR 2.495889 Loss 6.984914, Accuracy 81.709%\n",
      "Epoch 12, Batch 758, LR 2.495878 Loss 6.986067, Accuracy 81.700%\n",
      "Epoch 12, Batch 759, LR 2.495868 Loss 6.986698, Accuracy 81.692%\n",
      "Epoch 12, Batch 760, LR 2.495857 Loss 6.985759, Accuracy 81.699%\n",
      "Epoch 12, Batch 761, LR 2.495846 Loss 6.985197, Accuracy 81.699%\n",
      "Epoch 12, Batch 762, LR 2.495835 Loss 6.986038, Accuracy 81.694%\n",
      "Epoch 12, Batch 763, LR 2.495824 Loss 6.986929, Accuracy 81.686%\n",
      "Epoch 12, Batch 764, LR 2.495813 Loss 6.986373, Accuracy 81.692%\n",
      "Epoch 12, Batch 765, LR 2.495802 Loss 6.986597, Accuracy 81.689%\n",
      "Epoch 12, Batch 766, LR 2.495791 Loss 6.985975, Accuracy 81.693%\n",
      "Epoch 12, Batch 767, LR 2.495780 Loss 6.985712, Accuracy 81.688%\n",
      "Epoch 12, Batch 768, LR 2.495769 Loss 6.985642, Accuracy 81.686%\n",
      "Epoch 12, Batch 769, LR 2.495758 Loss 6.986186, Accuracy 81.685%\n",
      "Epoch 12, Batch 770, LR 2.495747 Loss 6.986562, Accuracy 81.682%\n",
      "Epoch 12, Batch 771, LR 2.495736 Loss 6.985709, Accuracy 81.686%\n",
      "Epoch 12, Batch 772, LR 2.495725 Loss 6.985390, Accuracy 81.689%\n",
      "Epoch 12, Batch 773, LR 2.495714 Loss 6.985317, Accuracy 81.693%\n",
      "Epoch 12, Batch 774, LR 2.495703 Loss 6.985155, Accuracy 81.689%\n",
      "Epoch 12, Batch 775, LR 2.495692 Loss 6.984317, Accuracy 81.691%\n",
      "Epoch 12, Batch 776, LR 2.495680 Loss 6.983831, Accuracy 81.689%\n",
      "Epoch 12, Batch 777, LR 2.495669 Loss 6.984082, Accuracy 81.687%\n",
      "Epoch 12, Batch 778, LR 2.495658 Loss 6.983775, Accuracy 81.690%\n",
      "Epoch 12, Batch 779, LR 2.495647 Loss 6.983399, Accuracy 81.697%\n",
      "Epoch 12, Batch 780, LR 2.495636 Loss 6.983221, Accuracy 81.698%\n",
      "Epoch 12, Batch 781, LR 2.495625 Loss 6.982397, Accuracy 81.700%\n",
      "Epoch 12, Batch 782, LR 2.495613 Loss 6.982163, Accuracy 81.707%\n",
      "Epoch 12, Batch 783, LR 2.495602 Loss 6.982102, Accuracy 81.706%\n",
      "Epoch 12, Batch 784, LR 2.495591 Loss 6.982132, Accuracy 81.704%\n",
      "Epoch 12, Batch 785, LR 2.495580 Loss 6.982493, Accuracy 81.700%\n",
      "Epoch 12, Batch 786, LR 2.495568 Loss 6.983931, Accuracy 81.694%\n",
      "Epoch 12, Batch 787, LR 2.495557 Loss 6.984126, Accuracy 81.693%\n",
      "Epoch 12, Batch 788, LR 2.495546 Loss 6.983729, Accuracy 81.694%\n",
      "Epoch 12, Batch 789, LR 2.495535 Loss 6.983305, Accuracy 81.695%\n",
      "Epoch 12, Batch 790, LR 2.495523 Loss 6.983400, Accuracy 81.698%\n",
      "Epoch 12, Batch 791, LR 2.495512 Loss 6.983960, Accuracy 81.699%\n",
      "Epoch 12, Batch 792, LR 2.495501 Loss 6.982897, Accuracy 81.705%\n",
      "Epoch 12, Batch 793, LR 2.495489 Loss 6.981901, Accuracy 81.710%\n",
      "Epoch 12, Batch 794, LR 2.495478 Loss 6.982122, Accuracy 81.706%\n",
      "Epoch 12, Batch 795, LR 2.495466 Loss 6.981359, Accuracy 81.710%\n",
      "Epoch 12, Batch 796, LR 2.495455 Loss 6.982329, Accuracy 81.710%\n",
      "Epoch 12, Batch 797, LR 2.495444 Loss 6.981946, Accuracy 81.715%\n",
      "Epoch 12, Batch 798, LR 2.495432 Loss 6.982419, Accuracy 81.715%\n",
      "Epoch 12, Batch 799, LR 2.495421 Loss 6.982373, Accuracy 81.715%\n",
      "Epoch 12, Batch 800, LR 2.495409 Loss 6.981488, Accuracy 81.724%\n",
      "Epoch 12, Batch 801, LR 2.495398 Loss 6.980858, Accuracy 81.731%\n",
      "Epoch 12, Batch 802, LR 2.495386 Loss 6.980086, Accuracy 81.736%\n",
      "Epoch 12, Batch 803, LR 2.495375 Loss 6.980123, Accuracy 81.738%\n",
      "Epoch 12, Batch 804, LR 2.495363 Loss 6.980531, Accuracy 81.731%\n",
      "Epoch 12, Batch 805, LR 2.495352 Loss 6.979907, Accuracy 81.734%\n",
      "Epoch 12, Batch 806, LR 2.495340 Loss 6.980457, Accuracy 81.735%\n",
      "Epoch 12, Batch 807, LR 2.495329 Loss 6.979856, Accuracy 81.741%\n",
      "Epoch 12, Batch 808, LR 2.495317 Loss 6.979662, Accuracy 81.741%\n",
      "Epoch 12, Batch 809, LR 2.495305 Loss 6.979676, Accuracy 81.740%\n",
      "Epoch 12, Batch 810, LR 2.495294 Loss 6.979522, Accuracy 81.742%\n",
      "Epoch 12, Batch 811, LR 2.495282 Loss 6.979015, Accuracy 81.740%\n",
      "Epoch 12, Batch 812, LR 2.495271 Loss 6.978812, Accuracy 81.745%\n",
      "Epoch 12, Batch 813, LR 2.495259 Loss 6.978420, Accuracy 81.746%\n",
      "Epoch 12, Batch 814, LR 2.495247 Loss 6.978702, Accuracy 81.747%\n",
      "Epoch 12, Batch 815, LR 2.495236 Loss 6.977866, Accuracy 81.752%\n",
      "Epoch 12, Batch 816, LR 2.495224 Loss 6.978189, Accuracy 81.753%\n",
      "Epoch 12, Batch 817, LR 2.495212 Loss 6.977429, Accuracy 81.762%\n",
      "Epoch 12, Batch 818, LR 2.495200 Loss 6.975990, Accuracy 81.772%\n",
      "Epoch 12, Batch 819, LR 2.495189 Loss 6.976602, Accuracy 81.760%\n",
      "Epoch 12, Batch 820, LR 2.495177 Loss 6.976386, Accuracy 81.759%\n",
      "Epoch 12, Batch 821, LR 2.495165 Loss 6.976391, Accuracy 81.753%\n",
      "Epoch 12, Batch 822, LR 2.495153 Loss 6.975746, Accuracy 81.753%\n",
      "Epoch 12, Batch 823, LR 2.495142 Loss 6.975100, Accuracy 81.756%\n",
      "Epoch 12, Batch 824, LR 2.495130 Loss 6.975189, Accuracy 81.757%\n",
      "Epoch 12, Batch 825, LR 2.495118 Loss 6.975702, Accuracy 81.755%\n",
      "Epoch 12, Batch 826, LR 2.495106 Loss 6.976222, Accuracy 81.750%\n",
      "Epoch 12, Batch 827, LR 2.495094 Loss 6.976016, Accuracy 81.753%\n",
      "Epoch 12, Batch 828, LR 2.495083 Loss 6.975969, Accuracy 81.750%\n",
      "Epoch 12, Batch 829, LR 2.495071 Loss 6.975744, Accuracy 81.753%\n",
      "Epoch 12, Batch 830, LR 2.495059 Loss 6.975497, Accuracy 81.761%\n",
      "Epoch 12, Batch 831, LR 2.495047 Loss 6.975864, Accuracy 81.755%\n",
      "Epoch 12, Batch 832, LR 2.495035 Loss 6.975544, Accuracy 81.760%\n",
      "Epoch 12, Batch 833, LR 2.495023 Loss 6.975231, Accuracy 81.764%\n",
      "Epoch 12, Batch 834, LR 2.495011 Loss 6.975998, Accuracy 81.759%\n",
      "Epoch 12, Batch 835, LR 2.494999 Loss 6.976593, Accuracy 81.751%\n",
      "Epoch 12, Batch 836, LR 2.494987 Loss 6.976419, Accuracy 81.752%\n",
      "Epoch 12, Batch 837, LR 2.494975 Loss 6.975935, Accuracy 81.758%\n",
      "Epoch 12, Batch 838, LR 2.494963 Loss 6.976320, Accuracy 81.753%\n",
      "Epoch 12, Batch 839, LR 2.494951 Loss 6.976210, Accuracy 81.757%\n",
      "Epoch 12, Batch 840, LR 2.494939 Loss 6.976194, Accuracy 81.758%\n",
      "Epoch 12, Batch 841, LR 2.494927 Loss 6.976132, Accuracy 81.756%\n",
      "Epoch 12, Batch 842, LR 2.494915 Loss 6.975350, Accuracy 81.763%\n",
      "Epoch 12, Batch 843, LR 2.494903 Loss 6.975273, Accuracy 81.760%\n",
      "Epoch 12, Batch 844, LR 2.494891 Loss 6.975164, Accuracy 81.761%\n",
      "Epoch 12, Batch 845, LR 2.494879 Loss 6.975042, Accuracy 81.762%\n",
      "Epoch 12, Batch 846, LR 2.494867 Loss 6.974258, Accuracy 81.763%\n",
      "Epoch 12, Batch 847, LR 2.494854 Loss 6.974198, Accuracy 81.765%\n",
      "Epoch 12, Batch 848, LR 2.494842 Loss 6.974239, Accuracy 81.761%\n",
      "Epoch 12, Batch 849, LR 2.494830 Loss 6.974819, Accuracy 81.762%\n",
      "Epoch 12, Batch 850, LR 2.494818 Loss 6.974434, Accuracy 81.765%\n",
      "Epoch 12, Batch 851, LR 2.494806 Loss 6.974708, Accuracy 81.759%\n",
      "Epoch 12, Batch 852, LR 2.494794 Loss 6.975138, Accuracy 81.753%\n",
      "Epoch 12, Batch 853, LR 2.494781 Loss 6.974945, Accuracy 81.753%\n",
      "Epoch 12, Batch 854, LR 2.494769 Loss 6.975226, Accuracy 81.751%\n",
      "Epoch 12, Batch 855, LR 2.494757 Loss 6.974968, Accuracy 81.751%\n",
      "Epoch 12, Batch 856, LR 2.494745 Loss 6.974993, Accuracy 81.750%\n",
      "Epoch 12, Batch 857, LR 2.494732 Loss 6.974618, Accuracy 81.752%\n",
      "Epoch 12, Batch 858, LR 2.494720 Loss 6.974206, Accuracy 81.754%\n",
      "Epoch 12, Batch 859, LR 2.494708 Loss 6.974854, Accuracy 81.752%\n",
      "Epoch 12, Batch 860, LR 2.494695 Loss 6.974970, Accuracy 81.751%\n",
      "Epoch 12, Batch 861, LR 2.494683 Loss 6.974256, Accuracy 81.759%\n",
      "Epoch 12, Batch 862, LR 2.494671 Loss 6.974246, Accuracy 81.759%\n",
      "Epoch 12, Batch 863, LR 2.494658 Loss 6.973429, Accuracy 81.767%\n",
      "Epoch 12, Batch 864, LR 2.494646 Loss 6.973088, Accuracy 81.765%\n",
      "Epoch 12, Batch 865, LR 2.494634 Loss 6.973625, Accuracy 81.765%\n",
      "Epoch 12, Batch 866, LR 2.494621 Loss 6.973697, Accuracy 81.768%\n",
      "Epoch 12, Batch 867, LR 2.494609 Loss 6.974054, Accuracy 81.764%\n",
      "Epoch 12, Batch 868, LR 2.494596 Loss 6.974127, Accuracy 81.759%\n",
      "Epoch 12, Batch 869, LR 2.494584 Loss 6.974327, Accuracy 81.755%\n",
      "Epoch 12, Batch 870, LR 2.494571 Loss 6.974450, Accuracy 81.753%\n",
      "Epoch 12, Batch 871, LR 2.494559 Loss 6.974042, Accuracy 81.752%\n",
      "Epoch 12, Batch 872, LR 2.494546 Loss 6.974042, Accuracy 81.749%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 873, LR 2.494534 Loss 6.974389, Accuracy 81.745%\n",
      "Epoch 12, Batch 874, LR 2.494521 Loss 6.974752, Accuracy 81.743%\n",
      "Epoch 12, Batch 875, LR 2.494509 Loss 6.974538, Accuracy 81.738%\n",
      "Epoch 12, Batch 876, LR 2.494496 Loss 6.973885, Accuracy 81.740%\n",
      "Epoch 12, Batch 877, LR 2.494484 Loss 6.974600, Accuracy 81.735%\n",
      "Epoch 12, Batch 878, LR 2.494471 Loss 6.974703, Accuracy 81.737%\n",
      "Epoch 12, Batch 879, LR 2.494459 Loss 6.974462, Accuracy 81.736%\n",
      "Epoch 12, Batch 880, LR 2.494446 Loss 6.975674, Accuracy 81.730%\n",
      "Epoch 12, Batch 881, LR 2.494433 Loss 6.976266, Accuracy 81.727%\n",
      "Epoch 12, Batch 882, LR 2.494421 Loss 6.975658, Accuracy 81.732%\n",
      "Epoch 12, Batch 883, LR 2.494408 Loss 6.975510, Accuracy 81.734%\n",
      "Epoch 12, Batch 884, LR 2.494395 Loss 6.975432, Accuracy 81.735%\n",
      "Epoch 12, Batch 885, LR 2.494383 Loss 6.975193, Accuracy 81.735%\n",
      "Epoch 12, Batch 886, LR 2.494370 Loss 6.975322, Accuracy 81.731%\n",
      "Epoch 12, Batch 887, LR 2.494357 Loss 6.975033, Accuracy 81.733%\n",
      "Epoch 12, Batch 888, LR 2.494345 Loss 6.974145, Accuracy 81.738%\n",
      "Epoch 12, Batch 889, LR 2.494332 Loss 6.974741, Accuracy 81.735%\n",
      "Epoch 12, Batch 890, LR 2.494319 Loss 6.973876, Accuracy 81.738%\n",
      "Epoch 12, Batch 891, LR 2.494306 Loss 6.973769, Accuracy 81.738%\n",
      "Epoch 12, Batch 892, LR 2.494294 Loss 6.974688, Accuracy 81.730%\n",
      "Epoch 12, Batch 893, LR 2.494281 Loss 6.974952, Accuracy 81.731%\n",
      "Epoch 12, Batch 894, LR 2.494268 Loss 6.975092, Accuracy 81.732%\n",
      "Epoch 12, Batch 895, LR 2.494255 Loss 6.974658, Accuracy 81.734%\n",
      "Epoch 12, Batch 896, LR 2.494242 Loss 6.975504, Accuracy 81.727%\n",
      "Epoch 12, Batch 897, LR 2.494229 Loss 6.975481, Accuracy 81.731%\n",
      "Epoch 12, Batch 898, LR 2.494217 Loss 6.975868, Accuracy 81.728%\n",
      "Epoch 12, Batch 899, LR 2.494204 Loss 6.975395, Accuracy 81.723%\n",
      "Epoch 12, Batch 900, LR 2.494191 Loss 6.975194, Accuracy 81.720%\n",
      "Epoch 12, Batch 901, LR 2.494178 Loss 6.974725, Accuracy 81.722%\n",
      "Epoch 12, Batch 902, LR 2.494165 Loss 6.974383, Accuracy 81.721%\n",
      "Epoch 12, Batch 903, LR 2.494152 Loss 6.974043, Accuracy 81.724%\n",
      "Epoch 12, Batch 904, LR 2.494139 Loss 6.974274, Accuracy 81.724%\n",
      "Epoch 12, Batch 905, LR 2.494126 Loss 6.975331, Accuracy 81.720%\n",
      "Epoch 12, Batch 906, LR 2.494113 Loss 6.975566, Accuracy 81.719%\n",
      "Epoch 12, Batch 907, LR 2.494100 Loss 6.975543, Accuracy 81.716%\n",
      "Epoch 12, Batch 908, LR 2.494087 Loss 6.974707, Accuracy 81.721%\n",
      "Epoch 12, Batch 909, LR 2.494074 Loss 6.975734, Accuracy 81.716%\n",
      "Epoch 12, Batch 910, LR 2.494061 Loss 6.975072, Accuracy 81.718%\n",
      "Epoch 12, Batch 911, LR 2.494048 Loss 6.975513, Accuracy 81.717%\n",
      "Epoch 12, Batch 912, LR 2.494035 Loss 6.975519, Accuracy 81.718%\n",
      "Epoch 12, Batch 913, LR 2.494022 Loss 6.974798, Accuracy 81.715%\n",
      "Epoch 12, Batch 914, LR 2.494009 Loss 6.974588, Accuracy 81.720%\n",
      "Epoch 12, Batch 915, LR 2.493996 Loss 6.974440, Accuracy 81.716%\n",
      "Epoch 12, Batch 916, LR 2.493983 Loss 6.976315, Accuracy 81.709%\n",
      "Epoch 12, Batch 917, LR 2.493969 Loss 6.975974, Accuracy 81.711%\n",
      "Epoch 12, Batch 918, LR 2.493956 Loss 6.976452, Accuracy 81.708%\n",
      "Epoch 12, Batch 919, LR 2.493943 Loss 6.976627, Accuracy 81.711%\n",
      "Epoch 12, Batch 920, LR 2.493930 Loss 6.975828, Accuracy 81.715%\n",
      "Epoch 12, Batch 921, LR 2.493917 Loss 6.976441, Accuracy 81.712%\n",
      "Epoch 12, Batch 922, LR 2.493904 Loss 6.975883, Accuracy 81.714%\n",
      "Epoch 12, Batch 923, LR 2.493890 Loss 6.975216, Accuracy 81.718%\n",
      "Epoch 12, Batch 924, LR 2.493877 Loss 6.975528, Accuracy 81.713%\n",
      "Epoch 12, Batch 925, LR 2.493864 Loss 6.975616, Accuracy 81.713%\n",
      "Epoch 12, Batch 926, LR 2.493851 Loss 6.975744, Accuracy 81.710%\n",
      "Epoch 12, Batch 927, LR 2.493837 Loss 6.975269, Accuracy 81.713%\n",
      "Epoch 12, Batch 928, LR 2.493824 Loss 6.975271, Accuracy 81.717%\n",
      "Epoch 12, Batch 929, LR 2.493811 Loss 6.975817, Accuracy 81.716%\n",
      "Epoch 12, Batch 930, LR 2.493797 Loss 6.976112, Accuracy 81.713%\n",
      "Epoch 12, Batch 931, LR 2.493784 Loss 6.975780, Accuracy 81.713%\n",
      "Epoch 12, Batch 932, LR 2.493771 Loss 6.976170, Accuracy 81.710%\n",
      "Epoch 12, Batch 933, LR 2.493757 Loss 6.975974, Accuracy 81.713%\n",
      "Epoch 12, Batch 934, LR 2.493744 Loss 6.975106, Accuracy 81.718%\n",
      "Epoch 12, Batch 935, LR 2.493731 Loss 6.975869, Accuracy 81.714%\n",
      "Epoch 12, Batch 936, LR 2.493717 Loss 6.975887, Accuracy 81.713%\n",
      "Epoch 12, Batch 937, LR 2.493704 Loss 6.976200, Accuracy 81.714%\n",
      "Epoch 12, Batch 938, LR 2.493690 Loss 6.976848, Accuracy 81.708%\n",
      "Epoch 12, Batch 939, LR 2.493677 Loss 6.976792, Accuracy 81.709%\n",
      "Epoch 12, Batch 940, LR 2.493663 Loss 6.975861, Accuracy 81.715%\n",
      "Epoch 12, Batch 941, LR 2.493650 Loss 6.975430, Accuracy 81.716%\n",
      "Epoch 12, Batch 942, LR 2.493636 Loss 6.974743, Accuracy 81.721%\n",
      "Epoch 12, Batch 943, LR 2.493623 Loss 6.974918, Accuracy 81.717%\n",
      "Epoch 12, Batch 944, LR 2.493609 Loss 6.975578, Accuracy 81.716%\n",
      "Epoch 12, Batch 945, LR 2.493596 Loss 6.975688, Accuracy 81.715%\n",
      "Epoch 12, Batch 946, LR 2.493582 Loss 6.975340, Accuracy 81.719%\n",
      "Epoch 12, Batch 947, LR 2.493569 Loss 6.975276, Accuracy 81.719%\n",
      "Epoch 12, Batch 948, LR 2.493555 Loss 6.975209, Accuracy 81.715%\n",
      "Epoch 12, Batch 949, LR 2.493542 Loss 6.975489, Accuracy 81.713%\n",
      "Epoch 12, Batch 950, LR 2.493528 Loss 6.975922, Accuracy 81.708%\n",
      "Epoch 12, Batch 951, LR 2.493514 Loss 6.975232, Accuracy 81.713%\n",
      "Epoch 12, Batch 952, LR 2.493501 Loss 6.975506, Accuracy 81.710%\n",
      "Epoch 12, Batch 953, LR 2.493487 Loss 6.975598, Accuracy 81.707%\n",
      "Epoch 12, Batch 954, LR 2.493473 Loss 6.975555, Accuracy 81.709%\n",
      "Epoch 12, Batch 955, LR 2.493460 Loss 6.975756, Accuracy 81.706%\n",
      "Epoch 12, Batch 956, LR 2.493446 Loss 6.974862, Accuracy 81.707%\n",
      "Epoch 12, Batch 957, LR 2.493432 Loss 6.974558, Accuracy 81.711%\n",
      "Epoch 12, Batch 958, LR 2.493419 Loss 6.974305, Accuracy 81.710%\n",
      "Epoch 12, Batch 959, LR 2.493405 Loss 6.974528, Accuracy 81.709%\n",
      "Epoch 12, Batch 960, LR 2.493391 Loss 6.974037, Accuracy 81.713%\n",
      "Epoch 12, Batch 961, LR 2.493377 Loss 6.973760, Accuracy 81.712%\n",
      "Epoch 12, Batch 962, LR 2.493364 Loss 6.973230, Accuracy 81.710%\n",
      "Epoch 12, Batch 963, LR 2.493350 Loss 6.973570, Accuracy 81.708%\n",
      "Epoch 12, Batch 964, LR 2.493336 Loss 6.973243, Accuracy 81.710%\n",
      "Epoch 12, Batch 965, LR 2.493322 Loss 6.972560, Accuracy 81.716%\n",
      "Epoch 12, Batch 966, LR 2.493308 Loss 6.972627, Accuracy 81.714%\n",
      "Epoch 12, Batch 967, LR 2.493294 Loss 6.972396, Accuracy 81.716%\n",
      "Epoch 12, Batch 968, LR 2.493281 Loss 6.971841, Accuracy 81.720%\n",
      "Epoch 12, Batch 969, LR 2.493267 Loss 6.971413, Accuracy 81.722%\n",
      "Epoch 12, Batch 970, LR 2.493253 Loss 6.970099, Accuracy 81.728%\n",
      "Epoch 12, Batch 971, LR 2.493239 Loss 6.970114, Accuracy 81.730%\n",
      "Epoch 12, Batch 972, LR 2.493225 Loss 6.969817, Accuracy 81.732%\n",
      "Epoch 12, Batch 973, LR 2.493211 Loss 6.969761, Accuracy 81.730%\n",
      "Epoch 12, Batch 974, LR 2.493197 Loss 6.969615, Accuracy 81.733%\n",
      "Epoch 12, Batch 975, LR 2.493183 Loss 6.969841, Accuracy 81.729%\n",
      "Epoch 12, Batch 976, LR 2.493169 Loss 6.969577, Accuracy 81.730%\n",
      "Epoch 12, Batch 977, LR 2.493155 Loss 6.969364, Accuracy 81.727%\n",
      "Epoch 12, Batch 978, LR 2.493141 Loss 6.968678, Accuracy 81.732%\n",
      "Epoch 12, Batch 979, LR 2.493127 Loss 6.968910, Accuracy 81.734%\n",
      "Epoch 12, Batch 980, LR 2.493113 Loss 6.968624, Accuracy 81.735%\n",
      "Epoch 12, Batch 981, LR 2.493099 Loss 6.968067, Accuracy 81.737%\n",
      "Epoch 12, Batch 982, LR 2.493085 Loss 6.967662, Accuracy 81.741%\n",
      "Epoch 12, Batch 983, LR 2.493071 Loss 6.967045, Accuracy 81.748%\n",
      "Epoch 12, Batch 984, LR 2.493057 Loss 6.966420, Accuracy 81.753%\n",
      "Epoch 12, Batch 985, LR 2.493043 Loss 6.966990, Accuracy 81.751%\n",
      "Epoch 12, Batch 986, LR 2.493029 Loss 6.966736, Accuracy 81.754%\n",
      "Epoch 12, Batch 987, LR 2.493014 Loss 6.966507, Accuracy 81.755%\n",
      "Epoch 12, Batch 988, LR 2.493000 Loss 6.965423, Accuracy 81.765%\n",
      "Epoch 12, Batch 989, LR 2.492986 Loss 6.964704, Accuracy 81.769%\n",
      "Epoch 12, Batch 990, LR 2.492972 Loss 6.964248, Accuracy 81.772%\n",
      "Epoch 12, Batch 991, LR 2.492958 Loss 6.964069, Accuracy 81.773%\n",
      "Epoch 12, Batch 992, LR 2.492944 Loss 6.964377, Accuracy 81.775%\n",
      "Epoch 12, Batch 993, LR 2.492929 Loss 6.963505, Accuracy 81.783%\n",
      "Epoch 12, Batch 994, LR 2.492915 Loss 6.963538, Accuracy 81.782%\n",
      "Epoch 12, Batch 995, LR 2.492901 Loss 6.963301, Accuracy 81.786%\n",
      "Epoch 12, Batch 996, LR 2.492887 Loss 6.963058, Accuracy 81.783%\n",
      "Epoch 12, Batch 997, LR 2.492872 Loss 6.963140, Accuracy 81.782%\n",
      "Epoch 12, Batch 998, LR 2.492858 Loss 6.962345, Accuracy 81.785%\n",
      "Epoch 12, Batch 999, LR 2.492844 Loss 6.962127, Accuracy 81.782%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 1000, LR 2.492829 Loss 6.962228, Accuracy 81.782%\n",
      "Epoch 12, Batch 1001, LR 2.492815 Loss 6.961544, Accuracy 81.788%\n",
      "Epoch 12, Batch 1002, LR 2.492801 Loss 6.961869, Accuracy 81.781%\n",
      "Epoch 12, Batch 1003, LR 2.492786 Loss 6.961587, Accuracy 81.780%\n",
      "Epoch 12, Batch 1004, LR 2.492772 Loss 6.961011, Accuracy 81.781%\n",
      "Epoch 12, Batch 1005, LR 2.492758 Loss 6.960803, Accuracy 81.784%\n",
      "Epoch 12, Batch 1006, LR 2.492743 Loss 6.960218, Accuracy 81.786%\n",
      "Epoch 12, Batch 1007, LR 2.492729 Loss 6.960011, Accuracy 81.790%\n",
      "Epoch 12, Batch 1008, LR 2.492714 Loss 6.959553, Accuracy 81.793%\n",
      "Epoch 12, Batch 1009, LR 2.492700 Loss 6.959525, Accuracy 81.794%\n",
      "Epoch 12, Batch 1010, LR 2.492685 Loss 6.959808, Accuracy 81.786%\n",
      "Epoch 12, Batch 1011, LR 2.492671 Loss 6.959201, Accuracy 81.792%\n",
      "Epoch 12, Batch 1012, LR 2.492656 Loss 6.958677, Accuracy 81.793%\n",
      "Epoch 12, Batch 1013, LR 2.492642 Loss 6.958618, Accuracy 81.794%\n",
      "Epoch 12, Batch 1014, LR 2.492627 Loss 6.958562, Accuracy 81.794%\n",
      "Epoch 12, Batch 1015, LR 2.492613 Loss 6.958721, Accuracy 81.792%\n",
      "Epoch 12, Batch 1016, LR 2.492598 Loss 6.958835, Accuracy 81.789%\n",
      "Epoch 12, Batch 1017, LR 2.492584 Loss 6.958329, Accuracy 81.791%\n",
      "Epoch 12, Batch 1018, LR 2.492569 Loss 6.958588, Accuracy 81.786%\n",
      "Epoch 12, Batch 1019, LR 2.492555 Loss 6.957437, Accuracy 81.793%\n",
      "Epoch 12, Batch 1020, LR 2.492540 Loss 6.957901, Accuracy 81.788%\n",
      "Epoch 12, Batch 1021, LR 2.492525 Loss 6.957458, Accuracy 81.788%\n",
      "Epoch 12, Batch 1022, LR 2.492511 Loss 6.957188, Accuracy 81.790%\n",
      "Epoch 12, Batch 1023, LR 2.492496 Loss 6.957203, Accuracy 81.791%\n",
      "Epoch 12, Batch 1024, LR 2.492481 Loss 6.956723, Accuracy 81.795%\n",
      "Epoch 12, Batch 1025, LR 2.492467 Loss 6.956662, Accuracy 81.795%\n",
      "Epoch 12, Batch 1026, LR 2.492452 Loss 6.957229, Accuracy 81.785%\n",
      "Epoch 12, Batch 1027, LR 2.492437 Loss 6.956945, Accuracy 81.785%\n",
      "Epoch 12, Batch 1028, LR 2.492423 Loss 6.956333, Accuracy 81.784%\n",
      "Epoch 12, Batch 1029, LR 2.492408 Loss 6.955968, Accuracy 81.784%\n",
      "Epoch 12, Batch 1030, LR 2.492393 Loss 6.955028, Accuracy 81.789%\n",
      "Epoch 12, Batch 1031, LR 2.492378 Loss 6.955414, Accuracy 81.790%\n",
      "Epoch 12, Batch 1032, LR 2.492364 Loss 6.955574, Accuracy 81.785%\n",
      "Epoch 12, Batch 1033, LR 2.492349 Loss 6.955850, Accuracy 81.784%\n",
      "Epoch 12, Batch 1034, LR 2.492334 Loss 6.955467, Accuracy 81.785%\n",
      "Epoch 12, Batch 1035, LR 2.492319 Loss 6.955471, Accuracy 81.786%\n",
      "Epoch 12, Batch 1036, LR 2.492304 Loss 6.956018, Accuracy 81.780%\n",
      "Epoch 12, Batch 1037, LR 2.492290 Loss 6.956158, Accuracy 81.783%\n",
      "Epoch 12, Batch 1038, LR 2.492275 Loss 6.955879, Accuracy 81.787%\n",
      "Epoch 12, Batch 1039, LR 2.492260 Loss 6.955983, Accuracy 81.785%\n",
      "Epoch 12, Batch 1040, LR 2.492245 Loss 6.956721, Accuracy 81.779%\n",
      "Epoch 12, Batch 1041, LR 2.492230 Loss 6.957319, Accuracy 81.775%\n",
      "Epoch 12, Batch 1042, LR 2.492215 Loss 6.957547, Accuracy 81.776%\n",
      "Epoch 12, Batch 1043, LR 2.492200 Loss 6.957703, Accuracy 81.777%\n",
      "Epoch 12, Batch 1044, LR 2.492185 Loss 6.956748, Accuracy 81.783%\n",
      "Epoch 12, Batch 1045, LR 2.492170 Loss 6.956444, Accuracy 81.787%\n",
      "Epoch 12, Batch 1046, LR 2.492155 Loss 6.956287, Accuracy 81.791%\n",
      "Epoch 12, Batch 1047, LR 2.492140 Loss 6.956027, Accuracy 81.794%\n",
      "Epoch 12, Loss (train set) 6.956027, Accuracy (train set) 81.794%\n",
      "Epoch 13, Batch 1, LR 2.492125 Loss 6.835870, Accuracy 81.250%\n",
      "Epoch 13, Batch 2, LR 2.492110 Loss 7.166450, Accuracy 80.859%\n",
      "Epoch 13, Batch 3, LR 2.492095 Loss 6.802756, Accuracy 82.031%\n",
      "Epoch 13, Batch 4, LR 2.492080 Loss 6.844151, Accuracy 81.641%\n",
      "Epoch 13, Batch 5, LR 2.492065 Loss 6.768378, Accuracy 81.719%\n",
      "Epoch 13, Batch 6, LR 2.492050 Loss 6.723203, Accuracy 82.161%\n",
      "Epoch 13, Batch 7, LR 2.492035 Loss 6.755575, Accuracy 81.920%\n",
      "Epoch 13, Batch 8, LR 2.492020 Loss 6.717357, Accuracy 82.715%\n",
      "Epoch 13, Batch 9, LR 2.492005 Loss 6.715786, Accuracy 82.465%\n",
      "Epoch 13, Batch 10, LR 2.491990 Loss 6.711121, Accuracy 82.969%\n",
      "Epoch 13, Batch 11, LR 2.491974 Loss 6.676353, Accuracy 83.026%\n",
      "Epoch 13, Batch 12, LR 2.491959 Loss 6.677442, Accuracy 83.138%\n",
      "Epoch 13, Batch 13, LR 2.491944 Loss 6.666736, Accuracy 82.993%\n",
      "Epoch 13, Batch 14, LR 2.491929 Loss 6.661152, Accuracy 82.812%\n",
      "Epoch 13, Batch 15, LR 2.491914 Loss 6.656234, Accuracy 82.760%\n",
      "Epoch 13, Batch 16, LR 2.491899 Loss 6.633785, Accuracy 83.105%\n",
      "Epoch 13, Batch 17, LR 2.491883 Loss 6.606305, Accuracy 83.180%\n",
      "Epoch 13, Batch 18, LR 2.491868 Loss 6.611188, Accuracy 83.030%\n",
      "Epoch 13, Batch 19, LR 2.491853 Loss 6.625746, Accuracy 82.812%\n",
      "Epoch 13, Batch 20, LR 2.491837 Loss 6.661494, Accuracy 82.383%\n",
      "Epoch 13, Batch 21, LR 2.491822 Loss 6.660616, Accuracy 82.478%\n",
      "Epoch 13, Batch 22, LR 2.491807 Loss 6.632317, Accuracy 82.528%\n",
      "Epoch 13, Batch 23, LR 2.491792 Loss 6.635742, Accuracy 82.609%\n",
      "Epoch 13, Batch 24, LR 2.491776 Loss 6.615683, Accuracy 82.780%\n",
      "Epoch 13, Batch 25, LR 2.491761 Loss 6.617323, Accuracy 82.812%\n",
      "Epoch 13, Batch 26, LR 2.491746 Loss 6.633061, Accuracy 82.722%\n",
      "Epoch 13, Batch 27, LR 2.491730 Loss 6.643621, Accuracy 82.726%\n",
      "Epoch 13, Batch 28, LR 2.491715 Loss 6.653425, Accuracy 82.645%\n",
      "Epoch 13, Batch 29, LR 2.491699 Loss 6.656050, Accuracy 82.651%\n",
      "Epoch 13, Batch 30, LR 2.491684 Loss 6.654769, Accuracy 82.786%\n",
      "Epoch 13, Batch 31, LR 2.491669 Loss 6.655020, Accuracy 82.888%\n",
      "Epoch 13, Batch 32, LR 2.491653 Loss 6.664564, Accuracy 82.739%\n",
      "Epoch 13, Batch 33, LR 2.491638 Loss 6.658038, Accuracy 82.765%\n",
      "Epoch 13, Batch 34, LR 2.491622 Loss 6.666832, Accuracy 82.812%\n",
      "Epoch 13, Batch 35, LR 2.491607 Loss 6.657041, Accuracy 82.902%\n",
      "Epoch 13, Batch 36, LR 2.491591 Loss 6.656658, Accuracy 82.878%\n",
      "Epoch 13, Batch 37, LR 2.491576 Loss 6.658995, Accuracy 82.876%\n",
      "Epoch 13, Batch 38, LR 2.491560 Loss 6.642751, Accuracy 83.018%\n",
      "Epoch 13, Batch 39, LR 2.491545 Loss 6.625335, Accuracy 83.193%\n",
      "Epoch 13, Batch 40, LR 2.491529 Loss 6.627660, Accuracy 83.262%\n",
      "Epoch 13, Batch 41, LR 2.491513 Loss 6.642365, Accuracy 83.175%\n",
      "Epoch 13, Batch 42, LR 2.491498 Loss 6.647061, Accuracy 83.147%\n",
      "Epoch 13, Batch 43, LR 2.491482 Loss 6.647209, Accuracy 83.176%\n",
      "Epoch 13, Batch 44, LR 2.491467 Loss 6.663059, Accuracy 83.079%\n",
      "Epoch 13, Batch 45, LR 2.491451 Loss 6.656475, Accuracy 83.142%\n",
      "Epoch 13, Batch 46, LR 2.491435 Loss 6.659433, Accuracy 83.135%\n",
      "Epoch 13, Batch 47, LR 2.491420 Loss 6.684789, Accuracy 82.979%\n",
      "Epoch 13, Batch 48, LR 2.491404 Loss 6.682502, Accuracy 83.089%\n",
      "Epoch 13, Batch 49, LR 2.491388 Loss 6.676435, Accuracy 83.131%\n",
      "Epoch 13, Batch 50, LR 2.491373 Loss 6.679312, Accuracy 83.109%\n",
      "Epoch 13, Batch 51, LR 2.491357 Loss 6.685574, Accuracy 83.088%\n",
      "Epoch 13, Batch 52, LR 2.491341 Loss 6.687474, Accuracy 83.173%\n",
      "Epoch 13, Batch 53, LR 2.491325 Loss 6.690435, Accuracy 83.137%\n",
      "Epoch 13, Batch 54, LR 2.491310 Loss 6.695527, Accuracy 83.073%\n",
      "Epoch 13, Batch 55, LR 2.491294 Loss 6.693299, Accuracy 83.068%\n",
      "Epoch 13, Batch 56, LR 2.491278 Loss 6.683481, Accuracy 83.092%\n",
      "Epoch 13, Batch 57, LR 2.491262 Loss 6.688388, Accuracy 83.087%\n",
      "Epoch 13, Batch 58, LR 2.491246 Loss 6.696989, Accuracy 83.136%\n",
      "Epoch 13, Batch 59, LR 2.491231 Loss 6.712317, Accuracy 82.998%\n",
      "Epoch 13, Batch 60, LR 2.491215 Loss 6.721390, Accuracy 82.956%\n",
      "Epoch 13, Batch 61, LR 2.491199 Loss 6.726002, Accuracy 82.877%\n",
      "Epoch 13, Batch 62, LR 2.491183 Loss 6.722126, Accuracy 82.913%\n",
      "Epoch 13, Batch 63, LR 2.491167 Loss 6.717536, Accuracy 82.961%\n",
      "Epoch 13, Batch 64, LR 2.491151 Loss 6.723855, Accuracy 82.922%\n",
      "Epoch 13, Batch 65, LR 2.491135 Loss 6.726444, Accuracy 82.885%\n",
      "Epoch 13, Batch 66, LR 2.491119 Loss 6.731767, Accuracy 82.895%\n",
      "Epoch 13, Batch 67, LR 2.491103 Loss 6.733089, Accuracy 82.906%\n",
      "Epoch 13, Batch 68, LR 2.491087 Loss 6.744046, Accuracy 82.778%\n",
      "Epoch 13, Batch 69, LR 2.491071 Loss 6.743471, Accuracy 82.801%\n",
      "Epoch 13, Batch 70, LR 2.491056 Loss 6.730657, Accuracy 82.879%\n",
      "Epoch 13, Batch 71, LR 2.491040 Loss 6.735650, Accuracy 82.812%\n",
      "Epoch 13, Batch 72, LR 2.491023 Loss 6.742919, Accuracy 82.726%\n",
      "Epoch 13, Batch 73, LR 2.491007 Loss 6.745052, Accuracy 82.748%\n",
      "Epoch 13, Batch 74, LR 2.490991 Loss 6.748383, Accuracy 82.707%\n",
      "Epoch 13, Batch 75, LR 2.490975 Loss 6.751863, Accuracy 82.656%\n",
      "Epoch 13, Batch 76, LR 2.490959 Loss 6.744932, Accuracy 82.741%\n",
      "Epoch 13, Batch 77, LR 2.490943 Loss 6.742721, Accuracy 82.792%\n",
      "Epoch 13, Batch 78, LR 2.490927 Loss 6.744380, Accuracy 82.762%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 79, LR 2.490911 Loss 6.743936, Accuracy 82.793%\n",
      "Epoch 13, Batch 80, LR 2.490895 Loss 6.744355, Accuracy 82.832%\n",
      "Epoch 13, Batch 81, LR 2.490879 Loss 6.737751, Accuracy 82.890%\n",
      "Epoch 13, Batch 82, LR 2.490863 Loss 6.730525, Accuracy 82.908%\n",
      "Epoch 13, Batch 83, LR 2.490846 Loss 6.721471, Accuracy 82.916%\n",
      "Epoch 13, Batch 84, LR 2.490830 Loss 6.723719, Accuracy 82.896%\n",
      "Epoch 13, Batch 85, LR 2.490814 Loss 6.726303, Accuracy 82.877%\n",
      "Epoch 13, Batch 86, LR 2.490798 Loss 6.730032, Accuracy 82.894%\n",
      "Epoch 13, Batch 87, LR 2.490782 Loss 6.726963, Accuracy 82.884%\n",
      "Epoch 13, Batch 88, LR 2.490765 Loss 6.726517, Accuracy 82.910%\n",
      "Epoch 13, Batch 89, LR 2.490749 Loss 6.724257, Accuracy 82.918%\n",
      "Epoch 13, Batch 90, LR 2.490733 Loss 6.728515, Accuracy 82.899%\n",
      "Epoch 13, Batch 91, LR 2.490716 Loss 6.732116, Accuracy 82.907%\n",
      "Epoch 13, Batch 92, LR 2.490700 Loss 6.736268, Accuracy 82.906%\n",
      "Epoch 13, Batch 93, LR 2.490684 Loss 6.733596, Accuracy 82.964%\n",
      "Epoch 13, Batch 94, LR 2.490667 Loss 6.741414, Accuracy 82.921%\n",
      "Epoch 13, Batch 95, LR 2.490651 Loss 6.740476, Accuracy 82.919%\n",
      "Epoch 13, Batch 96, LR 2.490635 Loss 6.740828, Accuracy 82.935%\n",
      "Epoch 13, Batch 97, LR 2.490618 Loss 6.739697, Accuracy 82.941%\n",
      "Epoch 13, Batch 98, LR 2.490602 Loss 6.743988, Accuracy 82.892%\n",
      "Epoch 13, Batch 99, LR 2.490586 Loss 6.744068, Accuracy 82.868%\n",
      "Epoch 13, Batch 100, LR 2.490569 Loss 6.748439, Accuracy 82.844%\n",
      "Epoch 13, Batch 101, LR 2.490553 Loss 6.749862, Accuracy 82.797%\n",
      "Epoch 13, Batch 102, LR 2.490536 Loss 6.752250, Accuracy 82.828%\n",
      "Epoch 13, Batch 103, LR 2.490520 Loss 6.751397, Accuracy 82.790%\n",
      "Epoch 13, Batch 104, LR 2.490503 Loss 6.755557, Accuracy 82.790%\n",
      "Epoch 13, Batch 105, LR 2.490487 Loss 6.754881, Accuracy 82.768%\n",
      "Epoch 13, Batch 106, LR 2.490470 Loss 6.763656, Accuracy 82.746%\n",
      "Epoch 13, Batch 107, LR 2.490454 Loss 6.769966, Accuracy 82.725%\n",
      "Epoch 13, Batch 108, LR 2.490437 Loss 6.770232, Accuracy 82.711%\n",
      "Epoch 13, Batch 109, LR 2.490421 Loss 6.774828, Accuracy 82.691%\n",
      "Epoch 13, Batch 110, LR 2.490404 Loss 6.770140, Accuracy 82.727%\n",
      "Epoch 13, Batch 111, LR 2.490388 Loss 6.771627, Accuracy 82.721%\n",
      "Epoch 13, Batch 112, LR 2.490371 Loss 6.774835, Accuracy 82.694%\n",
      "Epoch 13, Batch 113, LR 2.490354 Loss 6.776412, Accuracy 82.702%\n",
      "Epoch 13, Batch 114, LR 2.490338 Loss 6.774663, Accuracy 82.717%\n",
      "Epoch 13, Batch 115, LR 2.490321 Loss 6.776371, Accuracy 82.711%\n",
      "Epoch 13, Batch 116, LR 2.490305 Loss 6.777701, Accuracy 82.698%\n",
      "Epoch 13, Batch 117, LR 2.490288 Loss 6.774413, Accuracy 82.706%\n",
      "Epoch 13, Batch 118, LR 2.490271 Loss 6.776711, Accuracy 82.693%\n",
      "Epoch 13, Batch 119, LR 2.490255 Loss 6.771964, Accuracy 82.701%\n",
      "Epoch 13, Batch 120, LR 2.490238 Loss 6.771160, Accuracy 82.734%\n",
      "Epoch 13, Batch 121, LR 2.490221 Loss 6.771377, Accuracy 82.716%\n",
      "Epoch 13, Batch 122, LR 2.490204 Loss 6.776514, Accuracy 82.691%\n",
      "Epoch 13, Batch 123, LR 2.490188 Loss 6.778247, Accuracy 82.660%\n",
      "Epoch 13, Batch 124, LR 2.490171 Loss 6.781726, Accuracy 82.611%\n",
      "Epoch 13, Batch 125, LR 2.490154 Loss 6.782085, Accuracy 82.612%\n",
      "Epoch 13, Batch 126, LR 2.490137 Loss 6.783873, Accuracy 82.620%\n",
      "Epoch 13, Batch 127, LR 2.490121 Loss 6.785761, Accuracy 82.585%\n",
      "Epoch 13, Batch 128, LR 2.490104 Loss 6.784121, Accuracy 82.574%\n",
      "Epoch 13, Batch 129, LR 2.490087 Loss 6.786696, Accuracy 82.558%\n",
      "Epoch 13, Batch 130, LR 2.490070 Loss 6.785539, Accuracy 82.548%\n",
      "Epoch 13, Batch 131, LR 2.490053 Loss 6.787620, Accuracy 82.574%\n",
      "Epoch 13, Batch 132, LR 2.490036 Loss 6.790462, Accuracy 82.564%\n",
      "Epoch 13, Batch 133, LR 2.490019 Loss 6.790085, Accuracy 82.554%\n",
      "Epoch 13, Batch 134, LR 2.490003 Loss 6.792545, Accuracy 82.515%\n",
      "Epoch 13, Batch 135, LR 2.489986 Loss 6.793182, Accuracy 82.512%\n",
      "Epoch 13, Batch 136, LR 2.489969 Loss 6.791380, Accuracy 82.531%\n",
      "Epoch 13, Batch 137, LR 2.489952 Loss 6.791861, Accuracy 82.550%\n",
      "Epoch 13, Batch 138, LR 2.489935 Loss 6.791356, Accuracy 82.575%\n",
      "Epoch 13, Batch 139, LR 2.489918 Loss 6.793438, Accuracy 82.548%\n",
      "Epoch 13, Batch 140, LR 2.489901 Loss 6.787163, Accuracy 82.589%\n",
      "Epoch 13, Batch 141, LR 2.489884 Loss 6.792683, Accuracy 82.580%\n",
      "Epoch 13, Batch 142, LR 2.489867 Loss 6.793555, Accuracy 82.559%\n",
      "Epoch 13, Batch 143, LR 2.489850 Loss 6.792767, Accuracy 82.567%\n",
      "Epoch 13, Batch 144, LR 2.489833 Loss 6.787481, Accuracy 82.585%\n",
      "Epoch 13, Batch 145, LR 2.489816 Loss 6.783010, Accuracy 82.613%\n",
      "Epoch 13, Batch 146, LR 2.489799 Loss 6.781982, Accuracy 82.615%\n",
      "Epoch 13, Batch 147, LR 2.489782 Loss 6.780758, Accuracy 82.632%\n",
      "Epoch 13, Batch 148, LR 2.489764 Loss 6.780042, Accuracy 82.612%\n",
      "Epoch 13, Batch 149, LR 2.489747 Loss 6.779239, Accuracy 82.613%\n",
      "Epoch 13, Batch 150, LR 2.489730 Loss 6.778362, Accuracy 82.620%\n",
      "Epoch 13, Batch 151, LR 2.489713 Loss 6.779344, Accuracy 82.585%\n",
      "Epoch 13, Batch 152, LR 2.489696 Loss 6.785006, Accuracy 82.530%\n",
      "Epoch 13, Batch 153, LR 2.489679 Loss 6.786794, Accuracy 82.506%\n",
      "Epoch 13, Batch 154, LR 2.489662 Loss 6.782910, Accuracy 82.544%\n",
      "Epoch 13, Batch 155, LR 2.489644 Loss 6.780978, Accuracy 82.560%\n",
      "Epoch 13, Batch 156, LR 2.489627 Loss 6.777975, Accuracy 82.587%\n",
      "Epoch 13, Batch 157, LR 2.489610 Loss 6.778548, Accuracy 82.584%\n",
      "Epoch 13, Batch 158, LR 2.489593 Loss 6.776782, Accuracy 82.580%\n",
      "Epoch 13, Batch 159, LR 2.489575 Loss 6.776478, Accuracy 82.616%\n",
      "Epoch 13, Batch 160, LR 2.489558 Loss 6.775349, Accuracy 82.656%\n",
      "Epoch 13, Batch 161, LR 2.489541 Loss 6.775066, Accuracy 82.648%\n",
      "Epoch 13, Batch 162, LR 2.489524 Loss 6.778952, Accuracy 82.634%\n",
      "Epoch 13, Batch 163, LR 2.489506 Loss 6.778036, Accuracy 82.635%\n",
      "Epoch 13, Batch 164, LR 2.489489 Loss 6.783268, Accuracy 82.593%\n",
      "Epoch 13, Batch 165, LR 2.489472 Loss 6.783998, Accuracy 82.595%\n",
      "Epoch 13, Batch 166, LR 2.489454 Loss 6.784370, Accuracy 82.596%\n",
      "Epoch 13, Batch 167, LR 2.489437 Loss 6.781354, Accuracy 82.602%\n",
      "Epoch 13, Batch 168, LR 2.489419 Loss 6.780921, Accuracy 82.617%\n",
      "Epoch 13, Batch 169, LR 2.489402 Loss 6.779301, Accuracy 82.623%\n",
      "Epoch 13, Batch 170, LR 2.489385 Loss 6.781370, Accuracy 82.615%\n",
      "Epoch 13, Batch 171, LR 2.489367 Loss 6.780037, Accuracy 82.607%\n",
      "Epoch 13, Batch 172, LR 2.489350 Loss 6.779673, Accuracy 82.585%\n",
      "Epoch 13, Batch 173, LR 2.489332 Loss 6.779614, Accuracy 82.564%\n",
      "Epoch 13, Batch 174, LR 2.489315 Loss 6.774533, Accuracy 82.588%\n",
      "Epoch 13, Batch 175, LR 2.489297 Loss 6.772389, Accuracy 82.580%\n",
      "Epoch 13, Batch 176, LR 2.489280 Loss 6.770231, Accuracy 82.582%\n",
      "Epoch 13, Batch 177, LR 2.489262 Loss 6.768417, Accuracy 82.583%\n",
      "Epoch 13, Batch 178, LR 2.489245 Loss 6.771587, Accuracy 82.575%\n",
      "Epoch 13, Batch 179, LR 2.489227 Loss 6.771197, Accuracy 82.594%\n",
      "Epoch 13, Batch 180, LR 2.489210 Loss 6.770831, Accuracy 82.591%\n",
      "Epoch 13, Batch 181, LR 2.489192 Loss 6.768388, Accuracy 82.605%\n",
      "Epoch 13, Batch 182, LR 2.489175 Loss 6.766166, Accuracy 82.615%\n",
      "Epoch 13, Batch 183, LR 2.489157 Loss 6.768206, Accuracy 82.603%\n",
      "Epoch 13, Batch 184, LR 2.489139 Loss 6.766957, Accuracy 82.621%\n",
      "Epoch 13, Batch 185, LR 2.489122 Loss 6.763630, Accuracy 82.644%\n",
      "Epoch 13, Batch 186, LR 2.489104 Loss 6.761889, Accuracy 82.674%\n",
      "Epoch 13, Batch 187, LR 2.489086 Loss 6.762128, Accuracy 82.679%\n",
      "Epoch 13, Batch 188, LR 2.489069 Loss 6.759348, Accuracy 82.696%\n",
      "Epoch 13, Batch 189, LR 2.489051 Loss 6.759321, Accuracy 82.693%\n",
      "Epoch 13, Batch 190, LR 2.489033 Loss 6.753678, Accuracy 82.714%\n",
      "Epoch 13, Batch 191, LR 2.489016 Loss 6.753022, Accuracy 82.718%\n",
      "Epoch 13, Batch 192, LR 2.488998 Loss 6.752989, Accuracy 82.707%\n",
      "Epoch 13, Batch 193, LR 2.488980 Loss 6.754380, Accuracy 82.695%\n",
      "Epoch 13, Batch 194, LR 2.488962 Loss 6.753152, Accuracy 82.712%\n",
      "Epoch 13, Batch 195, LR 2.488945 Loss 6.752038, Accuracy 82.720%\n",
      "Epoch 13, Batch 196, LR 2.488927 Loss 6.750511, Accuracy 82.741%\n",
      "Epoch 13, Batch 197, LR 2.488909 Loss 6.749466, Accuracy 82.741%\n",
      "Epoch 13, Batch 198, LR 2.488891 Loss 6.751039, Accuracy 82.722%\n",
      "Epoch 13, Batch 199, LR 2.488873 Loss 6.749906, Accuracy 82.718%\n",
      "Epoch 13, Batch 200, LR 2.488856 Loss 6.749776, Accuracy 82.723%\n",
      "Epoch 13, Batch 201, LR 2.488838 Loss 6.750893, Accuracy 82.727%\n",
      "Epoch 13, Batch 202, LR 2.488820 Loss 6.750024, Accuracy 82.727%\n",
      "Epoch 13, Batch 203, LR 2.488802 Loss 6.750258, Accuracy 82.732%\n",
      "Epoch 13, Batch 204, LR 2.488784 Loss 6.750044, Accuracy 82.736%\n",
      "Epoch 13, Batch 205, LR 2.488766 Loss 6.747705, Accuracy 82.759%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 206, LR 2.488748 Loss 6.747276, Accuracy 82.733%\n",
      "Epoch 13, Batch 207, LR 2.488730 Loss 6.748796, Accuracy 82.729%\n",
      "Epoch 13, Batch 208, LR 2.488712 Loss 6.748541, Accuracy 82.719%\n",
      "Epoch 13, Batch 209, LR 2.488694 Loss 6.751769, Accuracy 82.693%\n",
      "Epoch 13, Batch 210, LR 2.488676 Loss 6.752215, Accuracy 82.675%\n",
      "Epoch 13, Batch 211, LR 2.488658 Loss 6.751387, Accuracy 82.701%\n",
      "Epoch 13, Batch 212, LR 2.488640 Loss 6.753712, Accuracy 82.706%\n",
      "Epoch 13, Batch 213, LR 2.488622 Loss 6.754816, Accuracy 82.699%\n",
      "Epoch 13, Batch 214, LR 2.488604 Loss 6.757722, Accuracy 82.659%\n",
      "Epoch 13, Batch 215, LR 2.488586 Loss 6.756669, Accuracy 82.664%\n",
      "Epoch 13, Batch 216, LR 2.488568 Loss 6.758379, Accuracy 82.643%\n",
      "Epoch 13, Batch 217, LR 2.488550 Loss 6.757122, Accuracy 82.632%\n",
      "Epoch 13, Batch 218, LR 2.488532 Loss 6.755906, Accuracy 82.637%\n",
      "Epoch 13, Batch 219, LR 2.488514 Loss 6.756484, Accuracy 82.634%\n",
      "Epoch 13, Batch 220, LR 2.488496 Loss 6.756670, Accuracy 82.631%\n",
      "Epoch 13, Batch 221, LR 2.488478 Loss 6.755486, Accuracy 82.625%\n",
      "Epoch 13, Batch 222, LR 2.488460 Loss 6.753565, Accuracy 82.612%\n",
      "Epoch 13, Batch 223, LR 2.488441 Loss 6.752477, Accuracy 82.627%\n",
      "Epoch 13, Batch 224, LR 2.488423 Loss 6.751360, Accuracy 82.631%\n",
      "Epoch 13, Batch 225, LR 2.488405 Loss 6.748117, Accuracy 82.646%\n",
      "Epoch 13, Batch 226, LR 2.488387 Loss 6.747457, Accuracy 82.674%\n",
      "Epoch 13, Batch 227, LR 2.488369 Loss 6.748414, Accuracy 82.658%\n",
      "Epoch 13, Batch 228, LR 2.488350 Loss 6.748323, Accuracy 82.655%\n",
      "Epoch 13, Batch 229, LR 2.488332 Loss 6.749267, Accuracy 82.645%\n",
      "Epoch 13, Batch 230, LR 2.488314 Loss 6.748689, Accuracy 82.666%\n",
      "Epoch 13, Batch 231, LR 2.488296 Loss 6.746671, Accuracy 82.667%\n",
      "Epoch 13, Batch 232, LR 2.488277 Loss 6.745680, Accuracy 82.685%\n",
      "Epoch 13, Batch 233, LR 2.488259 Loss 6.745896, Accuracy 82.685%\n",
      "Epoch 13, Batch 234, LR 2.488241 Loss 6.745656, Accuracy 82.666%\n",
      "Epoch 13, Batch 235, LR 2.488222 Loss 6.747761, Accuracy 82.643%\n",
      "Epoch 13, Batch 236, LR 2.488204 Loss 6.746774, Accuracy 82.650%\n",
      "Epoch 13, Batch 237, LR 2.488186 Loss 6.745028, Accuracy 82.651%\n",
      "Epoch 13, Batch 238, LR 2.488167 Loss 6.742773, Accuracy 82.675%\n",
      "Epoch 13, Batch 239, LR 2.488149 Loss 6.745268, Accuracy 82.652%\n",
      "Epoch 13, Batch 240, LR 2.488130 Loss 6.745541, Accuracy 82.653%\n",
      "Epoch 13, Batch 241, LR 2.488112 Loss 6.743856, Accuracy 82.670%\n",
      "Epoch 13, Batch 242, LR 2.488094 Loss 6.744322, Accuracy 82.664%\n",
      "Epoch 13, Batch 243, LR 2.488075 Loss 6.746835, Accuracy 82.655%\n",
      "Epoch 13, Batch 244, LR 2.488057 Loss 6.746147, Accuracy 82.672%\n",
      "Epoch 13, Batch 245, LR 2.488038 Loss 6.744235, Accuracy 82.675%\n",
      "Epoch 13, Batch 246, LR 2.488020 Loss 6.743342, Accuracy 82.685%\n",
      "Epoch 13, Batch 247, LR 2.488001 Loss 6.741732, Accuracy 82.689%\n",
      "Epoch 13, Batch 248, LR 2.487983 Loss 6.741996, Accuracy 82.705%\n",
      "Epoch 13, Batch 249, LR 2.487964 Loss 6.740242, Accuracy 82.715%\n",
      "Epoch 13, Batch 250, LR 2.487946 Loss 6.741587, Accuracy 82.719%\n",
      "Epoch 13, Batch 251, LR 2.487927 Loss 6.737914, Accuracy 82.750%\n",
      "Epoch 13, Batch 252, LR 2.487908 Loss 6.736856, Accuracy 82.757%\n",
      "Epoch 13, Batch 253, LR 2.487890 Loss 6.738460, Accuracy 82.745%\n",
      "Epoch 13, Batch 254, LR 2.487871 Loss 6.740064, Accuracy 82.748%\n",
      "Epoch 13, Batch 255, LR 2.487853 Loss 6.741685, Accuracy 82.748%\n",
      "Epoch 13, Batch 256, LR 2.487834 Loss 6.742617, Accuracy 82.730%\n",
      "Epoch 13, Batch 257, LR 2.487815 Loss 6.741353, Accuracy 82.743%\n",
      "Epoch 13, Batch 258, LR 2.487797 Loss 6.743721, Accuracy 82.725%\n",
      "Epoch 13, Batch 259, LR 2.487778 Loss 6.744576, Accuracy 82.734%\n",
      "Epoch 13, Batch 260, LR 2.487759 Loss 6.745037, Accuracy 82.734%\n",
      "Epoch 13, Batch 261, LR 2.487741 Loss 6.746054, Accuracy 82.735%\n",
      "Epoch 13, Batch 262, LR 2.487722 Loss 6.744906, Accuracy 82.759%\n",
      "Epoch 13, Batch 263, LR 2.487703 Loss 6.745046, Accuracy 82.747%\n",
      "Epoch 13, Batch 264, LR 2.487684 Loss 6.741124, Accuracy 82.774%\n",
      "Epoch 13, Batch 265, LR 2.487666 Loss 6.742755, Accuracy 82.754%\n",
      "Epoch 13, Batch 266, LR 2.487647 Loss 6.745078, Accuracy 82.733%\n",
      "Epoch 13, Batch 267, LR 2.487628 Loss 6.744050, Accuracy 82.736%\n",
      "Epoch 13, Batch 268, LR 2.487609 Loss 6.746127, Accuracy 82.722%\n",
      "Epoch 13, Batch 269, LR 2.487590 Loss 6.746825, Accuracy 82.705%\n",
      "Epoch 13, Batch 270, LR 2.487572 Loss 6.748367, Accuracy 82.697%\n",
      "Epoch 13, Batch 271, LR 2.487553 Loss 6.748274, Accuracy 82.694%\n",
      "Epoch 13, Batch 272, LR 2.487534 Loss 6.751059, Accuracy 82.669%\n",
      "Epoch 13, Batch 273, LR 2.487515 Loss 6.749878, Accuracy 82.669%\n",
      "Epoch 13, Batch 274, LR 2.487496 Loss 6.752381, Accuracy 82.659%\n",
      "Epoch 13, Batch 275, LR 2.487477 Loss 6.753526, Accuracy 82.656%\n",
      "Epoch 13, Batch 276, LR 2.487458 Loss 6.752544, Accuracy 82.657%\n",
      "Epoch 13, Batch 277, LR 2.487439 Loss 6.751815, Accuracy 82.666%\n",
      "Epoch 13, Batch 278, LR 2.487420 Loss 6.753937, Accuracy 82.664%\n",
      "Epoch 13, Batch 279, LR 2.487401 Loss 6.753100, Accuracy 82.670%\n",
      "Epoch 13, Batch 280, LR 2.487382 Loss 6.751447, Accuracy 82.670%\n",
      "Epoch 13, Batch 281, LR 2.487363 Loss 6.750553, Accuracy 82.676%\n",
      "Epoch 13, Batch 282, LR 2.487344 Loss 6.752516, Accuracy 82.674%\n",
      "Epoch 13, Batch 283, LR 2.487325 Loss 6.751869, Accuracy 82.677%\n",
      "Epoch 13, Batch 284, LR 2.487306 Loss 6.749326, Accuracy 82.705%\n",
      "Epoch 13, Batch 285, LR 2.487287 Loss 6.749063, Accuracy 82.719%\n",
      "Epoch 13, Batch 286, LR 2.487268 Loss 6.749551, Accuracy 82.731%\n",
      "Epoch 13, Batch 287, LR 2.487249 Loss 6.752104, Accuracy 82.712%\n",
      "Epoch 13, Batch 288, LR 2.487230 Loss 6.753731, Accuracy 82.709%\n",
      "Epoch 13, Batch 289, LR 2.487211 Loss 6.755256, Accuracy 82.702%\n",
      "Epoch 13, Batch 290, LR 2.487192 Loss 6.756513, Accuracy 82.697%\n",
      "Epoch 13, Batch 291, LR 2.487173 Loss 6.757377, Accuracy 82.697%\n",
      "Epoch 13, Batch 292, LR 2.487154 Loss 6.758029, Accuracy 82.676%\n",
      "Epoch 13, Batch 293, LR 2.487134 Loss 6.759547, Accuracy 82.666%\n",
      "Epoch 13, Batch 294, LR 2.487115 Loss 6.757509, Accuracy 82.682%\n",
      "Epoch 13, Batch 295, LR 2.487096 Loss 6.756136, Accuracy 82.685%\n",
      "Epoch 13, Batch 296, LR 2.487077 Loss 6.755471, Accuracy 82.691%\n",
      "Epoch 13, Batch 297, LR 2.487058 Loss 6.755194, Accuracy 82.699%\n",
      "Epoch 13, Batch 298, LR 2.487038 Loss 6.753087, Accuracy 82.710%\n",
      "Epoch 13, Batch 299, LR 2.487019 Loss 6.753750, Accuracy 82.716%\n",
      "Epoch 13, Batch 300, LR 2.487000 Loss 6.756742, Accuracy 82.701%\n",
      "Epoch 13, Batch 301, LR 2.486981 Loss 6.756083, Accuracy 82.706%\n",
      "Epoch 13, Batch 302, LR 2.486961 Loss 6.757218, Accuracy 82.699%\n",
      "Epoch 13, Batch 303, LR 2.486942 Loss 6.756725, Accuracy 82.707%\n",
      "Epoch 13, Batch 304, LR 2.486923 Loss 6.756543, Accuracy 82.725%\n",
      "Epoch 13, Batch 305, LR 2.486903 Loss 6.755262, Accuracy 82.736%\n",
      "Epoch 13, Batch 306, LR 2.486884 Loss 6.756337, Accuracy 82.733%\n",
      "Epoch 13, Batch 307, LR 2.486865 Loss 6.756311, Accuracy 82.734%\n",
      "Epoch 13, Batch 308, LR 2.486845 Loss 6.756320, Accuracy 82.749%\n",
      "Epoch 13, Batch 309, LR 2.486826 Loss 6.756383, Accuracy 82.749%\n",
      "Epoch 13, Batch 310, LR 2.486806 Loss 6.755246, Accuracy 82.749%\n",
      "Epoch 13, Batch 311, LR 2.486787 Loss 6.755552, Accuracy 82.742%\n",
      "Epoch 13, Batch 312, LR 2.486768 Loss 6.753425, Accuracy 82.745%\n",
      "Epoch 13, Batch 313, LR 2.486748 Loss 6.754073, Accuracy 82.733%\n",
      "Epoch 13, Batch 314, LR 2.486729 Loss 6.754092, Accuracy 82.730%\n",
      "Epoch 13, Batch 315, LR 2.486709 Loss 6.751853, Accuracy 82.743%\n",
      "Epoch 13, Batch 316, LR 2.486690 Loss 6.752992, Accuracy 82.741%\n",
      "Epoch 13, Batch 317, LR 2.486670 Loss 6.753689, Accuracy 82.739%\n",
      "Epoch 13, Batch 318, LR 2.486651 Loss 6.752446, Accuracy 82.749%\n",
      "Epoch 13, Batch 319, LR 2.486631 Loss 6.751736, Accuracy 82.749%\n",
      "Epoch 13, Batch 320, LR 2.486612 Loss 6.750679, Accuracy 82.756%\n",
      "Epoch 13, Batch 321, LR 2.486592 Loss 6.750809, Accuracy 82.757%\n",
      "Epoch 13, Batch 322, LR 2.486572 Loss 6.750100, Accuracy 82.759%\n",
      "Epoch 13, Batch 323, LR 2.486553 Loss 6.749554, Accuracy 82.762%\n",
      "Epoch 13, Batch 324, LR 2.486533 Loss 6.750071, Accuracy 82.762%\n",
      "Epoch 13, Batch 325, LR 2.486514 Loss 6.747925, Accuracy 82.786%\n",
      "Epoch 13, Batch 326, LR 2.486494 Loss 6.748922, Accuracy 82.779%\n",
      "Epoch 13, Batch 327, LR 2.486474 Loss 6.748234, Accuracy 82.781%\n",
      "Epoch 13, Batch 328, LR 2.486455 Loss 6.745468, Accuracy 82.805%\n",
      "Epoch 13, Batch 329, LR 2.486435 Loss 6.747288, Accuracy 82.791%\n",
      "Epoch 13, Batch 330, LR 2.486415 Loss 6.747850, Accuracy 82.791%\n",
      "Epoch 13, Batch 331, LR 2.486396 Loss 6.748101, Accuracy 82.791%\n",
      "Epoch 13, Batch 332, LR 2.486376 Loss 6.748984, Accuracy 82.787%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 333, LR 2.486356 Loss 6.750955, Accuracy 82.773%\n",
      "Epoch 13, Batch 334, LR 2.486336 Loss 6.751345, Accuracy 82.761%\n",
      "Epoch 13, Batch 335, LR 2.486317 Loss 6.751515, Accuracy 82.766%\n",
      "Epoch 13, Batch 336, LR 2.486297 Loss 6.748845, Accuracy 82.778%\n",
      "Epoch 13, Batch 337, LR 2.486277 Loss 6.747831, Accuracy 82.782%\n",
      "Epoch 13, Batch 338, LR 2.486257 Loss 6.747464, Accuracy 82.792%\n",
      "Epoch 13, Batch 339, LR 2.486238 Loss 6.747459, Accuracy 82.796%\n",
      "Epoch 13, Batch 340, LR 2.486218 Loss 6.746998, Accuracy 82.796%\n",
      "Epoch 13, Batch 341, LR 2.486198 Loss 6.747332, Accuracy 82.801%\n",
      "Epoch 13, Batch 342, LR 2.486178 Loss 6.747088, Accuracy 82.806%\n",
      "Epoch 13, Batch 343, LR 2.486158 Loss 6.746608, Accuracy 82.810%\n",
      "Epoch 13, Batch 344, LR 2.486138 Loss 6.745487, Accuracy 82.826%\n",
      "Epoch 13, Batch 345, LR 2.486118 Loss 6.745482, Accuracy 82.817%\n",
      "Epoch 13, Batch 346, LR 2.486098 Loss 6.744049, Accuracy 82.824%\n",
      "Epoch 13, Batch 347, LR 2.486079 Loss 6.744333, Accuracy 82.828%\n",
      "Epoch 13, Batch 348, LR 2.486059 Loss 6.744343, Accuracy 82.839%\n",
      "Epoch 13, Batch 349, LR 2.486039 Loss 6.744355, Accuracy 82.846%\n",
      "Epoch 13, Batch 350, LR 2.486019 Loss 6.741783, Accuracy 82.868%\n",
      "Epoch 13, Batch 351, LR 2.485999 Loss 6.743752, Accuracy 82.853%\n",
      "Epoch 13, Batch 352, LR 2.485979 Loss 6.743742, Accuracy 82.855%\n",
      "Epoch 13, Batch 353, LR 2.485959 Loss 6.744191, Accuracy 82.848%\n",
      "Epoch 13, Batch 354, LR 2.485939 Loss 6.745278, Accuracy 82.846%\n",
      "Epoch 13, Batch 355, LR 2.485919 Loss 6.743176, Accuracy 82.861%\n",
      "Epoch 13, Batch 356, LR 2.485899 Loss 6.743145, Accuracy 82.843%\n",
      "Epoch 13, Batch 357, LR 2.485878 Loss 6.743411, Accuracy 82.850%\n",
      "Epoch 13, Batch 358, LR 2.485858 Loss 6.743174, Accuracy 82.852%\n",
      "Epoch 13, Batch 359, LR 2.485838 Loss 6.744514, Accuracy 82.841%\n",
      "Epoch 13, Batch 360, LR 2.485818 Loss 6.744049, Accuracy 82.860%\n",
      "Epoch 13, Batch 361, LR 2.485798 Loss 6.742868, Accuracy 82.864%\n",
      "Epoch 13, Batch 362, LR 2.485778 Loss 6.740841, Accuracy 82.875%\n",
      "Epoch 13, Batch 363, LR 2.485758 Loss 6.740211, Accuracy 82.888%\n",
      "Epoch 13, Batch 364, LR 2.485738 Loss 6.739845, Accuracy 82.888%\n",
      "Epoch 13, Batch 365, LR 2.485717 Loss 6.740088, Accuracy 82.883%\n",
      "Epoch 13, Batch 366, LR 2.485697 Loss 6.740805, Accuracy 82.885%\n",
      "Epoch 13, Batch 367, LR 2.485677 Loss 6.739330, Accuracy 82.902%\n",
      "Epoch 13, Batch 368, LR 2.485657 Loss 6.739100, Accuracy 82.893%\n",
      "Epoch 13, Batch 369, LR 2.485636 Loss 6.739888, Accuracy 82.891%\n",
      "Epoch 13, Batch 370, LR 2.485616 Loss 6.740725, Accuracy 82.886%\n",
      "Epoch 13, Batch 371, LR 2.485596 Loss 6.739466, Accuracy 82.888%\n",
      "Epoch 13, Batch 372, LR 2.485576 Loss 6.737916, Accuracy 82.884%\n",
      "Epoch 13, Batch 373, LR 2.485555 Loss 6.737822, Accuracy 82.875%\n",
      "Epoch 13, Batch 374, LR 2.485535 Loss 6.739901, Accuracy 82.863%\n",
      "Epoch 13, Batch 375, LR 2.485515 Loss 6.742072, Accuracy 82.846%\n",
      "Epoch 13, Batch 376, LR 2.485494 Loss 6.744206, Accuracy 82.821%\n",
      "Epoch 13, Batch 377, LR 2.485474 Loss 6.746218, Accuracy 82.808%\n",
      "Epoch 13, Batch 378, LR 2.485454 Loss 6.746405, Accuracy 82.806%\n",
      "Epoch 13, Batch 379, LR 2.485433 Loss 6.747820, Accuracy 82.800%\n",
      "Epoch 13, Batch 380, LR 2.485413 Loss 6.746381, Accuracy 82.817%\n",
      "Epoch 13, Batch 381, LR 2.485392 Loss 6.747428, Accuracy 82.819%\n",
      "Epoch 13, Batch 382, LR 2.485372 Loss 6.746378, Accuracy 82.829%\n",
      "Epoch 13, Batch 383, LR 2.485352 Loss 6.744681, Accuracy 82.851%\n",
      "Epoch 13, Batch 384, LR 2.485331 Loss 6.745449, Accuracy 82.843%\n",
      "Epoch 13, Batch 385, LR 2.485311 Loss 6.745239, Accuracy 82.839%\n",
      "Epoch 13, Batch 386, LR 2.485290 Loss 6.746812, Accuracy 82.827%\n",
      "Epoch 13, Batch 387, LR 2.485270 Loss 6.745127, Accuracy 82.841%\n",
      "Epoch 13, Batch 388, LR 2.485249 Loss 6.744888, Accuracy 82.839%\n",
      "Epoch 13, Batch 389, LR 2.485229 Loss 6.743577, Accuracy 82.845%\n",
      "Epoch 13, Batch 390, LR 2.485208 Loss 6.744028, Accuracy 82.833%\n",
      "Epoch 13, Batch 391, LR 2.485188 Loss 6.743770, Accuracy 82.834%\n",
      "Epoch 13, Batch 392, LR 2.485167 Loss 6.744101, Accuracy 82.834%\n",
      "Epoch 13, Batch 393, LR 2.485146 Loss 6.743310, Accuracy 82.850%\n",
      "Epoch 13, Batch 394, LR 2.485126 Loss 6.743165, Accuracy 82.856%\n",
      "Epoch 13, Batch 395, LR 2.485105 Loss 6.743711, Accuracy 82.862%\n",
      "Epoch 13, Batch 396, LR 2.485085 Loss 6.743654, Accuracy 82.872%\n",
      "Epoch 13, Batch 397, LR 2.485064 Loss 6.742368, Accuracy 82.885%\n",
      "Epoch 13, Batch 398, LR 2.485043 Loss 6.742766, Accuracy 82.881%\n",
      "Epoch 13, Batch 399, LR 2.485023 Loss 6.743305, Accuracy 82.883%\n",
      "Epoch 13, Batch 400, LR 2.485002 Loss 6.744239, Accuracy 82.871%\n",
      "Epoch 13, Batch 401, LR 2.484981 Loss 6.745617, Accuracy 82.853%\n",
      "Epoch 13, Batch 402, LR 2.484961 Loss 6.743378, Accuracy 82.861%\n",
      "Epoch 13, Batch 403, LR 2.484940 Loss 6.743830, Accuracy 82.857%\n",
      "Epoch 13, Batch 404, LR 2.484919 Loss 6.741220, Accuracy 82.878%\n",
      "Epoch 13, Batch 405, LR 2.484898 Loss 6.743805, Accuracy 82.861%\n",
      "Epoch 13, Batch 406, LR 2.484878 Loss 6.742371, Accuracy 82.864%\n",
      "Epoch 13, Batch 407, LR 2.484857 Loss 6.742970, Accuracy 82.853%\n",
      "Epoch 13, Batch 408, LR 2.484836 Loss 6.742036, Accuracy 82.860%\n",
      "Epoch 13, Batch 409, LR 2.484815 Loss 6.740128, Accuracy 82.885%\n",
      "Epoch 13, Batch 410, LR 2.484794 Loss 6.741646, Accuracy 82.879%\n",
      "Epoch 13, Batch 411, LR 2.484774 Loss 6.743029, Accuracy 82.860%\n",
      "Epoch 13, Batch 412, LR 2.484753 Loss 6.740945, Accuracy 82.873%\n",
      "Epoch 13, Batch 413, LR 2.484732 Loss 6.740215, Accuracy 82.875%\n",
      "Epoch 13, Batch 414, LR 2.484711 Loss 6.741786, Accuracy 82.873%\n",
      "Epoch 13, Batch 415, LR 2.484690 Loss 6.742896, Accuracy 82.865%\n",
      "Epoch 13, Batch 416, LR 2.484669 Loss 6.743513, Accuracy 82.856%\n",
      "Epoch 13, Batch 417, LR 2.484648 Loss 6.743449, Accuracy 82.856%\n",
      "Epoch 13, Batch 418, LR 2.484627 Loss 6.743024, Accuracy 82.865%\n",
      "Epoch 13, Batch 419, LR 2.484606 Loss 6.741807, Accuracy 82.868%\n",
      "Epoch 13, Batch 420, LR 2.484585 Loss 6.741895, Accuracy 82.870%\n",
      "Epoch 13, Batch 421, LR 2.484564 Loss 6.739376, Accuracy 82.883%\n",
      "Epoch 13, Batch 422, LR 2.484543 Loss 6.738390, Accuracy 82.890%\n",
      "Epoch 13, Batch 423, LR 2.484522 Loss 6.738070, Accuracy 82.888%\n",
      "Epoch 13, Batch 424, LR 2.484501 Loss 6.740004, Accuracy 82.873%\n",
      "Epoch 13, Batch 425, LR 2.484480 Loss 6.739167, Accuracy 82.873%\n",
      "Epoch 13, Batch 426, LR 2.484459 Loss 6.740127, Accuracy 82.869%\n",
      "Epoch 13, Batch 427, LR 2.484438 Loss 6.738355, Accuracy 82.880%\n",
      "Epoch 13, Batch 428, LR 2.484417 Loss 6.736674, Accuracy 82.893%\n",
      "Epoch 13, Batch 429, LR 2.484396 Loss 6.736634, Accuracy 82.896%\n",
      "Epoch 13, Batch 430, LR 2.484375 Loss 6.735539, Accuracy 82.912%\n",
      "Epoch 13, Batch 431, LR 2.484354 Loss 6.734396, Accuracy 82.918%\n",
      "Epoch 13, Batch 432, LR 2.484333 Loss 6.735137, Accuracy 82.901%\n",
      "Epoch 13, Batch 433, LR 2.484312 Loss 6.736386, Accuracy 82.890%\n",
      "Epoch 13, Batch 434, LR 2.484290 Loss 6.737248, Accuracy 82.879%\n",
      "Epoch 13, Batch 435, LR 2.484269 Loss 6.736541, Accuracy 82.881%\n",
      "Epoch 13, Batch 436, LR 2.484248 Loss 6.736242, Accuracy 82.888%\n",
      "Epoch 13, Batch 437, LR 2.484227 Loss 6.737395, Accuracy 82.888%\n",
      "Epoch 13, Batch 438, LR 2.484206 Loss 6.738534, Accuracy 82.880%\n",
      "Epoch 13, Batch 439, LR 2.484184 Loss 6.737569, Accuracy 82.887%\n",
      "Epoch 13, Batch 440, LR 2.484163 Loss 6.738254, Accuracy 82.875%\n",
      "Epoch 13, Batch 441, LR 2.484142 Loss 6.737478, Accuracy 82.878%\n",
      "Epoch 13, Batch 442, LR 2.484121 Loss 6.736789, Accuracy 82.887%\n",
      "Epoch 13, Batch 443, LR 2.484099 Loss 6.736894, Accuracy 82.881%\n",
      "Epoch 13, Batch 444, LR 2.484078 Loss 6.736607, Accuracy 82.879%\n",
      "Epoch 13, Batch 445, LR 2.484057 Loss 6.738164, Accuracy 82.863%\n",
      "Epoch 13, Batch 446, LR 2.484035 Loss 6.737780, Accuracy 82.874%\n",
      "Epoch 13, Batch 447, LR 2.484014 Loss 6.737287, Accuracy 82.888%\n",
      "Epoch 13, Batch 448, LR 2.483993 Loss 6.736851, Accuracy 82.887%\n",
      "Epoch 13, Batch 449, LR 2.483971 Loss 6.737325, Accuracy 82.880%\n",
      "Epoch 13, Batch 450, LR 2.483950 Loss 6.736743, Accuracy 82.889%\n",
      "Epoch 13, Batch 451, LR 2.483928 Loss 6.736164, Accuracy 82.897%\n",
      "Epoch 13, Batch 452, LR 2.483907 Loss 6.735591, Accuracy 82.897%\n",
      "Epoch 13, Batch 453, LR 2.483886 Loss 6.735541, Accuracy 82.897%\n",
      "Epoch 13, Batch 454, LR 2.483864 Loss 6.736328, Accuracy 82.895%\n",
      "Epoch 13, Batch 455, LR 2.483843 Loss 6.735470, Accuracy 82.897%\n",
      "Epoch 13, Batch 456, LR 2.483821 Loss 6.735553, Accuracy 82.891%\n",
      "Epoch 13, Batch 457, LR 2.483800 Loss 6.734389, Accuracy 82.895%\n",
      "Epoch 13, Batch 458, LR 2.483778 Loss 6.735524, Accuracy 82.893%\n",
      "Epoch 13, Batch 459, LR 2.483757 Loss 6.734910, Accuracy 82.904%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 460, LR 2.483735 Loss 6.733925, Accuracy 82.914%\n",
      "Epoch 13, Batch 461, LR 2.483714 Loss 6.733403, Accuracy 82.911%\n",
      "Epoch 13, Batch 462, LR 2.483692 Loss 6.732005, Accuracy 82.924%\n",
      "Epoch 13, Batch 463, LR 2.483670 Loss 6.731396, Accuracy 82.927%\n",
      "Epoch 13, Batch 464, LR 2.483649 Loss 6.730195, Accuracy 82.939%\n",
      "Epoch 13, Batch 465, LR 2.483627 Loss 6.729583, Accuracy 82.937%\n",
      "Epoch 13, Batch 466, LR 2.483606 Loss 6.727786, Accuracy 82.953%\n",
      "Epoch 13, Batch 467, LR 2.483584 Loss 6.726855, Accuracy 82.955%\n",
      "Epoch 13, Batch 468, LR 2.483562 Loss 6.726692, Accuracy 82.949%\n",
      "Epoch 13, Batch 469, LR 2.483541 Loss 6.726617, Accuracy 82.944%\n",
      "Epoch 13, Batch 470, LR 2.483519 Loss 6.725739, Accuracy 82.937%\n",
      "Epoch 13, Batch 471, LR 2.483497 Loss 6.725920, Accuracy 82.937%\n",
      "Epoch 13, Batch 472, LR 2.483476 Loss 6.726641, Accuracy 82.940%\n",
      "Epoch 13, Batch 473, LR 2.483454 Loss 6.726187, Accuracy 82.945%\n",
      "Epoch 13, Batch 474, LR 2.483432 Loss 6.725540, Accuracy 82.948%\n",
      "Epoch 13, Batch 475, LR 2.483410 Loss 6.725835, Accuracy 82.951%\n",
      "Epoch 13, Batch 476, LR 2.483389 Loss 6.725417, Accuracy 82.949%\n",
      "Epoch 13, Batch 477, LR 2.483367 Loss 6.726599, Accuracy 82.940%\n",
      "Epoch 13, Batch 478, LR 2.483345 Loss 6.728870, Accuracy 82.925%\n",
      "Epoch 13, Batch 479, LR 2.483323 Loss 6.728386, Accuracy 82.932%\n",
      "Epoch 13, Batch 480, LR 2.483302 Loss 6.727698, Accuracy 82.941%\n",
      "Epoch 13, Batch 481, LR 2.483280 Loss 6.728465, Accuracy 82.933%\n",
      "Epoch 13, Batch 482, LR 2.483258 Loss 6.727680, Accuracy 82.934%\n",
      "Epoch 13, Batch 483, LR 2.483236 Loss 6.727553, Accuracy 82.926%\n",
      "Epoch 13, Batch 484, LR 2.483214 Loss 6.726663, Accuracy 82.930%\n",
      "Epoch 13, Batch 485, LR 2.483192 Loss 6.726907, Accuracy 82.925%\n",
      "Epoch 13, Batch 486, LR 2.483170 Loss 6.726145, Accuracy 82.923%\n",
      "Epoch 13, Batch 487, LR 2.483148 Loss 6.725360, Accuracy 82.926%\n",
      "Epoch 13, Batch 488, LR 2.483126 Loss 6.726430, Accuracy 82.921%\n",
      "Epoch 13, Batch 489, LR 2.483105 Loss 6.725342, Accuracy 82.929%\n",
      "Epoch 13, Batch 490, LR 2.483083 Loss 6.726557, Accuracy 82.919%\n",
      "Epoch 13, Batch 491, LR 2.483061 Loss 6.725623, Accuracy 82.921%\n",
      "Epoch 13, Batch 492, LR 2.483039 Loss 6.725193, Accuracy 82.922%\n",
      "Epoch 13, Batch 493, LR 2.483017 Loss 6.726577, Accuracy 82.912%\n",
      "Epoch 13, Batch 494, LR 2.482995 Loss 6.727006, Accuracy 82.909%\n",
      "Epoch 13, Batch 495, LR 2.482973 Loss 6.725113, Accuracy 82.918%\n",
      "Epoch 13, Batch 496, LR 2.482951 Loss 6.724259, Accuracy 82.924%\n",
      "Epoch 13, Batch 497, LR 2.482928 Loss 6.723554, Accuracy 82.923%\n",
      "Epoch 13, Batch 498, LR 2.482906 Loss 6.723289, Accuracy 82.922%\n",
      "Epoch 13, Batch 499, LR 2.482884 Loss 6.723644, Accuracy 82.921%\n",
      "Epoch 13, Batch 500, LR 2.482862 Loss 6.722813, Accuracy 82.925%\n",
      "Epoch 13, Batch 501, LR 2.482840 Loss 6.722631, Accuracy 82.922%\n",
      "Epoch 13, Batch 502, LR 2.482818 Loss 6.722992, Accuracy 82.918%\n",
      "Epoch 13, Batch 503, LR 2.482796 Loss 6.723476, Accuracy 82.915%\n",
      "Epoch 13, Batch 504, LR 2.482774 Loss 6.723357, Accuracy 82.912%\n",
      "Epoch 13, Batch 505, LR 2.482752 Loss 6.723405, Accuracy 82.910%\n",
      "Epoch 13, Batch 506, LR 2.482729 Loss 6.722980, Accuracy 82.919%\n",
      "Epoch 13, Batch 507, LR 2.482707 Loss 6.721694, Accuracy 82.931%\n",
      "Epoch 13, Batch 508, LR 2.482685 Loss 6.721401, Accuracy 82.926%\n",
      "Epoch 13, Batch 509, LR 2.482663 Loss 6.721555, Accuracy 82.925%\n",
      "Epoch 13, Batch 510, LR 2.482640 Loss 6.721074, Accuracy 82.924%\n",
      "Epoch 13, Batch 511, LR 2.482618 Loss 6.720928, Accuracy 82.929%\n",
      "Epoch 13, Batch 512, LR 2.482596 Loss 6.721398, Accuracy 82.925%\n",
      "Epoch 13, Batch 513, LR 2.482574 Loss 6.721329, Accuracy 82.925%\n",
      "Epoch 13, Batch 514, LR 2.482551 Loss 6.721724, Accuracy 82.914%\n",
      "Epoch 13, Batch 515, LR 2.482529 Loss 6.722580, Accuracy 82.914%\n",
      "Epoch 13, Batch 516, LR 2.482507 Loss 6.723424, Accuracy 82.915%\n",
      "Epoch 13, Batch 517, LR 2.482484 Loss 6.722909, Accuracy 82.912%\n",
      "Epoch 13, Batch 518, LR 2.482462 Loss 6.723247, Accuracy 82.908%\n",
      "Epoch 13, Batch 519, LR 2.482440 Loss 6.722480, Accuracy 82.910%\n",
      "Epoch 13, Batch 520, LR 2.482417 Loss 6.722861, Accuracy 82.900%\n",
      "Epoch 13, Batch 521, LR 2.482395 Loss 6.722585, Accuracy 82.895%\n",
      "Epoch 13, Batch 522, LR 2.482373 Loss 6.723866, Accuracy 82.886%\n",
      "Epoch 13, Batch 523, LR 2.482350 Loss 6.723876, Accuracy 82.893%\n",
      "Epoch 13, Batch 524, LR 2.482328 Loss 6.723541, Accuracy 82.892%\n",
      "Epoch 13, Batch 525, LR 2.482305 Loss 6.722146, Accuracy 82.899%\n",
      "Epoch 13, Batch 526, LR 2.482283 Loss 6.721798, Accuracy 82.902%\n",
      "Epoch 13, Batch 527, LR 2.482260 Loss 6.720945, Accuracy 82.904%\n",
      "Epoch 13, Batch 528, LR 2.482238 Loss 6.719455, Accuracy 82.910%\n",
      "Epoch 13, Batch 529, LR 2.482215 Loss 6.720220, Accuracy 82.907%\n",
      "Epoch 13, Batch 530, LR 2.482193 Loss 6.719689, Accuracy 82.901%\n",
      "Epoch 13, Batch 531, LR 2.482170 Loss 6.719665, Accuracy 82.895%\n",
      "Epoch 13, Batch 532, LR 2.482148 Loss 6.720164, Accuracy 82.899%\n",
      "Epoch 13, Batch 533, LR 2.482125 Loss 6.720811, Accuracy 82.900%\n",
      "Epoch 13, Batch 534, LR 2.482102 Loss 6.721343, Accuracy 82.899%\n",
      "Epoch 13, Batch 535, LR 2.482080 Loss 6.720938, Accuracy 82.915%\n",
      "Epoch 13, Batch 536, LR 2.482057 Loss 6.721087, Accuracy 82.912%\n",
      "Epoch 13, Batch 537, LR 2.482035 Loss 6.721780, Accuracy 82.913%\n",
      "Epoch 13, Batch 538, LR 2.482012 Loss 6.724160, Accuracy 82.895%\n",
      "Epoch 13, Batch 539, LR 2.481989 Loss 6.725942, Accuracy 82.884%\n",
      "Epoch 13, Batch 540, LR 2.481967 Loss 6.726431, Accuracy 82.876%\n",
      "Epoch 13, Batch 541, LR 2.481944 Loss 6.726101, Accuracy 82.875%\n",
      "Epoch 13, Batch 542, LR 2.481921 Loss 6.726653, Accuracy 82.872%\n",
      "Epoch 13, Batch 543, LR 2.481899 Loss 6.727907, Accuracy 82.861%\n",
      "Epoch 13, Batch 544, LR 2.481876 Loss 6.729567, Accuracy 82.850%\n",
      "Epoch 13, Batch 545, LR 2.481853 Loss 6.730157, Accuracy 82.850%\n",
      "Epoch 13, Batch 546, LR 2.481830 Loss 6.730183, Accuracy 82.845%\n",
      "Epoch 13, Batch 547, LR 2.481808 Loss 6.731293, Accuracy 82.838%\n",
      "Epoch 13, Batch 548, LR 2.481785 Loss 6.731558, Accuracy 82.838%\n",
      "Epoch 13, Batch 549, LR 2.481762 Loss 6.731813, Accuracy 82.832%\n",
      "Epoch 13, Batch 550, LR 2.481739 Loss 6.733389, Accuracy 82.811%\n",
      "Epoch 13, Batch 551, LR 2.481716 Loss 6.732650, Accuracy 82.811%\n",
      "Epoch 13, Batch 552, LR 2.481694 Loss 6.732206, Accuracy 82.808%\n",
      "Epoch 13, Batch 553, LR 2.481671 Loss 6.730857, Accuracy 82.818%\n",
      "Epoch 13, Batch 554, LR 2.481648 Loss 6.730739, Accuracy 82.825%\n",
      "Epoch 13, Batch 555, LR 2.481625 Loss 6.731099, Accuracy 82.828%\n",
      "Epoch 13, Batch 556, LR 2.481602 Loss 6.729359, Accuracy 82.842%\n",
      "Epoch 13, Batch 557, LR 2.481579 Loss 6.730513, Accuracy 82.827%\n",
      "Epoch 13, Batch 558, LR 2.481556 Loss 6.730780, Accuracy 82.827%\n",
      "Epoch 13, Batch 559, LR 2.481533 Loss 6.730771, Accuracy 82.826%\n",
      "Epoch 13, Batch 560, LR 2.481510 Loss 6.730526, Accuracy 82.824%\n",
      "Epoch 13, Batch 561, LR 2.481487 Loss 6.729314, Accuracy 82.832%\n",
      "Epoch 13, Batch 562, LR 2.481465 Loss 6.729243, Accuracy 82.833%\n",
      "Epoch 13, Batch 563, LR 2.481442 Loss 6.729990, Accuracy 82.828%\n",
      "Epoch 13, Batch 564, LR 2.481419 Loss 6.729314, Accuracy 82.831%\n",
      "Epoch 13, Batch 565, LR 2.481396 Loss 6.728351, Accuracy 82.835%\n",
      "Epoch 13, Batch 566, LR 2.481372 Loss 6.728977, Accuracy 82.829%\n",
      "Epoch 13, Batch 567, LR 2.481349 Loss 6.728937, Accuracy 82.832%\n",
      "Epoch 13, Batch 568, LR 2.481326 Loss 6.728004, Accuracy 82.830%\n",
      "Epoch 13, Batch 569, LR 2.481303 Loss 6.726824, Accuracy 82.841%\n",
      "Epoch 13, Batch 570, LR 2.481280 Loss 6.727047, Accuracy 82.840%\n",
      "Epoch 13, Batch 571, LR 2.481257 Loss 6.726722, Accuracy 82.841%\n",
      "Epoch 13, Batch 572, LR 2.481234 Loss 6.726428, Accuracy 82.844%\n",
      "Epoch 13, Batch 573, LR 2.481211 Loss 6.726754, Accuracy 82.841%\n",
      "Epoch 13, Batch 574, LR 2.481188 Loss 6.725308, Accuracy 82.849%\n",
      "Epoch 13, Batch 575, LR 2.481165 Loss 6.725759, Accuracy 82.845%\n",
      "Epoch 13, Batch 576, LR 2.481141 Loss 6.726274, Accuracy 82.845%\n",
      "Epoch 13, Batch 577, LR 2.481118 Loss 6.725763, Accuracy 82.854%\n",
      "Epoch 13, Batch 578, LR 2.481095 Loss 6.726313, Accuracy 82.856%\n",
      "Epoch 13, Batch 579, LR 2.481072 Loss 6.726897, Accuracy 82.852%\n",
      "Epoch 13, Batch 580, LR 2.481049 Loss 6.727931, Accuracy 82.848%\n",
      "Epoch 13, Batch 581, LR 2.481025 Loss 6.728296, Accuracy 82.841%\n",
      "Epoch 13, Batch 582, LR 2.481002 Loss 6.728205, Accuracy 82.842%\n",
      "Epoch 13, Batch 583, LR 2.480979 Loss 6.727959, Accuracy 82.841%\n",
      "Epoch 13, Batch 584, LR 2.480955 Loss 6.728053, Accuracy 82.838%\n",
      "Epoch 13, Batch 585, LR 2.480932 Loss 6.728148, Accuracy 82.837%\n",
      "Epoch 13, Batch 586, LR 2.480909 Loss 6.727328, Accuracy 82.840%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 587, LR 2.480886 Loss 6.727395, Accuracy 82.846%\n",
      "Epoch 13, Batch 588, LR 2.480862 Loss 6.726358, Accuracy 82.852%\n",
      "Epoch 13, Batch 589, LR 2.480839 Loss 6.727023, Accuracy 82.851%\n",
      "Epoch 13, Batch 590, LR 2.480815 Loss 6.726262, Accuracy 82.859%\n",
      "Epoch 13, Batch 591, LR 2.480792 Loss 6.725679, Accuracy 82.860%\n",
      "Epoch 13, Batch 592, LR 2.480769 Loss 6.726010, Accuracy 82.859%\n",
      "Epoch 13, Batch 593, LR 2.480745 Loss 6.725815, Accuracy 82.852%\n",
      "Epoch 13, Batch 594, LR 2.480722 Loss 6.725945, Accuracy 82.848%\n",
      "Epoch 13, Batch 595, LR 2.480698 Loss 6.726956, Accuracy 82.840%\n",
      "Epoch 13, Batch 596, LR 2.480675 Loss 6.726140, Accuracy 82.847%\n",
      "Epoch 13, Batch 597, LR 2.480651 Loss 6.725821, Accuracy 82.843%\n",
      "Epoch 13, Batch 598, LR 2.480628 Loss 6.726398, Accuracy 82.837%\n",
      "Epoch 13, Batch 599, LR 2.480604 Loss 6.725491, Accuracy 82.842%\n",
      "Epoch 13, Batch 600, LR 2.480581 Loss 6.726000, Accuracy 82.837%\n",
      "Epoch 13, Batch 601, LR 2.480557 Loss 6.726525, Accuracy 82.838%\n",
      "Epoch 13, Batch 602, LR 2.480534 Loss 6.726813, Accuracy 82.838%\n",
      "Epoch 13, Batch 603, LR 2.480510 Loss 6.726780, Accuracy 82.844%\n",
      "Epoch 13, Batch 604, LR 2.480487 Loss 6.725886, Accuracy 82.850%\n",
      "Epoch 13, Batch 605, LR 2.480463 Loss 6.726709, Accuracy 82.846%\n",
      "Epoch 13, Batch 606, LR 2.480440 Loss 6.726762, Accuracy 82.845%\n",
      "Epoch 13, Batch 607, LR 2.480416 Loss 6.726330, Accuracy 82.852%\n",
      "Epoch 13, Batch 608, LR 2.480392 Loss 6.726933, Accuracy 82.850%\n",
      "Epoch 13, Batch 609, LR 2.480369 Loss 6.727262, Accuracy 82.848%\n",
      "Epoch 13, Batch 610, LR 2.480345 Loss 6.727897, Accuracy 82.848%\n",
      "Epoch 13, Batch 611, LR 2.480321 Loss 6.727867, Accuracy 82.844%\n",
      "Epoch 13, Batch 612, LR 2.480298 Loss 6.727889, Accuracy 82.847%\n",
      "Epoch 13, Batch 613, LR 2.480274 Loss 6.727475, Accuracy 82.844%\n",
      "Epoch 13, Batch 614, LR 2.480250 Loss 6.727708, Accuracy 82.834%\n",
      "Epoch 13, Batch 615, LR 2.480227 Loss 6.727457, Accuracy 82.835%\n",
      "Epoch 13, Batch 616, LR 2.480203 Loss 6.726772, Accuracy 82.832%\n",
      "Epoch 13, Batch 617, LR 2.480179 Loss 6.727110, Accuracy 82.828%\n",
      "Epoch 13, Batch 618, LR 2.480155 Loss 6.727036, Accuracy 82.829%\n",
      "Epoch 13, Batch 619, LR 2.480132 Loss 6.726659, Accuracy 82.829%\n",
      "Epoch 13, Batch 620, LR 2.480108 Loss 6.726050, Accuracy 82.834%\n",
      "Epoch 13, Batch 621, LR 2.480084 Loss 6.726480, Accuracy 82.833%\n",
      "Epoch 13, Batch 622, LR 2.480060 Loss 6.726675, Accuracy 82.835%\n",
      "Epoch 13, Batch 623, LR 2.480036 Loss 6.727966, Accuracy 82.831%\n",
      "Epoch 13, Batch 624, LR 2.480012 Loss 6.727985, Accuracy 82.830%\n",
      "Epoch 13, Batch 625, LR 2.479989 Loss 6.727526, Accuracy 82.830%\n",
      "Epoch 13, Batch 626, LR 2.479965 Loss 6.727084, Accuracy 82.831%\n",
      "Epoch 13, Batch 627, LR 2.479941 Loss 6.727308, Accuracy 82.830%\n",
      "Epoch 13, Batch 628, LR 2.479917 Loss 6.728630, Accuracy 82.821%\n",
      "Epoch 13, Batch 629, LR 2.479893 Loss 6.728191, Accuracy 82.826%\n",
      "Epoch 13, Batch 630, LR 2.479869 Loss 6.728031, Accuracy 82.826%\n",
      "Epoch 13, Batch 631, LR 2.479845 Loss 6.728074, Accuracy 82.830%\n",
      "Epoch 13, Batch 632, LR 2.479821 Loss 6.727492, Accuracy 82.827%\n",
      "Epoch 13, Batch 633, LR 2.479797 Loss 6.726827, Accuracy 82.836%\n",
      "Epoch 13, Batch 634, LR 2.479773 Loss 6.726588, Accuracy 82.836%\n",
      "Epoch 13, Batch 635, LR 2.479749 Loss 6.726871, Accuracy 82.828%\n",
      "Epoch 13, Batch 636, LR 2.479725 Loss 6.727035, Accuracy 82.827%\n",
      "Epoch 13, Batch 637, LR 2.479701 Loss 6.727180, Accuracy 82.825%\n",
      "Epoch 13, Batch 638, LR 2.479677 Loss 6.727624, Accuracy 82.820%\n",
      "Epoch 13, Batch 639, LR 2.479653 Loss 6.727350, Accuracy 82.821%\n",
      "Epoch 13, Batch 640, LR 2.479629 Loss 6.727466, Accuracy 82.816%\n",
      "Epoch 13, Batch 641, LR 2.479605 Loss 6.726422, Accuracy 82.822%\n",
      "Epoch 13, Batch 642, LR 2.479581 Loss 6.726568, Accuracy 82.826%\n",
      "Epoch 13, Batch 643, LR 2.479557 Loss 6.725806, Accuracy 82.827%\n",
      "Epoch 13, Batch 644, LR 2.479532 Loss 6.726581, Accuracy 82.827%\n",
      "Epoch 13, Batch 645, LR 2.479508 Loss 6.727123, Accuracy 82.833%\n",
      "Epoch 13, Batch 646, LR 2.479484 Loss 6.726866, Accuracy 82.839%\n",
      "Epoch 13, Batch 647, LR 2.479460 Loss 6.727923, Accuracy 82.829%\n",
      "Epoch 13, Batch 648, LR 2.479436 Loss 6.728343, Accuracy 82.829%\n",
      "Epoch 13, Batch 649, LR 2.479412 Loss 6.728364, Accuracy 82.834%\n",
      "Epoch 13, Batch 650, LR 2.479387 Loss 6.728130, Accuracy 82.839%\n",
      "Epoch 13, Batch 651, LR 2.479363 Loss 6.728603, Accuracy 82.828%\n",
      "Epoch 13, Batch 652, LR 2.479339 Loss 6.728435, Accuracy 82.828%\n",
      "Epoch 13, Batch 653, LR 2.479315 Loss 6.728154, Accuracy 82.830%\n",
      "Epoch 13, Batch 654, LR 2.479290 Loss 6.727645, Accuracy 82.828%\n",
      "Epoch 13, Batch 655, LR 2.479266 Loss 6.725914, Accuracy 82.838%\n",
      "Epoch 13, Batch 656, LR 2.479242 Loss 6.726936, Accuracy 82.829%\n",
      "Epoch 13, Batch 657, LR 2.479217 Loss 6.727312, Accuracy 82.822%\n",
      "Epoch 13, Batch 658, LR 2.479193 Loss 6.727756, Accuracy 82.822%\n",
      "Epoch 13, Batch 659, LR 2.479169 Loss 6.727398, Accuracy 82.826%\n",
      "Epoch 13, Batch 660, LR 2.479144 Loss 6.726855, Accuracy 82.831%\n",
      "Epoch 13, Batch 661, LR 2.479120 Loss 6.726766, Accuracy 82.833%\n",
      "Epoch 13, Batch 662, LR 2.479096 Loss 6.727228, Accuracy 82.831%\n",
      "Epoch 13, Batch 663, LR 2.479071 Loss 6.726931, Accuracy 82.831%\n",
      "Epoch 13, Batch 664, LR 2.479047 Loss 6.726552, Accuracy 82.836%\n",
      "Epoch 13, Batch 665, LR 2.479022 Loss 6.727308, Accuracy 82.835%\n",
      "Epoch 13, Batch 666, LR 2.478998 Loss 6.727548, Accuracy 82.836%\n",
      "Epoch 13, Batch 667, LR 2.478973 Loss 6.727688, Accuracy 82.832%\n",
      "Epoch 13, Batch 668, LR 2.478949 Loss 6.727715, Accuracy 82.832%\n",
      "Epoch 13, Batch 669, LR 2.478924 Loss 6.727899, Accuracy 82.832%\n",
      "Epoch 13, Batch 670, LR 2.478900 Loss 6.728102, Accuracy 82.829%\n",
      "Epoch 13, Batch 671, LR 2.478875 Loss 6.727403, Accuracy 82.832%\n",
      "Epoch 13, Batch 672, LR 2.478851 Loss 6.727608, Accuracy 82.828%\n",
      "Epoch 13, Batch 673, LR 2.478826 Loss 6.727512, Accuracy 82.824%\n",
      "Epoch 13, Batch 674, LR 2.478802 Loss 6.727060, Accuracy 82.822%\n",
      "Epoch 13, Batch 675, LR 2.478777 Loss 6.727437, Accuracy 82.816%\n",
      "Epoch 13, Batch 676, LR 2.478753 Loss 6.726761, Accuracy 82.823%\n",
      "Epoch 13, Batch 677, LR 2.478728 Loss 6.726468, Accuracy 82.823%\n",
      "Epoch 13, Batch 678, LR 2.478703 Loss 6.726811, Accuracy 82.819%\n",
      "Epoch 13, Batch 679, LR 2.478679 Loss 6.726902, Accuracy 82.821%\n",
      "Epoch 13, Batch 680, LR 2.478654 Loss 6.726588, Accuracy 82.818%\n",
      "Epoch 13, Batch 681, LR 2.478630 Loss 6.727019, Accuracy 82.816%\n",
      "Epoch 13, Batch 682, LR 2.478605 Loss 6.726652, Accuracy 82.821%\n",
      "Epoch 13, Batch 683, LR 2.478580 Loss 6.727214, Accuracy 82.816%\n",
      "Epoch 13, Batch 684, LR 2.478555 Loss 6.727375, Accuracy 82.814%\n",
      "Epoch 13, Batch 685, LR 2.478531 Loss 6.726924, Accuracy 82.815%\n",
      "Epoch 13, Batch 686, LR 2.478506 Loss 6.726136, Accuracy 82.818%\n",
      "Epoch 13, Batch 687, LR 2.478481 Loss 6.724951, Accuracy 82.833%\n",
      "Epoch 13, Batch 688, LR 2.478457 Loss 6.724740, Accuracy 82.832%\n",
      "Epoch 13, Batch 689, LR 2.478432 Loss 6.725339, Accuracy 82.828%\n",
      "Epoch 13, Batch 690, LR 2.478407 Loss 6.725743, Accuracy 82.823%\n",
      "Epoch 13, Batch 691, LR 2.478382 Loss 6.725387, Accuracy 82.827%\n",
      "Epoch 13, Batch 692, LR 2.478357 Loss 6.725144, Accuracy 82.832%\n",
      "Epoch 13, Batch 693, LR 2.478333 Loss 6.725727, Accuracy 82.824%\n",
      "Epoch 13, Batch 694, LR 2.478308 Loss 6.727007, Accuracy 82.812%\n",
      "Epoch 13, Batch 695, LR 2.478283 Loss 6.727301, Accuracy 82.815%\n",
      "Epoch 13, Batch 696, LR 2.478258 Loss 6.726326, Accuracy 82.821%\n",
      "Epoch 13, Batch 697, LR 2.478233 Loss 6.726554, Accuracy 82.823%\n",
      "Epoch 13, Batch 698, LR 2.478208 Loss 6.727399, Accuracy 82.810%\n",
      "Epoch 13, Batch 699, LR 2.478183 Loss 6.727278, Accuracy 82.810%\n",
      "Epoch 13, Batch 700, LR 2.478158 Loss 6.726337, Accuracy 82.820%\n",
      "Epoch 13, Batch 701, LR 2.478133 Loss 6.725429, Accuracy 82.821%\n",
      "Epoch 13, Batch 702, LR 2.478108 Loss 6.725685, Accuracy 82.814%\n",
      "Epoch 13, Batch 703, LR 2.478084 Loss 6.724825, Accuracy 82.825%\n",
      "Epoch 13, Batch 704, LR 2.478059 Loss 6.724507, Accuracy 82.826%\n",
      "Epoch 13, Batch 705, LR 2.478034 Loss 6.725390, Accuracy 82.824%\n",
      "Epoch 13, Batch 706, LR 2.478009 Loss 6.724761, Accuracy 82.834%\n",
      "Epoch 13, Batch 707, LR 2.477983 Loss 6.725029, Accuracy 82.831%\n",
      "Epoch 13, Batch 708, LR 2.477958 Loss 6.726084, Accuracy 82.820%\n",
      "Epoch 13, Batch 709, LR 2.477933 Loss 6.725514, Accuracy 82.820%\n",
      "Epoch 13, Batch 710, LR 2.477908 Loss 6.724584, Accuracy 82.830%\n",
      "Epoch 13, Batch 711, LR 2.477883 Loss 6.725658, Accuracy 82.822%\n",
      "Epoch 13, Batch 712, LR 2.477858 Loss 6.726250, Accuracy 82.822%\n",
      "Epoch 13, Batch 713, LR 2.477833 Loss 6.725787, Accuracy 82.826%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 714, LR 2.477808 Loss 6.725980, Accuracy 82.827%\n",
      "Epoch 13, Batch 715, LR 2.477783 Loss 6.726622, Accuracy 82.827%\n",
      "Epoch 13, Batch 716, LR 2.477758 Loss 6.727208, Accuracy 82.827%\n",
      "Epoch 13, Batch 717, LR 2.477732 Loss 6.727783, Accuracy 82.826%\n",
      "Epoch 13, Batch 718, LR 2.477707 Loss 6.727972, Accuracy 82.824%\n",
      "Epoch 13, Batch 719, LR 2.477682 Loss 6.727716, Accuracy 82.830%\n",
      "Epoch 13, Batch 720, LR 2.477657 Loss 6.726739, Accuracy 82.839%\n",
      "Epoch 13, Batch 721, LR 2.477632 Loss 6.726500, Accuracy 82.839%\n",
      "Epoch 13, Batch 722, LR 2.477606 Loss 6.726356, Accuracy 82.836%\n",
      "Epoch 13, Batch 723, LR 2.477581 Loss 6.726467, Accuracy 82.842%\n",
      "Epoch 13, Batch 724, LR 2.477556 Loss 6.726592, Accuracy 82.841%\n",
      "Epoch 13, Batch 725, LR 2.477531 Loss 6.726426, Accuracy 82.839%\n",
      "Epoch 13, Batch 726, LR 2.477505 Loss 6.726778, Accuracy 82.848%\n",
      "Epoch 13, Batch 727, LR 2.477480 Loss 6.726910, Accuracy 82.845%\n",
      "Epoch 13, Batch 728, LR 2.477455 Loss 6.726731, Accuracy 82.841%\n",
      "Epoch 13, Batch 729, LR 2.477429 Loss 6.726574, Accuracy 82.841%\n",
      "Epoch 13, Batch 730, LR 2.477404 Loss 6.726307, Accuracy 82.845%\n",
      "Epoch 13, Batch 731, LR 2.477379 Loss 6.726633, Accuracy 82.841%\n",
      "Epoch 13, Batch 732, LR 2.477353 Loss 6.726265, Accuracy 82.835%\n",
      "Epoch 13, Batch 733, LR 2.477328 Loss 6.726158, Accuracy 82.833%\n",
      "Epoch 13, Batch 734, LR 2.477303 Loss 6.726196, Accuracy 82.835%\n",
      "Epoch 13, Batch 735, LR 2.477277 Loss 6.725462, Accuracy 82.836%\n",
      "Epoch 13, Batch 736, LR 2.477252 Loss 6.725559, Accuracy 82.835%\n",
      "Epoch 13, Batch 737, LR 2.477226 Loss 6.726514, Accuracy 82.831%\n",
      "Epoch 13, Batch 738, LR 2.477201 Loss 6.726609, Accuracy 82.828%\n",
      "Epoch 13, Batch 739, LR 2.477175 Loss 6.726849, Accuracy 82.828%\n",
      "Epoch 13, Batch 740, LR 2.477150 Loss 6.727018, Accuracy 82.827%\n",
      "Epoch 13, Batch 741, LR 2.477124 Loss 6.727023, Accuracy 82.837%\n",
      "Epoch 13, Batch 742, LR 2.477099 Loss 6.726887, Accuracy 82.842%\n",
      "Epoch 13, Batch 743, LR 2.477073 Loss 6.726704, Accuracy 82.845%\n",
      "Epoch 13, Batch 744, LR 2.477048 Loss 6.727142, Accuracy 82.840%\n",
      "Epoch 13, Batch 745, LR 2.477022 Loss 6.727184, Accuracy 82.836%\n",
      "Epoch 13, Batch 746, LR 2.476997 Loss 6.727654, Accuracy 82.831%\n",
      "Epoch 13, Batch 747, LR 2.476971 Loss 6.728176, Accuracy 82.827%\n",
      "Epoch 13, Batch 748, LR 2.476945 Loss 6.728996, Accuracy 82.823%\n",
      "Epoch 13, Batch 749, LR 2.476920 Loss 6.729290, Accuracy 82.817%\n",
      "Epoch 13, Batch 750, LR 2.476894 Loss 6.729808, Accuracy 82.807%\n",
      "Epoch 13, Batch 751, LR 2.476869 Loss 6.729852, Accuracy 82.806%\n",
      "Epoch 13, Batch 752, LR 2.476843 Loss 6.729946, Accuracy 82.799%\n",
      "Epoch 13, Batch 753, LR 2.476817 Loss 6.729264, Accuracy 82.800%\n",
      "Epoch 13, Batch 754, LR 2.476791 Loss 6.729500, Accuracy 82.797%\n",
      "Epoch 13, Batch 755, LR 2.476766 Loss 6.729170, Accuracy 82.802%\n",
      "Epoch 13, Batch 756, LR 2.476740 Loss 6.730282, Accuracy 82.802%\n",
      "Epoch 13, Batch 757, LR 2.476714 Loss 6.729893, Accuracy 82.811%\n",
      "Epoch 13, Batch 758, LR 2.476689 Loss 6.730154, Accuracy 82.810%\n",
      "Epoch 13, Batch 759, LR 2.476663 Loss 6.729861, Accuracy 82.811%\n",
      "Epoch 13, Batch 760, LR 2.476637 Loss 6.730254, Accuracy 82.812%\n",
      "Epoch 13, Batch 761, LR 2.476611 Loss 6.730234, Accuracy 82.810%\n",
      "Epoch 13, Batch 762, LR 2.476586 Loss 6.729573, Accuracy 82.819%\n",
      "Epoch 13, Batch 763, LR 2.476560 Loss 6.729347, Accuracy 82.819%\n",
      "Epoch 13, Batch 764, LR 2.476534 Loss 6.729452, Accuracy 82.815%\n",
      "Epoch 13, Batch 765, LR 2.476508 Loss 6.729995, Accuracy 82.814%\n",
      "Epoch 13, Batch 766, LR 2.476482 Loss 6.729415, Accuracy 82.816%\n",
      "Epoch 13, Batch 767, LR 2.476456 Loss 6.729798, Accuracy 82.809%\n",
      "Epoch 13, Batch 768, LR 2.476430 Loss 6.730654, Accuracy 82.806%\n",
      "Epoch 13, Batch 769, LR 2.476405 Loss 6.730441, Accuracy 82.801%\n",
      "Epoch 13, Batch 770, LR 2.476379 Loss 6.729846, Accuracy 82.802%\n",
      "Epoch 13, Batch 771, LR 2.476353 Loss 6.729662, Accuracy 82.797%\n",
      "Epoch 13, Batch 772, LR 2.476327 Loss 6.730046, Accuracy 82.797%\n",
      "Epoch 13, Batch 773, LR 2.476301 Loss 6.729525, Accuracy 82.801%\n",
      "Epoch 13, Batch 774, LR 2.476275 Loss 6.730088, Accuracy 82.790%\n",
      "Epoch 13, Batch 775, LR 2.476249 Loss 6.730102, Accuracy 82.791%\n",
      "Epoch 13, Batch 776, LR 2.476223 Loss 6.730526, Accuracy 82.791%\n",
      "Epoch 13, Batch 777, LR 2.476197 Loss 6.731142, Accuracy 82.784%\n",
      "Epoch 13, Batch 778, LR 2.476171 Loss 6.732275, Accuracy 82.775%\n",
      "Epoch 13, Batch 779, LR 2.476145 Loss 6.733037, Accuracy 82.770%\n",
      "Epoch 13, Batch 780, LR 2.476119 Loss 6.732776, Accuracy 82.772%\n",
      "Epoch 13, Batch 781, LR 2.476093 Loss 6.732839, Accuracy 82.770%\n",
      "Epoch 13, Batch 782, LR 2.476067 Loss 6.733151, Accuracy 82.772%\n",
      "Epoch 13, Batch 783, LR 2.476040 Loss 6.732795, Accuracy 82.777%\n",
      "Epoch 13, Batch 784, LR 2.476014 Loss 6.731756, Accuracy 82.786%\n",
      "Epoch 13, Batch 785, LR 2.475988 Loss 6.730857, Accuracy 82.789%\n",
      "Epoch 13, Batch 786, LR 2.475962 Loss 6.730423, Accuracy 82.789%\n",
      "Epoch 13, Batch 787, LR 2.475936 Loss 6.730288, Accuracy 82.791%\n",
      "Epoch 13, Batch 788, LR 2.475910 Loss 6.730496, Accuracy 82.795%\n",
      "Epoch 13, Batch 789, LR 2.475884 Loss 6.730723, Accuracy 82.796%\n",
      "Epoch 13, Batch 790, LR 2.475857 Loss 6.729860, Accuracy 82.802%\n",
      "Epoch 13, Batch 791, LR 2.475831 Loss 6.729302, Accuracy 82.804%\n",
      "Epoch 13, Batch 792, LR 2.475805 Loss 6.729570, Accuracy 82.805%\n",
      "Epoch 13, Batch 793, LR 2.475779 Loss 6.729406, Accuracy 82.805%\n",
      "Epoch 13, Batch 794, LR 2.475753 Loss 6.729280, Accuracy 82.804%\n",
      "Epoch 13, Batch 795, LR 2.475726 Loss 6.729667, Accuracy 82.805%\n",
      "Epoch 13, Batch 796, LR 2.475700 Loss 6.729353, Accuracy 82.807%\n",
      "Epoch 13, Batch 797, LR 2.475674 Loss 6.729769, Accuracy 82.800%\n",
      "Epoch 13, Batch 798, LR 2.475647 Loss 6.730238, Accuracy 82.797%\n",
      "Epoch 13, Batch 799, LR 2.475621 Loss 6.730291, Accuracy 82.794%\n",
      "Epoch 13, Batch 800, LR 2.475595 Loss 6.729529, Accuracy 82.794%\n",
      "Epoch 13, Batch 801, LR 2.475568 Loss 6.729480, Accuracy 82.794%\n",
      "Epoch 13, Batch 802, LR 2.475542 Loss 6.730484, Accuracy 82.786%\n",
      "Epoch 13, Batch 803, LR 2.475516 Loss 6.730567, Accuracy 82.779%\n",
      "Epoch 13, Batch 804, LR 2.475489 Loss 6.730550, Accuracy 82.781%\n",
      "Epoch 13, Batch 805, LR 2.475463 Loss 6.731066, Accuracy 82.777%\n",
      "Epoch 13, Batch 806, LR 2.475436 Loss 6.731989, Accuracy 82.777%\n",
      "Epoch 13, Batch 807, LR 2.475410 Loss 6.731658, Accuracy 82.777%\n",
      "Epoch 13, Batch 808, LR 2.475384 Loss 6.731992, Accuracy 82.774%\n",
      "Epoch 13, Batch 809, LR 2.475357 Loss 6.732734, Accuracy 82.768%\n",
      "Epoch 13, Batch 810, LR 2.475331 Loss 6.733345, Accuracy 82.763%\n",
      "Epoch 13, Batch 811, LR 2.475304 Loss 6.732999, Accuracy 82.768%\n",
      "Epoch 13, Batch 812, LR 2.475278 Loss 6.732828, Accuracy 82.768%\n",
      "Epoch 13, Batch 813, LR 2.475251 Loss 6.731838, Accuracy 82.775%\n",
      "Epoch 13, Batch 814, LR 2.475225 Loss 6.732129, Accuracy 82.772%\n",
      "Epoch 13, Batch 815, LR 2.475198 Loss 6.731888, Accuracy 82.770%\n",
      "Epoch 13, Batch 816, LR 2.475171 Loss 6.731568, Accuracy 82.776%\n",
      "Epoch 13, Batch 817, LR 2.475145 Loss 6.731039, Accuracy 82.778%\n",
      "Epoch 13, Batch 818, LR 2.475118 Loss 6.731450, Accuracy 82.778%\n",
      "Epoch 13, Batch 819, LR 2.475092 Loss 6.731458, Accuracy 82.776%\n",
      "Epoch 13, Batch 820, LR 2.475065 Loss 6.731971, Accuracy 82.780%\n",
      "Epoch 13, Batch 821, LR 2.475038 Loss 6.731897, Accuracy 82.783%\n",
      "Epoch 13, Batch 822, LR 2.475012 Loss 6.731795, Accuracy 82.786%\n",
      "Epoch 13, Batch 823, LR 2.474985 Loss 6.731705, Accuracy 82.791%\n",
      "Epoch 13, Batch 824, LR 2.474959 Loss 6.732067, Accuracy 82.789%\n",
      "Epoch 13, Batch 825, LR 2.474932 Loss 6.731685, Accuracy 82.789%\n",
      "Epoch 13, Batch 826, LR 2.474905 Loss 6.731913, Accuracy 82.790%\n",
      "Epoch 13, Batch 827, LR 2.474878 Loss 6.732426, Accuracy 82.785%\n",
      "Epoch 13, Batch 828, LR 2.474852 Loss 6.732134, Accuracy 82.788%\n",
      "Epoch 13, Batch 829, LR 2.474825 Loss 6.731864, Accuracy 82.789%\n",
      "Epoch 13, Batch 830, LR 2.474798 Loss 6.733001, Accuracy 82.782%\n",
      "Epoch 13, Batch 831, LR 2.474771 Loss 6.733240, Accuracy 82.784%\n",
      "Epoch 13, Batch 832, LR 2.474745 Loss 6.733173, Accuracy 82.781%\n",
      "Epoch 13, Batch 833, LR 2.474718 Loss 6.732174, Accuracy 82.786%\n",
      "Epoch 13, Batch 834, LR 2.474691 Loss 6.732059, Accuracy 82.788%\n",
      "Epoch 13, Batch 835, LR 2.474664 Loss 6.732793, Accuracy 82.782%\n",
      "Epoch 13, Batch 836, LR 2.474637 Loss 6.732716, Accuracy 82.784%\n",
      "Epoch 13, Batch 837, LR 2.474610 Loss 6.732067, Accuracy 82.788%\n",
      "Epoch 13, Batch 838, LR 2.474584 Loss 6.731677, Accuracy 82.783%\n",
      "Epoch 13, Batch 839, LR 2.474557 Loss 6.731044, Accuracy 82.783%\n",
      "Epoch 13, Batch 840, LR 2.474530 Loss 6.731435, Accuracy 82.776%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 841, LR 2.474503 Loss 6.731750, Accuracy 82.774%\n",
      "Epoch 13, Batch 842, LR 2.474476 Loss 6.731993, Accuracy 82.773%\n",
      "Epoch 13, Batch 843, LR 2.474449 Loss 6.732130, Accuracy 82.770%\n",
      "Epoch 13, Batch 844, LR 2.474422 Loss 6.732343, Accuracy 82.766%\n",
      "Epoch 13, Batch 845, LR 2.474395 Loss 6.732482, Accuracy 82.763%\n",
      "Epoch 13, Batch 846, LR 2.474368 Loss 6.732110, Accuracy 82.760%\n",
      "Epoch 13, Batch 847, LR 2.474341 Loss 6.732426, Accuracy 82.755%\n",
      "Epoch 13, Batch 848, LR 2.474314 Loss 6.731923, Accuracy 82.759%\n",
      "Epoch 13, Batch 849, LR 2.474287 Loss 6.731480, Accuracy 82.759%\n",
      "Epoch 13, Batch 850, LR 2.474260 Loss 6.731450, Accuracy 82.761%\n",
      "Epoch 13, Batch 851, LR 2.474233 Loss 6.730791, Accuracy 82.762%\n",
      "Epoch 13, Batch 852, LR 2.474206 Loss 6.730909, Accuracy 82.760%\n",
      "Epoch 13, Batch 853, LR 2.474179 Loss 6.730585, Accuracy 82.763%\n",
      "Epoch 13, Batch 854, LR 2.474152 Loss 6.729875, Accuracy 82.763%\n",
      "Epoch 13, Batch 855, LR 2.474125 Loss 6.729441, Accuracy 82.765%\n",
      "Epoch 13, Batch 856, LR 2.474098 Loss 6.729351, Accuracy 82.763%\n",
      "Epoch 13, Batch 857, LR 2.474070 Loss 6.728533, Accuracy 82.770%\n",
      "Epoch 13, Batch 858, LR 2.474043 Loss 6.728296, Accuracy 82.772%\n",
      "Epoch 13, Batch 859, LR 2.474016 Loss 6.728099, Accuracy 82.766%\n",
      "Epoch 13, Batch 860, LR 2.473989 Loss 6.728033, Accuracy 82.764%\n",
      "Epoch 13, Batch 861, LR 2.473962 Loss 6.727631, Accuracy 82.768%\n",
      "Epoch 13, Batch 862, LR 2.473935 Loss 6.727376, Accuracy 82.767%\n",
      "Epoch 13, Batch 863, LR 2.473907 Loss 6.728000, Accuracy 82.765%\n",
      "Epoch 13, Batch 864, LR 2.473880 Loss 6.727517, Accuracy 82.769%\n",
      "Epoch 13, Batch 865, LR 2.473853 Loss 6.728176, Accuracy 82.763%\n",
      "Epoch 13, Batch 866, LR 2.473826 Loss 6.728130, Accuracy 82.764%\n",
      "Epoch 13, Batch 867, LR 2.473798 Loss 6.727712, Accuracy 82.772%\n",
      "Epoch 13, Batch 868, LR 2.473771 Loss 6.726888, Accuracy 82.777%\n",
      "Epoch 13, Batch 869, LR 2.473744 Loss 6.726059, Accuracy 82.782%\n",
      "Epoch 13, Batch 870, LR 2.473716 Loss 6.725951, Accuracy 82.782%\n",
      "Epoch 13, Batch 871, LR 2.473689 Loss 6.725216, Accuracy 82.788%\n",
      "Epoch 13, Batch 872, LR 2.473662 Loss 6.725673, Accuracy 82.786%\n",
      "Epoch 13, Batch 873, LR 2.473634 Loss 6.725303, Accuracy 82.786%\n",
      "Epoch 13, Batch 874, LR 2.473607 Loss 6.724775, Accuracy 82.786%\n",
      "Epoch 13, Batch 875, LR 2.473580 Loss 6.724933, Accuracy 82.787%\n",
      "Epoch 13, Batch 876, LR 2.473552 Loss 6.725297, Accuracy 82.783%\n",
      "Epoch 13, Batch 877, LR 2.473525 Loss 6.724500, Accuracy 82.788%\n",
      "Epoch 13, Batch 878, LR 2.473497 Loss 6.724068, Accuracy 82.789%\n",
      "Epoch 13, Batch 879, LR 2.473470 Loss 6.724377, Accuracy 82.782%\n",
      "Epoch 13, Batch 880, LR 2.473442 Loss 6.723665, Accuracy 82.784%\n",
      "Epoch 13, Batch 881, LR 2.473415 Loss 6.724359, Accuracy 82.776%\n",
      "Epoch 13, Batch 882, LR 2.473388 Loss 6.724116, Accuracy 82.777%\n",
      "Epoch 13, Batch 883, LR 2.473360 Loss 6.724041, Accuracy 82.776%\n",
      "Epoch 13, Batch 884, LR 2.473332 Loss 6.724431, Accuracy 82.773%\n",
      "Epoch 13, Batch 885, LR 2.473305 Loss 6.724512, Accuracy 82.775%\n",
      "Epoch 13, Batch 886, LR 2.473277 Loss 6.724336, Accuracy 82.777%\n",
      "Epoch 13, Batch 887, LR 2.473250 Loss 6.725254, Accuracy 82.769%\n",
      "Epoch 13, Batch 888, LR 2.473222 Loss 6.725819, Accuracy 82.766%\n",
      "Epoch 13, Batch 889, LR 2.473195 Loss 6.725838, Accuracy 82.760%\n",
      "Epoch 13, Batch 890, LR 2.473167 Loss 6.726079, Accuracy 82.763%\n",
      "Epoch 13, Batch 891, LR 2.473139 Loss 6.726250, Accuracy 82.756%\n",
      "Epoch 13, Batch 892, LR 2.473112 Loss 6.726235, Accuracy 82.755%\n",
      "Epoch 13, Batch 893, LR 2.473084 Loss 6.725924, Accuracy 82.756%\n",
      "Epoch 13, Batch 894, LR 2.473057 Loss 6.726813, Accuracy 82.748%\n",
      "Epoch 13, Batch 895, LR 2.473029 Loss 6.726220, Accuracy 82.750%\n",
      "Epoch 13, Batch 896, LR 2.473001 Loss 6.726025, Accuracy 82.753%\n",
      "Epoch 13, Batch 897, LR 2.472974 Loss 6.726206, Accuracy 82.752%\n",
      "Epoch 13, Batch 898, LR 2.472946 Loss 6.726278, Accuracy 82.752%\n",
      "Epoch 13, Batch 899, LR 2.472918 Loss 6.725661, Accuracy 82.754%\n",
      "Epoch 13, Batch 900, LR 2.472890 Loss 6.724824, Accuracy 82.760%\n",
      "Epoch 13, Batch 901, LR 2.472863 Loss 6.724885, Accuracy 82.759%\n",
      "Epoch 13, Batch 902, LR 2.472835 Loss 6.725033, Accuracy 82.755%\n",
      "Epoch 13, Batch 903, LR 2.472807 Loss 6.724445, Accuracy 82.762%\n",
      "Epoch 13, Batch 904, LR 2.472779 Loss 6.724768, Accuracy 82.756%\n",
      "Epoch 13, Batch 905, LR 2.472751 Loss 6.724711, Accuracy 82.756%\n",
      "Epoch 13, Batch 906, LR 2.472724 Loss 6.725066, Accuracy 82.756%\n",
      "Epoch 13, Batch 907, LR 2.472696 Loss 6.725427, Accuracy 82.748%\n",
      "Epoch 13, Batch 908, LR 2.472668 Loss 6.725356, Accuracy 82.747%\n",
      "Epoch 13, Batch 909, LR 2.472640 Loss 6.725555, Accuracy 82.743%\n",
      "Epoch 13, Batch 910, LR 2.472612 Loss 6.726115, Accuracy 82.738%\n",
      "Epoch 13, Batch 911, LR 2.472584 Loss 6.725113, Accuracy 82.739%\n",
      "Epoch 13, Batch 912, LR 2.472556 Loss 6.725081, Accuracy 82.735%\n",
      "Epoch 13, Batch 913, LR 2.472528 Loss 6.725187, Accuracy 82.734%\n",
      "Epoch 13, Batch 914, LR 2.472501 Loss 6.725397, Accuracy 82.735%\n",
      "Epoch 13, Batch 915, LR 2.472473 Loss 6.725203, Accuracy 82.738%\n",
      "Epoch 13, Batch 916, LR 2.472445 Loss 6.725368, Accuracy 82.740%\n",
      "Epoch 13, Batch 917, LR 2.472417 Loss 6.724899, Accuracy 82.745%\n",
      "Epoch 13, Batch 918, LR 2.472389 Loss 6.725351, Accuracy 82.741%\n",
      "Epoch 13, Batch 919, LR 2.472361 Loss 6.724965, Accuracy 82.741%\n",
      "Epoch 13, Batch 920, LR 2.472333 Loss 6.724703, Accuracy 82.741%\n",
      "Epoch 13, Batch 921, LR 2.472305 Loss 6.724434, Accuracy 82.743%\n",
      "Epoch 13, Batch 922, LR 2.472277 Loss 6.724649, Accuracy 82.738%\n",
      "Epoch 13, Batch 923, LR 2.472248 Loss 6.724433, Accuracy 82.741%\n",
      "Epoch 13, Batch 924, LR 2.472220 Loss 6.725202, Accuracy 82.736%\n",
      "Epoch 13, Batch 925, LR 2.472192 Loss 6.724856, Accuracy 82.735%\n",
      "Epoch 13, Batch 926, LR 2.472164 Loss 6.724438, Accuracy 82.740%\n",
      "Epoch 13, Batch 927, LR 2.472136 Loss 6.724577, Accuracy 82.743%\n",
      "Epoch 13, Batch 928, LR 2.472108 Loss 6.724671, Accuracy 82.739%\n",
      "Epoch 13, Batch 929, LR 2.472080 Loss 6.724294, Accuracy 82.739%\n",
      "Epoch 13, Batch 930, LR 2.472052 Loss 6.723792, Accuracy 82.744%\n",
      "Epoch 13, Batch 931, LR 2.472023 Loss 6.722451, Accuracy 82.752%\n",
      "Epoch 13, Batch 932, LR 2.471995 Loss 6.722262, Accuracy 82.755%\n",
      "Epoch 13, Batch 933, LR 2.471967 Loss 6.722537, Accuracy 82.759%\n",
      "Epoch 13, Batch 934, LR 2.471939 Loss 6.722761, Accuracy 82.757%\n",
      "Epoch 13, Batch 935, LR 2.471911 Loss 6.722035, Accuracy 82.762%\n",
      "Epoch 13, Batch 936, LR 2.471882 Loss 6.722039, Accuracy 82.762%\n",
      "Epoch 13, Batch 937, LR 2.471854 Loss 6.722180, Accuracy 82.761%\n",
      "Epoch 13, Batch 938, LR 2.471826 Loss 6.721673, Accuracy 82.763%\n",
      "Epoch 13, Batch 939, LR 2.471798 Loss 6.722225, Accuracy 82.761%\n",
      "Epoch 13, Batch 940, LR 2.471769 Loss 6.722170, Accuracy 82.763%\n",
      "Epoch 13, Batch 941, LR 2.471741 Loss 6.722414, Accuracy 82.762%\n",
      "Epoch 13, Batch 942, LR 2.471713 Loss 6.723466, Accuracy 82.755%\n",
      "Epoch 13, Batch 943, LR 2.471684 Loss 6.723541, Accuracy 82.755%\n",
      "Epoch 13, Batch 944, LR 2.471656 Loss 6.722937, Accuracy 82.755%\n",
      "Epoch 13, Batch 945, LR 2.471628 Loss 6.722926, Accuracy 82.755%\n",
      "Epoch 13, Batch 946, LR 2.471599 Loss 6.721821, Accuracy 82.761%\n",
      "Epoch 13, Batch 947, LR 2.471571 Loss 6.721413, Accuracy 82.761%\n",
      "Epoch 13, Batch 948, LR 2.471542 Loss 6.721684, Accuracy 82.764%\n",
      "Epoch 13, Batch 949, LR 2.471514 Loss 6.721470, Accuracy 82.769%\n",
      "Epoch 13, Batch 950, LR 2.471485 Loss 6.721495, Accuracy 82.769%\n",
      "Epoch 13, Batch 951, LR 2.471457 Loss 6.722029, Accuracy 82.766%\n",
      "Epoch 13, Batch 952, LR 2.471429 Loss 6.722075, Accuracy 82.769%\n",
      "Epoch 13, Batch 953, LR 2.471400 Loss 6.722199, Accuracy 82.763%\n",
      "Epoch 13, Batch 954, LR 2.471372 Loss 6.722387, Accuracy 82.761%\n",
      "Epoch 13, Batch 955, LR 2.471343 Loss 6.722278, Accuracy 82.763%\n",
      "Epoch 13, Batch 956, LR 2.471315 Loss 6.721938, Accuracy 82.764%\n",
      "Epoch 13, Batch 957, LR 2.471286 Loss 6.721899, Accuracy 82.767%\n",
      "Epoch 13, Batch 958, LR 2.471257 Loss 6.721842, Accuracy 82.764%\n",
      "Epoch 13, Batch 959, LR 2.471229 Loss 6.722594, Accuracy 82.765%\n",
      "Epoch 13, Batch 960, LR 2.471200 Loss 6.722186, Accuracy 82.768%\n",
      "Epoch 13, Batch 961, LR 2.471172 Loss 6.722134, Accuracy 82.766%\n",
      "Epoch 13, Batch 962, LR 2.471143 Loss 6.722311, Accuracy 82.762%\n",
      "Epoch 13, Batch 963, LR 2.471114 Loss 6.721663, Accuracy 82.765%\n",
      "Epoch 13, Batch 964, LR 2.471086 Loss 6.721334, Accuracy 82.765%\n",
      "Epoch 13, Batch 965, LR 2.471057 Loss 6.720885, Accuracy 82.770%\n",
      "Epoch 13, Batch 966, LR 2.471029 Loss 6.720576, Accuracy 82.772%\n",
      "Epoch 13, Batch 967, LR 2.471000 Loss 6.720034, Accuracy 82.779%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 968, LR 2.470971 Loss 6.720210, Accuracy 82.779%\n",
      "Epoch 13, Batch 969, LR 2.470942 Loss 6.720564, Accuracy 82.779%\n",
      "Epoch 13, Batch 970, LR 2.470914 Loss 6.720256, Accuracy 82.779%\n",
      "Epoch 13, Batch 971, LR 2.470885 Loss 6.720368, Accuracy 82.776%\n",
      "Epoch 13, Batch 972, LR 2.470856 Loss 6.720803, Accuracy 82.776%\n",
      "Epoch 13, Batch 973, LR 2.470827 Loss 6.720127, Accuracy 82.777%\n",
      "Epoch 13, Batch 974, LR 2.470799 Loss 6.719960, Accuracy 82.780%\n",
      "Epoch 13, Batch 975, LR 2.470770 Loss 6.719649, Accuracy 82.782%\n",
      "Epoch 13, Batch 976, LR 2.470741 Loss 6.720176, Accuracy 82.780%\n",
      "Epoch 13, Batch 977, LR 2.470712 Loss 6.719558, Accuracy 82.783%\n",
      "Epoch 13, Batch 978, LR 2.470683 Loss 6.719637, Accuracy 82.785%\n",
      "Epoch 13, Batch 979, LR 2.470655 Loss 6.719640, Accuracy 82.777%\n",
      "Epoch 13, Batch 980, LR 2.470626 Loss 6.719181, Accuracy 82.777%\n",
      "Epoch 13, Batch 981, LR 2.470597 Loss 6.719268, Accuracy 82.775%\n",
      "Epoch 13, Batch 982, LR 2.470568 Loss 6.719898, Accuracy 82.770%\n",
      "Epoch 13, Batch 983, LR 2.470539 Loss 6.718986, Accuracy 82.775%\n",
      "Epoch 13, Batch 984, LR 2.470510 Loss 6.719691, Accuracy 82.772%\n",
      "Epoch 13, Batch 985, LR 2.470481 Loss 6.718958, Accuracy 82.778%\n",
      "Epoch 13, Batch 986, LR 2.470452 Loss 6.718870, Accuracy 82.782%\n",
      "Epoch 13, Batch 987, LR 2.470423 Loss 6.718455, Accuracy 82.788%\n",
      "Epoch 13, Batch 988, LR 2.470394 Loss 6.718931, Accuracy 82.783%\n",
      "Epoch 13, Batch 989, LR 2.470365 Loss 6.718661, Accuracy 82.786%\n",
      "Epoch 13, Batch 990, LR 2.470336 Loss 6.719846, Accuracy 82.777%\n",
      "Epoch 13, Batch 991, LR 2.470307 Loss 6.719548, Accuracy 82.779%\n",
      "Epoch 13, Batch 992, LR 2.470278 Loss 6.719247, Accuracy 82.781%\n",
      "Epoch 13, Batch 993, LR 2.470249 Loss 6.718651, Accuracy 82.784%\n",
      "Epoch 13, Batch 994, LR 2.470220 Loss 6.718489, Accuracy 82.787%\n",
      "Epoch 13, Batch 995, LR 2.470191 Loss 6.718968, Accuracy 82.784%\n",
      "Epoch 13, Batch 996, LR 2.470162 Loss 6.719012, Accuracy 82.786%\n",
      "Epoch 13, Batch 997, LR 2.470133 Loss 6.718857, Accuracy 82.788%\n",
      "Epoch 13, Batch 998, LR 2.470104 Loss 6.719159, Accuracy 82.785%\n",
      "Epoch 13, Batch 999, LR 2.470075 Loss 6.718884, Accuracy 82.784%\n",
      "Epoch 13, Batch 1000, LR 2.470046 Loss 6.718846, Accuracy 82.784%\n",
      "Epoch 13, Batch 1001, LR 2.470016 Loss 6.718499, Accuracy 82.788%\n",
      "Epoch 13, Batch 1002, LR 2.469987 Loss 6.717873, Accuracy 82.792%\n",
      "Epoch 13, Batch 1003, LR 2.469958 Loss 6.717949, Accuracy 82.787%\n",
      "Epoch 13, Batch 1004, LR 2.469929 Loss 6.718569, Accuracy 82.777%\n",
      "Epoch 13, Batch 1005, LR 2.469900 Loss 6.718519, Accuracy 82.773%\n",
      "Epoch 13, Batch 1006, LR 2.469870 Loss 6.717985, Accuracy 82.777%\n",
      "Epoch 13, Batch 1007, LR 2.469841 Loss 6.717932, Accuracy 82.777%\n",
      "Epoch 13, Batch 1008, LR 2.469812 Loss 6.718081, Accuracy 82.776%\n",
      "Epoch 13, Batch 1009, LR 2.469783 Loss 6.717680, Accuracy 82.780%\n",
      "Epoch 13, Batch 1010, LR 2.469753 Loss 6.718104, Accuracy 82.777%\n",
      "Epoch 13, Batch 1011, LR 2.469724 Loss 6.718467, Accuracy 82.775%\n",
      "Epoch 13, Batch 1012, LR 2.469695 Loss 6.718349, Accuracy 82.775%\n",
      "Epoch 13, Batch 1013, LR 2.469665 Loss 6.718851, Accuracy 82.771%\n",
      "Epoch 13, Batch 1014, LR 2.469636 Loss 6.719318, Accuracy 82.768%\n",
      "Epoch 13, Batch 1015, LR 2.469607 Loss 6.719157, Accuracy 82.770%\n",
      "Epoch 13, Batch 1016, LR 2.469577 Loss 6.719088, Accuracy 82.775%\n",
      "Epoch 13, Batch 1017, LR 2.469548 Loss 6.718544, Accuracy 82.774%\n",
      "Epoch 13, Batch 1018, LR 2.469519 Loss 6.718322, Accuracy 82.773%\n",
      "Epoch 13, Batch 1019, LR 2.469489 Loss 6.718904, Accuracy 82.768%\n",
      "Epoch 13, Batch 1020, LR 2.469460 Loss 6.718623, Accuracy 82.767%\n",
      "Epoch 13, Batch 1021, LR 2.469430 Loss 6.718549, Accuracy 82.769%\n",
      "Epoch 13, Batch 1022, LR 2.469401 Loss 6.718550, Accuracy 82.765%\n",
      "Epoch 13, Batch 1023, LR 2.469371 Loss 6.718750, Accuracy 82.767%\n",
      "Epoch 13, Batch 1024, LR 2.469342 Loss 6.719558, Accuracy 82.760%\n",
      "Epoch 13, Batch 1025, LR 2.469312 Loss 6.719778, Accuracy 82.761%\n",
      "Epoch 13, Batch 1026, LR 2.469283 Loss 6.719564, Accuracy 82.761%\n",
      "Epoch 13, Batch 1027, LR 2.469253 Loss 6.719813, Accuracy 82.757%\n",
      "Epoch 13, Batch 1028, LR 2.469224 Loss 6.719951, Accuracy 82.759%\n",
      "Epoch 13, Batch 1029, LR 2.469194 Loss 6.719600, Accuracy 82.762%\n",
      "Epoch 13, Batch 1030, LR 2.469165 Loss 6.719294, Accuracy 82.763%\n",
      "Epoch 13, Batch 1031, LR 2.469135 Loss 6.719895, Accuracy 82.759%\n",
      "Epoch 13, Batch 1032, LR 2.469106 Loss 6.720605, Accuracy 82.754%\n",
      "Epoch 13, Batch 1033, LR 2.469076 Loss 6.720749, Accuracy 82.757%\n",
      "Epoch 13, Batch 1034, LR 2.469046 Loss 6.721250, Accuracy 82.751%\n",
      "Epoch 13, Batch 1035, LR 2.469017 Loss 6.722168, Accuracy 82.745%\n",
      "Epoch 13, Batch 1036, LR 2.468987 Loss 6.722278, Accuracy 82.745%\n",
      "Epoch 13, Batch 1037, LR 2.468957 Loss 6.722431, Accuracy 82.743%\n",
      "Epoch 13, Batch 1038, LR 2.468928 Loss 6.722379, Accuracy 82.743%\n",
      "Epoch 13, Batch 1039, LR 2.468898 Loss 6.721924, Accuracy 82.747%\n",
      "Epoch 13, Batch 1040, LR 2.468868 Loss 6.721246, Accuracy 82.750%\n",
      "Epoch 13, Batch 1041, LR 2.468839 Loss 6.721399, Accuracy 82.747%\n",
      "Epoch 13, Batch 1042, LR 2.468809 Loss 6.720644, Accuracy 82.750%\n",
      "Epoch 13, Batch 1043, LR 2.468779 Loss 6.720074, Accuracy 82.750%\n",
      "Epoch 13, Batch 1044, LR 2.468749 Loss 6.719292, Accuracy 82.754%\n",
      "Epoch 13, Batch 1045, LR 2.468720 Loss 6.719499, Accuracy 82.754%\n",
      "Epoch 13, Batch 1046, LR 2.468690 Loss 6.719399, Accuracy 82.755%\n",
      "Epoch 13, Batch 1047, LR 2.468660 Loss 6.718900, Accuracy 82.760%\n",
      "Epoch 13, Loss (train set) 6.718900, Accuracy (train set) 82.760%\n",
      "Epoch 14, Batch 1, LR 2.468630 Loss 6.190905, Accuracy 85.156%\n",
      "Epoch 14, Batch 2, LR 2.468600 Loss 6.270079, Accuracy 86.328%\n",
      "Epoch 14, Batch 3, LR 2.468571 Loss 6.468359, Accuracy 85.417%\n",
      "Epoch 14, Batch 4, LR 2.468541 Loss 6.391518, Accuracy 85.156%\n",
      "Epoch 14, Batch 5, LR 2.468511 Loss 6.349184, Accuracy 85.938%\n",
      "Epoch 14, Batch 6, LR 2.468481 Loss 6.466113, Accuracy 85.026%\n",
      "Epoch 14, Batch 7, LR 2.468451 Loss 6.493163, Accuracy 84.710%\n",
      "Epoch 14, Batch 8, LR 2.468421 Loss 6.542761, Accuracy 83.887%\n",
      "Epoch 14, Batch 9, LR 2.468391 Loss 6.561561, Accuracy 83.767%\n",
      "Epoch 14, Batch 10, LR 2.468361 Loss 6.554361, Accuracy 84.062%\n",
      "Epoch 14, Batch 11, LR 2.468331 Loss 6.581002, Accuracy 83.523%\n",
      "Epoch 14, Batch 12, LR 2.468301 Loss 6.595088, Accuracy 83.854%\n",
      "Epoch 14, Batch 13, LR 2.468271 Loss 6.563392, Accuracy 83.954%\n",
      "Epoch 14, Batch 14, LR 2.468241 Loss 6.501111, Accuracy 84.375%\n",
      "Epoch 14, Batch 15, LR 2.468211 Loss 6.482998, Accuracy 84.583%\n",
      "Epoch 14, Batch 16, LR 2.468181 Loss 6.524973, Accuracy 84.473%\n",
      "Epoch 14, Batch 17, LR 2.468151 Loss 6.532691, Accuracy 84.421%\n",
      "Epoch 14, Batch 18, LR 2.468121 Loss 6.527556, Accuracy 84.201%\n",
      "Epoch 14, Batch 19, LR 2.468091 Loss 6.536543, Accuracy 84.087%\n",
      "Epoch 14, Batch 20, LR 2.468061 Loss 6.548448, Accuracy 84.062%\n",
      "Epoch 14, Batch 21, LR 2.468031 Loss 6.527287, Accuracy 84.301%\n",
      "Epoch 14, Batch 22, LR 2.468001 Loss 6.554779, Accuracy 84.233%\n",
      "Epoch 14, Batch 23, LR 2.467971 Loss 6.528893, Accuracy 84.375%\n",
      "Epoch 14, Batch 24, LR 2.467941 Loss 6.529973, Accuracy 84.375%\n",
      "Epoch 14, Batch 25, LR 2.467910 Loss 6.530436, Accuracy 84.406%\n",
      "Epoch 14, Batch 26, LR 2.467880 Loss 6.511154, Accuracy 84.495%\n",
      "Epoch 14, Batch 27, LR 2.467850 Loss 6.527521, Accuracy 84.259%\n",
      "Epoch 14, Batch 28, LR 2.467820 Loss 6.530518, Accuracy 84.208%\n",
      "Epoch 14, Batch 29, LR 2.467790 Loss 6.517190, Accuracy 84.106%\n",
      "Epoch 14, Batch 30, LR 2.467760 Loss 6.513339, Accuracy 84.141%\n",
      "Epoch 14, Batch 31, LR 2.467729 Loss 6.535728, Accuracy 83.947%\n",
      "Epoch 14, Batch 32, LR 2.467699 Loss 6.511315, Accuracy 84.082%\n",
      "Epoch 14, Batch 33, LR 2.467669 Loss 6.506665, Accuracy 84.020%\n",
      "Epoch 14, Batch 34, LR 2.467639 Loss 6.522387, Accuracy 83.915%\n",
      "Epoch 14, Batch 35, LR 2.467608 Loss 6.522138, Accuracy 83.862%\n",
      "Epoch 14, Batch 36, LR 2.467578 Loss 6.540464, Accuracy 83.789%\n",
      "Epoch 14, Batch 37, LR 2.467548 Loss 6.524977, Accuracy 83.763%\n",
      "Epoch 14, Batch 38, LR 2.467517 Loss 6.525259, Accuracy 83.697%\n",
      "Epoch 14, Batch 39, LR 2.467487 Loss 6.554569, Accuracy 83.654%\n",
      "Epoch 14, Batch 40, LR 2.467457 Loss 6.551446, Accuracy 83.613%\n",
      "Epoch 14, Batch 41, LR 2.467426 Loss 6.531168, Accuracy 83.651%\n",
      "Epoch 14, Batch 42, LR 2.467396 Loss 6.537152, Accuracy 83.557%\n",
      "Epoch 14, Batch 43, LR 2.467365 Loss 6.538602, Accuracy 83.594%\n",
      "Epoch 14, Batch 44, LR 2.467335 Loss 6.540278, Accuracy 83.540%\n",
      "Epoch 14, Batch 45, LR 2.467305 Loss 6.517754, Accuracy 83.646%\n",
      "Epoch 14, Batch 46, LR 2.467274 Loss 6.513918, Accuracy 83.679%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 47, LR 2.467244 Loss 6.510256, Accuracy 83.727%\n",
      "Epoch 14, Batch 48, LR 2.467213 Loss 6.529475, Accuracy 83.626%\n",
      "Epoch 14, Batch 49, LR 2.467183 Loss 6.521341, Accuracy 83.721%\n",
      "Epoch 14, Batch 50, LR 2.467152 Loss 6.511493, Accuracy 83.797%\n",
      "Epoch 14, Batch 51, LR 2.467122 Loss 6.512429, Accuracy 83.778%\n",
      "Epoch 14, Batch 52, LR 2.467091 Loss 6.516914, Accuracy 83.714%\n",
      "Epoch 14, Batch 53, LR 2.467061 Loss 6.524117, Accuracy 83.682%\n",
      "Epoch 14, Batch 54, LR 2.467030 Loss 6.507940, Accuracy 83.883%\n",
      "Epoch 14, Batch 55, LR 2.466999 Loss 6.500213, Accuracy 83.949%\n",
      "Epoch 14, Batch 56, LR 2.466969 Loss 6.503109, Accuracy 84.026%\n",
      "Epoch 14, Batch 57, LR 2.466938 Loss 6.500702, Accuracy 83.978%\n",
      "Epoch 14, Batch 58, LR 2.466908 Loss 6.500728, Accuracy 83.957%\n",
      "Epoch 14, Batch 59, LR 2.466877 Loss 6.494038, Accuracy 83.965%\n",
      "Epoch 14, Batch 60, LR 2.466846 Loss 6.495994, Accuracy 83.958%\n",
      "Epoch 14, Batch 61, LR 2.466816 Loss 6.491166, Accuracy 83.927%\n",
      "Epoch 14, Batch 62, LR 2.466785 Loss 6.500924, Accuracy 83.884%\n",
      "Epoch 14, Batch 63, LR 2.466754 Loss 6.503594, Accuracy 83.916%\n",
      "Epoch 14, Batch 64, LR 2.466724 Loss 6.509606, Accuracy 83.899%\n",
      "Epoch 14, Batch 65, LR 2.466693 Loss 6.515352, Accuracy 83.906%\n",
      "Epoch 14, Batch 66, LR 2.466662 Loss 6.512649, Accuracy 83.973%\n",
      "Epoch 14, Batch 67, LR 2.466632 Loss 6.521168, Accuracy 83.885%\n",
      "Epoch 14, Batch 68, LR 2.466601 Loss 6.524291, Accuracy 83.858%\n",
      "Epoch 14, Batch 69, LR 2.466570 Loss 6.521309, Accuracy 83.843%\n",
      "Epoch 14, Batch 70, LR 2.466539 Loss 6.525345, Accuracy 83.862%\n",
      "Epoch 14, Batch 71, LR 2.466508 Loss 6.521498, Accuracy 83.880%\n",
      "Epoch 14, Batch 72, LR 2.466478 Loss 6.516541, Accuracy 83.887%\n",
      "Epoch 14, Batch 73, LR 2.466447 Loss 6.511609, Accuracy 83.947%\n",
      "Epoch 14, Batch 74, LR 2.466416 Loss 6.512377, Accuracy 83.900%\n",
      "Epoch 14, Batch 75, LR 2.466385 Loss 6.507723, Accuracy 83.885%\n",
      "Epoch 14, Batch 76, LR 2.466354 Loss 6.511684, Accuracy 83.820%\n",
      "Epoch 14, Batch 77, LR 2.466323 Loss 6.514134, Accuracy 83.787%\n",
      "Epoch 14, Batch 78, LR 2.466293 Loss 6.502619, Accuracy 83.814%\n",
      "Epoch 14, Batch 79, LR 2.466262 Loss 6.502590, Accuracy 83.792%\n",
      "Epoch 14, Batch 80, LR 2.466231 Loss 6.502042, Accuracy 83.779%\n",
      "Epoch 14, Batch 81, LR 2.466200 Loss 6.500050, Accuracy 83.787%\n",
      "Epoch 14, Batch 82, LR 2.466169 Loss 6.503573, Accuracy 83.746%\n",
      "Epoch 14, Batch 83, LR 2.466138 Loss 6.505301, Accuracy 83.716%\n",
      "Epoch 14, Batch 84, LR 2.466107 Loss 6.498340, Accuracy 83.724%\n",
      "Epoch 14, Batch 85, LR 2.466076 Loss 6.491915, Accuracy 83.759%\n",
      "Epoch 14, Batch 86, LR 2.466045 Loss 6.490995, Accuracy 83.803%\n",
      "Epoch 14, Batch 87, LR 2.466014 Loss 6.486116, Accuracy 83.791%\n",
      "Epoch 14, Batch 88, LR 2.465983 Loss 6.487000, Accuracy 83.798%\n",
      "Epoch 14, Batch 89, LR 2.465952 Loss 6.486009, Accuracy 83.778%\n",
      "Epoch 14, Batch 90, LR 2.465921 Loss 6.486226, Accuracy 83.802%\n",
      "Epoch 14, Batch 91, LR 2.465890 Loss 6.485337, Accuracy 83.868%\n",
      "Epoch 14, Batch 92, LR 2.465859 Loss 6.485792, Accuracy 83.865%\n",
      "Epoch 14, Batch 93, LR 2.465827 Loss 6.488929, Accuracy 83.837%\n",
      "Epoch 14, Batch 94, LR 2.465796 Loss 6.488820, Accuracy 83.851%\n",
      "Epoch 14, Batch 95, LR 2.465765 Loss 6.485666, Accuracy 83.849%\n",
      "Epoch 14, Batch 96, LR 2.465734 Loss 6.483821, Accuracy 83.854%\n",
      "Epoch 14, Batch 97, LR 2.465703 Loss 6.476792, Accuracy 83.916%\n",
      "Epoch 14, Batch 98, LR 2.465672 Loss 6.477573, Accuracy 83.889%\n",
      "Epoch 14, Batch 99, LR 2.465641 Loss 6.480177, Accuracy 83.894%\n",
      "Epoch 14, Batch 100, LR 2.465609 Loss 6.475193, Accuracy 83.945%\n",
      "Epoch 14, Batch 101, LR 2.465578 Loss 6.473574, Accuracy 83.942%\n",
      "Epoch 14, Batch 102, LR 2.465547 Loss 6.478481, Accuracy 83.946%\n",
      "Epoch 14, Batch 103, LR 2.465516 Loss 6.475755, Accuracy 83.996%\n",
      "Epoch 14, Batch 104, LR 2.465484 Loss 6.477247, Accuracy 83.977%\n",
      "Epoch 14, Batch 105, LR 2.465453 Loss 6.473505, Accuracy 84.025%\n",
      "Epoch 14, Batch 106, LR 2.465422 Loss 6.479708, Accuracy 83.984%\n",
      "Epoch 14, Batch 107, LR 2.465391 Loss 6.480824, Accuracy 83.995%\n",
      "Epoch 14, Batch 108, LR 2.465359 Loss 6.483111, Accuracy 83.955%\n",
      "Epoch 14, Batch 109, LR 2.465328 Loss 6.484177, Accuracy 83.981%\n",
      "Epoch 14, Batch 110, LR 2.465297 Loss 6.490002, Accuracy 83.984%\n",
      "Epoch 14, Batch 111, LR 2.465265 Loss 6.491640, Accuracy 83.974%\n",
      "Epoch 14, Batch 112, LR 2.465234 Loss 6.488860, Accuracy 83.984%\n",
      "Epoch 14, Batch 113, LR 2.465203 Loss 6.485373, Accuracy 84.015%\n",
      "Epoch 14, Batch 114, LR 2.465171 Loss 6.486118, Accuracy 84.005%\n",
      "Epoch 14, Batch 115, LR 2.465140 Loss 6.483193, Accuracy 84.069%\n",
      "Epoch 14, Batch 116, LR 2.465108 Loss 6.481750, Accuracy 84.072%\n",
      "Epoch 14, Batch 117, LR 2.465077 Loss 6.475360, Accuracy 84.115%\n",
      "Epoch 14, Batch 118, LR 2.465045 Loss 6.476501, Accuracy 84.084%\n",
      "Epoch 14, Batch 119, LR 2.465014 Loss 6.480968, Accuracy 84.040%\n",
      "Epoch 14, Batch 120, LR 2.464982 Loss 6.479734, Accuracy 84.043%\n",
      "Epoch 14, Batch 121, LR 2.464951 Loss 6.476732, Accuracy 84.059%\n",
      "Epoch 14, Batch 122, LR 2.464920 Loss 6.473079, Accuracy 84.068%\n",
      "Epoch 14, Batch 123, LR 2.464888 Loss 6.474022, Accuracy 84.076%\n",
      "Epoch 14, Batch 124, LR 2.464856 Loss 6.471799, Accuracy 84.060%\n",
      "Epoch 14, Batch 125, LR 2.464825 Loss 6.477507, Accuracy 84.013%\n",
      "Epoch 14, Batch 126, LR 2.464793 Loss 6.479846, Accuracy 84.040%\n",
      "Epoch 14, Batch 127, LR 2.464762 Loss 6.479147, Accuracy 84.031%\n",
      "Epoch 14, Batch 128, LR 2.464730 Loss 6.482070, Accuracy 84.027%\n",
      "Epoch 14, Batch 129, LR 2.464699 Loss 6.483767, Accuracy 84.018%\n",
      "Epoch 14, Batch 130, LR 2.464667 Loss 6.482531, Accuracy 84.014%\n",
      "Epoch 14, Batch 131, LR 2.464635 Loss 6.477301, Accuracy 84.023%\n",
      "Epoch 14, Batch 132, LR 2.464604 Loss 6.474532, Accuracy 84.038%\n",
      "Epoch 14, Batch 133, LR 2.464572 Loss 6.476316, Accuracy 84.005%\n",
      "Epoch 14, Batch 134, LR 2.464540 Loss 6.476059, Accuracy 84.037%\n",
      "Epoch 14, Batch 135, LR 2.464509 Loss 6.477025, Accuracy 84.068%\n",
      "Epoch 14, Batch 136, LR 2.464477 Loss 6.473794, Accuracy 84.088%\n",
      "Epoch 14, Batch 137, LR 2.464445 Loss 6.466989, Accuracy 84.113%\n",
      "Epoch 14, Batch 138, LR 2.464414 Loss 6.469361, Accuracy 84.103%\n",
      "Epoch 14, Batch 139, LR 2.464382 Loss 6.470068, Accuracy 84.105%\n",
      "Epoch 14, Batch 140, LR 2.464350 Loss 6.471855, Accuracy 84.107%\n",
      "Epoch 14, Batch 141, LR 2.464318 Loss 6.474939, Accuracy 84.081%\n",
      "Epoch 14, Batch 142, LR 2.464286 Loss 6.471324, Accuracy 84.072%\n",
      "Epoch 14, Batch 143, LR 2.464255 Loss 6.468595, Accuracy 84.096%\n",
      "Epoch 14, Batch 144, LR 2.464223 Loss 6.472574, Accuracy 84.049%\n",
      "Epoch 14, Batch 145, LR 2.464191 Loss 6.474466, Accuracy 84.030%\n",
      "Epoch 14, Batch 146, LR 2.464159 Loss 6.471340, Accuracy 84.049%\n",
      "Epoch 14, Batch 147, LR 2.464127 Loss 6.468637, Accuracy 84.072%\n",
      "Epoch 14, Batch 148, LR 2.464095 Loss 6.468273, Accuracy 84.048%\n",
      "Epoch 14, Batch 149, LR 2.464064 Loss 6.465300, Accuracy 84.050%\n",
      "Epoch 14, Batch 150, LR 2.464032 Loss 6.461955, Accuracy 84.068%\n",
      "Epoch 14, Batch 151, LR 2.464000 Loss 6.462072, Accuracy 84.090%\n",
      "Epoch 14, Batch 152, LR 2.463968 Loss 6.465686, Accuracy 84.077%\n",
      "Epoch 14, Batch 153, LR 2.463936 Loss 6.466193, Accuracy 84.058%\n",
      "Epoch 14, Batch 154, LR 2.463904 Loss 6.458982, Accuracy 84.091%\n",
      "Epoch 14, Batch 155, LR 2.463872 Loss 6.460205, Accuracy 84.083%\n",
      "Epoch 14, Batch 156, LR 2.463840 Loss 6.462702, Accuracy 84.080%\n",
      "Epoch 14, Batch 157, LR 2.463808 Loss 6.462298, Accuracy 84.086%\n",
      "Epoch 14, Batch 158, LR 2.463776 Loss 6.460222, Accuracy 84.098%\n",
      "Epoch 14, Batch 159, LR 2.463744 Loss 6.459169, Accuracy 84.119%\n",
      "Epoch 14, Batch 160, LR 2.463712 Loss 6.461116, Accuracy 84.116%\n",
      "Epoch 14, Batch 161, LR 2.463680 Loss 6.464564, Accuracy 84.103%\n",
      "Epoch 14, Batch 162, LR 2.463648 Loss 6.466573, Accuracy 84.100%\n",
      "Epoch 14, Batch 163, LR 2.463616 Loss 6.466007, Accuracy 84.107%\n",
      "Epoch 14, Batch 164, LR 2.463584 Loss 6.462978, Accuracy 84.118%\n",
      "Epoch 14, Batch 165, LR 2.463552 Loss 6.459943, Accuracy 84.167%\n",
      "Epoch 14, Batch 166, LR 2.463519 Loss 6.458036, Accuracy 84.182%\n",
      "Epoch 14, Batch 167, LR 2.463487 Loss 6.455186, Accuracy 84.188%\n",
      "Epoch 14, Batch 168, LR 2.463455 Loss 6.460349, Accuracy 84.147%\n",
      "Epoch 14, Batch 169, LR 2.463423 Loss 6.462554, Accuracy 84.125%\n",
      "Epoch 14, Batch 170, LR 2.463391 Loss 6.461629, Accuracy 84.113%\n",
      "Epoch 14, Batch 171, LR 2.463359 Loss 6.462069, Accuracy 84.083%\n",
      "Epoch 14, Batch 172, LR 2.463326 Loss 6.465062, Accuracy 84.075%\n",
      "Epoch 14, Batch 173, LR 2.463294 Loss 6.464009, Accuracy 84.063%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 174, LR 2.463262 Loss 6.463384, Accuracy 84.079%\n",
      "Epoch 14, Batch 175, LR 2.463230 Loss 6.461453, Accuracy 84.062%\n",
      "Epoch 14, Batch 176, LR 2.463198 Loss 6.462855, Accuracy 84.042%\n",
      "Epoch 14, Batch 177, LR 2.463165 Loss 6.460740, Accuracy 84.066%\n",
      "Epoch 14, Batch 178, LR 2.463133 Loss 6.460181, Accuracy 84.068%\n",
      "Epoch 14, Batch 179, LR 2.463101 Loss 6.453932, Accuracy 84.104%\n",
      "Epoch 14, Batch 180, LR 2.463068 Loss 6.450865, Accuracy 84.106%\n",
      "Epoch 14, Batch 181, LR 2.463036 Loss 6.453779, Accuracy 84.086%\n",
      "Epoch 14, Batch 182, LR 2.463004 Loss 6.455879, Accuracy 84.087%\n",
      "Epoch 14, Batch 183, LR 2.462971 Loss 6.455442, Accuracy 84.093%\n",
      "Epoch 14, Batch 184, LR 2.462939 Loss 6.457193, Accuracy 84.078%\n",
      "Epoch 14, Batch 185, LR 2.462907 Loss 6.456747, Accuracy 84.067%\n",
      "Epoch 14, Batch 186, LR 2.462874 Loss 6.458390, Accuracy 84.085%\n",
      "Epoch 14, Batch 187, LR 2.462842 Loss 6.464714, Accuracy 84.070%\n",
      "Epoch 14, Batch 188, LR 2.462809 Loss 6.463240, Accuracy 84.072%\n",
      "Epoch 14, Batch 189, LR 2.462777 Loss 6.467867, Accuracy 84.044%\n",
      "Epoch 14, Batch 190, LR 2.462744 Loss 6.468766, Accuracy 84.046%\n",
      "Epoch 14, Batch 191, LR 2.462712 Loss 6.470194, Accuracy 84.048%\n",
      "Epoch 14, Batch 192, LR 2.462679 Loss 6.471701, Accuracy 84.037%\n",
      "Epoch 14, Batch 193, LR 2.462647 Loss 6.474954, Accuracy 84.023%\n",
      "Epoch 14, Batch 194, LR 2.462614 Loss 6.471020, Accuracy 84.065%\n",
      "Epoch 14, Batch 195, LR 2.462582 Loss 6.468398, Accuracy 84.079%\n",
      "Epoch 14, Batch 196, LR 2.462549 Loss 6.465700, Accuracy 84.096%\n",
      "Epoch 14, Batch 197, LR 2.462517 Loss 6.468833, Accuracy 84.062%\n",
      "Epoch 14, Batch 198, LR 2.462484 Loss 6.467624, Accuracy 84.059%\n",
      "Epoch 14, Batch 199, LR 2.462452 Loss 6.469378, Accuracy 84.053%\n",
      "Epoch 14, Batch 200, LR 2.462419 Loss 6.469217, Accuracy 84.062%\n",
      "Epoch 14, Batch 201, LR 2.462387 Loss 6.471160, Accuracy 84.045%\n",
      "Epoch 14, Batch 202, LR 2.462354 Loss 6.473402, Accuracy 84.031%\n",
      "Epoch 14, Batch 203, LR 2.462321 Loss 6.474105, Accuracy 84.048%\n",
      "Epoch 14, Batch 204, LR 2.462289 Loss 6.476528, Accuracy 84.038%\n",
      "Epoch 14, Batch 205, LR 2.462256 Loss 6.475464, Accuracy 84.059%\n",
      "Epoch 14, Batch 206, LR 2.462223 Loss 6.476031, Accuracy 84.064%\n",
      "Epoch 14, Batch 207, LR 2.462191 Loss 6.474327, Accuracy 84.066%\n",
      "Epoch 14, Batch 208, LR 2.462158 Loss 6.472567, Accuracy 84.071%\n",
      "Epoch 14, Batch 209, LR 2.462125 Loss 6.473900, Accuracy 84.061%\n",
      "Epoch 14, Batch 210, LR 2.462092 Loss 6.472074, Accuracy 84.074%\n",
      "Epoch 14, Batch 211, LR 2.462060 Loss 6.472643, Accuracy 84.064%\n",
      "Epoch 14, Batch 212, LR 2.462027 Loss 6.473279, Accuracy 84.051%\n",
      "Epoch 14, Batch 213, LR 2.461994 Loss 6.476230, Accuracy 84.030%\n",
      "Epoch 14, Batch 214, LR 2.461961 Loss 6.477063, Accuracy 84.032%\n",
      "Epoch 14, Batch 215, LR 2.461929 Loss 6.477730, Accuracy 84.048%\n",
      "Epoch 14, Batch 216, LR 2.461896 Loss 6.476114, Accuracy 84.057%\n",
      "Epoch 14, Batch 217, LR 2.461863 Loss 6.474576, Accuracy 84.051%\n",
      "Epoch 14, Batch 218, LR 2.461830 Loss 6.473033, Accuracy 84.063%\n",
      "Epoch 14, Batch 219, LR 2.461797 Loss 6.469613, Accuracy 84.079%\n",
      "Epoch 14, Batch 220, LR 2.461764 Loss 6.469886, Accuracy 84.087%\n",
      "Epoch 14, Batch 221, LR 2.461731 Loss 6.470877, Accuracy 84.103%\n",
      "Epoch 14, Batch 222, LR 2.461699 Loss 6.468789, Accuracy 84.101%\n",
      "Epoch 14, Batch 223, LR 2.461666 Loss 6.468077, Accuracy 84.102%\n",
      "Epoch 14, Batch 224, LR 2.461633 Loss 6.467458, Accuracy 84.096%\n",
      "Epoch 14, Batch 225, LR 2.461600 Loss 6.465349, Accuracy 84.108%\n",
      "Epoch 14, Batch 226, LR 2.461567 Loss 6.462837, Accuracy 84.133%\n",
      "Epoch 14, Batch 227, LR 2.461534 Loss 6.465851, Accuracy 84.110%\n",
      "Epoch 14, Batch 228, LR 2.461501 Loss 6.473051, Accuracy 84.067%\n",
      "Epoch 14, Batch 229, LR 2.461468 Loss 6.474143, Accuracy 84.061%\n",
      "Epoch 14, Batch 230, LR 2.461435 Loss 6.475392, Accuracy 84.049%\n",
      "Epoch 14, Batch 231, LR 2.461402 Loss 6.477686, Accuracy 84.054%\n",
      "Epoch 14, Batch 232, LR 2.461369 Loss 6.477574, Accuracy 84.045%\n",
      "Epoch 14, Batch 233, LR 2.461336 Loss 6.477133, Accuracy 84.043%\n",
      "Epoch 14, Batch 234, LR 2.461303 Loss 6.479751, Accuracy 84.041%\n",
      "Epoch 14, Batch 235, LR 2.461270 Loss 6.476038, Accuracy 84.066%\n",
      "Epoch 14, Batch 236, LR 2.461237 Loss 6.475675, Accuracy 84.067%\n",
      "Epoch 14, Batch 237, LR 2.461203 Loss 6.474992, Accuracy 84.068%\n",
      "Epoch 14, Batch 238, LR 2.461170 Loss 6.475116, Accuracy 84.063%\n",
      "Epoch 14, Batch 239, LR 2.461137 Loss 6.472866, Accuracy 84.074%\n",
      "Epoch 14, Batch 240, LR 2.461104 Loss 6.471174, Accuracy 84.092%\n",
      "Epoch 14, Batch 241, LR 2.461071 Loss 6.470999, Accuracy 84.086%\n",
      "Epoch 14, Batch 242, LR 2.461038 Loss 6.472064, Accuracy 84.088%\n",
      "Epoch 14, Batch 243, LR 2.461005 Loss 6.473506, Accuracy 84.079%\n",
      "Epoch 14, Batch 244, LR 2.460971 Loss 6.474014, Accuracy 84.077%\n",
      "Epoch 14, Batch 245, LR 2.460938 Loss 6.474264, Accuracy 84.085%\n",
      "Epoch 14, Batch 246, LR 2.460905 Loss 6.475007, Accuracy 84.083%\n",
      "Epoch 14, Batch 247, LR 2.460872 Loss 6.473081, Accuracy 84.081%\n",
      "Epoch 14, Batch 248, LR 2.460838 Loss 6.474062, Accuracy 84.063%\n",
      "Epoch 14, Batch 249, LR 2.460805 Loss 6.474562, Accuracy 84.058%\n",
      "Epoch 14, Batch 250, LR 2.460772 Loss 6.474630, Accuracy 84.041%\n",
      "Epoch 14, Batch 251, LR 2.460738 Loss 6.473937, Accuracy 84.042%\n",
      "Epoch 14, Batch 252, LR 2.460705 Loss 6.475227, Accuracy 84.046%\n",
      "Epoch 14, Batch 253, LR 2.460672 Loss 6.474500, Accuracy 84.051%\n",
      "Epoch 14, Batch 254, LR 2.460638 Loss 6.476465, Accuracy 84.031%\n",
      "Epoch 14, Batch 255, LR 2.460605 Loss 6.476013, Accuracy 84.035%\n",
      "Epoch 14, Batch 256, LR 2.460572 Loss 6.478026, Accuracy 84.024%\n",
      "Epoch 14, Batch 257, LR 2.460538 Loss 6.478367, Accuracy 84.022%\n",
      "Epoch 14, Batch 258, LR 2.460505 Loss 6.477678, Accuracy 84.030%\n",
      "Epoch 14, Batch 259, LR 2.460472 Loss 6.477881, Accuracy 84.016%\n",
      "Epoch 14, Batch 260, LR 2.460438 Loss 6.477873, Accuracy 84.017%\n",
      "Epoch 14, Batch 261, LR 2.460405 Loss 6.478610, Accuracy 84.016%\n",
      "Epoch 14, Batch 262, LR 2.460371 Loss 6.477715, Accuracy 84.035%\n",
      "Epoch 14, Batch 263, LR 2.460338 Loss 6.476457, Accuracy 84.048%\n",
      "Epoch 14, Batch 264, LR 2.460304 Loss 6.478868, Accuracy 84.020%\n",
      "Epoch 14, Batch 265, LR 2.460271 Loss 6.479935, Accuracy 84.012%\n",
      "Epoch 14, Batch 266, LR 2.460237 Loss 6.482565, Accuracy 84.011%\n",
      "Epoch 14, Batch 267, LR 2.460204 Loss 6.482740, Accuracy 84.006%\n",
      "Epoch 14, Batch 268, LR 2.460170 Loss 6.481591, Accuracy 84.011%\n",
      "Epoch 14, Batch 269, LR 2.460137 Loss 6.482579, Accuracy 84.006%\n",
      "Epoch 14, Batch 270, LR 2.460103 Loss 6.480513, Accuracy 84.028%\n",
      "Epoch 14, Batch 271, LR 2.460070 Loss 6.481105, Accuracy 84.035%\n",
      "Epoch 14, Batch 272, LR 2.460036 Loss 6.481363, Accuracy 84.045%\n",
      "Epoch 14, Batch 273, LR 2.460002 Loss 6.481253, Accuracy 84.052%\n",
      "Epoch 14, Batch 274, LR 2.459969 Loss 6.484628, Accuracy 84.021%\n",
      "Epoch 14, Batch 275, LR 2.459935 Loss 6.485985, Accuracy 84.023%\n",
      "Epoch 14, Batch 276, LR 2.459901 Loss 6.487498, Accuracy 84.021%\n",
      "Epoch 14, Batch 277, LR 2.459868 Loss 6.489458, Accuracy 83.991%\n",
      "Epoch 14, Batch 278, LR 2.459834 Loss 6.488950, Accuracy 83.987%\n",
      "Epoch 14, Batch 279, LR 2.459800 Loss 6.488060, Accuracy 83.991%\n",
      "Epoch 14, Batch 280, LR 2.459767 Loss 6.486455, Accuracy 84.004%\n",
      "Epoch 14, Batch 281, LR 2.459733 Loss 6.486649, Accuracy 84.002%\n",
      "Epoch 14, Batch 282, LR 2.459699 Loss 6.488037, Accuracy 83.993%\n",
      "Epoch 14, Batch 283, LR 2.459665 Loss 6.487513, Accuracy 83.986%\n",
      "Epoch 14, Batch 284, LR 2.459632 Loss 6.486408, Accuracy 83.995%\n",
      "Epoch 14, Batch 285, LR 2.459598 Loss 6.487016, Accuracy 83.988%\n",
      "Epoch 14, Batch 286, LR 2.459564 Loss 6.486791, Accuracy 83.990%\n",
      "Epoch 14, Batch 287, LR 2.459530 Loss 6.487105, Accuracy 83.986%\n",
      "Epoch 14, Batch 288, LR 2.459497 Loss 6.487444, Accuracy 83.971%\n",
      "Epoch 14, Batch 289, LR 2.459463 Loss 6.486482, Accuracy 83.959%\n",
      "Epoch 14, Batch 290, LR 2.459429 Loss 6.485954, Accuracy 83.952%\n",
      "Epoch 14, Batch 291, LR 2.459395 Loss 6.487651, Accuracy 83.935%\n",
      "Epoch 14, Batch 292, LR 2.459361 Loss 6.487002, Accuracy 83.931%\n",
      "Epoch 14, Batch 293, LR 2.459327 Loss 6.487433, Accuracy 83.924%\n",
      "Epoch 14, Batch 294, LR 2.459293 Loss 6.486171, Accuracy 83.931%\n",
      "Epoch 14, Batch 295, LR 2.459259 Loss 6.484708, Accuracy 83.927%\n",
      "Epoch 14, Batch 296, LR 2.459226 Loss 6.486838, Accuracy 83.905%\n",
      "Epoch 14, Batch 297, LR 2.459192 Loss 6.487439, Accuracy 83.902%\n",
      "Epoch 14, Batch 298, LR 2.459158 Loss 6.487919, Accuracy 83.895%\n",
      "Epoch 14, Batch 299, LR 2.459124 Loss 6.489296, Accuracy 83.879%\n",
      "Epoch 14, Batch 300, LR 2.459090 Loss 6.486627, Accuracy 83.891%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 301, LR 2.459056 Loss 6.489078, Accuracy 83.890%\n",
      "Epoch 14, Batch 302, LR 2.459022 Loss 6.489377, Accuracy 83.883%\n",
      "Epoch 14, Batch 303, LR 2.458988 Loss 6.490105, Accuracy 83.867%\n",
      "Epoch 14, Batch 304, LR 2.458954 Loss 6.490676, Accuracy 83.858%\n",
      "Epoch 14, Batch 305, LR 2.458920 Loss 6.489872, Accuracy 83.860%\n",
      "Epoch 14, Batch 306, LR 2.458885 Loss 6.490075, Accuracy 83.862%\n",
      "Epoch 14, Batch 307, LR 2.458851 Loss 6.490712, Accuracy 83.856%\n",
      "Epoch 14, Batch 308, LR 2.458817 Loss 6.490609, Accuracy 83.855%\n",
      "Epoch 14, Batch 309, LR 2.458783 Loss 6.490758, Accuracy 83.854%\n",
      "Epoch 14, Batch 310, LR 2.458749 Loss 6.490787, Accuracy 83.848%\n",
      "Epoch 14, Batch 311, LR 2.458715 Loss 6.491460, Accuracy 83.855%\n",
      "Epoch 14, Batch 312, LR 2.458681 Loss 6.491787, Accuracy 83.849%\n",
      "Epoch 14, Batch 313, LR 2.458647 Loss 6.494228, Accuracy 83.841%\n",
      "Epoch 14, Batch 314, LR 2.458612 Loss 6.493588, Accuracy 83.848%\n",
      "Epoch 14, Batch 315, LR 2.458578 Loss 6.493590, Accuracy 83.844%\n",
      "Epoch 14, Batch 316, LR 2.458544 Loss 6.495788, Accuracy 83.826%\n",
      "Epoch 14, Batch 317, LR 2.458510 Loss 6.497037, Accuracy 83.820%\n",
      "Epoch 14, Batch 318, LR 2.458476 Loss 6.496873, Accuracy 83.810%\n",
      "Epoch 14, Batch 319, LR 2.458441 Loss 6.498479, Accuracy 83.809%\n",
      "Epoch 14, Batch 320, LR 2.458407 Loss 6.499470, Accuracy 83.806%\n",
      "Epoch 14, Batch 321, LR 2.458373 Loss 6.498032, Accuracy 83.810%\n",
      "Epoch 14, Batch 322, LR 2.458339 Loss 6.497031, Accuracy 83.819%\n",
      "Epoch 14, Batch 323, LR 2.458304 Loss 6.496212, Accuracy 83.836%\n",
      "Epoch 14, Batch 324, LR 2.458270 Loss 6.499441, Accuracy 83.811%\n",
      "Epoch 14, Batch 325, LR 2.458236 Loss 6.498224, Accuracy 83.817%\n",
      "Epoch 14, Batch 326, LR 2.458201 Loss 6.499471, Accuracy 83.805%\n",
      "Epoch 14, Batch 327, LR 2.458167 Loss 6.500245, Accuracy 83.797%\n",
      "Epoch 14, Batch 328, LR 2.458133 Loss 6.500265, Accuracy 83.806%\n",
      "Epoch 14, Batch 329, LR 2.458098 Loss 6.499560, Accuracy 83.807%\n",
      "Epoch 14, Batch 330, LR 2.458064 Loss 6.500213, Accuracy 83.814%\n",
      "Epoch 14, Batch 331, LR 2.458029 Loss 6.500472, Accuracy 83.813%\n",
      "Epoch 14, Batch 332, LR 2.457995 Loss 6.501516, Accuracy 83.820%\n",
      "Epoch 14, Batch 333, LR 2.457960 Loss 6.503760, Accuracy 83.821%\n",
      "Epoch 14, Batch 334, LR 2.457926 Loss 6.503748, Accuracy 83.828%\n",
      "Epoch 14, Batch 335, LR 2.457892 Loss 6.502812, Accuracy 83.839%\n",
      "Epoch 14, Batch 336, LR 2.457857 Loss 6.503184, Accuracy 83.838%\n",
      "Epoch 14, Batch 337, LR 2.457823 Loss 6.501639, Accuracy 83.846%\n",
      "Epoch 14, Batch 338, LR 2.457788 Loss 6.500250, Accuracy 83.853%\n",
      "Epoch 14, Batch 339, LR 2.457753 Loss 6.501205, Accuracy 83.840%\n",
      "Epoch 14, Batch 340, LR 2.457719 Loss 6.500632, Accuracy 83.833%\n",
      "Epoch 14, Batch 341, LR 2.457684 Loss 6.502963, Accuracy 83.811%\n",
      "Epoch 14, Batch 342, LR 2.457650 Loss 6.504045, Accuracy 83.799%\n",
      "Epoch 14, Batch 343, LR 2.457615 Loss 6.503187, Accuracy 83.801%\n",
      "Epoch 14, Batch 344, LR 2.457581 Loss 6.502549, Accuracy 83.812%\n",
      "Epoch 14, Batch 345, LR 2.457546 Loss 6.503455, Accuracy 83.811%\n",
      "Epoch 14, Batch 346, LR 2.457511 Loss 6.502470, Accuracy 83.820%\n",
      "Epoch 14, Batch 347, LR 2.457477 Loss 6.504574, Accuracy 83.801%\n",
      "Epoch 14, Batch 348, LR 2.457442 Loss 6.504388, Accuracy 83.812%\n",
      "Epoch 14, Batch 349, LR 2.457407 Loss 6.504200, Accuracy 83.818%\n",
      "Epoch 14, Batch 350, LR 2.457373 Loss 6.504404, Accuracy 83.817%\n",
      "Epoch 14, Batch 351, LR 2.457338 Loss 6.505383, Accuracy 83.805%\n",
      "Epoch 14, Batch 352, LR 2.457303 Loss 6.505850, Accuracy 83.800%\n",
      "Epoch 14, Batch 353, LR 2.457269 Loss 6.503645, Accuracy 83.806%\n",
      "Epoch 14, Batch 354, LR 2.457234 Loss 6.502799, Accuracy 83.812%\n",
      "Epoch 14, Batch 355, LR 2.457199 Loss 6.503633, Accuracy 83.809%\n",
      "Epoch 14, Batch 356, LR 2.457164 Loss 6.502163, Accuracy 83.815%\n",
      "Epoch 14, Batch 357, LR 2.457130 Loss 6.504397, Accuracy 83.806%\n",
      "Epoch 14, Batch 358, LR 2.457095 Loss 6.503988, Accuracy 83.805%\n",
      "Epoch 14, Batch 359, LR 2.457060 Loss 6.504213, Accuracy 83.809%\n",
      "Epoch 14, Batch 360, LR 2.457025 Loss 6.504295, Accuracy 83.806%\n",
      "Epoch 14, Batch 361, LR 2.456990 Loss 6.504541, Accuracy 83.804%\n",
      "Epoch 14, Batch 362, LR 2.456956 Loss 6.503354, Accuracy 83.810%\n",
      "Epoch 14, Batch 363, LR 2.456921 Loss 6.504511, Accuracy 83.818%\n",
      "Epoch 14, Batch 364, LR 2.456886 Loss 6.504732, Accuracy 83.819%\n",
      "Epoch 14, Batch 365, LR 2.456851 Loss 6.505457, Accuracy 83.818%\n",
      "Epoch 14, Batch 366, LR 2.456816 Loss 6.507766, Accuracy 83.797%\n",
      "Epoch 14, Batch 367, LR 2.456781 Loss 6.506395, Accuracy 83.807%\n",
      "Epoch 14, Batch 368, LR 2.456746 Loss 6.506251, Accuracy 83.806%\n",
      "Epoch 14, Batch 369, LR 2.456711 Loss 6.504815, Accuracy 83.816%\n",
      "Epoch 14, Batch 370, LR 2.456676 Loss 6.504510, Accuracy 83.813%\n",
      "Epoch 14, Batch 371, LR 2.456641 Loss 6.505023, Accuracy 83.817%\n",
      "Epoch 14, Batch 372, LR 2.456606 Loss 6.503664, Accuracy 83.827%\n",
      "Epoch 14, Batch 373, LR 2.456571 Loss 6.503499, Accuracy 83.820%\n",
      "Epoch 14, Batch 374, LR 2.456536 Loss 6.503531, Accuracy 83.817%\n",
      "Epoch 14, Batch 375, LR 2.456501 Loss 6.503322, Accuracy 83.831%\n",
      "Epoch 14, Batch 376, LR 2.456466 Loss 6.504614, Accuracy 83.820%\n",
      "Epoch 14, Batch 377, LR 2.456431 Loss 6.505684, Accuracy 83.815%\n",
      "Epoch 14, Batch 378, LR 2.456396 Loss 6.505983, Accuracy 83.811%\n",
      "Epoch 14, Batch 379, LR 2.456361 Loss 6.505414, Accuracy 83.802%\n",
      "Epoch 14, Batch 380, LR 2.456326 Loss 6.507739, Accuracy 83.791%\n",
      "Epoch 14, Batch 381, LR 2.456291 Loss 6.507269, Accuracy 83.793%\n",
      "Epoch 14, Batch 382, LR 2.456256 Loss 6.509139, Accuracy 83.784%\n",
      "Epoch 14, Batch 383, LR 2.456221 Loss 6.509190, Accuracy 83.777%\n",
      "Epoch 14, Batch 384, LR 2.456186 Loss 6.509937, Accuracy 83.771%\n",
      "Epoch 14, Batch 385, LR 2.456150 Loss 6.508769, Accuracy 83.784%\n",
      "Epoch 14, Batch 386, LR 2.456115 Loss 6.507863, Accuracy 83.794%\n",
      "Epoch 14, Batch 387, LR 2.456080 Loss 6.506499, Accuracy 83.808%\n",
      "Epoch 14, Batch 388, LR 2.456045 Loss 6.503657, Accuracy 83.827%\n",
      "Epoch 14, Batch 389, LR 2.456010 Loss 6.503606, Accuracy 83.829%\n",
      "Epoch 14, Batch 390, LR 2.455974 Loss 6.503063, Accuracy 83.838%\n",
      "Epoch 14, Batch 391, LR 2.455939 Loss 6.502284, Accuracy 83.846%\n",
      "Epoch 14, Batch 392, LR 2.455904 Loss 6.502811, Accuracy 83.841%\n",
      "Epoch 14, Batch 393, LR 2.455869 Loss 6.501686, Accuracy 83.850%\n",
      "Epoch 14, Batch 394, LR 2.455833 Loss 6.503534, Accuracy 83.834%\n",
      "Epoch 14, Batch 395, LR 2.455798 Loss 6.503422, Accuracy 83.829%\n",
      "Epoch 14, Batch 396, LR 2.455763 Loss 6.504913, Accuracy 83.827%\n",
      "Epoch 14, Batch 397, LR 2.455727 Loss 6.504203, Accuracy 83.832%\n",
      "Epoch 14, Batch 398, LR 2.455692 Loss 6.503824, Accuracy 83.833%\n",
      "Epoch 14, Batch 399, LR 2.455657 Loss 6.502246, Accuracy 83.839%\n",
      "Epoch 14, Batch 400, LR 2.455621 Loss 6.501783, Accuracy 83.840%\n",
      "Epoch 14, Batch 401, LR 2.455586 Loss 6.501889, Accuracy 83.839%\n",
      "Epoch 14, Batch 402, LR 2.455551 Loss 6.502775, Accuracy 83.835%\n",
      "Epoch 14, Batch 403, LR 2.455515 Loss 6.505675, Accuracy 83.828%\n",
      "Epoch 14, Batch 404, LR 2.455480 Loss 6.504709, Accuracy 83.832%\n",
      "Epoch 14, Batch 405, LR 2.455444 Loss 6.503542, Accuracy 83.837%\n",
      "Epoch 14, Batch 406, LR 2.455409 Loss 6.504043, Accuracy 83.836%\n",
      "Epoch 14, Batch 407, LR 2.455373 Loss 6.502181, Accuracy 83.836%\n",
      "Epoch 14, Batch 408, LR 2.455338 Loss 6.501862, Accuracy 83.845%\n",
      "Epoch 14, Batch 409, LR 2.455302 Loss 6.503251, Accuracy 83.844%\n",
      "Epoch 14, Batch 410, LR 2.455267 Loss 6.502691, Accuracy 83.845%\n",
      "Epoch 14, Batch 411, LR 2.455231 Loss 6.503525, Accuracy 83.837%\n",
      "Epoch 14, Batch 412, LR 2.455196 Loss 6.503709, Accuracy 83.829%\n",
      "Epoch 14, Batch 413, LR 2.455160 Loss 6.504859, Accuracy 83.825%\n",
      "Epoch 14, Batch 414, LR 2.455125 Loss 6.504639, Accuracy 83.826%\n",
      "Epoch 14, Batch 415, LR 2.455089 Loss 6.505829, Accuracy 83.808%\n",
      "Epoch 14, Batch 416, LR 2.455054 Loss 6.504993, Accuracy 83.819%\n",
      "Epoch 14, Batch 417, LR 2.455018 Loss 6.504024, Accuracy 83.820%\n",
      "Epoch 14, Batch 418, LR 2.454982 Loss 6.503006, Accuracy 83.829%\n",
      "Epoch 14, Batch 419, LR 2.454947 Loss 6.500931, Accuracy 83.840%\n",
      "Epoch 14, Batch 420, LR 2.454911 Loss 6.500604, Accuracy 83.847%\n",
      "Epoch 14, Batch 421, LR 2.454875 Loss 6.501974, Accuracy 83.844%\n",
      "Epoch 14, Batch 422, LR 2.454840 Loss 6.502199, Accuracy 83.846%\n",
      "Epoch 14, Batch 423, LR 2.454804 Loss 6.503898, Accuracy 83.841%\n",
      "Epoch 14, Batch 424, LR 2.454768 Loss 6.505864, Accuracy 83.833%\n",
      "Epoch 14, Batch 425, LR 2.454733 Loss 6.508306, Accuracy 83.816%\n",
      "Epoch 14, Batch 426, LR 2.454697 Loss 6.508503, Accuracy 83.808%\n",
      "Epoch 14, Batch 427, LR 2.454661 Loss 6.508272, Accuracy 83.808%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 428, LR 2.454625 Loss 6.508321, Accuracy 83.807%\n",
      "Epoch 14, Batch 429, LR 2.454590 Loss 6.506838, Accuracy 83.814%\n",
      "Epoch 14, Batch 430, LR 2.454554 Loss 6.508757, Accuracy 83.797%\n",
      "Epoch 14, Batch 431, LR 2.454518 Loss 6.508337, Accuracy 83.797%\n",
      "Epoch 14, Batch 432, LR 2.454482 Loss 6.508215, Accuracy 83.802%\n",
      "Epoch 14, Batch 433, LR 2.454446 Loss 6.508178, Accuracy 83.805%\n",
      "Epoch 14, Batch 434, LR 2.454411 Loss 6.507293, Accuracy 83.808%\n",
      "Epoch 14, Batch 435, LR 2.454375 Loss 6.506185, Accuracy 83.811%\n",
      "Epoch 14, Batch 436, LR 2.454339 Loss 6.506457, Accuracy 83.821%\n",
      "Epoch 14, Batch 437, LR 2.454303 Loss 6.507075, Accuracy 83.821%\n",
      "Epoch 14, Batch 438, LR 2.454267 Loss 6.507527, Accuracy 83.829%\n",
      "Epoch 14, Batch 439, LR 2.454231 Loss 6.506982, Accuracy 83.839%\n",
      "Epoch 14, Batch 440, LR 2.454195 Loss 6.506575, Accuracy 83.849%\n",
      "Epoch 14, Batch 441, LR 2.454159 Loss 6.506986, Accuracy 83.844%\n",
      "Epoch 14, Batch 442, LR 2.454123 Loss 6.506538, Accuracy 83.850%\n",
      "Epoch 14, Batch 443, LR 2.454087 Loss 6.506198, Accuracy 83.855%\n",
      "Epoch 14, Batch 444, LR 2.454051 Loss 6.505326, Accuracy 83.856%\n",
      "Epoch 14, Batch 445, LR 2.454015 Loss 6.504755, Accuracy 83.864%\n",
      "Epoch 14, Batch 446, LR 2.453979 Loss 6.502966, Accuracy 83.864%\n",
      "Epoch 14, Batch 447, LR 2.453943 Loss 6.504622, Accuracy 83.844%\n",
      "Epoch 14, Batch 448, LR 2.453907 Loss 6.505945, Accuracy 83.841%\n",
      "Epoch 14, Batch 449, LR 2.453871 Loss 6.507083, Accuracy 83.843%\n",
      "Epoch 14, Batch 450, LR 2.453835 Loss 6.505499, Accuracy 83.844%\n",
      "Epoch 14, Batch 451, LR 2.453799 Loss 6.504357, Accuracy 83.848%\n",
      "Epoch 14, Batch 452, LR 2.453763 Loss 6.504536, Accuracy 83.855%\n",
      "Epoch 14, Batch 453, LR 2.453727 Loss 6.505624, Accuracy 83.849%\n",
      "Epoch 14, Batch 454, LR 2.453691 Loss 6.506526, Accuracy 83.847%\n",
      "Epoch 14, Batch 455, LR 2.453655 Loss 6.507456, Accuracy 83.838%\n",
      "Epoch 14, Batch 456, LR 2.453619 Loss 6.508381, Accuracy 83.832%\n",
      "Epoch 14, Batch 457, LR 2.453582 Loss 6.508022, Accuracy 83.840%\n",
      "Epoch 14, Batch 458, LR 2.453546 Loss 6.506960, Accuracy 83.848%\n",
      "Epoch 14, Batch 459, LR 2.453510 Loss 6.507503, Accuracy 83.846%\n",
      "Epoch 14, Batch 460, LR 2.453474 Loss 6.506183, Accuracy 83.857%\n",
      "Epoch 14, Batch 461, LR 2.453438 Loss 6.507309, Accuracy 83.841%\n",
      "Epoch 14, Batch 462, LR 2.453401 Loss 6.506739, Accuracy 83.851%\n",
      "Epoch 14, Batch 463, LR 2.453365 Loss 6.505763, Accuracy 83.857%\n",
      "Epoch 14, Batch 464, LR 2.453329 Loss 6.505917, Accuracy 83.856%\n",
      "Epoch 14, Batch 465, LR 2.453293 Loss 6.504795, Accuracy 83.856%\n",
      "Epoch 14, Batch 466, LR 2.453256 Loss 6.505437, Accuracy 83.844%\n",
      "Epoch 14, Batch 467, LR 2.453220 Loss 6.504876, Accuracy 83.851%\n",
      "Epoch 14, Batch 468, LR 2.453184 Loss 6.503979, Accuracy 83.863%\n",
      "Epoch 14, Batch 469, LR 2.453147 Loss 6.505868, Accuracy 83.850%\n",
      "Epoch 14, Batch 470, LR 2.453111 Loss 6.506044, Accuracy 83.848%\n",
      "Epoch 14, Batch 471, LR 2.453075 Loss 6.506574, Accuracy 83.853%\n",
      "Epoch 14, Batch 472, LR 2.453038 Loss 6.507466, Accuracy 83.852%\n",
      "Epoch 14, Batch 473, LR 2.453002 Loss 6.507121, Accuracy 83.856%\n",
      "Epoch 14, Batch 474, LR 2.452966 Loss 6.507038, Accuracy 83.862%\n",
      "Epoch 14, Batch 475, LR 2.452929 Loss 6.507119, Accuracy 83.862%\n",
      "Epoch 14, Batch 476, LR 2.452893 Loss 6.506369, Accuracy 83.865%\n",
      "Epoch 14, Batch 477, LR 2.452856 Loss 6.505993, Accuracy 83.864%\n",
      "Epoch 14, Batch 478, LR 2.452820 Loss 6.506654, Accuracy 83.852%\n",
      "Epoch 14, Batch 479, LR 2.452783 Loss 6.507449, Accuracy 83.853%\n",
      "Epoch 14, Batch 480, LR 2.452747 Loss 6.506737, Accuracy 83.861%\n",
      "Epoch 14, Batch 481, LR 2.452710 Loss 6.508496, Accuracy 83.852%\n",
      "Epoch 14, Batch 482, LR 2.452674 Loss 6.506756, Accuracy 83.866%\n",
      "Epoch 14, Batch 483, LR 2.452637 Loss 6.506719, Accuracy 83.862%\n",
      "Epoch 14, Batch 484, LR 2.452601 Loss 6.505786, Accuracy 83.867%\n",
      "Epoch 14, Batch 485, LR 2.452564 Loss 6.505392, Accuracy 83.871%\n",
      "Epoch 14, Batch 486, LR 2.452528 Loss 6.504020, Accuracy 83.885%\n",
      "Epoch 14, Batch 487, LR 2.452491 Loss 6.502476, Accuracy 83.894%\n",
      "Epoch 14, Batch 488, LR 2.452455 Loss 6.501942, Accuracy 83.890%\n",
      "Epoch 14, Batch 489, LR 2.452418 Loss 6.501763, Accuracy 83.891%\n",
      "Epoch 14, Batch 490, LR 2.452381 Loss 6.502262, Accuracy 83.890%\n",
      "Epoch 14, Batch 491, LR 2.452345 Loss 6.502011, Accuracy 83.883%\n",
      "Epoch 14, Batch 492, LR 2.452308 Loss 6.501980, Accuracy 83.883%\n",
      "Epoch 14, Batch 493, LR 2.452272 Loss 6.502053, Accuracy 83.877%\n",
      "Epoch 14, Batch 494, LR 2.452235 Loss 6.503178, Accuracy 83.874%\n",
      "Epoch 14, Batch 495, LR 2.452198 Loss 6.503523, Accuracy 83.868%\n",
      "Epoch 14, Batch 496, LR 2.452161 Loss 6.503319, Accuracy 83.873%\n",
      "Epoch 14, Batch 497, LR 2.452125 Loss 6.502769, Accuracy 83.878%\n",
      "Epoch 14, Batch 498, LR 2.452088 Loss 6.503312, Accuracy 83.876%\n",
      "Epoch 14, Batch 499, LR 2.452051 Loss 6.504240, Accuracy 83.877%\n",
      "Epoch 14, Batch 500, LR 2.452015 Loss 6.504471, Accuracy 83.880%\n",
      "Epoch 14, Batch 501, LR 2.451978 Loss 6.503977, Accuracy 83.874%\n",
      "Epoch 14, Batch 502, LR 2.451941 Loss 6.504815, Accuracy 83.875%\n",
      "Epoch 14, Batch 503, LR 2.451904 Loss 6.503593, Accuracy 83.883%\n",
      "Epoch 14, Batch 504, LR 2.451867 Loss 6.504637, Accuracy 83.879%\n",
      "Epoch 14, Batch 505, LR 2.451831 Loss 6.505035, Accuracy 83.878%\n",
      "Epoch 14, Batch 506, LR 2.451794 Loss 6.504628, Accuracy 83.887%\n",
      "Epoch 14, Batch 507, LR 2.451757 Loss 6.505478, Accuracy 83.880%\n",
      "Epoch 14, Batch 508, LR 2.451720 Loss 6.506002, Accuracy 83.881%\n",
      "Epoch 14, Batch 509, LR 2.451683 Loss 6.505900, Accuracy 83.882%\n",
      "Epoch 14, Batch 510, LR 2.451646 Loss 6.506867, Accuracy 83.879%\n",
      "Epoch 14, Batch 511, LR 2.451609 Loss 6.506984, Accuracy 83.878%\n",
      "Epoch 14, Batch 512, LR 2.451572 Loss 6.507302, Accuracy 83.871%\n",
      "Epoch 14, Batch 513, LR 2.451536 Loss 6.507565, Accuracy 83.869%\n",
      "Epoch 14, Batch 514, LR 2.451499 Loss 6.507004, Accuracy 83.866%\n",
      "Epoch 14, Batch 515, LR 2.451462 Loss 6.508905, Accuracy 83.847%\n",
      "Epoch 14, Batch 516, LR 2.451425 Loss 6.508382, Accuracy 83.850%\n",
      "Epoch 14, Batch 517, LR 2.451388 Loss 6.508316, Accuracy 83.857%\n",
      "Epoch 14, Batch 518, LR 2.451351 Loss 6.507065, Accuracy 83.862%\n",
      "Epoch 14, Batch 519, LR 2.451314 Loss 6.507510, Accuracy 83.856%\n",
      "Epoch 14, Batch 520, LR 2.451277 Loss 6.505714, Accuracy 83.875%\n",
      "Epoch 14, Batch 521, LR 2.451240 Loss 6.504117, Accuracy 83.889%\n",
      "Epoch 14, Batch 522, LR 2.451203 Loss 6.504688, Accuracy 83.880%\n",
      "Epoch 14, Batch 523, LR 2.451165 Loss 6.504120, Accuracy 83.887%\n",
      "Epoch 14, Batch 524, LR 2.451128 Loss 6.503318, Accuracy 83.892%\n",
      "Epoch 14, Batch 525, LR 2.451091 Loss 6.503020, Accuracy 83.890%\n",
      "Epoch 14, Batch 526, LR 2.451054 Loss 6.503671, Accuracy 83.880%\n",
      "Epoch 14, Batch 527, LR 2.451017 Loss 6.504876, Accuracy 83.871%\n",
      "Epoch 14, Batch 528, LR 2.450980 Loss 6.506133, Accuracy 83.866%\n",
      "Epoch 14, Batch 529, LR 2.450943 Loss 6.507104, Accuracy 83.860%\n",
      "Epoch 14, Batch 530, LR 2.450906 Loss 6.506656, Accuracy 83.858%\n",
      "Epoch 14, Batch 531, LR 2.450868 Loss 6.506935, Accuracy 83.857%\n",
      "Epoch 14, Batch 532, LR 2.450831 Loss 6.507472, Accuracy 83.857%\n",
      "Epoch 14, Batch 533, LR 2.450794 Loss 6.507937, Accuracy 83.861%\n",
      "Epoch 14, Batch 534, LR 2.450757 Loss 6.507041, Accuracy 83.864%\n",
      "Epoch 14, Batch 535, LR 2.450720 Loss 6.507133, Accuracy 83.865%\n",
      "Epoch 14, Batch 536, LR 2.450682 Loss 6.508999, Accuracy 83.853%\n",
      "Epoch 14, Batch 537, LR 2.450645 Loss 6.509670, Accuracy 83.854%\n",
      "Epoch 14, Batch 538, LR 2.450608 Loss 6.510493, Accuracy 83.851%\n",
      "Epoch 14, Batch 539, LR 2.450570 Loss 6.509986, Accuracy 83.850%\n",
      "Epoch 14, Batch 540, LR 2.450533 Loss 6.508817, Accuracy 83.860%\n",
      "Epoch 14, Batch 541, LR 2.450496 Loss 6.508410, Accuracy 83.864%\n",
      "Epoch 14, Batch 542, LR 2.450459 Loss 6.507902, Accuracy 83.868%\n",
      "Epoch 14, Batch 543, LR 2.450421 Loss 6.508407, Accuracy 83.867%\n",
      "Epoch 14, Batch 544, LR 2.450384 Loss 6.507380, Accuracy 83.868%\n",
      "Epoch 14, Batch 545, LR 2.450346 Loss 6.506471, Accuracy 83.875%\n",
      "Epoch 14, Batch 546, LR 2.450309 Loss 6.506914, Accuracy 83.873%\n",
      "Epoch 14, Batch 547, LR 2.450272 Loss 6.505712, Accuracy 83.881%\n",
      "Epoch 14, Batch 548, LR 2.450234 Loss 6.504486, Accuracy 83.896%\n",
      "Epoch 14, Batch 549, LR 2.450197 Loss 6.504391, Accuracy 83.901%\n",
      "Epoch 14, Batch 550, LR 2.450159 Loss 6.503547, Accuracy 83.912%\n",
      "Epoch 14, Batch 551, LR 2.450122 Loss 6.502711, Accuracy 83.914%\n",
      "Epoch 14, Batch 552, LR 2.450084 Loss 6.504008, Accuracy 83.908%\n",
      "Epoch 14, Batch 553, LR 2.450047 Loss 6.502762, Accuracy 83.919%\n",
      "Epoch 14, Batch 554, LR 2.450009 Loss 6.502078, Accuracy 83.922%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 555, LR 2.449972 Loss 6.502524, Accuracy 83.922%\n",
      "Epoch 14, Batch 556, LR 2.449934 Loss 6.501855, Accuracy 83.924%\n",
      "Epoch 14, Batch 557, LR 2.449897 Loss 6.501922, Accuracy 83.932%\n",
      "Epoch 14, Batch 558, LR 2.449859 Loss 6.503312, Accuracy 83.927%\n",
      "Epoch 14, Batch 559, LR 2.449822 Loss 6.503808, Accuracy 83.922%\n",
      "Epoch 14, Batch 560, LR 2.449784 Loss 6.504036, Accuracy 83.926%\n",
      "Epoch 14, Batch 561, LR 2.449747 Loss 6.503854, Accuracy 83.924%\n",
      "Epoch 14, Batch 562, LR 2.449709 Loss 6.502699, Accuracy 83.933%\n",
      "Epoch 14, Batch 563, LR 2.449671 Loss 6.502535, Accuracy 83.937%\n",
      "Epoch 14, Batch 564, LR 2.449634 Loss 6.502534, Accuracy 83.939%\n",
      "Epoch 14, Batch 565, LR 2.449596 Loss 6.501483, Accuracy 83.945%\n",
      "Epoch 14, Batch 566, LR 2.449558 Loss 6.501372, Accuracy 83.946%\n",
      "Epoch 14, Batch 567, LR 2.449521 Loss 6.499387, Accuracy 83.955%\n",
      "Epoch 14, Batch 568, LR 2.449483 Loss 6.499307, Accuracy 83.954%\n",
      "Epoch 14, Batch 569, LR 2.449445 Loss 6.497576, Accuracy 83.964%\n",
      "Epoch 14, Batch 570, LR 2.449408 Loss 6.497472, Accuracy 83.965%\n",
      "Epoch 14, Batch 571, LR 2.449370 Loss 6.498026, Accuracy 83.959%\n",
      "Epoch 14, Batch 572, LR 2.449332 Loss 6.499052, Accuracy 83.953%\n",
      "Epoch 14, Batch 573, LR 2.449294 Loss 6.498071, Accuracy 83.952%\n",
      "Epoch 14, Batch 574, LR 2.449257 Loss 6.499679, Accuracy 83.948%\n",
      "Epoch 14, Batch 575, LR 2.449219 Loss 6.499681, Accuracy 83.947%\n",
      "Epoch 14, Batch 576, LR 2.449181 Loss 6.499315, Accuracy 83.952%\n",
      "Epoch 14, Batch 577, LR 2.449143 Loss 6.497505, Accuracy 83.959%\n",
      "Epoch 14, Batch 578, LR 2.449105 Loss 6.497784, Accuracy 83.964%\n",
      "Epoch 14, Batch 579, LR 2.449068 Loss 6.498196, Accuracy 83.962%\n",
      "Epoch 14, Batch 580, LR 2.449030 Loss 6.498565, Accuracy 83.959%\n",
      "Epoch 14, Batch 581, LR 2.448992 Loss 6.498289, Accuracy 83.957%\n",
      "Epoch 14, Batch 582, LR 2.448954 Loss 6.497734, Accuracy 83.952%\n",
      "Epoch 14, Batch 583, LR 2.448916 Loss 6.499432, Accuracy 83.938%\n",
      "Epoch 14, Batch 584, LR 2.448878 Loss 6.500147, Accuracy 83.944%\n",
      "Epoch 14, Batch 585, LR 2.448840 Loss 6.499379, Accuracy 83.946%\n",
      "Epoch 14, Batch 586, LR 2.448802 Loss 6.498537, Accuracy 83.943%\n",
      "Epoch 14, Batch 587, LR 2.448764 Loss 6.497351, Accuracy 83.949%\n",
      "Epoch 14, Batch 588, LR 2.448726 Loss 6.497323, Accuracy 83.946%\n",
      "Epoch 14, Batch 589, LR 2.448688 Loss 6.497266, Accuracy 83.948%\n",
      "Epoch 14, Batch 590, LR 2.448650 Loss 6.498277, Accuracy 83.945%\n",
      "Epoch 14, Batch 591, LR 2.448612 Loss 6.498728, Accuracy 83.943%\n",
      "Epoch 14, Batch 592, LR 2.448574 Loss 6.497923, Accuracy 83.940%\n",
      "Epoch 14, Batch 593, LR 2.448536 Loss 6.496450, Accuracy 83.951%\n",
      "Epoch 14, Batch 594, LR 2.448498 Loss 6.496914, Accuracy 83.949%\n",
      "Epoch 14, Batch 595, LR 2.448460 Loss 6.496788, Accuracy 83.950%\n",
      "Epoch 14, Batch 596, LR 2.448422 Loss 6.497370, Accuracy 83.946%\n",
      "Epoch 14, Batch 597, LR 2.448384 Loss 6.497456, Accuracy 83.948%\n",
      "Epoch 14, Batch 598, LR 2.448346 Loss 6.498216, Accuracy 83.939%\n",
      "Epoch 14, Batch 599, LR 2.448308 Loss 6.497981, Accuracy 83.938%\n",
      "Epoch 14, Batch 600, LR 2.448270 Loss 6.497780, Accuracy 83.935%\n",
      "Epoch 14, Batch 601, LR 2.448232 Loss 6.497312, Accuracy 83.938%\n",
      "Epoch 14, Batch 602, LR 2.448193 Loss 6.498788, Accuracy 83.934%\n",
      "Epoch 14, Batch 603, LR 2.448155 Loss 6.499681, Accuracy 83.933%\n",
      "Epoch 14, Batch 604, LR 2.448117 Loss 6.500733, Accuracy 83.927%\n",
      "Epoch 14, Batch 605, LR 2.448079 Loss 6.500558, Accuracy 83.929%\n",
      "Epoch 14, Batch 606, LR 2.448041 Loss 6.500869, Accuracy 83.930%\n",
      "Epoch 14, Batch 607, LR 2.448002 Loss 6.501531, Accuracy 83.918%\n",
      "Epoch 14, Batch 608, LR 2.447964 Loss 6.501751, Accuracy 83.920%\n",
      "Epoch 14, Batch 609, LR 2.447926 Loss 6.501363, Accuracy 83.921%\n",
      "Epoch 14, Batch 610, LR 2.447888 Loss 6.502400, Accuracy 83.915%\n",
      "Epoch 14, Batch 611, LR 2.447849 Loss 6.502115, Accuracy 83.916%\n",
      "Epoch 14, Batch 612, LR 2.447811 Loss 6.502902, Accuracy 83.913%\n",
      "Epoch 14, Batch 613, LR 2.447773 Loss 6.503381, Accuracy 83.911%\n",
      "Epoch 14, Batch 614, LR 2.447734 Loss 6.503697, Accuracy 83.913%\n",
      "Epoch 14, Batch 615, LR 2.447696 Loss 6.503223, Accuracy 83.916%\n",
      "Epoch 14, Batch 616, LR 2.447658 Loss 6.502631, Accuracy 83.922%\n",
      "Epoch 14, Batch 617, LR 2.447619 Loss 6.502202, Accuracy 83.927%\n",
      "Epoch 14, Batch 618, LR 2.447581 Loss 6.502407, Accuracy 83.921%\n",
      "Epoch 14, Batch 619, LR 2.447543 Loss 6.503151, Accuracy 83.919%\n",
      "Epoch 14, Batch 620, LR 2.447504 Loss 6.502802, Accuracy 83.920%\n",
      "Epoch 14, Batch 621, LR 2.447466 Loss 6.503404, Accuracy 83.915%\n",
      "Epoch 14, Batch 622, LR 2.447427 Loss 6.504032, Accuracy 83.909%\n",
      "Epoch 14, Batch 623, LR 2.447389 Loss 6.504656, Accuracy 83.901%\n",
      "Epoch 14, Batch 624, LR 2.447350 Loss 6.504019, Accuracy 83.908%\n",
      "Epoch 14, Batch 625, LR 2.447312 Loss 6.504959, Accuracy 83.906%\n",
      "Epoch 14, Batch 626, LR 2.447274 Loss 6.504896, Accuracy 83.911%\n",
      "Epoch 14, Batch 627, LR 2.447235 Loss 6.506169, Accuracy 83.900%\n",
      "Epoch 14, Batch 628, LR 2.447196 Loss 6.505773, Accuracy 83.902%\n",
      "Epoch 14, Batch 629, LR 2.447158 Loss 6.505373, Accuracy 83.906%\n",
      "Epoch 14, Batch 630, LR 2.447119 Loss 6.504983, Accuracy 83.904%\n",
      "Epoch 14, Batch 631, LR 2.447081 Loss 6.505354, Accuracy 83.897%\n",
      "Epoch 14, Batch 632, LR 2.447042 Loss 6.505264, Accuracy 83.900%\n",
      "Epoch 14, Batch 633, LR 2.447004 Loss 6.504641, Accuracy 83.906%\n",
      "Epoch 14, Batch 634, LR 2.446965 Loss 6.506814, Accuracy 83.893%\n",
      "Epoch 14, Batch 635, LR 2.446926 Loss 6.506555, Accuracy 83.895%\n",
      "Epoch 14, Batch 636, LR 2.446888 Loss 6.505709, Accuracy 83.901%\n",
      "Epoch 14, Batch 637, LR 2.446849 Loss 6.505714, Accuracy 83.897%\n",
      "Epoch 14, Batch 638, LR 2.446811 Loss 6.505863, Accuracy 83.895%\n",
      "Epoch 14, Batch 639, LR 2.446772 Loss 6.507237, Accuracy 83.879%\n",
      "Epoch 14, Batch 640, LR 2.446733 Loss 6.507147, Accuracy 83.875%\n",
      "Epoch 14, Batch 641, LR 2.446695 Loss 6.508294, Accuracy 83.869%\n",
      "Epoch 14, Batch 642, LR 2.446656 Loss 6.508370, Accuracy 83.865%\n",
      "Epoch 14, Batch 643, LR 2.446617 Loss 6.508610, Accuracy 83.860%\n",
      "Epoch 14, Batch 644, LR 2.446578 Loss 6.508549, Accuracy 83.859%\n",
      "Epoch 14, Batch 645, LR 2.446540 Loss 6.508417, Accuracy 83.864%\n",
      "Epoch 14, Batch 646, LR 2.446501 Loss 6.507938, Accuracy 83.867%\n",
      "Epoch 14, Batch 647, LR 2.446462 Loss 6.507666, Accuracy 83.867%\n",
      "Epoch 14, Batch 648, LR 2.446423 Loss 6.506757, Accuracy 83.865%\n",
      "Epoch 14, Batch 649, LR 2.446384 Loss 6.506608, Accuracy 83.863%\n",
      "Epoch 14, Batch 650, LR 2.446346 Loss 6.505784, Accuracy 83.867%\n",
      "Epoch 14, Batch 651, LR 2.446307 Loss 6.506070, Accuracy 83.864%\n",
      "Epoch 14, Batch 652, LR 2.446268 Loss 6.505508, Accuracy 83.855%\n",
      "Epoch 14, Batch 653, LR 2.446229 Loss 6.506091, Accuracy 83.850%\n",
      "Epoch 14, Batch 654, LR 2.446190 Loss 6.504928, Accuracy 83.855%\n",
      "Epoch 14, Batch 655, LR 2.446151 Loss 6.505392, Accuracy 83.862%\n",
      "Epoch 14, Batch 656, LR 2.446112 Loss 6.506908, Accuracy 83.857%\n",
      "Epoch 14, Batch 657, LR 2.446074 Loss 6.507628, Accuracy 83.849%\n",
      "Epoch 14, Batch 658, LR 2.446035 Loss 6.507361, Accuracy 83.847%\n",
      "Epoch 14, Batch 659, LR 2.445996 Loss 6.507661, Accuracy 83.846%\n",
      "Epoch 14, Batch 660, LR 2.445957 Loss 6.507219, Accuracy 83.851%\n",
      "Epoch 14, Batch 661, LR 2.445918 Loss 6.507179, Accuracy 83.853%\n",
      "Epoch 14, Batch 662, LR 2.445879 Loss 6.507251, Accuracy 83.855%\n",
      "Epoch 14, Batch 663, LR 2.445840 Loss 6.506620, Accuracy 83.854%\n",
      "Epoch 14, Batch 664, LR 2.445801 Loss 6.506446, Accuracy 83.855%\n",
      "Epoch 14, Batch 665, LR 2.445762 Loss 6.505152, Accuracy 83.866%\n",
      "Epoch 14, Batch 666, LR 2.445723 Loss 6.507128, Accuracy 83.853%\n",
      "Epoch 14, Batch 667, LR 2.445684 Loss 6.507163, Accuracy 83.851%\n",
      "Epoch 14, Batch 668, LR 2.445645 Loss 6.508053, Accuracy 83.843%\n",
      "Epoch 14, Batch 669, LR 2.445606 Loss 6.507182, Accuracy 83.847%\n",
      "Epoch 14, Batch 670, LR 2.445566 Loss 6.507546, Accuracy 83.844%\n",
      "Epoch 14, Batch 671, LR 2.445527 Loss 6.508103, Accuracy 83.843%\n",
      "Epoch 14, Batch 672, LR 2.445488 Loss 6.508599, Accuracy 83.833%\n",
      "Epoch 14, Batch 673, LR 2.445449 Loss 6.508558, Accuracy 83.829%\n",
      "Epoch 14, Batch 674, LR 2.445410 Loss 6.508239, Accuracy 83.828%\n",
      "Epoch 14, Batch 675, LR 2.445371 Loss 6.508550, Accuracy 83.819%\n",
      "Epoch 14, Batch 676, LR 2.445332 Loss 6.509893, Accuracy 83.816%\n",
      "Epoch 14, Batch 677, LR 2.445292 Loss 6.510067, Accuracy 83.811%\n",
      "Epoch 14, Batch 678, LR 2.445253 Loss 6.509722, Accuracy 83.818%\n",
      "Epoch 14, Batch 679, LR 2.445214 Loss 6.510249, Accuracy 83.811%\n",
      "Epoch 14, Batch 680, LR 2.445175 Loss 6.508965, Accuracy 83.819%\n",
      "Epoch 14, Batch 681, LR 2.445136 Loss 6.508258, Accuracy 83.825%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 682, LR 2.445096 Loss 6.508269, Accuracy 83.827%\n",
      "Epoch 14, Batch 683, LR 2.445057 Loss 6.507739, Accuracy 83.831%\n",
      "Epoch 14, Batch 684, LR 2.445018 Loss 6.507130, Accuracy 83.832%\n",
      "Epoch 14, Batch 685, LR 2.444978 Loss 6.507562, Accuracy 83.828%\n",
      "Epoch 14, Batch 686, LR 2.444939 Loss 6.507308, Accuracy 83.827%\n",
      "Epoch 14, Batch 687, LR 2.444900 Loss 6.506565, Accuracy 83.830%\n",
      "Epoch 14, Batch 688, LR 2.444860 Loss 6.505803, Accuracy 83.830%\n",
      "Epoch 14, Batch 689, LR 2.444821 Loss 6.505569, Accuracy 83.833%\n",
      "Epoch 14, Batch 690, LR 2.444782 Loss 6.505102, Accuracy 83.837%\n",
      "Epoch 14, Batch 691, LR 2.444742 Loss 6.504988, Accuracy 83.836%\n",
      "Epoch 14, Batch 692, LR 2.444703 Loss 6.505493, Accuracy 83.834%\n",
      "Epoch 14, Batch 693, LR 2.444664 Loss 6.504918, Accuracy 83.837%\n",
      "Epoch 14, Batch 694, LR 2.444624 Loss 6.505491, Accuracy 83.836%\n",
      "Epoch 14, Batch 695, LR 2.444585 Loss 6.506999, Accuracy 83.828%\n",
      "Epoch 14, Batch 696, LR 2.444545 Loss 6.506460, Accuracy 83.831%\n",
      "Epoch 14, Batch 697, LR 2.444506 Loss 6.505843, Accuracy 83.835%\n",
      "Epoch 14, Batch 698, LR 2.444466 Loss 6.505645, Accuracy 83.834%\n",
      "Epoch 14, Batch 699, LR 2.444427 Loss 6.505458, Accuracy 83.839%\n",
      "Epoch 14, Batch 700, LR 2.444387 Loss 6.504776, Accuracy 83.845%\n",
      "Epoch 14, Batch 701, LR 2.444348 Loss 6.505983, Accuracy 83.838%\n",
      "Epoch 14, Batch 702, LR 2.444308 Loss 6.505438, Accuracy 83.843%\n",
      "Epoch 14, Batch 703, LR 2.444269 Loss 6.505696, Accuracy 83.848%\n",
      "Epoch 14, Batch 704, LR 2.444229 Loss 6.505102, Accuracy 83.853%\n",
      "Epoch 14, Batch 705, LR 2.444190 Loss 6.505306, Accuracy 83.853%\n",
      "Epoch 14, Batch 706, LR 2.444150 Loss 6.505299, Accuracy 83.850%\n",
      "Epoch 14, Batch 707, LR 2.444110 Loss 6.503806, Accuracy 83.853%\n",
      "Epoch 14, Batch 708, LR 2.444071 Loss 6.503785, Accuracy 83.851%\n",
      "Epoch 14, Batch 709, LR 2.444031 Loss 6.503474, Accuracy 83.858%\n",
      "Epoch 14, Batch 710, LR 2.443991 Loss 6.503478, Accuracy 83.858%\n",
      "Epoch 14, Batch 711, LR 2.443952 Loss 6.502838, Accuracy 83.865%\n",
      "Epoch 14, Batch 712, LR 2.443912 Loss 6.502718, Accuracy 83.865%\n",
      "Epoch 14, Batch 713, LR 2.443872 Loss 6.501680, Accuracy 83.868%\n",
      "Epoch 14, Batch 714, LR 2.443833 Loss 6.502092, Accuracy 83.861%\n",
      "Epoch 14, Batch 715, LR 2.443793 Loss 6.502141, Accuracy 83.859%\n",
      "Epoch 14, Batch 716, LR 2.443753 Loss 6.502819, Accuracy 83.859%\n",
      "Epoch 14, Batch 717, LR 2.443714 Loss 6.502401, Accuracy 83.860%\n",
      "Epoch 14, Batch 718, LR 2.443674 Loss 6.501717, Accuracy 83.861%\n",
      "Epoch 14, Batch 719, LR 2.443634 Loss 6.501852, Accuracy 83.861%\n",
      "Epoch 14, Batch 720, LR 2.443594 Loss 6.501226, Accuracy 83.868%\n",
      "Epoch 14, Batch 721, LR 2.443555 Loss 6.501235, Accuracy 83.868%\n",
      "Epoch 14, Batch 722, LR 2.443515 Loss 6.501083, Accuracy 83.873%\n",
      "Epoch 14, Batch 723, LR 2.443475 Loss 6.501093, Accuracy 83.870%\n",
      "Epoch 14, Batch 724, LR 2.443435 Loss 6.500038, Accuracy 83.872%\n",
      "Epoch 14, Batch 725, LR 2.443395 Loss 6.500774, Accuracy 83.863%\n",
      "Epoch 14, Batch 726, LR 2.443355 Loss 6.500730, Accuracy 83.862%\n",
      "Epoch 14, Batch 727, LR 2.443316 Loss 6.499972, Accuracy 83.865%\n",
      "Epoch 14, Batch 728, LR 2.443276 Loss 6.499322, Accuracy 83.864%\n",
      "Epoch 14, Batch 729, LR 2.443236 Loss 6.499375, Accuracy 83.864%\n",
      "Epoch 14, Batch 730, LR 2.443196 Loss 6.498745, Accuracy 83.862%\n",
      "Epoch 14, Batch 731, LR 2.443156 Loss 6.500089, Accuracy 83.853%\n",
      "Epoch 14, Batch 732, LR 2.443116 Loss 6.499651, Accuracy 83.862%\n",
      "Epoch 14, Batch 733, LR 2.443076 Loss 6.499290, Accuracy 83.868%\n",
      "Epoch 14, Batch 734, LR 2.443036 Loss 6.500413, Accuracy 83.859%\n",
      "Epoch 14, Batch 735, LR 2.442996 Loss 6.500439, Accuracy 83.856%\n",
      "Epoch 14, Batch 736, LR 2.442956 Loss 6.500677, Accuracy 83.857%\n",
      "Epoch 14, Batch 737, LR 2.442916 Loss 6.501568, Accuracy 83.852%\n",
      "Epoch 14, Batch 738, LR 2.442876 Loss 6.500738, Accuracy 83.854%\n",
      "Epoch 14, Batch 739, LR 2.442836 Loss 6.501146, Accuracy 83.857%\n",
      "Epoch 14, Batch 740, LR 2.442796 Loss 6.500755, Accuracy 83.862%\n",
      "Epoch 14, Batch 741, LR 2.442756 Loss 6.500726, Accuracy 83.862%\n",
      "Epoch 14, Batch 742, LR 2.442716 Loss 6.499853, Accuracy 83.868%\n",
      "Epoch 14, Batch 743, LR 2.442676 Loss 6.500414, Accuracy 83.862%\n",
      "Epoch 14, Batch 744, LR 2.442636 Loss 6.501218, Accuracy 83.856%\n",
      "Epoch 14, Batch 745, LR 2.442595 Loss 6.500493, Accuracy 83.864%\n",
      "Epoch 14, Batch 746, LR 2.442555 Loss 6.499691, Accuracy 83.870%\n",
      "Epoch 14, Batch 747, LR 2.442515 Loss 6.498243, Accuracy 83.881%\n",
      "Epoch 14, Batch 748, LR 2.442475 Loss 6.497289, Accuracy 83.886%\n",
      "Epoch 14, Batch 749, LR 2.442435 Loss 6.497070, Accuracy 83.888%\n",
      "Epoch 14, Batch 750, LR 2.442395 Loss 6.496356, Accuracy 83.885%\n",
      "Epoch 14, Batch 751, LR 2.442354 Loss 6.495925, Accuracy 83.885%\n",
      "Epoch 14, Batch 752, LR 2.442314 Loss 6.496892, Accuracy 83.884%\n",
      "Epoch 14, Batch 753, LR 2.442274 Loss 6.496502, Accuracy 83.881%\n",
      "Epoch 14, Batch 754, LR 2.442234 Loss 6.495713, Accuracy 83.890%\n",
      "Epoch 14, Batch 755, LR 2.442193 Loss 6.495557, Accuracy 83.892%\n",
      "Epoch 14, Batch 756, LR 2.442153 Loss 6.495918, Accuracy 83.890%\n",
      "Epoch 14, Batch 757, LR 2.442113 Loss 6.495490, Accuracy 83.892%\n",
      "Epoch 14, Batch 758, LR 2.442073 Loss 6.495036, Accuracy 83.894%\n",
      "Epoch 14, Batch 759, LR 2.442032 Loss 6.495414, Accuracy 83.894%\n",
      "Epoch 14, Batch 760, LR 2.441992 Loss 6.495411, Accuracy 83.897%\n",
      "Epoch 14, Batch 761, LR 2.441952 Loss 6.496357, Accuracy 83.890%\n",
      "Epoch 14, Batch 762, LR 2.441911 Loss 6.497068, Accuracy 83.884%\n",
      "Epoch 14, Batch 763, LR 2.441871 Loss 6.497249, Accuracy 83.880%\n",
      "Epoch 14, Batch 764, LR 2.441831 Loss 6.496492, Accuracy 83.879%\n",
      "Epoch 14, Batch 765, LR 2.441790 Loss 6.496122, Accuracy 83.882%\n",
      "Epoch 14, Batch 766, LR 2.441750 Loss 6.496176, Accuracy 83.881%\n",
      "Epoch 14, Batch 767, LR 2.441709 Loss 6.496317, Accuracy 83.875%\n",
      "Epoch 14, Batch 768, LR 2.441669 Loss 6.495804, Accuracy 83.876%\n",
      "Epoch 14, Batch 769, LR 2.441628 Loss 6.496634, Accuracy 83.872%\n",
      "Epoch 14, Batch 770, LR 2.441588 Loss 6.497057, Accuracy 83.865%\n",
      "Epoch 14, Batch 771, LR 2.441547 Loss 6.497284, Accuracy 83.862%\n",
      "Epoch 14, Batch 772, LR 2.441507 Loss 6.496661, Accuracy 83.865%\n",
      "Epoch 14, Batch 773, LR 2.441466 Loss 6.496817, Accuracy 83.864%\n",
      "Epoch 14, Batch 774, LR 2.441426 Loss 6.497288, Accuracy 83.859%\n",
      "Epoch 14, Batch 775, LR 2.441385 Loss 6.497370, Accuracy 83.857%\n",
      "Epoch 14, Batch 776, LR 2.441345 Loss 6.497385, Accuracy 83.861%\n",
      "Epoch 14, Batch 777, LR 2.441304 Loss 6.498453, Accuracy 83.858%\n",
      "Epoch 14, Batch 778, LR 2.441264 Loss 6.498833, Accuracy 83.856%\n",
      "Epoch 14, Batch 779, LR 2.441223 Loss 6.499426, Accuracy 83.847%\n",
      "Epoch 14, Batch 780, LR 2.441183 Loss 6.499514, Accuracy 83.849%\n",
      "Epoch 14, Batch 781, LR 2.441142 Loss 6.499655, Accuracy 83.846%\n",
      "Epoch 14, Batch 782, LR 2.441101 Loss 6.500578, Accuracy 83.837%\n",
      "Epoch 14, Batch 783, LR 2.441061 Loss 6.500329, Accuracy 83.838%\n",
      "Epoch 14, Batch 784, LR 2.441020 Loss 6.500868, Accuracy 83.836%\n",
      "Epoch 14, Batch 785, LR 2.440979 Loss 6.500700, Accuracy 83.833%\n",
      "Epoch 14, Batch 786, LR 2.440939 Loss 6.501962, Accuracy 83.824%\n",
      "Epoch 14, Batch 787, LR 2.440898 Loss 6.501108, Accuracy 83.827%\n",
      "Epoch 14, Batch 788, LR 2.440857 Loss 6.500628, Accuracy 83.829%\n",
      "Epoch 14, Batch 789, LR 2.440817 Loss 6.500735, Accuracy 83.831%\n",
      "Epoch 14, Batch 790, LR 2.440776 Loss 6.500412, Accuracy 83.829%\n",
      "Epoch 14, Batch 791, LR 2.440735 Loss 6.500184, Accuracy 83.831%\n",
      "Epoch 14, Batch 792, LR 2.440694 Loss 6.499550, Accuracy 83.834%\n",
      "Epoch 14, Batch 793, LR 2.440654 Loss 6.499714, Accuracy 83.834%\n",
      "Epoch 14, Batch 794, LR 2.440613 Loss 6.498929, Accuracy 83.841%\n",
      "Epoch 14, Batch 795, LR 2.440572 Loss 6.498537, Accuracy 83.844%\n",
      "Epoch 14, Batch 796, LR 2.440531 Loss 6.498442, Accuracy 83.844%\n",
      "Epoch 14, Batch 797, LR 2.440490 Loss 6.498622, Accuracy 83.845%\n",
      "Epoch 14, Batch 798, LR 2.440449 Loss 6.498492, Accuracy 83.844%\n",
      "Epoch 14, Batch 799, LR 2.440409 Loss 6.499023, Accuracy 83.841%\n",
      "Epoch 14, Batch 800, LR 2.440368 Loss 6.499582, Accuracy 83.836%\n",
      "Epoch 14, Batch 801, LR 2.440327 Loss 6.499276, Accuracy 83.839%\n",
      "Epoch 14, Batch 802, LR 2.440286 Loss 6.498875, Accuracy 83.835%\n",
      "Epoch 14, Batch 803, LR 2.440245 Loss 6.499452, Accuracy 83.837%\n",
      "Epoch 14, Batch 804, LR 2.440204 Loss 6.500327, Accuracy 83.832%\n",
      "Epoch 14, Batch 805, LR 2.440163 Loss 6.499878, Accuracy 83.832%\n",
      "Epoch 14, Batch 806, LR 2.440122 Loss 6.500192, Accuracy 83.826%\n",
      "Epoch 14, Batch 807, LR 2.440081 Loss 6.500559, Accuracy 83.818%\n",
      "Epoch 14, Batch 808, LR 2.440040 Loss 6.500637, Accuracy 83.816%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 809, LR 2.439999 Loss 6.500469, Accuracy 83.815%\n",
      "Epoch 14, Batch 810, LR 2.439958 Loss 6.501237, Accuracy 83.816%\n",
      "Epoch 14, Batch 811, LR 2.439917 Loss 6.501820, Accuracy 83.810%\n",
      "Epoch 14, Batch 812, LR 2.439876 Loss 6.502058, Accuracy 83.807%\n",
      "Epoch 14, Batch 813, LR 2.439835 Loss 6.501647, Accuracy 83.806%\n",
      "Epoch 14, Batch 814, LR 2.439794 Loss 6.501038, Accuracy 83.808%\n",
      "Epoch 14, Batch 815, LR 2.439753 Loss 6.501173, Accuracy 83.807%\n",
      "Epoch 14, Batch 816, LR 2.439712 Loss 6.500866, Accuracy 83.810%\n",
      "Epoch 14, Batch 817, LR 2.439671 Loss 6.500946, Accuracy 83.811%\n",
      "Epoch 14, Batch 818, LR 2.439630 Loss 6.500645, Accuracy 83.810%\n",
      "Epoch 14, Batch 819, LR 2.439589 Loss 6.500164, Accuracy 83.816%\n",
      "Epoch 14, Batch 820, LR 2.439547 Loss 6.500485, Accuracy 83.812%\n",
      "Epoch 14, Batch 821, LR 2.439506 Loss 6.500143, Accuracy 83.809%\n",
      "Epoch 14, Batch 822, LR 2.439465 Loss 6.500001, Accuracy 83.808%\n",
      "Epoch 14, Batch 823, LR 2.439424 Loss 6.500065, Accuracy 83.809%\n",
      "Epoch 14, Batch 824, LR 2.439383 Loss 6.499369, Accuracy 83.812%\n",
      "Epoch 14, Batch 825, LR 2.439341 Loss 6.498595, Accuracy 83.814%\n",
      "Epoch 14, Batch 826, LR 2.439300 Loss 6.499061, Accuracy 83.812%\n",
      "Epoch 14, Batch 827, LR 2.439259 Loss 6.499222, Accuracy 83.811%\n",
      "Epoch 14, Batch 828, LR 2.439218 Loss 6.499355, Accuracy 83.810%\n",
      "Epoch 14, Batch 829, LR 2.439176 Loss 6.499445, Accuracy 83.811%\n",
      "Epoch 14, Batch 830, LR 2.439135 Loss 6.500177, Accuracy 83.808%\n",
      "Epoch 14, Batch 831, LR 2.439094 Loss 6.500212, Accuracy 83.812%\n",
      "Epoch 14, Batch 832, LR 2.439053 Loss 6.500563, Accuracy 83.810%\n",
      "Epoch 14, Batch 833, LR 2.439011 Loss 6.499992, Accuracy 83.810%\n",
      "Epoch 14, Batch 834, LR 2.438970 Loss 6.500063, Accuracy 83.811%\n",
      "Epoch 14, Batch 835, LR 2.438929 Loss 6.500152, Accuracy 83.809%\n",
      "Epoch 14, Batch 836, LR 2.438887 Loss 6.500007, Accuracy 83.811%\n",
      "Epoch 14, Batch 837, LR 2.438846 Loss 6.500512, Accuracy 83.807%\n",
      "Epoch 14, Batch 838, LR 2.438804 Loss 6.499824, Accuracy 83.810%\n",
      "Epoch 14, Batch 839, LR 2.438763 Loss 6.499707, Accuracy 83.809%\n",
      "Epoch 14, Batch 840, LR 2.438722 Loss 6.499351, Accuracy 83.807%\n",
      "Epoch 14, Batch 841, LR 2.438680 Loss 6.499082, Accuracy 83.809%\n",
      "Epoch 14, Batch 842, LR 2.438639 Loss 6.498523, Accuracy 83.814%\n",
      "Epoch 14, Batch 843, LR 2.438597 Loss 6.498726, Accuracy 83.811%\n",
      "Epoch 14, Batch 844, LR 2.438556 Loss 6.499248, Accuracy 83.809%\n",
      "Epoch 14, Batch 845, LR 2.438514 Loss 6.498558, Accuracy 83.812%\n",
      "Epoch 14, Batch 846, LR 2.438473 Loss 6.499079, Accuracy 83.812%\n",
      "Epoch 14, Batch 847, LR 2.438431 Loss 6.499037, Accuracy 83.809%\n",
      "Epoch 14, Batch 848, LR 2.438390 Loss 6.499140, Accuracy 83.807%\n",
      "Epoch 14, Batch 849, LR 2.438348 Loss 6.498991, Accuracy 83.809%\n",
      "Epoch 14, Batch 850, LR 2.438307 Loss 6.498860, Accuracy 83.811%\n",
      "Epoch 14, Batch 851, LR 2.438265 Loss 6.499095, Accuracy 83.811%\n",
      "Epoch 14, Batch 852, LR 2.438223 Loss 6.498549, Accuracy 83.817%\n",
      "Epoch 14, Batch 853, LR 2.438182 Loss 6.498127, Accuracy 83.816%\n",
      "Epoch 14, Batch 854, LR 2.438140 Loss 6.498994, Accuracy 83.810%\n",
      "Epoch 14, Batch 855, LR 2.438099 Loss 6.498729, Accuracy 83.811%\n",
      "Epoch 14, Batch 856, LR 2.438057 Loss 6.498372, Accuracy 83.816%\n",
      "Epoch 14, Batch 857, LR 2.438015 Loss 6.498739, Accuracy 83.815%\n",
      "Epoch 14, Batch 858, LR 2.437974 Loss 6.499417, Accuracy 83.808%\n",
      "Epoch 14, Batch 859, LR 2.437932 Loss 6.499847, Accuracy 83.804%\n",
      "Epoch 14, Batch 860, LR 2.437890 Loss 6.500094, Accuracy 83.802%\n",
      "Epoch 14, Batch 861, LR 2.437849 Loss 6.500739, Accuracy 83.796%\n",
      "Epoch 14, Batch 862, LR 2.437807 Loss 6.501186, Accuracy 83.795%\n",
      "Epoch 14, Batch 863, LR 2.437765 Loss 6.501427, Accuracy 83.791%\n",
      "Epoch 14, Batch 864, LR 2.437723 Loss 6.501160, Accuracy 83.791%\n",
      "Epoch 14, Batch 865, LR 2.437682 Loss 6.501424, Accuracy 83.792%\n",
      "Epoch 14, Batch 866, LR 2.437640 Loss 6.500859, Accuracy 83.799%\n",
      "Epoch 14, Batch 867, LR 2.437598 Loss 6.501031, Accuracy 83.794%\n",
      "Epoch 14, Batch 868, LR 2.437556 Loss 6.501621, Accuracy 83.789%\n",
      "Epoch 14, Batch 869, LR 2.437514 Loss 6.501155, Accuracy 83.791%\n",
      "Epoch 14, Batch 870, LR 2.437473 Loss 6.501220, Accuracy 83.791%\n",
      "Epoch 14, Batch 871, LR 2.437431 Loss 6.500082, Accuracy 83.797%\n",
      "Epoch 14, Batch 872, LR 2.437389 Loss 6.500516, Accuracy 83.798%\n",
      "Epoch 14, Batch 873, LR 2.437347 Loss 6.500778, Accuracy 83.797%\n",
      "Epoch 14, Batch 874, LR 2.437305 Loss 6.501457, Accuracy 83.795%\n",
      "Epoch 14, Batch 875, LR 2.437263 Loss 6.500946, Accuracy 83.793%\n",
      "Epoch 14, Batch 876, LR 2.437221 Loss 6.501306, Accuracy 83.794%\n",
      "Epoch 14, Batch 877, LR 2.437179 Loss 6.501759, Accuracy 83.793%\n",
      "Epoch 14, Batch 878, LR 2.437138 Loss 6.501383, Accuracy 83.790%\n",
      "Epoch 14, Batch 879, LR 2.437096 Loss 6.501298, Accuracy 83.791%\n",
      "Epoch 14, Batch 880, LR 2.437054 Loss 6.501715, Accuracy 83.792%\n",
      "Epoch 14, Batch 881, LR 2.437012 Loss 6.500548, Accuracy 83.799%\n",
      "Epoch 14, Batch 882, LR 2.436970 Loss 6.500471, Accuracy 83.804%\n",
      "Epoch 14, Batch 883, LR 2.436928 Loss 6.500237, Accuracy 83.803%\n",
      "Epoch 14, Batch 884, LR 2.436886 Loss 6.500336, Accuracy 83.801%\n",
      "Epoch 14, Batch 885, LR 2.436844 Loss 6.499530, Accuracy 83.806%\n",
      "Epoch 14, Batch 886, LR 2.436802 Loss 6.499967, Accuracy 83.804%\n",
      "Epoch 14, Batch 887, LR 2.436759 Loss 6.500790, Accuracy 83.802%\n",
      "Epoch 14, Batch 888, LR 2.436717 Loss 6.500248, Accuracy 83.802%\n",
      "Epoch 14, Batch 889, LR 2.436675 Loss 6.500497, Accuracy 83.798%\n",
      "Epoch 14, Batch 890, LR 2.436633 Loss 6.500048, Accuracy 83.799%\n",
      "Epoch 14, Batch 891, LR 2.436591 Loss 6.500130, Accuracy 83.792%\n",
      "Epoch 14, Batch 892, LR 2.436549 Loss 6.499803, Accuracy 83.792%\n",
      "Epoch 14, Batch 893, LR 2.436507 Loss 6.499847, Accuracy 83.790%\n",
      "Epoch 14, Batch 894, LR 2.436465 Loss 6.500250, Accuracy 83.783%\n",
      "Epoch 14, Batch 895, LR 2.436422 Loss 6.499811, Accuracy 83.788%\n",
      "Epoch 14, Batch 896, LR 2.436380 Loss 6.499458, Accuracy 83.786%\n",
      "Epoch 14, Batch 897, LR 2.436338 Loss 6.499903, Accuracy 83.784%\n",
      "Epoch 14, Batch 898, LR 2.436296 Loss 6.500073, Accuracy 83.787%\n",
      "Epoch 14, Batch 899, LR 2.436254 Loss 6.499897, Accuracy 83.791%\n",
      "Epoch 14, Batch 900, LR 2.436211 Loss 6.499732, Accuracy 83.788%\n",
      "Epoch 14, Batch 901, LR 2.436169 Loss 6.500476, Accuracy 83.785%\n",
      "Epoch 14, Batch 902, LR 2.436127 Loss 6.501678, Accuracy 83.776%\n",
      "Epoch 14, Batch 903, LR 2.436085 Loss 6.501208, Accuracy 83.775%\n",
      "Epoch 14, Batch 904, LR 2.436042 Loss 6.501724, Accuracy 83.775%\n",
      "Epoch 14, Batch 905, LR 2.436000 Loss 6.501588, Accuracy 83.777%\n",
      "Epoch 14, Batch 906, LR 2.435958 Loss 6.501702, Accuracy 83.777%\n",
      "Epoch 14, Batch 907, LR 2.435915 Loss 6.502070, Accuracy 83.776%\n",
      "Epoch 14, Batch 908, LR 2.435873 Loss 6.502159, Accuracy 83.774%\n",
      "Epoch 14, Batch 909, LR 2.435831 Loss 6.502203, Accuracy 83.774%\n",
      "Epoch 14, Batch 910, LR 2.435788 Loss 6.501664, Accuracy 83.777%\n",
      "Epoch 14, Batch 911, LR 2.435746 Loss 6.501015, Accuracy 83.784%\n",
      "Epoch 14, Batch 912, LR 2.435703 Loss 6.500873, Accuracy 83.784%\n",
      "Epoch 14, Batch 913, LR 2.435661 Loss 6.500895, Accuracy 83.783%\n",
      "Epoch 14, Batch 914, LR 2.435619 Loss 6.500392, Accuracy 83.784%\n",
      "Epoch 14, Batch 915, LR 2.435576 Loss 6.499891, Accuracy 83.786%\n",
      "Epoch 14, Batch 916, LR 2.435534 Loss 6.500159, Accuracy 83.784%\n",
      "Epoch 14, Batch 917, LR 2.435491 Loss 6.500193, Accuracy 83.786%\n",
      "Epoch 14, Batch 918, LR 2.435449 Loss 6.499831, Accuracy 83.792%\n",
      "Epoch 14, Batch 919, LR 2.435406 Loss 6.500079, Accuracy 83.790%\n",
      "Epoch 14, Batch 920, LR 2.435364 Loss 6.499988, Accuracy 83.785%\n",
      "Epoch 14, Batch 921, LR 2.435321 Loss 6.498988, Accuracy 83.791%\n",
      "Epoch 14, Batch 922, LR 2.435279 Loss 6.499539, Accuracy 83.787%\n",
      "Epoch 14, Batch 923, LR 2.435236 Loss 6.499573, Accuracy 83.786%\n",
      "Epoch 14, Batch 924, LR 2.435194 Loss 6.498957, Accuracy 83.789%\n",
      "Epoch 14, Batch 925, LR 2.435151 Loss 6.498348, Accuracy 83.794%\n",
      "Epoch 14, Batch 926, LR 2.435108 Loss 6.498683, Accuracy 83.789%\n",
      "Epoch 14, Batch 927, LR 2.435066 Loss 6.498366, Accuracy 83.793%\n",
      "Epoch 14, Batch 928, LR 2.435023 Loss 6.498923, Accuracy 83.790%\n",
      "Epoch 14, Batch 929, LR 2.434981 Loss 6.499578, Accuracy 83.783%\n",
      "Epoch 14, Batch 930, LR 2.434938 Loss 6.499676, Accuracy 83.785%\n",
      "Epoch 14, Batch 931, LR 2.434895 Loss 6.499707, Accuracy 83.783%\n",
      "Epoch 14, Batch 932, LR 2.434853 Loss 6.500861, Accuracy 83.782%\n",
      "Epoch 14, Batch 933, LR 2.434810 Loss 6.500978, Accuracy 83.778%\n",
      "Epoch 14, Batch 934, LR 2.434767 Loss 6.501696, Accuracy 83.775%\n",
      "Epoch 14, Batch 935, LR 2.434725 Loss 6.501683, Accuracy 83.778%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 936, LR 2.434682 Loss 6.500752, Accuracy 83.787%\n",
      "Epoch 14, Batch 937, LR 2.434639 Loss 6.499840, Accuracy 83.791%\n",
      "Epoch 14, Batch 938, LR 2.434596 Loss 6.499613, Accuracy 83.792%\n",
      "Epoch 14, Batch 939, LR 2.434554 Loss 6.499229, Accuracy 83.794%\n",
      "Epoch 14, Batch 940, LR 2.434511 Loss 6.499484, Accuracy 83.792%\n",
      "Epoch 14, Batch 941, LR 2.434468 Loss 6.499374, Accuracy 83.795%\n",
      "Epoch 14, Batch 942, LR 2.434425 Loss 6.499565, Accuracy 83.794%\n",
      "Epoch 14, Batch 943, LR 2.434382 Loss 6.499896, Accuracy 83.790%\n",
      "Epoch 14, Batch 944, LR 2.434339 Loss 6.500462, Accuracy 83.782%\n",
      "Epoch 14, Batch 945, LR 2.434297 Loss 6.500307, Accuracy 83.781%\n",
      "Epoch 14, Batch 946, LR 2.434254 Loss 6.500539, Accuracy 83.782%\n",
      "Epoch 14, Batch 947, LR 2.434211 Loss 6.501290, Accuracy 83.781%\n",
      "Epoch 14, Batch 948, LR 2.434168 Loss 6.501045, Accuracy 83.784%\n",
      "Epoch 14, Batch 949, LR 2.434125 Loss 6.500625, Accuracy 83.784%\n",
      "Epoch 14, Batch 950, LR 2.434082 Loss 6.501126, Accuracy 83.780%\n",
      "Epoch 14, Batch 951, LR 2.434039 Loss 6.500280, Accuracy 83.786%\n",
      "Epoch 14, Batch 952, LR 2.433996 Loss 6.500458, Accuracy 83.787%\n",
      "Epoch 14, Batch 953, LR 2.433953 Loss 6.500040, Accuracy 83.788%\n",
      "Epoch 14, Batch 954, LR 2.433910 Loss 6.499576, Accuracy 83.791%\n",
      "Epoch 14, Batch 955, LR 2.433867 Loss 6.500108, Accuracy 83.788%\n",
      "Epoch 14, Batch 956, LR 2.433824 Loss 6.501011, Accuracy 83.783%\n",
      "Epoch 14, Batch 957, LR 2.433781 Loss 6.501917, Accuracy 83.777%\n",
      "Epoch 14, Batch 958, LR 2.433738 Loss 6.501803, Accuracy 83.779%\n",
      "Epoch 14, Batch 959, LR 2.433695 Loss 6.501477, Accuracy 83.775%\n",
      "Epoch 14, Batch 960, LR 2.433652 Loss 6.502647, Accuracy 83.767%\n",
      "Epoch 14, Batch 961, LR 2.433609 Loss 6.502737, Accuracy 83.767%\n",
      "Epoch 14, Batch 962, LR 2.433566 Loss 6.502211, Accuracy 83.768%\n",
      "Epoch 14, Batch 963, LR 2.433523 Loss 6.502113, Accuracy 83.764%\n",
      "Epoch 14, Batch 964, LR 2.433480 Loss 6.501749, Accuracy 83.767%\n",
      "Epoch 14, Batch 965, LR 2.433437 Loss 6.501624, Accuracy 83.767%\n",
      "Epoch 14, Batch 966, LR 2.433394 Loss 6.501913, Accuracy 83.766%\n",
      "Epoch 14, Batch 967, LR 2.433350 Loss 6.501945, Accuracy 83.766%\n",
      "Epoch 14, Batch 968, LR 2.433307 Loss 6.502304, Accuracy 83.769%\n",
      "Epoch 14, Batch 969, LR 2.433264 Loss 6.502430, Accuracy 83.766%\n",
      "Epoch 14, Batch 970, LR 2.433221 Loss 6.502597, Accuracy 83.765%\n",
      "Epoch 14, Batch 971, LR 2.433178 Loss 6.502067, Accuracy 83.765%\n",
      "Epoch 14, Batch 972, LR 2.433135 Loss 6.501838, Accuracy 83.770%\n",
      "Epoch 14, Batch 973, LR 2.433091 Loss 6.500966, Accuracy 83.776%\n",
      "Epoch 14, Batch 974, LR 2.433048 Loss 6.500930, Accuracy 83.777%\n",
      "Epoch 14, Batch 975, LR 2.433005 Loss 6.501214, Accuracy 83.782%\n",
      "Epoch 14, Batch 976, LR 2.432962 Loss 6.500838, Accuracy 83.785%\n",
      "Epoch 14, Batch 977, LR 2.432918 Loss 6.500526, Accuracy 83.786%\n",
      "Epoch 14, Batch 978, LR 2.432875 Loss 6.500808, Accuracy 83.788%\n",
      "Epoch 14, Batch 979, LR 2.432832 Loss 6.500780, Accuracy 83.782%\n",
      "Epoch 14, Batch 980, LR 2.432788 Loss 6.501339, Accuracy 83.780%\n",
      "Epoch 14, Batch 981, LR 2.432745 Loss 6.501600, Accuracy 83.779%\n",
      "Epoch 14, Batch 982, LR 2.432702 Loss 6.501844, Accuracy 83.774%\n",
      "Epoch 14, Batch 983, LR 2.432658 Loss 6.501969, Accuracy 83.775%\n",
      "Epoch 14, Batch 984, LR 2.432615 Loss 6.501912, Accuracy 83.775%\n",
      "Epoch 14, Batch 985, LR 2.432571 Loss 6.502082, Accuracy 83.774%\n",
      "Epoch 14, Batch 986, LR 2.432528 Loss 6.501962, Accuracy 83.774%\n",
      "Epoch 14, Batch 987, LR 2.432485 Loss 6.501576, Accuracy 83.778%\n",
      "Epoch 14, Batch 988, LR 2.432441 Loss 6.501318, Accuracy 83.776%\n",
      "Epoch 14, Batch 989, LR 2.432398 Loss 6.500710, Accuracy 83.776%\n",
      "Epoch 14, Batch 990, LR 2.432354 Loss 6.500838, Accuracy 83.773%\n",
      "Epoch 14, Batch 991, LR 2.432311 Loss 6.500666, Accuracy 83.770%\n",
      "Epoch 14, Batch 992, LR 2.432267 Loss 6.500895, Accuracy 83.767%\n",
      "Epoch 14, Batch 993, LR 2.432224 Loss 6.500439, Accuracy 83.769%\n",
      "Epoch 14, Batch 994, LR 2.432180 Loss 6.500027, Accuracy 83.770%\n",
      "Epoch 14, Batch 995, LR 2.432137 Loss 6.500102, Accuracy 83.770%\n",
      "Epoch 14, Batch 996, LR 2.432093 Loss 6.500612, Accuracy 83.767%\n",
      "Epoch 14, Batch 997, LR 2.432050 Loss 6.500340, Accuracy 83.770%\n",
      "Epoch 14, Batch 998, LR 2.432006 Loss 6.500607, Accuracy 83.769%\n",
      "Epoch 14, Batch 999, LR 2.431963 Loss 6.501009, Accuracy 83.764%\n",
      "Epoch 14, Batch 1000, LR 2.431919 Loss 6.500378, Accuracy 83.767%\n",
      "Epoch 14, Batch 1001, LR 2.431875 Loss 6.500604, Accuracy 83.767%\n",
      "Epoch 14, Batch 1002, LR 2.431832 Loss 6.500696, Accuracy 83.769%\n",
      "Epoch 14, Batch 1003, LR 2.431788 Loss 6.500319, Accuracy 83.776%\n",
      "Epoch 14, Batch 1004, LR 2.431744 Loss 6.499800, Accuracy 83.781%\n",
      "Epoch 14, Batch 1005, LR 2.431701 Loss 6.499933, Accuracy 83.781%\n",
      "Epoch 14, Batch 1006, LR 2.431657 Loss 6.499850, Accuracy 83.780%\n",
      "Epoch 14, Batch 1007, LR 2.431613 Loss 6.500481, Accuracy 83.780%\n",
      "Epoch 14, Batch 1008, LR 2.431570 Loss 6.499829, Accuracy 83.781%\n",
      "Epoch 14, Batch 1009, LR 2.431526 Loss 6.500541, Accuracy 83.781%\n",
      "Epoch 14, Batch 1010, LR 2.431482 Loss 6.500536, Accuracy 83.783%\n",
      "Epoch 14, Batch 1011, LR 2.431438 Loss 6.500858, Accuracy 83.782%\n",
      "Epoch 14, Batch 1012, LR 2.431395 Loss 6.500197, Accuracy 83.785%\n",
      "Epoch 14, Batch 1013, LR 2.431351 Loss 6.500160, Accuracy 83.786%\n",
      "Epoch 14, Batch 1014, LR 2.431307 Loss 6.500030, Accuracy 83.787%\n",
      "Epoch 14, Batch 1015, LR 2.431263 Loss 6.499662, Accuracy 83.788%\n",
      "Epoch 14, Batch 1016, LR 2.431220 Loss 6.500034, Accuracy 83.784%\n",
      "Epoch 14, Batch 1017, LR 2.431176 Loss 6.499300, Accuracy 83.788%\n",
      "Epoch 14, Batch 1018, LR 2.431132 Loss 6.499293, Accuracy 83.786%\n",
      "Epoch 14, Batch 1019, LR 2.431088 Loss 6.499489, Accuracy 83.783%\n",
      "Epoch 14, Batch 1020, LR 2.431044 Loss 6.498662, Accuracy 83.785%\n",
      "Epoch 14, Batch 1021, LR 2.431000 Loss 6.498668, Accuracy 83.785%\n",
      "Epoch 14, Batch 1022, LR 2.430956 Loss 6.499091, Accuracy 83.785%\n",
      "Epoch 14, Batch 1023, LR 2.430912 Loss 6.498968, Accuracy 83.782%\n",
      "Epoch 14, Batch 1024, LR 2.430868 Loss 6.499127, Accuracy 83.781%\n",
      "Epoch 14, Batch 1025, LR 2.430825 Loss 6.499234, Accuracy 83.784%\n",
      "Epoch 14, Batch 1026, LR 2.430781 Loss 6.499399, Accuracy 83.782%\n",
      "Epoch 14, Batch 1027, LR 2.430737 Loss 6.499632, Accuracy 83.784%\n",
      "Epoch 14, Batch 1028, LR 2.430693 Loss 6.499654, Accuracy 83.782%\n",
      "Epoch 14, Batch 1029, LR 2.430649 Loss 6.499181, Accuracy 83.788%\n",
      "Epoch 14, Batch 1030, LR 2.430605 Loss 6.499537, Accuracy 83.788%\n",
      "Epoch 14, Batch 1031, LR 2.430561 Loss 6.499523, Accuracy 83.787%\n",
      "Epoch 14, Batch 1032, LR 2.430517 Loss 6.499981, Accuracy 83.783%\n",
      "Epoch 14, Batch 1033, LR 2.430473 Loss 6.500320, Accuracy 83.781%\n",
      "Epoch 14, Batch 1034, LR 2.430429 Loss 6.500418, Accuracy 83.782%\n",
      "Epoch 14, Batch 1035, LR 2.430384 Loss 6.500540, Accuracy 83.783%\n",
      "Epoch 14, Batch 1036, LR 2.430340 Loss 6.500080, Accuracy 83.785%\n",
      "Epoch 14, Batch 1037, LR 2.430296 Loss 6.500104, Accuracy 83.786%\n",
      "Epoch 14, Batch 1038, LR 2.430252 Loss 6.500391, Accuracy 83.781%\n",
      "Epoch 14, Batch 1039, LR 2.430208 Loss 6.500911, Accuracy 83.777%\n",
      "Epoch 14, Batch 1040, LR 2.430164 Loss 6.500984, Accuracy 83.780%\n",
      "Epoch 14, Batch 1041, LR 2.430120 Loss 6.500818, Accuracy 83.781%\n",
      "Epoch 14, Batch 1042, LR 2.430076 Loss 6.500757, Accuracy 83.780%\n",
      "Epoch 14, Batch 1043, LR 2.430031 Loss 6.500940, Accuracy 83.781%\n",
      "Epoch 14, Batch 1044, LR 2.429987 Loss 6.500874, Accuracy 83.779%\n",
      "Epoch 14, Batch 1045, LR 2.429943 Loss 6.500590, Accuracy 83.781%\n",
      "Epoch 14, Batch 1046, LR 2.429899 Loss 6.499905, Accuracy 83.783%\n",
      "Epoch 14, Batch 1047, LR 2.429855 Loss 6.500304, Accuracy 83.778%\n",
      "Epoch 14, Loss (train set) 6.500304, Accuracy (train set) 83.778%\n",
      "Epoch 14, Accuracy (validation set) 54.323%\n",
      "Epoch 15, Batch 1, LR 2.429810 Loss 6.340859, Accuracy 84.375%\n",
      "Epoch 15, Batch 2, LR 2.429766 Loss 6.336487, Accuracy 85.156%\n",
      "Epoch 15, Batch 3, LR 2.429722 Loss 6.318179, Accuracy 85.156%\n",
      "Epoch 15, Batch 4, LR 2.429677 Loss 6.229919, Accuracy 85.938%\n",
      "Epoch 15, Batch 5, LR 2.429633 Loss 6.242116, Accuracy 85.938%\n",
      "Epoch 15, Batch 6, LR 2.429589 Loss 6.293239, Accuracy 85.417%\n",
      "Epoch 15, Batch 7, LR 2.429544 Loss 6.302299, Accuracy 84.933%\n",
      "Epoch 15, Batch 8, LR 2.429500 Loss 6.224704, Accuracy 85.352%\n",
      "Epoch 15, Batch 9, LR 2.429456 Loss 6.232990, Accuracy 85.330%\n",
      "Epoch 15, Batch 10, LR 2.429411 Loss 6.191091, Accuracy 85.547%\n",
      "Epoch 15, Batch 11, LR 2.429367 Loss 6.087500, Accuracy 86.009%\n",
      "Epoch 15, Batch 12, LR 2.429323 Loss 6.081041, Accuracy 86.198%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 13, LR 2.429278 Loss 6.114646, Accuracy 85.998%\n",
      "Epoch 15, Batch 14, LR 2.429234 Loss 6.153829, Accuracy 85.435%\n",
      "Epoch 15, Batch 15, LR 2.429189 Loss 6.189344, Accuracy 85.156%\n",
      "Epoch 15, Batch 16, LR 2.429145 Loss 6.180209, Accuracy 85.156%\n",
      "Epoch 15, Batch 17, LR 2.429100 Loss 6.194665, Accuracy 85.110%\n",
      "Epoch 15, Batch 18, LR 2.429056 Loss 6.266235, Accuracy 84.722%\n",
      "Epoch 15, Batch 19, LR 2.429011 Loss 6.260438, Accuracy 84.539%\n",
      "Epoch 15, Batch 20, LR 2.428967 Loss 6.286304, Accuracy 84.727%\n",
      "Epoch 15, Batch 21, LR 2.428922 Loss 6.280310, Accuracy 84.821%\n",
      "Epoch 15, Batch 22, LR 2.428878 Loss 6.290259, Accuracy 84.766%\n",
      "Epoch 15, Batch 23, LR 2.428833 Loss 6.285826, Accuracy 84.647%\n",
      "Epoch 15, Batch 24, LR 2.428789 Loss 6.301139, Accuracy 84.603%\n",
      "Epoch 15, Batch 25, LR 2.428744 Loss 6.317107, Accuracy 84.562%\n",
      "Epoch 15, Batch 26, LR 2.428700 Loss 6.313068, Accuracy 84.706%\n",
      "Epoch 15, Batch 27, LR 2.428655 Loss 6.360304, Accuracy 84.520%\n",
      "Epoch 15, Batch 28, LR 2.428610 Loss 6.378474, Accuracy 84.263%\n",
      "Epoch 15, Batch 29, LR 2.428566 Loss 6.367029, Accuracy 84.402%\n",
      "Epoch 15, Batch 30, LR 2.428521 Loss 6.380598, Accuracy 84.245%\n",
      "Epoch 15, Batch 31, LR 2.428477 Loss 6.360975, Accuracy 84.400%\n",
      "Epoch 15, Batch 32, LR 2.428432 Loss 6.357448, Accuracy 84.473%\n",
      "Epoch 15, Batch 33, LR 2.428387 Loss 6.347776, Accuracy 84.541%\n",
      "Epoch 15, Batch 34, LR 2.428342 Loss 6.324287, Accuracy 84.582%\n",
      "Epoch 15, Batch 35, LR 2.428298 Loss 6.332539, Accuracy 84.554%\n",
      "Epoch 15, Batch 36, LR 2.428253 Loss 6.347637, Accuracy 84.418%\n",
      "Epoch 15, Batch 37, LR 2.428208 Loss 6.356996, Accuracy 84.354%\n",
      "Epoch 15, Batch 38, LR 2.428164 Loss 6.349560, Accuracy 84.457%\n",
      "Epoch 15, Batch 39, LR 2.428119 Loss 6.355036, Accuracy 84.415%\n",
      "Epoch 15, Batch 40, LR 2.428074 Loss 6.360984, Accuracy 84.453%\n",
      "Epoch 15, Batch 41, LR 2.428029 Loss 6.358482, Accuracy 84.470%\n",
      "Epoch 15, Batch 42, LR 2.427984 Loss 6.344999, Accuracy 84.580%\n",
      "Epoch 15, Batch 43, LR 2.427940 Loss 6.358183, Accuracy 84.557%\n",
      "Epoch 15, Batch 44, LR 2.427895 Loss 6.361291, Accuracy 84.499%\n",
      "Epoch 15, Batch 45, LR 2.427850 Loss 6.354861, Accuracy 84.549%\n",
      "Epoch 15, Batch 46, LR 2.427805 Loss 6.350474, Accuracy 84.630%\n",
      "Epoch 15, Batch 47, LR 2.427760 Loss 6.360202, Accuracy 84.574%\n",
      "Epoch 15, Batch 48, LR 2.427715 Loss 6.358256, Accuracy 84.603%\n",
      "Epoch 15, Batch 49, LR 2.427670 Loss 6.349152, Accuracy 84.678%\n",
      "Epoch 15, Batch 50, LR 2.427625 Loss 6.343179, Accuracy 84.719%\n",
      "Epoch 15, Batch 51, LR 2.427581 Loss 6.330961, Accuracy 84.835%\n",
      "Epoch 15, Batch 52, LR 2.427536 Loss 6.326151, Accuracy 84.841%\n",
      "Epoch 15, Batch 53, LR 2.427491 Loss 6.314908, Accuracy 84.891%\n",
      "Epoch 15, Batch 54, LR 2.427446 Loss 6.333375, Accuracy 84.823%\n",
      "Epoch 15, Batch 55, LR 2.427401 Loss 6.322733, Accuracy 84.844%\n",
      "Epoch 15, Batch 56, LR 2.427356 Loss 6.317160, Accuracy 84.877%\n",
      "Epoch 15, Batch 57, LR 2.427311 Loss 6.309902, Accuracy 84.978%\n",
      "Epoch 15, Batch 58, LR 2.427266 Loss 6.312257, Accuracy 85.022%\n",
      "Epoch 15, Batch 59, LR 2.427221 Loss 6.313158, Accuracy 84.931%\n",
      "Epoch 15, Batch 60, LR 2.427176 Loss 6.314644, Accuracy 84.922%\n",
      "Epoch 15, Batch 61, LR 2.427131 Loss 6.307170, Accuracy 84.951%\n",
      "Epoch 15, Batch 62, LR 2.427085 Loss 6.305932, Accuracy 84.980%\n",
      "Epoch 15, Batch 63, LR 2.427040 Loss 6.306704, Accuracy 84.983%\n",
      "Epoch 15, Batch 64, LR 2.426995 Loss 6.309438, Accuracy 85.010%\n",
      "Epoch 15, Batch 65, LR 2.426950 Loss 6.331077, Accuracy 84.868%\n",
      "Epoch 15, Batch 66, LR 2.426905 Loss 6.330383, Accuracy 84.860%\n",
      "Epoch 15, Batch 67, LR 2.426860 Loss 6.328355, Accuracy 84.841%\n",
      "Epoch 15, Batch 68, LR 2.426815 Loss 6.314535, Accuracy 84.915%\n",
      "Epoch 15, Batch 69, LR 2.426770 Loss 6.314866, Accuracy 84.918%\n",
      "Epoch 15, Batch 70, LR 2.426724 Loss 6.313790, Accuracy 84.911%\n",
      "Epoch 15, Batch 71, LR 2.426679 Loss 6.309293, Accuracy 84.914%\n",
      "Epoch 15, Batch 72, LR 2.426634 Loss 6.303136, Accuracy 84.939%\n",
      "Epoch 15, Batch 73, LR 2.426589 Loss 6.308965, Accuracy 84.899%\n",
      "Epoch 15, Batch 74, LR 2.426544 Loss 6.300478, Accuracy 84.882%\n",
      "Epoch 15, Batch 75, LR 2.426498 Loss 6.290524, Accuracy 84.958%\n",
      "Epoch 15, Batch 76, LR 2.426453 Loss 6.291814, Accuracy 84.920%\n",
      "Epoch 15, Batch 77, LR 2.426408 Loss 6.291992, Accuracy 84.903%\n",
      "Epoch 15, Batch 78, LR 2.426362 Loss 6.296315, Accuracy 84.866%\n",
      "Epoch 15, Batch 79, LR 2.426317 Loss 6.293781, Accuracy 84.860%\n",
      "Epoch 15, Batch 80, LR 2.426272 Loss 6.290329, Accuracy 84.863%\n",
      "Epoch 15, Batch 81, LR 2.426227 Loss 6.296672, Accuracy 84.790%\n",
      "Epoch 15, Batch 82, LR 2.426181 Loss 6.291007, Accuracy 84.794%\n",
      "Epoch 15, Batch 83, LR 2.426136 Loss 6.292873, Accuracy 84.799%\n",
      "Epoch 15, Batch 84, LR 2.426090 Loss 6.291445, Accuracy 84.794%\n",
      "Epoch 15, Batch 85, LR 2.426045 Loss 6.295305, Accuracy 84.752%\n",
      "Epoch 15, Batch 86, LR 2.426000 Loss 6.296128, Accuracy 84.738%\n",
      "Epoch 15, Batch 87, LR 2.425954 Loss 6.291831, Accuracy 84.788%\n",
      "Epoch 15, Batch 88, LR 2.425909 Loss 6.290173, Accuracy 84.837%\n",
      "Epoch 15, Batch 89, LR 2.425863 Loss 6.287970, Accuracy 84.893%\n",
      "Epoch 15, Batch 90, LR 2.425818 Loss 6.288480, Accuracy 84.896%\n",
      "Epoch 15, Batch 91, LR 2.425772 Loss 6.283431, Accuracy 84.924%\n",
      "Epoch 15, Batch 92, LR 2.425727 Loss 6.282855, Accuracy 84.952%\n",
      "Epoch 15, Batch 93, LR 2.425681 Loss 6.281292, Accuracy 84.971%\n",
      "Epoch 15, Batch 94, LR 2.425636 Loss 6.278088, Accuracy 85.040%\n",
      "Epoch 15, Batch 95, LR 2.425590 Loss 6.270906, Accuracy 85.066%\n",
      "Epoch 15, Batch 96, LR 2.425545 Loss 6.273110, Accuracy 85.018%\n",
      "Epoch 15, Batch 97, LR 2.425499 Loss 6.268678, Accuracy 85.068%\n",
      "Epoch 15, Batch 98, LR 2.425454 Loss 6.263091, Accuracy 85.085%\n",
      "Epoch 15, Batch 99, LR 2.425408 Loss 6.259875, Accuracy 85.117%\n",
      "Epoch 15, Batch 100, LR 2.425363 Loss 6.263297, Accuracy 85.062%\n",
      "Epoch 15, Batch 101, LR 2.425317 Loss 6.267545, Accuracy 85.025%\n",
      "Epoch 15, Batch 102, LR 2.425271 Loss 6.262935, Accuracy 85.011%\n",
      "Epoch 15, Batch 103, LR 2.425226 Loss 6.272164, Accuracy 84.997%\n",
      "Epoch 15, Batch 104, LR 2.425180 Loss 6.275554, Accuracy 84.953%\n",
      "Epoch 15, Batch 105, LR 2.425135 Loss 6.272123, Accuracy 84.993%\n",
      "Epoch 15, Batch 106, LR 2.425089 Loss 6.273015, Accuracy 85.001%\n",
      "Epoch 15, Batch 107, LR 2.425043 Loss 6.270562, Accuracy 85.003%\n",
      "Epoch 15, Batch 108, LR 2.424997 Loss 6.272511, Accuracy 85.004%\n",
      "Epoch 15, Batch 109, LR 2.424952 Loss 6.275049, Accuracy 84.977%\n",
      "Epoch 15, Batch 110, LR 2.424906 Loss 6.279875, Accuracy 84.915%\n",
      "Epoch 15, Batch 111, LR 2.424860 Loss 6.275942, Accuracy 84.952%\n",
      "Epoch 15, Batch 112, LR 2.424815 Loss 6.278492, Accuracy 84.982%\n",
      "Epoch 15, Batch 113, LR 2.424769 Loss 6.265984, Accuracy 85.011%\n",
      "Epoch 15, Batch 114, LR 2.424723 Loss 6.267686, Accuracy 84.971%\n",
      "Epoch 15, Batch 115, LR 2.424677 Loss 6.269391, Accuracy 85.041%\n",
      "Epoch 15, Batch 116, LR 2.424631 Loss 6.268047, Accuracy 85.055%\n",
      "Epoch 15, Batch 117, LR 2.424586 Loss 6.275980, Accuracy 85.009%\n",
      "Epoch 15, Batch 118, LR 2.424540 Loss 6.270391, Accuracy 85.011%\n",
      "Epoch 15, Batch 119, LR 2.424494 Loss 6.267973, Accuracy 85.032%\n",
      "Epoch 15, Batch 120, LR 2.424448 Loss 6.263480, Accuracy 85.059%\n",
      "Epoch 15, Batch 121, LR 2.424402 Loss 6.260432, Accuracy 85.072%\n",
      "Epoch 15, Batch 122, LR 2.424356 Loss 6.261704, Accuracy 85.092%\n",
      "Epoch 15, Batch 123, LR 2.424310 Loss 6.253461, Accuracy 85.105%\n",
      "Epoch 15, Batch 124, LR 2.424265 Loss 6.249776, Accuracy 85.112%\n",
      "Epoch 15, Batch 125, LR 2.424219 Loss 6.249218, Accuracy 85.112%\n",
      "Epoch 15, Batch 126, LR 2.424173 Loss 6.246382, Accuracy 85.138%\n",
      "Epoch 15, Batch 127, LR 2.424127 Loss 6.244335, Accuracy 85.138%\n",
      "Epoch 15, Batch 128, LR 2.424081 Loss 6.242490, Accuracy 85.120%\n",
      "Epoch 15, Batch 129, LR 2.424035 Loss 6.238929, Accuracy 85.138%\n",
      "Epoch 15, Batch 130, LR 2.423989 Loss 6.239280, Accuracy 85.084%\n",
      "Epoch 15, Batch 131, LR 2.423943 Loss 6.236910, Accuracy 85.055%\n",
      "Epoch 15, Batch 132, LR 2.423897 Loss 6.231992, Accuracy 85.079%\n",
      "Epoch 15, Batch 133, LR 2.423851 Loss 6.238425, Accuracy 85.021%\n",
      "Epoch 15, Batch 134, LR 2.423805 Loss 6.238563, Accuracy 85.034%\n",
      "Epoch 15, Batch 135, LR 2.423759 Loss 6.233129, Accuracy 85.064%\n",
      "Epoch 15, Batch 136, LR 2.423713 Loss 6.235240, Accuracy 85.047%\n",
      "Epoch 15, Batch 137, LR 2.423666 Loss 6.239537, Accuracy 84.985%\n",
      "Epoch 15, Batch 138, LR 2.423620 Loss 6.239851, Accuracy 84.952%\n",
      "Epoch 15, Batch 139, LR 2.423574 Loss 6.238235, Accuracy 84.976%\n",
      "Epoch 15, Batch 140, LR 2.423528 Loss 6.234522, Accuracy 84.983%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 141, LR 2.423482 Loss 6.231043, Accuracy 85.012%\n",
      "Epoch 15, Batch 142, LR 2.423436 Loss 6.232705, Accuracy 84.986%\n",
      "Epoch 15, Batch 143, LR 2.423390 Loss 6.230456, Accuracy 84.992%\n",
      "Epoch 15, Batch 144, LR 2.423343 Loss 6.226533, Accuracy 84.993%\n",
      "Epoch 15, Batch 145, LR 2.423297 Loss 6.224392, Accuracy 85.022%\n",
      "Epoch 15, Batch 146, LR 2.423251 Loss 6.223684, Accuracy 85.044%\n",
      "Epoch 15, Batch 147, LR 2.423205 Loss 6.225974, Accuracy 85.023%\n",
      "Epoch 15, Batch 148, LR 2.423159 Loss 6.227888, Accuracy 85.045%\n",
      "Epoch 15, Batch 149, LR 2.423112 Loss 6.222338, Accuracy 85.093%\n",
      "Epoch 15, Batch 150, LR 2.423066 Loss 6.224524, Accuracy 85.078%\n",
      "Epoch 15, Batch 151, LR 2.423020 Loss 6.224467, Accuracy 85.063%\n",
      "Epoch 15, Batch 152, LR 2.422974 Loss 6.223497, Accuracy 85.074%\n",
      "Epoch 15, Batch 153, LR 2.422927 Loss 6.222992, Accuracy 85.075%\n",
      "Epoch 15, Batch 154, LR 2.422881 Loss 6.225606, Accuracy 85.055%\n",
      "Epoch 15, Batch 155, LR 2.422835 Loss 6.223366, Accuracy 85.066%\n",
      "Epoch 15, Batch 156, LR 2.422788 Loss 6.223413, Accuracy 85.076%\n",
      "Epoch 15, Batch 157, LR 2.422742 Loss 6.224068, Accuracy 85.037%\n",
      "Epoch 15, Batch 158, LR 2.422696 Loss 6.228210, Accuracy 85.028%\n",
      "Epoch 15, Batch 159, LR 2.422649 Loss 6.228663, Accuracy 85.028%\n",
      "Epoch 15, Batch 160, LR 2.422603 Loss 6.233747, Accuracy 85.015%\n",
      "Epoch 15, Batch 161, LR 2.422556 Loss 6.235015, Accuracy 85.011%\n",
      "Epoch 15, Batch 162, LR 2.422510 Loss 6.233879, Accuracy 85.021%\n",
      "Epoch 15, Batch 163, LR 2.422463 Loss 6.234424, Accuracy 85.012%\n",
      "Epoch 15, Batch 164, LR 2.422417 Loss 6.233651, Accuracy 85.037%\n",
      "Epoch 15, Batch 165, LR 2.422371 Loss 6.230907, Accuracy 85.066%\n",
      "Epoch 15, Batch 166, LR 2.422324 Loss 6.230333, Accuracy 85.057%\n",
      "Epoch 15, Batch 167, LR 2.422278 Loss 6.234386, Accuracy 85.016%\n",
      "Epoch 15, Batch 168, LR 2.422231 Loss 6.234786, Accuracy 85.035%\n",
      "Epoch 15, Batch 169, LR 2.422185 Loss 6.236599, Accuracy 85.018%\n",
      "Epoch 15, Batch 170, LR 2.422138 Loss 6.235096, Accuracy 85.032%\n",
      "Epoch 15, Batch 171, LR 2.422091 Loss 6.238650, Accuracy 85.019%\n",
      "Epoch 15, Batch 172, LR 2.422045 Loss 6.238124, Accuracy 85.029%\n",
      "Epoch 15, Batch 173, LR 2.421998 Loss 6.237417, Accuracy 85.016%\n",
      "Epoch 15, Batch 174, LR 2.421952 Loss 6.236378, Accuracy 85.040%\n",
      "Epoch 15, Batch 175, LR 2.421905 Loss 6.238682, Accuracy 85.013%\n",
      "Epoch 15, Batch 176, LR 2.421859 Loss 6.238939, Accuracy 84.996%\n",
      "Epoch 15, Batch 177, LR 2.421812 Loss 6.241148, Accuracy 84.966%\n",
      "Epoch 15, Batch 178, LR 2.421765 Loss 6.242885, Accuracy 84.950%\n",
      "Epoch 15, Batch 179, LR 2.421719 Loss 6.240509, Accuracy 84.951%\n",
      "Epoch 15, Batch 180, LR 2.421672 Loss 6.245800, Accuracy 84.913%\n",
      "Epoch 15, Batch 181, LR 2.421625 Loss 6.244508, Accuracy 84.949%\n",
      "Epoch 15, Batch 182, LR 2.421579 Loss 6.245572, Accuracy 84.942%\n",
      "Epoch 15, Batch 183, LR 2.421532 Loss 6.246148, Accuracy 84.930%\n",
      "Epoch 15, Batch 184, LR 2.421485 Loss 6.250137, Accuracy 84.918%\n",
      "Epoch 15, Batch 185, LR 2.421438 Loss 6.252226, Accuracy 84.924%\n",
      "Epoch 15, Batch 186, LR 2.421392 Loss 6.249718, Accuracy 84.934%\n",
      "Epoch 15, Batch 187, LR 2.421345 Loss 6.250864, Accuracy 84.931%\n",
      "Epoch 15, Batch 188, LR 2.421298 Loss 6.250102, Accuracy 84.936%\n",
      "Epoch 15, Batch 189, LR 2.421251 Loss 6.248673, Accuracy 84.933%\n",
      "Epoch 15, Batch 190, LR 2.421205 Loss 6.244730, Accuracy 84.951%\n",
      "Epoch 15, Batch 191, LR 2.421158 Loss 6.246302, Accuracy 84.948%\n",
      "Epoch 15, Batch 192, LR 2.421111 Loss 6.244795, Accuracy 84.945%\n",
      "Epoch 15, Batch 193, LR 2.421064 Loss 6.247136, Accuracy 84.966%\n",
      "Epoch 15, Batch 194, LR 2.421017 Loss 6.247671, Accuracy 84.971%\n",
      "Epoch 15, Batch 195, LR 2.420970 Loss 6.245627, Accuracy 84.976%\n",
      "Epoch 15, Batch 196, LR 2.420923 Loss 6.250010, Accuracy 84.961%\n",
      "Epoch 15, Batch 197, LR 2.420877 Loss 6.251728, Accuracy 84.926%\n",
      "Epoch 15, Batch 198, LR 2.420830 Loss 6.252573, Accuracy 84.916%\n",
      "Epoch 15, Batch 199, LR 2.420783 Loss 6.249170, Accuracy 84.929%\n",
      "Epoch 15, Batch 200, LR 2.420736 Loss 6.249966, Accuracy 84.930%\n",
      "Epoch 15, Batch 201, LR 2.420689 Loss 6.250088, Accuracy 84.927%\n",
      "Epoch 15, Batch 202, LR 2.420642 Loss 6.251469, Accuracy 84.913%\n",
      "Epoch 15, Batch 203, LR 2.420595 Loss 6.256256, Accuracy 84.883%\n",
      "Epoch 15, Batch 204, LR 2.420548 Loss 6.255673, Accuracy 84.896%\n",
      "Epoch 15, Batch 205, LR 2.420501 Loss 6.255895, Accuracy 84.901%\n",
      "Epoch 15, Batch 206, LR 2.420454 Loss 6.255679, Accuracy 84.887%\n",
      "Epoch 15, Batch 207, LR 2.420407 Loss 6.258285, Accuracy 84.877%\n",
      "Epoch 15, Batch 208, LR 2.420360 Loss 6.261982, Accuracy 84.841%\n",
      "Epoch 15, Batch 209, LR 2.420313 Loss 6.263048, Accuracy 84.827%\n",
      "Epoch 15, Batch 210, LR 2.420266 Loss 6.262495, Accuracy 84.855%\n",
      "Epoch 15, Batch 211, LR 2.420219 Loss 6.262077, Accuracy 84.842%\n",
      "Epoch 15, Batch 212, LR 2.420172 Loss 6.261767, Accuracy 84.828%\n",
      "Epoch 15, Batch 213, LR 2.420124 Loss 6.261122, Accuracy 84.841%\n",
      "Epoch 15, Batch 214, LR 2.420077 Loss 6.262580, Accuracy 84.831%\n",
      "Epoch 15, Batch 215, LR 2.420030 Loss 6.263194, Accuracy 84.800%\n",
      "Epoch 15, Batch 216, LR 2.419983 Loss 6.264425, Accuracy 84.784%\n",
      "Epoch 15, Batch 217, LR 2.419936 Loss 6.267554, Accuracy 84.775%\n",
      "Epoch 15, Batch 218, LR 2.419889 Loss 6.266106, Accuracy 84.773%\n",
      "Epoch 15, Batch 219, LR 2.419842 Loss 6.267779, Accuracy 84.764%\n",
      "Epoch 15, Batch 220, LR 2.419794 Loss 6.268122, Accuracy 84.773%\n",
      "Epoch 15, Batch 221, LR 2.419747 Loss 6.265603, Accuracy 84.796%\n",
      "Epoch 15, Batch 222, LR 2.419700 Loss 6.263599, Accuracy 84.787%\n",
      "Epoch 15, Batch 223, LR 2.419653 Loss 6.265977, Accuracy 84.788%\n",
      "Epoch 15, Batch 224, LR 2.419605 Loss 6.266494, Accuracy 84.790%\n",
      "Epoch 15, Batch 225, LR 2.419558 Loss 6.266046, Accuracy 84.802%\n",
      "Epoch 15, Batch 226, LR 2.419511 Loss 6.264351, Accuracy 84.793%\n",
      "Epoch 15, Batch 227, LR 2.419464 Loss 6.263909, Accuracy 84.809%\n",
      "Epoch 15, Batch 228, LR 2.419416 Loss 6.266857, Accuracy 84.786%\n",
      "Epoch 15, Batch 229, LR 2.419369 Loss 6.265693, Accuracy 84.784%\n",
      "Epoch 15, Batch 230, LR 2.419322 Loss 6.263989, Accuracy 84.803%\n",
      "Epoch 15, Batch 231, LR 2.419274 Loss 6.263433, Accuracy 84.808%\n",
      "Epoch 15, Batch 232, LR 2.419227 Loss 6.266303, Accuracy 84.779%\n",
      "Epoch 15, Batch 233, LR 2.419179 Loss 6.266755, Accuracy 84.774%\n",
      "Epoch 15, Batch 234, LR 2.419132 Loss 6.265982, Accuracy 84.759%\n",
      "Epoch 15, Batch 235, LR 2.419085 Loss 6.268217, Accuracy 84.744%\n",
      "Epoch 15, Batch 236, LR 2.419037 Loss 6.264221, Accuracy 84.752%\n",
      "Epoch 15, Batch 237, LR 2.418990 Loss 6.266393, Accuracy 84.751%\n",
      "Epoch 15, Batch 238, LR 2.418942 Loss 6.263610, Accuracy 84.749%\n",
      "Epoch 15, Batch 239, LR 2.418895 Loss 6.261765, Accuracy 84.771%\n",
      "Epoch 15, Batch 240, LR 2.418847 Loss 6.262512, Accuracy 84.772%\n",
      "Epoch 15, Batch 241, LR 2.418800 Loss 6.260649, Accuracy 84.787%\n",
      "Epoch 15, Batch 242, LR 2.418752 Loss 6.261811, Accuracy 84.779%\n",
      "Epoch 15, Batch 243, LR 2.418705 Loss 6.264353, Accuracy 84.761%\n",
      "Epoch 15, Batch 244, LR 2.418657 Loss 6.264551, Accuracy 84.762%\n",
      "Epoch 15, Batch 245, LR 2.418610 Loss 6.267685, Accuracy 84.739%\n",
      "Epoch 15, Batch 246, LR 2.418562 Loss 6.267042, Accuracy 84.740%\n",
      "Epoch 15, Batch 247, LR 2.418515 Loss 6.265119, Accuracy 84.758%\n",
      "Epoch 15, Batch 248, LR 2.418467 Loss 6.267786, Accuracy 84.744%\n",
      "Epoch 15, Batch 249, LR 2.418420 Loss 6.265823, Accuracy 84.742%\n",
      "Epoch 15, Batch 250, LR 2.418372 Loss 6.265652, Accuracy 84.744%\n",
      "Epoch 15, Batch 251, LR 2.418324 Loss 6.266605, Accuracy 84.739%\n",
      "Epoch 15, Batch 252, LR 2.418277 Loss 6.269359, Accuracy 84.722%\n",
      "Epoch 15, Batch 253, LR 2.418229 Loss 6.268913, Accuracy 84.724%\n",
      "Epoch 15, Batch 254, LR 2.418181 Loss 6.270107, Accuracy 84.719%\n",
      "Epoch 15, Batch 255, LR 2.418134 Loss 6.268718, Accuracy 84.703%\n",
      "Epoch 15, Batch 256, LR 2.418086 Loss 6.267845, Accuracy 84.708%\n",
      "Epoch 15, Batch 257, LR 2.418038 Loss 6.266197, Accuracy 84.719%\n",
      "Epoch 15, Batch 258, LR 2.417991 Loss 6.267363, Accuracy 84.723%\n",
      "Epoch 15, Batch 259, LR 2.417943 Loss 6.266325, Accuracy 84.743%\n",
      "Epoch 15, Batch 260, LR 2.417895 Loss 6.265505, Accuracy 84.739%\n",
      "Epoch 15, Batch 261, LR 2.417847 Loss 6.263093, Accuracy 84.731%\n",
      "Epoch 15, Batch 262, LR 2.417800 Loss 6.266419, Accuracy 84.715%\n",
      "Epoch 15, Batch 263, LR 2.417752 Loss 6.268059, Accuracy 84.723%\n",
      "Epoch 15, Batch 264, LR 2.417704 Loss 6.267881, Accuracy 84.730%\n",
      "Epoch 15, Batch 265, LR 2.417656 Loss 6.265891, Accuracy 84.726%\n",
      "Epoch 15, Batch 266, LR 2.417608 Loss 6.263661, Accuracy 84.727%\n",
      "Epoch 15, Batch 267, LR 2.417561 Loss 6.263305, Accuracy 84.729%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 268, LR 2.417513 Loss 6.263621, Accuracy 84.736%\n",
      "Epoch 15, Batch 269, LR 2.417465 Loss 6.265809, Accuracy 84.703%\n",
      "Epoch 15, Batch 270, LR 2.417417 Loss 6.266265, Accuracy 84.702%\n",
      "Epoch 15, Batch 271, LR 2.417369 Loss 6.266465, Accuracy 84.701%\n",
      "Epoch 15, Batch 272, LR 2.417321 Loss 6.265386, Accuracy 84.697%\n",
      "Epoch 15, Batch 273, LR 2.417273 Loss 6.265054, Accuracy 84.696%\n",
      "Epoch 15, Batch 274, LR 2.417225 Loss 6.262782, Accuracy 84.714%\n",
      "Epoch 15, Batch 275, LR 2.417177 Loss 6.262972, Accuracy 84.716%\n",
      "Epoch 15, Batch 276, LR 2.417129 Loss 6.262090, Accuracy 84.723%\n",
      "Epoch 15, Batch 277, LR 2.417082 Loss 6.262622, Accuracy 84.708%\n",
      "Epoch 15, Batch 278, LR 2.417034 Loss 6.262237, Accuracy 84.715%\n",
      "Epoch 15, Batch 279, LR 2.416986 Loss 6.262415, Accuracy 84.722%\n",
      "Epoch 15, Batch 280, LR 2.416938 Loss 6.261458, Accuracy 84.721%\n",
      "Epoch 15, Batch 281, LR 2.416890 Loss 6.264620, Accuracy 84.695%\n",
      "Epoch 15, Batch 282, LR 2.416841 Loss 6.263169, Accuracy 84.713%\n",
      "Epoch 15, Batch 283, LR 2.416793 Loss 6.261705, Accuracy 84.720%\n",
      "Epoch 15, Batch 284, LR 2.416745 Loss 6.262327, Accuracy 84.713%\n",
      "Epoch 15, Batch 285, LR 2.416697 Loss 6.262261, Accuracy 84.715%\n",
      "Epoch 15, Batch 286, LR 2.416649 Loss 6.261656, Accuracy 84.706%\n",
      "Epoch 15, Batch 287, LR 2.416601 Loss 6.261323, Accuracy 84.718%\n",
      "Epoch 15, Batch 288, LR 2.416553 Loss 6.261542, Accuracy 84.725%\n",
      "Epoch 15, Batch 289, LR 2.416505 Loss 6.260402, Accuracy 84.729%\n",
      "Epoch 15, Batch 290, LR 2.416457 Loss 6.262363, Accuracy 84.720%\n",
      "Epoch 15, Batch 291, LR 2.416409 Loss 6.261948, Accuracy 84.724%\n",
      "Epoch 15, Batch 292, LR 2.416360 Loss 6.259187, Accuracy 84.747%\n",
      "Epoch 15, Batch 293, LR 2.416312 Loss 6.258583, Accuracy 84.748%\n",
      "Epoch 15, Batch 294, LR 2.416264 Loss 6.257986, Accuracy 84.752%\n",
      "Epoch 15, Batch 295, LR 2.416216 Loss 6.260495, Accuracy 84.740%\n",
      "Epoch 15, Batch 296, LR 2.416168 Loss 6.260694, Accuracy 84.742%\n",
      "Epoch 15, Batch 297, LR 2.416119 Loss 6.259480, Accuracy 84.754%\n",
      "Epoch 15, Batch 298, LR 2.416071 Loss 6.260036, Accuracy 84.745%\n",
      "Epoch 15, Batch 299, LR 2.416023 Loss 6.262499, Accuracy 84.733%\n",
      "Epoch 15, Batch 300, LR 2.415975 Loss 6.260630, Accuracy 84.747%\n",
      "Epoch 15, Batch 301, LR 2.415926 Loss 6.259562, Accuracy 84.757%\n",
      "Epoch 15, Batch 302, LR 2.415878 Loss 6.258671, Accuracy 84.771%\n",
      "Epoch 15, Batch 303, LR 2.415830 Loss 6.258532, Accuracy 84.775%\n",
      "Epoch 15, Batch 304, LR 2.415781 Loss 6.258916, Accuracy 84.773%\n",
      "Epoch 15, Batch 305, LR 2.415733 Loss 6.258837, Accuracy 84.785%\n",
      "Epoch 15, Batch 306, LR 2.415685 Loss 6.258809, Accuracy 84.783%\n",
      "Epoch 15, Batch 307, LR 2.415636 Loss 6.257270, Accuracy 84.785%\n",
      "Epoch 15, Batch 308, LR 2.415588 Loss 6.256813, Accuracy 84.778%\n",
      "Epoch 15, Batch 309, LR 2.415539 Loss 6.256954, Accuracy 84.772%\n",
      "Epoch 15, Batch 310, LR 2.415491 Loss 6.258280, Accuracy 84.761%\n",
      "Epoch 15, Batch 311, LR 2.415443 Loss 6.255585, Accuracy 84.767%\n",
      "Epoch 15, Batch 312, LR 2.415394 Loss 6.257890, Accuracy 84.761%\n",
      "Epoch 15, Batch 313, LR 2.415346 Loss 6.256179, Accuracy 84.767%\n",
      "Epoch 15, Batch 314, LR 2.415297 Loss 6.258083, Accuracy 84.746%\n",
      "Epoch 15, Batch 315, LR 2.415249 Loss 6.257992, Accuracy 84.735%\n",
      "Epoch 15, Batch 316, LR 2.415200 Loss 6.260836, Accuracy 84.721%\n",
      "Epoch 15, Batch 317, LR 2.415152 Loss 6.259840, Accuracy 84.735%\n",
      "Epoch 15, Batch 318, LR 2.415103 Loss 6.258120, Accuracy 84.748%\n",
      "Epoch 15, Batch 319, LR 2.415055 Loss 6.259683, Accuracy 84.740%\n",
      "Epoch 15, Batch 320, LR 2.415006 Loss 6.259025, Accuracy 84.751%\n",
      "Epoch 15, Batch 321, LR 2.414958 Loss 6.258106, Accuracy 84.760%\n",
      "Epoch 15, Batch 322, LR 2.414909 Loss 6.256757, Accuracy 84.768%\n",
      "Epoch 15, Batch 323, LR 2.414861 Loss 6.257319, Accuracy 84.764%\n",
      "Epoch 15, Batch 324, LR 2.414812 Loss 6.256403, Accuracy 84.773%\n",
      "Epoch 15, Batch 325, LR 2.414763 Loss 6.258402, Accuracy 84.764%\n",
      "Epoch 15, Batch 326, LR 2.414715 Loss 6.257169, Accuracy 84.770%\n",
      "Epoch 15, Batch 327, LR 2.414666 Loss 6.258861, Accuracy 84.762%\n",
      "Epoch 15, Batch 328, LR 2.414617 Loss 6.260576, Accuracy 84.739%\n",
      "Epoch 15, Batch 329, LR 2.414569 Loss 6.259999, Accuracy 84.748%\n",
      "Epoch 15, Batch 330, LR 2.414520 Loss 6.261032, Accuracy 84.744%\n",
      "Epoch 15, Batch 331, LR 2.414471 Loss 6.261663, Accuracy 84.736%\n",
      "Epoch 15, Batch 332, LR 2.414423 Loss 6.262119, Accuracy 84.726%\n",
      "Epoch 15, Batch 333, LR 2.414374 Loss 6.264910, Accuracy 84.715%\n",
      "Epoch 15, Batch 334, LR 2.414325 Loss 6.262477, Accuracy 84.733%\n",
      "Epoch 15, Batch 335, LR 2.414276 Loss 6.259323, Accuracy 84.748%\n",
      "Epoch 15, Batch 336, LR 2.414228 Loss 6.257827, Accuracy 84.759%\n",
      "Epoch 15, Batch 337, LR 2.414179 Loss 6.258116, Accuracy 84.748%\n",
      "Epoch 15, Batch 338, LR 2.414130 Loss 6.259165, Accuracy 84.745%\n",
      "Epoch 15, Batch 339, LR 2.414081 Loss 6.259949, Accuracy 84.739%\n",
      "Epoch 15, Batch 340, LR 2.414033 Loss 6.259894, Accuracy 84.747%\n",
      "Epoch 15, Batch 341, LR 2.413984 Loss 6.258940, Accuracy 84.758%\n",
      "Epoch 15, Batch 342, LR 2.413935 Loss 6.258136, Accuracy 84.772%\n",
      "Epoch 15, Batch 343, LR 2.413886 Loss 6.260185, Accuracy 84.758%\n",
      "Epoch 15, Batch 344, LR 2.413837 Loss 6.259369, Accuracy 84.766%\n",
      "Epoch 15, Batch 345, LR 2.413788 Loss 6.261474, Accuracy 84.758%\n",
      "Epoch 15, Batch 346, LR 2.413739 Loss 6.261705, Accuracy 84.750%\n",
      "Epoch 15, Batch 347, LR 2.413690 Loss 6.259864, Accuracy 84.751%\n",
      "Epoch 15, Batch 348, LR 2.413642 Loss 6.260329, Accuracy 84.748%\n",
      "Epoch 15, Batch 349, LR 2.413593 Loss 6.260481, Accuracy 84.751%\n",
      "Epoch 15, Batch 350, LR 2.413544 Loss 6.262417, Accuracy 84.730%\n",
      "Epoch 15, Batch 351, LR 2.413495 Loss 6.264750, Accuracy 84.722%\n",
      "Epoch 15, Batch 352, LR 2.413446 Loss 6.266543, Accuracy 84.715%\n",
      "Epoch 15, Batch 353, LR 2.413397 Loss 6.266330, Accuracy 84.707%\n",
      "Epoch 15, Batch 354, LR 2.413348 Loss 6.267954, Accuracy 84.717%\n",
      "Epoch 15, Batch 355, LR 2.413299 Loss 6.268206, Accuracy 84.712%\n",
      "Epoch 15, Batch 356, LR 2.413250 Loss 6.267342, Accuracy 84.728%\n",
      "Epoch 15, Batch 357, LR 2.413201 Loss 6.270201, Accuracy 84.714%\n",
      "Epoch 15, Batch 358, LR 2.413152 Loss 6.270203, Accuracy 84.709%\n",
      "Epoch 15, Batch 359, LR 2.413103 Loss 6.270569, Accuracy 84.712%\n",
      "Epoch 15, Batch 360, LR 2.413053 Loss 6.270839, Accuracy 84.722%\n",
      "Epoch 15, Batch 361, LR 2.413004 Loss 6.271232, Accuracy 84.713%\n",
      "Epoch 15, Batch 362, LR 2.412955 Loss 6.271005, Accuracy 84.716%\n",
      "Epoch 15, Batch 363, LR 2.412906 Loss 6.270135, Accuracy 84.726%\n",
      "Epoch 15, Batch 364, LR 2.412857 Loss 6.270304, Accuracy 84.723%\n",
      "Epoch 15, Batch 365, LR 2.412808 Loss 6.268816, Accuracy 84.732%\n",
      "Epoch 15, Batch 366, LR 2.412759 Loss 6.268591, Accuracy 84.744%\n",
      "Epoch 15, Batch 367, LR 2.412710 Loss 6.268628, Accuracy 84.743%\n",
      "Epoch 15, Batch 368, LR 2.412660 Loss 6.268095, Accuracy 84.751%\n",
      "Epoch 15, Batch 369, LR 2.412611 Loss 6.270099, Accuracy 84.746%\n",
      "Epoch 15, Batch 370, LR 2.412562 Loss 6.271363, Accuracy 84.740%\n",
      "Epoch 15, Batch 371, LR 2.412513 Loss 6.271514, Accuracy 84.741%\n",
      "Epoch 15, Batch 372, LR 2.412463 Loss 6.272764, Accuracy 84.740%\n",
      "Epoch 15, Batch 373, LR 2.412414 Loss 6.272989, Accuracy 84.731%\n",
      "Epoch 15, Batch 374, LR 2.412365 Loss 6.273222, Accuracy 84.724%\n",
      "Epoch 15, Batch 375, LR 2.412316 Loss 6.273195, Accuracy 84.727%\n",
      "Epoch 15, Batch 376, LR 2.412266 Loss 6.271373, Accuracy 84.734%\n",
      "Epoch 15, Batch 377, LR 2.412217 Loss 6.272950, Accuracy 84.723%\n",
      "Epoch 15, Batch 378, LR 2.412168 Loss 6.273853, Accuracy 84.706%\n",
      "Epoch 15, Batch 379, LR 2.412118 Loss 6.272792, Accuracy 84.707%\n",
      "Epoch 15, Batch 380, LR 2.412069 Loss 6.274022, Accuracy 84.698%\n",
      "Epoch 15, Batch 381, LR 2.412020 Loss 6.273678, Accuracy 84.707%\n",
      "Epoch 15, Batch 382, LR 2.411970 Loss 6.273128, Accuracy 84.710%\n",
      "Epoch 15, Batch 383, LR 2.411921 Loss 6.272425, Accuracy 84.707%\n",
      "Epoch 15, Batch 384, LR 2.411872 Loss 6.273585, Accuracy 84.692%\n",
      "Epoch 15, Batch 385, LR 2.411822 Loss 6.274531, Accuracy 84.692%\n",
      "Epoch 15, Batch 386, LR 2.411773 Loss 6.274273, Accuracy 84.699%\n",
      "Epoch 15, Batch 387, LR 2.411723 Loss 6.275357, Accuracy 84.696%\n",
      "Epoch 15, Batch 388, LR 2.411674 Loss 6.274854, Accuracy 84.697%\n",
      "Epoch 15, Batch 389, LR 2.411624 Loss 6.273465, Accuracy 84.702%\n",
      "Epoch 15, Batch 390, LR 2.411575 Loss 6.274824, Accuracy 84.694%\n",
      "Epoch 15, Batch 391, LR 2.411525 Loss 6.275391, Accuracy 84.693%\n",
      "Epoch 15, Batch 392, LR 2.411476 Loss 6.275665, Accuracy 84.682%\n",
      "Epoch 15, Batch 393, LR 2.411426 Loss 6.275527, Accuracy 84.689%\n",
      "Epoch 15, Batch 394, LR 2.411377 Loss 6.275950, Accuracy 84.680%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 395, LR 2.411327 Loss 6.274899, Accuracy 84.689%\n",
      "Epoch 15, Batch 396, LR 2.411278 Loss 6.275439, Accuracy 84.699%\n",
      "Epoch 15, Batch 397, LR 2.411228 Loss 6.275384, Accuracy 84.704%\n",
      "Epoch 15, Batch 398, LR 2.411179 Loss 6.276668, Accuracy 84.695%\n",
      "Epoch 15, Batch 399, LR 2.411129 Loss 6.278144, Accuracy 84.688%\n",
      "Epoch 15, Batch 400, LR 2.411079 Loss 6.279531, Accuracy 84.672%\n",
      "Epoch 15, Batch 401, LR 2.411030 Loss 6.279444, Accuracy 84.669%\n",
      "Epoch 15, Batch 402, LR 2.410980 Loss 6.279624, Accuracy 84.670%\n",
      "Epoch 15, Batch 403, LR 2.410930 Loss 6.280391, Accuracy 84.670%\n",
      "Epoch 15, Batch 404, LR 2.410881 Loss 6.281313, Accuracy 84.671%\n",
      "Epoch 15, Batch 405, LR 2.410831 Loss 6.281798, Accuracy 84.670%\n",
      "Epoch 15, Batch 406, LR 2.410781 Loss 6.280145, Accuracy 84.681%\n",
      "Epoch 15, Batch 407, LR 2.410732 Loss 6.279429, Accuracy 84.684%\n",
      "Epoch 15, Batch 408, LR 2.410682 Loss 6.279425, Accuracy 84.691%\n",
      "Epoch 15, Batch 409, LR 2.410632 Loss 6.279078, Accuracy 84.692%\n",
      "Epoch 15, Batch 410, LR 2.410583 Loss 6.279033, Accuracy 84.701%\n",
      "Epoch 15, Batch 411, LR 2.410533 Loss 6.279163, Accuracy 84.698%\n",
      "Epoch 15, Batch 412, LR 2.410483 Loss 6.279935, Accuracy 84.686%\n",
      "Epoch 15, Batch 413, LR 2.410433 Loss 6.280511, Accuracy 84.683%\n",
      "Epoch 15, Batch 414, LR 2.410383 Loss 6.281480, Accuracy 84.677%\n",
      "Epoch 15, Batch 415, LR 2.410334 Loss 6.281329, Accuracy 84.684%\n",
      "Epoch 15, Batch 416, LR 2.410284 Loss 6.283096, Accuracy 84.683%\n",
      "Epoch 15, Batch 417, LR 2.410234 Loss 6.283709, Accuracy 84.682%\n",
      "Epoch 15, Batch 418, LR 2.410184 Loss 6.284815, Accuracy 84.676%\n",
      "Epoch 15, Batch 419, LR 2.410134 Loss 6.284681, Accuracy 84.686%\n",
      "Epoch 15, Batch 420, LR 2.410084 Loss 6.285568, Accuracy 84.678%\n",
      "Epoch 15, Batch 421, LR 2.410034 Loss 6.284822, Accuracy 84.685%\n",
      "Epoch 15, Batch 422, LR 2.409985 Loss 6.285298, Accuracy 84.680%\n",
      "Epoch 15, Batch 423, LR 2.409935 Loss 6.285311, Accuracy 84.685%\n",
      "Epoch 15, Batch 424, LR 2.409885 Loss 6.283652, Accuracy 84.685%\n",
      "Epoch 15, Batch 425, LR 2.409835 Loss 6.283823, Accuracy 84.682%\n",
      "Epoch 15, Batch 426, LR 2.409785 Loss 6.283314, Accuracy 84.674%\n",
      "Epoch 15, Batch 427, LR 2.409735 Loss 6.282226, Accuracy 84.690%\n",
      "Epoch 15, Batch 428, LR 2.409685 Loss 6.281598, Accuracy 84.702%\n",
      "Epoch 15, Batch 429, LR 2.409635 Loss 6.282606, Accuracy 84.705%\n",
      "Epoch 15, Batch 430, LR 2.409585 Loss 6.283347, Accuracy 84.704%\n",
      "Epoch 15, Batch 431, LR 2.409535 Loss 6.281037, Accuracy 84.716%\n",
      "Epoch 15, Batch 432, LR 2.409485 Loss 6.279620, Accuracy 84.724%\n",
      "Epoch 15, Batch 433, LR 2.409435 Loss 6.280774, Accuracy 84.723%\n",
      "Epoch 15, Batch 434, LR 2.409385 Loss 6.279856, Accuracy 84.739%\n",
      "Epoch 15, Batch 435, LR 2.409335 Loss 6.281339, Accuracy 84.727%\n",
      "Epoch 15, Batch 436, LR 2.409284 Loss 6.282518, Accuracy 84.710%\n",
      "Epoch 15, Batch 437, LR 2.409234 Loss 6.281535, Accuracy 84.716%\n",
      "Epoch 15, Batch 438, LR 2.409184 Loss 6.282833, Accuracy 84.719%\n",
      "Epoch 15, Batch 439, LR 2.409134 Loss 6.280544, Accuracy 84.729%\n",
      "Epoch 15, Batch 440, LR 2.409084 Loss 6.280970, Accuracy 84.728%\n",
      "Epoch 15, Batch 441, LR 2.409034 Loss 6.282655, Accuracy 84.719%\n",
      "Epoch 15, Batch 442, LR 2.408984 Loss 6.283064, Accuracy 84.707%\n",
      "Epoch 15, Batch 443, LR 2.408933 Loss 6.281928, Accuracy 84.710%\n",
      "Epoch 15, Batch 444, LR 2.408883 Loss 6.281855, Accuracy 84.708%\n",
      "Epoch 15, Batch 445, LR 2.408833 Loss 6.281290, Accuracy 84.709%\n",
      "Epoch 15, Batch 446, LR 2.408783 Loss 6.281348, Accuracy 84.697%\n",
      "Epoch 15, Batch 447, LR 2.408733 Loss 6.282542, Accuracy 84.693%\n",
      "Epoch 15, Batch 448, LR 2.408682 Loss 6.283371, Accuracy 84.692%\n",
      "Epoch 15, Batch 449, LR 2.408632 Loss 6.283410, Accuracy 84.693%\n",
      "Epoch 15, Batch 450, LR 2.408582 Loss 6.283850, Accuracy 84.693%\n",
      "Epoch 15, Batch 451, LR 2.408531 Loss 6.283602, Accuracy 84.701%\n",
      "Epoch 15, Batch 452, LR 2.408481 Loss 6.281537, Accuracy 84.709%\n",
      "Epoch 15, Batch 453, LR 2.408431 Loss 6.280240, Accuracy 84.713%\n",
      "Epoch 15, Batch 454, LR 2.408381 Loss 6.280668, Accuracy 84.709%\n",
      "Epoch 15, Batch 455, LR 2.408330 Loss 6.279781, Accuracy 84.720%\n",
      "Epoch 15, Batch 456, LR 2.408280 Loss 6.279719, Accuracy 84.725%\n",
      "Epoch 15, Batch 457, LR 2.408229 Loss 6.279689, Accuracy 84.734%\n",
      "Epoch 15, Batch 458, LR 2.408179 Loss 6.278744, Accuracy 84.740%\n",
      "Epoch 15, Batch 459, LR 2.408129 Loss 6.279331, Accuracy 84.732%\n",
      "Epoch 15, Batch 460, LR 2.408078 Loss 6.279228, Accuracy 84.733%\n",
      "Epoch 15, Batch 461, LR 2.408028 Loss 6.279450, Accuracy 84.716%\n",
      "Epoch 15, Batch 462, LR 2.407977 Loss 6.279208, Accuracy 84.713%\n",
      "Epoch 15, Batch 463, LR 2.407927 Loss 6.278894, Accuracy 84.719%\n",
      "Epoch 15, Batch 464, LR 2.407876 Loss 6.279136, Accuracy 84.718%\n",
      "Epoch 15, Batch 465, LR 2.407826 Loss 6.279018, Accuracy 84.723%\n",
      "Epoch 15, Batch 466, LR 2.407775 Loss 6.281902, Accuracy 84.709%\n",
      "Epoch 15, Batch 467, LR 2.407725 Loss 6.281263, Accuracy 84.710%\n",
      "Epoch 15, Batch 468, LR 2.407674 Loss 6.280925, Accuracy 84.704%\n",
      "Epoch 15, Batch 469, LR 2.407624 Loss 6.280537, Accuracy 84.708%\n",
      "Epoch 15, Batch 470, LR 2.407573 Loss 6.279628, Accuracy 84.717%\n",
      "Epoch 15, Batch 471, LR 2.407523 Loss 6.280016, Accuracy 84.723%\n",
      "Epoch 15, Batch 472, LR 2.407472 Loss 6.279889, Accuracy 84.719%\n",
      "Epoch 15, Batch 473, LR 2.407422 Loss 6.280024, Accuracy 84.717%\n",
      "Epoch 15, Batch 474, LR 2.407371 Loss 6.280486, Accuracy 84.716%\n",
      "Epoch 15, Batch 475, LR 2.407320 Loss 6.281596, Accuracy 84.704%\n",
      "Epoch 15, Batch 476, LR 2.407270 Loss 6.279940, Accuracy 84.711%\n",
      "Epoch 15, Batch 477, LR 2.407219 Loss 6.279311, Accuracy 84.707%\n",
      "Epoch 15, Batch 478, LR 2.407169 Loss 6.279546, Accuracy 84.707%\n",
      "Epoch 15, Batch 479, LR 2.407118 Loss 6.280445, Accuracy 84.704%\n",
      "Epoch 15, Batch 480, LR 2.407067 Loss 6.280615, Accuracy 84.704%\n",
      "Epoch 15, Batch 481, LR 2.407017 Loss 6.280594, Accuracy 84.701%\n",
      "Epoch 15, Batch 482, LR 2.406966 Loss 6.279207, Accuracy 84.711%\n",
      "Epoch 15, Batch 483, LR 2.406915 Loss 6.279765, Accuracy 84.695%\n",
      "Epoch 15, Batch 484, LR 2.406864 Loss 6.279339, Accuracy 84.703%\n",
      "Epoch 15, Batch 485, LR 2.406814 Loss 6.279303, Accuracy 84.697%\n",
      "Epoch 15, Batch 486, LR 2.406763 Loss 6.279202, Accuracy 84.703%\n",
      "Epoch 15, Batch 487, LR 2.406712 Loss 6.281388, Accuracy 84.686%\n",
      "Epoch 15, Batch 488, LR 2.406661 Loss 6.282751, Accuracy 84.678%\n",
      "Epoch 15, Batch 489, LR 2.406611 Loss 6.282395, Accuracy 84.682%\n",
      "Epoch 15, Batch 490, LR 2.406560 Loss 6.282170, Accuracy 84.681%\n",
      "Epoch 15, Batch 491, LR 2.406509 Loss 6.281096, Accuracy 84.682%\n",
      "Epoch 15, Batch 492, LR 2.406458 Loss 6.281380, Accuracy 84.686%\n",
      "Epoch 15, Batch 493, LR 2.406407 Loss 6.281488, Accuracy 84.682%\n",
      "Epoch 15, Batch 494, LR 2.406356 Loss 6.282303, Accuracy 84.675%\n",
      "Epoch 15, Batch 495, LR 2.406305 Loss 6.282303, Accuracy 84.680%\n",
      "Epoch 15, Batch 496, LR 2.406255 Loss 6.282182, Accuracy 84.688%\n",
      "Epoch 15, Batch 497, LR 2.406204 Loss 6.281568, Accuracy 84.696%\n",
      "Epoch 15, Batch 498, LR 2.406153 Loss 6.281378, Accuracy 84.695%\n",
      "Epoch 15, Batch 499, LR 2.406102 Loss 6.280579, Accuracy 84.702%\n",
      "Epoch 15, Batch 500, LR 2.406051 Loss 6.280203, Accuracy 84.705%\n",
      "Epoch 15, Batch 501, LR 2.406000 Loss 6.279430, Accuracy 84.710%\n",
      "Epoch 15, Batch 502, LR 2.405949 Loss 6.279029, Accuracy 84.705%\n",
      "Epoch 15, Batch 503, LR 2.405898 Loss 6.279230, Accuracy 84.704%\n",
      "Epoch 15, Batch 504, LR 2.405847 Loss 6.278022, Accuracy 84.707%\n",
      "Epoch 15, Batch 505, LR 2.405796 Loss 6.278962, Accuracy 84.703%\n",
      "Epoch 15, Batch 506, LR 2.405745 Loss 6.279033, Accuracy 84.705%\n",
      "Epoch 15, Batch 507, LR 2.405694 Loss 6.279240, Accuracy 84.699%\n",
      "Epoch 15, Batch 508, LR 2.405643 Loss 6.278954, Accuracy 84.698%\n",
      "Epoch 15, Batch 509, LR 2.405592 Loss 6.279078, Accuracy 84.691%\n",
      "Epoch 15, Batch 510, LR 2.405541 Loss 6.278644, Accuracy 84.686%\n",
      "Epoch 15, Batch 511, LR 2.405490 Loss 6.278848, Accuracy 84.679%\n",
      "Epoch 15, Batch 512, LR 2.405439 Loss 6.278979, Accuracy 84.674%\n",
      "Epoch 15, Batch 513, LR 2.405387 Loss 6.279740, Accuracy 84.672%\n",
      "Epoch 15, Batch 514, LR 2.405336 Loss 6.279415, Accuracy 84.676%\n",
      "Epoch 15, Batch 515, LR 2.405285 Loss 6.280226, Accuracy 84.672%\n",
      "Epoch 15, Batch 516, LR 2.405234 Loss 6.280864, Accuracy 84.660%\n",
      "Epoch 15, Batch 517, LR 2.405183 Loss 6.281460, Accuracy 84.650%\n",
      "Epoch 15, Batch 518, LR 2.405132 Loss 6.281028, Accuracy 84.649%\n",
      "Epoch 15, Batch 519, LR 2.405080 Loss 6.281761, Accuracy 84.637%\n",
      "Epoch 15, Batch 520, LR 2.405029 Loss 6.282091, Accuracy 84.632%\n",
      "Epoch 15, Batch 521, LR 2.404978 Loss 6.282752, Accuracy 84.631%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 522, LR 2.404927 Loss 6.281899, Accuracy 84.638%\n",
      "Epoch 15, Batch 523, LR 2.404876 Loss 6.283309, Accuracy 84.629%\n",
      "Epoch 15, Batch 524, LR 2.404824 Loss 6.284467, Accuracy 84.622%\n",
      "Epoch 15, Batch 525, LR 2.404773 Loss 6.283722, Accuracy 84.625%\n",
      "Epoch 15, Batch 526, LR 2.404722 Loss 6.283910, Accuracy 84.626%\n",
      "Epoch 15, Batch 527, LR 2.404670 Loss 6.284964, Accuracy 84.623%\n",
      "Epoch 15, Batch 528, LR 2.404619 Loss 6.285826, Accuracy 84.616%\n",
      "Epoch 15, Batch 529, LR 2.404568 Loss 6.286722, Accuracy 84.613%\n",
      "Epoch 15, Batch 530, LR 2.404516 Loss 6.288115, Accuracy 84.601%\n",
      "Epoch 15, Batch 531, LR 2.404465 Loss 6.288187, Accuracy 84.607%\n",
      "Epoch 15, Batch 532, LR 2.404414 Loss 6.288120, Accuracy 84.613%\n",
      "Epoch 15, Batch 533, LR 2.404362 Loss 6.287504, Accuracy 84.618%\n",
      "Epoch 15, Batch 534, LR 2.404311 Loss 6.286247, Accuracy 84.619%\n",
      "Epoch 15, Batch 535, LR 2.404260 Loss 6.286591, Accuracy 84.622%\n",
      "Epoch 15, Batch 536, LR 2.404208 Loss 6.284962, Accuracy 84.629%\n",
      "Epoch 15, Batch 537, LR 2.404157 Loss 6.284674, Accuracy 84.630%\n",
      "Epoch 15, Batch 538, LR 2.404105 Loss 6.283980, Accuracy 84.636%\n",
      "Epoch 15, Batch 539, LR 2.404054 Loss 6.284307, Accuracy 84.636%\n",
      "Epoch 15, Batch 540, LR 2.404002 Loss 6.284433, Accuracy 84.633%\n",
      "Epoch 15, Batch 541, LR 2.403951 Loss 6.284155, Accuracy 84.633%\n",
      "Epoch 15, Batch 542, LR 2.403899 Loss 6.283397, Accuracy 84.642%\n",
      "Epoch 15, Batch 543, LR 2.403848 Loss 6.282677, Accuracy 84.651%\n",
      "Epoch 15, Batch 544, LR 2.403796 Loss 6.282635, Accuracy 84.649%\n",
      "Epoch 15, Batch 545, LR 2.403745 Loss 6.282336, Accuracy 84.652%\n",
      "Epoch 15, Batch 546, LR 2.403693 Loss 6.280608, Accuracy 84.664%\n",
      "Epoch 15, Batch 547, LR 2.403642 Loss 6.280619, Accuracy 84.669%\n",
      "Epoch 15, Batch 548, LR 2.403590 Loss 6.279856, Accuracy 84.676%\n",
      "Epoch 15, Batch 549, LR 2.403538 Loss 6.281097, Accuracy 84.672%\n",
      "Epoch 15, Batch 550, LR 2.403487 Loss 6.282097, Accuracy 84.672%\n",
      "Epoch 15, Batch 551, LR 2.403435 Loss 6.283265, Accuracy 84.666%\n",
      "Epoch 15, Batch 552, LR 2.403384 Loss 6.283742, Accuracy 84.655%\n",
      "Epoch 15, Batch 553, LR 2.403332 Loss 6.283149, Accuracy 84.652%\n",
      "Epoch 15, Batch 554, LR 2.403280 Loss 6.283463, Accuracy 84.650%\n",
      "Epoch 15, Batch 555, LR 2.403229 Loss 6.283714, Accuracy 84.641%\n",
      "Epoch 15, Batch 556, LR 2.403177 Loss 6.284353, Accuracy 84.631%\n",
      "Epoch 15, Batch 557, LR 2.403125 Loss 6.284791, Accuracy 84.623%\n",
      "Epoch 15, Batch 558, LR 2.403073 Loss 6.284984, Accuracy 84.620%\n",
      "Epoch 15, Batch 559, LR 2.403022 Loss 6.284854, Accuracy 84.620%\n",
      "Epoch 15, Batch 560, LR 2.402970 Loss 6.285009, Accuracy 84.615%\n",
      "Epoch 15, Batch 561, LR 2.402918 Loss 6.285695, Accuracy 84.617%\n",
      "Epoch 15, Batch 562, LR 2.402866 Loss 6.285455, Accuracy 84.618%\n",
      "Epoch 15, Batch 563, LR 2.402815 Loss 6.284379, Accuracy 84.619%\n",
      "Epoch 15, Batch 564, LR 2.402763 Loss 6.283525, Accuracy 84.617%\n",
      "Epoch 15, Batch 565, LR 2.402711 Loss 6.282836, Accuracy 84.623%\n",
      "Epoch 15, Batch 566, LR 2.402659 Loss 6.282572, Accuracy 84.625%\n",
      "Epoch 15, Batch 567, LR 2.402607 Loss 6.280481, Accuracy 84.634%\n",
      "Epoch 15, Batch 568, LR 2.402556 Loss 6.280300, Accuracy 84.635%\n",
      "Epoch 15, Batch 569, LR 2.402504 Loss 6.279552, Accuracy 84.644%\n",
      "Epoch 15, Batch 570, LR 2.402452 Loss 6.279939, Accuracy 84.641%\n",
      "Epoch 15, Batch 571, LR 2.402400 Loss 6.280761, Accuracy 84.640%\n",
      "Epoch 15, Batch 572, LR 2.402348 Loss 6.281004, Accuracy 84.633%\n",
      "Epoch 15, Batch 573, LR 2.402296 Loss 6.280163, Accuracy 84.634%\n",
      "Epoch 15, Batch 574, LR 2.402244 Loss 6.280511, Accuracy 84.634%\n",
      "Epoch 15, Batch 575, LR 2.402192 Loss 6.279961, Accuracy 84.641%\n",
      "Epoch 15, Batch 576, LR 2.402140 Loss 6.279853, Accuracy 84.638%\n",
      "Epoch 15, Batch 577, LR 2.402088 Loss 6.280245, Accuracy 84.634%\n",
      "Epoch 15, Batch 578, LR 2.402036 Loss 6.279767, Accuracy 84.637%\n",
      "Epoch 15, Batch 579, LR 2.401984 Loss 6.278501, Accuracy 84.648%\n",
      "Epoch 15, Batch 580, LR 2.401932 Loss 6.277674, Accuracy 84.650%\n",
      "Epoch 15, Batch 581, LR 2.401880 Loss 6.277153, Accuracy 84.657%\n",
      "Epoch 15, Batch 582, LR 2.401828 Loss 6.277972, Accuracy 84.654%\n",
      "Epoch 15, Batch 583, LR 2.401776 Loss 6.276862, Accuracy 84.663%\n",
      "Epoch 15, Batch 584, LR 2.401724 Loss 6.277555, Accuracy 84.663%\n",
      "Epoch 15, Batch 585, LR 2.401672 Loss 6.276604, Accuracy 84.670%\n",
      "Epoch 15, Batch 586, LR 2.401620 Loss 6.276502, Accuracy 84.675%\n",
      "Epoch 15, Batch 587, LR 2.401568 Loss 6.278125, Accuracy 84.672%\n",
      "Epoch 15, Batch 588, LR 2.401516 Loss 6.277615, Accuracy 84.671%\n",
      "Epoch 15, Batch 589, LR 2.401464 Loss 6.279736, Accuracy 84.664%\n",
      "Epoch 15, Batch 590, LR 2.401412 Loss 6.278511, Accuracy 84.674%\n",
      "Epoch 15, Batch 591, LR 2.401360 Loss 6.278936, Accuracy 84.668%\n",
      "Epoch 15, Batch 592, LR 2.401307 Loss 6.279404, Accuracy 84.665%\n",
      "Epoch 15, Batch 593, LR 2.401255 Loss 6.279270, Accuracy 84.666%\n",
      "Epoch 15, Batch 594, LR 2.401203 Loss 6.277692, Accuracy 84.668%\n",
      "Epoch 15, Batch 595, LR 2.401151 Loss 6.278667, Accuracy 84.664%\n",
      "Epoch 15, Batch 596, LR 2.401099 Loss 6.278638, Accuracy 84.666%\n",
      "Epoch 15, Batch 597, LR 2.401046 Loss 6.278487, Accuracy 84.663%\n",
      "Epoch 15, Batch 598, LR 2.400994 Loss 6.277707, Accuracy 84.670%\n",
      "Epoch 15, Batch 599, LR 2.400942 Loss 6.277997, Accuracy 84.671%\n",
      "Epoch 15, Batch 600, LR 2.400890 Loss 6.278612, Accuracy 84.671%\n",
      "Epoch 15, Batch 601, LR 2.400837 Loss 6.279624, Accuracy 84.666%\n",
      "Epoch 15, Batch 602, LR 2.400785 Loss 6.279517, Accuracy 84.668%\n",
      "Epoch 15, Batch 603, LR 2.400733 Loss 6.279490, Accuracy 84.679%\n",
      "Epoch 15, Batch 604, LR 2.400680 Loss 6.278564, Accuracy 84.688%\n",
      "Epoch 15, Batch 605, LR 2.400628 Loss 6.278377, Accuracy 84.688%\n",
      "Epoch 15, Batch 606, LR 2.400576 Loss 6.278625, Accuracy 84.684%\n",
      "Epoch 15, Batch 607, LR 2.400523 Loss 6.278124, Accuracy 84.686%\n",
      "Epoch 15, Batch 608, LR 2.400471 Loss 6.277999, Accuracy 84.687%\n",
      "Epoch 15, Batch 609, LR 2.400419 Loss 6.277407, Accuracy 84.693%\n",
      "Epoch 15, Batch 610, LR 2.400366 Loss 6.277528, Accuracy 84.693%\n",
      "Epoch 15, Batch 611, LR 2.400314 Loss 6.278131, Accuracy 84.691%\n",
      "Epoch 15, Batch 612, LR 2.400261 Loss 6.277887, Accuracy 84.692%\n",
      "Epoch 15, Batch 613, LR 2.400209 Loss 6.276720, Accuracy 84.701%\n",
      "Epoch 15, Batch 614, LR 2.400156 Loss 6.278501, Accuracy 84.688%\n",
      "Epoch 15, Batch 615, LR 2.400104 Loss 6.279513, Accuracy 84.681%\n",
      "Epoch 15, Batch 616, LR 2.400052 Loss 6.278530, Accuracy 84.684%\n",
      "Epoch 15, Batch 617, LR 2.399999 Loss 6.279079, Accuracy 84.675%\n",
      "Epoch 15, Batch 618, LR 2.399947 Loss 6.279226, Accuracy 84.670%\n",
      "Epoch 15, Batch 619, LR 2.399894 Loss 6.279997, Accuracy 84.663%\n",
      "Epoch 15, Batch 620, LR 2.399841 Loss 6.280167, Accuracy 84.661%\n",
      "Epoch 15, Batch 621, LR 2.399789 Loss 6.281278, Accuracy 84.657%\n",
      "Epoch 15, Batch 622, LR 2.399736 Loss 6.281198, Accuracy 84.656%\n",
      "Epoch 15, Batch 623, LR 2.399684 Loss 6.281204, Accuracy 84.655%\n",
      "Epoch 15, Batch 624, LR 2.399631 Loss 6.282509, Accuracy 84.645%\n",
      "Epoch 15, Batch 625, LR 2.399579 Loss 6.281361, Accuracy 84.647%\n",
      "Epoch 15, Batch 626, LR 2.399526 Loss 6.280249, Accuracy 84.653%\n",
      "Epoch 15, Batch 627, LR 2.399473 Loss 6.281402, Accuracy 84.648%\n",
      "Epoch 15, Batch 628, LR 2.399421 Loss 6.281405, Accuracy 84.651%\n",
      "Epoch 15, Batch 629, LR 2.399368 Loss 6.279930, Accuracy 84.663%\n",
      "Epoch 15, Batch 630, LR 2.399315 Loss 6.281408, Accuracy 84.649%\n",
      "Epoch 15, Batch 631, LR 2.399263 Loss 6.281764, Accuracy 84.647%\n",
      "Epoch 15, Batch 632, LR 2.399210 Loss 6.280532, Accuracy 84.652%\n",
      "Epoch 15, Batch 633, LR 2.399157 Loss 6.281827, Accuracy 84.645%\n",
      "Epoch 15, Batch 634, LR 2.399105 Loss 6.281954, Accuracy 84.641%\n",
      "Epoch 15, Batch 635, LR 2.399052 Loss 6.281973, Accuracy 84.641%\n",
      "Epoch 15, Batch 636, LR 2.398999 Loss 6.280232, Accuracy 84.643%\n",
      "Epoch 15, Batch 637, LR 2.398946 Loss 6.280320, Accuracy 84.640%\n",
      "Epoch 15, Batch 638, LR 2.398894 Loss 6.280383, Accuracy 84.646%\n",
      "Epoch 15, Batch 639, LR 2.398841 Loss 6.280596, Accuracy 84.648%\n",
      "Epoch 15, Batch 640, LR 2.398788 Loss 6.280784, Accuracy 84.644%\n",
      "Epoch 15, Batch 641, LR 2.398735 Loss 6.281270, Accuracy 84.641%\n",
      "Epoch 15, Batch 642, LR 2.398682 Loss 6.281555, Accuracy 84.638%\n",
      "Epoch 15, Batch 643, LR 2.398630 Loss 6.281262, Accuracy 84.641%\n",
      "Epoch 15, Batch 644, LR 2.398577 Loss 6.281768, Accuracy 84.638%\n",
      "Epoch 15, Batch 645, LR 2.398524 Loss 6.281406, Accuracy 84.641%\n",
      "Epoch 15, Batch 646, LR 2.398471 Loss 6.281092, Accuracy 84.639%\n",
      "Epoch 15, Batch 647, LR 2.398418 Loss 6.281978, Accuracy 84.632%\n",
      "Epoch 15, Batch 648, LR 2.398365 Loss 6.282498, Accuracy 84.632%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 649, LR 2.398312 Loss 6.283442, Accuracy 84.628%\n",
      "Epoch 15, Batch 650, LR 2.398259 Loss 6.283312, Accuracy 84.631%\n",
      "Epoch 15, Batch 651, LR 2.398206 Loss 6.283550, Accuracy 84.625%\n",
      "Epoch 15, Batch 652, LR 2.398153 Loss 6.283535, Accuracy 84.625%\n",
      "Epoch 15, Batch 653, LR 2.398101 Loss 6.283150, Accuracy 84.627%\n",
      "Epoch 15, Batch 654, LR 2.398048 Loss 6.283146, Accuracy 84.625%\n",
      "Epoch 15, Batch 655, LR 2.397995 Loss 6.283290, Accuracy 84.623%\n",
      "Epoch 15, Batch 656, LR 2.397942 Loss 6.283894, Accuracy 84.613%\n",
      "Epoch 15, Batch 657, LR 2.397889 Loss 6.283597, Accuracy 84.616%\n",
      "Epoch 15, Batch 658, LR 2.397835 Loss 6.283235, Accuracy 84.628%\n",
      "Epoch 15, Batch 659, LR 2.397782 Loss 6.283561, Accuracy 84.623%\n",
      "Epoch 15, Batch 660, LR 2.397729 Loss 6.284593, Accuracy 84.615%\n",
      "Epoch 15, Batch 661, LR 2.397676 Loss 6.285390, Accuracy 84.616%\n",
      "Epoch 15, Batch 662, LR 2.397623 Loss 6.285265, Accuracy 84.613%\n",
      "Epoch 15, Batch 663, LR 2.397570 Loss 6.285847, Accuracy 84.609%\n",
      "Epoch 15, Batch 664, LR 2.397517 Loss 6.285612, Accuracy 84.611%\n",
      "Epoch 15, Batch 665, LR 2.397464 Loss 6.285961, Accuracy 84.611%\n",
      "Epoch 15, Batch 666, LR 2.397411 Loss 6.287140, Accuracy 84.604%\n",
      "Epoch 15, Batch 667, LR 2.397358 Loss 6.286641, Accuracy 84.609%\n",
      "Epoch 15, Batch 668, LR 2.397304 Loss 6.286125, Accuracy 84.612%\n",
      "Epoch 15, Batch 669, LR 2.397251 Loss 6.285112, Accuracy 84.614%\n",
      "Epoch 15, Batch 670, LR 2.397198 Loss 6.285698, Accuracy 84.611%\n",
      "Epoch 15, Batch 671, LR 2.397145 Loss 6.285523, Accuracy 84.608%\n",
      "Epoch 15, Batch 672, LR 2.397092 Loss 6.286237, Accuracy 84.608%\n",
      "Epoch 15, Batch 673, LR 2.397038 Loss 6.286872, Accuracy 84.605%\n",
      "Epoch 15, Batch 674, LR 2.396985 Loss 6.286852, Accuracy 84.603%\n",
      "Epoch 15, Batch 675, LR 2.396932 Loss 6.286153, Accuracy 84.604%\n",
      "Epoch 15, Batch 676, LR 2.396879 Loss 6.285662, Accuracy 84.599%\n",
      "Epoch 15, Batch 677, LR 2.396825 Loss 6.286137, Accuracy 84.591%\n",
      "Epoch 15, Batch 678, LR 2.396772 Loss 6.286496, Accuracy 84.585%\n",
      "Epoch 15, Batch 679, LR 2.396719 Loss 6.286092, Accuracy 84.589%\n",
      "Epoch 15, Batch 680, LR 2.396665 Loss 6.285700, Accuracy 84.590%\n",
      "Epoch 15, Batch 681, LR 2.396612 Loss 6.286711, Accuracy 84.591%\n",
      "Epoch 15, Batch 682, LR 2.396559 Loss 6.287196, Accuracy 84.588%\n",
      "Epoch 15, Batch 683, LR 2.396505 Loss 6.286670, Accuracy 84.590%\n",
      "Epoch 15, Batch 684, LR 2.396452 Loss 6.286038, Accuracy 84.598%\n",
      "Epoch 15, Batch 685, LR 2.396399 Loss 6.285512, Accuracy 84.599%\n",
      "Epoch 15, Batch 686, LR 2.396345 Loss 6.285435, Accuracy 84.605%\n",
      "Epoch 15, Batch 687, LR 2.396292 Loss 6.285763, Accuracy 84.607%\n",
      "Epoch 15, Batch 688, LR 2.396238 Loss 6.286318, Accuracy 84.601%\n",
      "Epoch 15, Batch 689, LR 2.396185 Loss 6.285807, Accuracy 84.604%\n",
      "Epoch 15, Batch 690, LR 2.396131 Loss 6.285629, Accuracy 84.603%\n",
      "Epoch 15, Batch 691, LR 2.396078 Loss 6.286664, Accuracy 84.597%\n",
      "Epoch 15, Batch 692, LR 2.396025 Loss 6.287218, Accuracy 84.588%\n",
      "Epoch 15, Batch 693, LR 2.395971 Loss 6.288234, Accuracy 84.584%\n",
      "Epoch 15, Batch 694, LR 2.395918 Loss 6.288373, Accuracy 84.584%\n",
      "Epoch 15, Batch 695, LR 2.395864 Loss 6.288819, Accuracy 84.582%\n",
      "Epoch 15, Batch 696, LR 2.395810 Loss 6.287964, Accuracy 84.585%\n",
      "Epoch 15, Batch 697, LR 2.395757 Loss 6.288679, Accuracy 84.582%\n",
      "Epoch 15, Batch 698, LR 2.395703 Loss 6.288283, Accuracy 84.580%\n",
      "Epoch 15, Batch 699, LR 2.395650 Loss 6.289002, Accuracy 84.575%\n",
      "Epoch 15, Batch 700, LR 2.395596 Loss 6.288845, Accuracy 84.580%\n",
      "Epoch 15, Batch 701, LR 2.395543 Loss 6.288645, Accuracy 84.581%\n",
      "Epoch 15, Batch 702, LR 2.395489 Loss 6.288735, Accuracy 84.581%\n",
      "Epoch 15, Batch 703, LR 2.395435 Loss 6.288683, Accuracy 84.583%\n",
      "Epoch 15, Batch 704, LR 2.395382 Loss 6.289263, Accuracy 84.580%\n",
      "Epoch 15, Batch 705, LR 2.395328 Loss 6.289469, Accuracy 84.577%\n",
      "Epoch 15, Batch 706, LR 2.395274 Loss 6.289485, Accuracy 84.576%\n",
      "Epoch 15, Batch 707, LR 2.395221 Loss 6.288416, Accuracy 84.587%\n",
      "Epoch 15, Batch 708, LR 2.395167 Loss 6.289227, Accuracy 84.582%\n",
      "Epoch 15, Batch 709, LR 2.395113 Loss 6.289951, Accuracy 84.577%\n",
      "Epoch 15, Batch 710, LR 2.395060 Loss 6.290336, Accuracy 84.577%\n",
      "Epoch 15, Batch 711, LR 2.395006 Loss 6.290993, Accuracy 84.574%\n",
      "Epoch 15, Batch 712, LR 2.394952 Loss 6.290875, Accuracy 84.579%\n",
      "Epoch 15, Batch 713, LR 2.394898 Loss 6.290897, Accuracy 84.579%\n",
      "Epoch 15, Batch 714, LR 2.394845 Loss 6.290863, Accuracy 84.574%\n",
      "Epoch 15, Batch 715, LR 2.394791 Loss 6.291300, Accuracy 84.569%\n",
      "Epoch 15, Batch 716, LR 2.394737 Loss 6.290718, Accuracy 84.568%\n",
      "Epoch 15, Batch 717, LR 2.394683 Loss 6.290513, Accuracy 84.565%\n",
      "Epoch 15, Batch 718, LR 2.394629 Loss 6.291235, Accuracy 84.563%\n",
      "Epoch 15, Batch 719, LR 2.394576 Loss 6.289854, Accuracy 84.569%\n",
      "Epoch 15, Batch 720, LR 2.394522 Loss 6.289626, Accuracy 84.576%\n",
      "Epoch 15, Batch 721, LR 2.394468 Loss 6.289235, Accuracy 84.582%\n",
      "Epoch 15, Batch 722, LR 2.394414 Loss 6.288276, Accuracy 84.585%\n",
      "Epoch 15, Batch 723, LR 2.394360 Loss 6.287398, Accuracy 84.591%\n",
      "Epoch 15, Batch 724, LR 2.394306 Loss 6.286418, Accuracy 84.599%\n",
      "Epoch 15, Batch 725, LR 2.394252 Loss 6.285177, Accuracy 84.608%\n",
      "Epoch 15, Batch 726, LR 2.394198 Loss 6.285584, Accuracy 84.607%\n",
      "Epoch 15, Batch 727, LR 2.394144 Loss 6.285151, Accuracy 84.606%\n",
      "Epoch 15, Batch 728, LR 2.394090 Loss 6.286020, Accuracy 84.599%\n",
      "Epoch 15, Batch 729, LR 2.394036 Loss 6.286670, Accuracy 84.591%\n",
      "Epoch 15, Batch 730, LR 2.393982 Loss 6.286032, Accuracy 84.593%\n",
      "Epoch 15, Batch 731, LR 2.393928 Loss 6.285748, Accuracy 84.591%\n",
      "Epoch 15, Batch 732, LR 2.393874 Loss 6.284866, Accuracy 84.597%\n",
      "Epoch 15, Batch 733, LR 2.393820 Loss 6.284871, Accuracy 84.602%\n",
      "Epoch 15, Batch 734, LR 2.393766 Loss 6.285826, Accuracy 84.599%\n",
      "Epoch 15, Batch 735, LR 2.393712 Loss 6.285676, Accuracy 84.599%\n",
      "Epoch 15, Batch 736, LR 2.393658 Loss 6.285703, Accuracy 84.603%\n",
      "Epoch 15, Batch 737, LR 2.393604 Loss 6.285630, Accuracy 84.600%\n",
      "Epoch 15, Batch 738, LR 2.393550 Loss 6.285888, Accuracy 84.595%\n",
      "Epoch 15, Batch 739, LR 2.393496 Loss 6.286200, Accuracy 84.593%\n",
      "Epoch 15, Batch 740, LR 2.393442 Loss 6.285775, Accuracy 84.589%\n",
      "Epoch 15, Batch 741, LR 2.393388 Loss 6.285175, Accuracy 84.597%\n",
      "Epoch 15, Batch 742, LR 2.393334 Loss 6.285098, Accuracy 84.597%\n",
      "Epoch 15, Batch 743, LR 2.393280 Loss 6.283787, Accuracy 84.611%\n",
      "Epoch 15, Batch 744, LR 2.393225 Loss 6.284373, Accuracy 84.608%\n",
      "Epoch 15, Batch 745, LR 2.393171 Loss 6.285103, Accuracy 84.607%\n",
      "Epoch 15, Batch 746, LR 2.393117 Loss 6.284679, Accuracy 84.607%\n",
      "Epoch 15, Batch 747, LR 2.393063 Loss 6.284659, Accuracy 84.609%\n",
      "Epoch 15, Batch 748, LR 2.393009 Loss 6.284391, Accuracy 84.613%\n",
      "Epoch 15, Batch 749, LR 2.392954 Loss 6.284192, Accuracy 84.614%\n",
      "Epoch 15, Batch 750, LR 2.392900 Loss 6.284760, Accuracy 84.608%\n",
      "Epoch 15, Batch 751, LR 2.392846 Loss 6.285642, Accuracy 84.607%\n",
      "Epoch 15, Batch 752, LR 2.392792 Loss 6.286198, Accuracy 84.597%\n",
      "Epoch 15, Batch 753, LR 2.392737 Loss 6.286043, Accuracy 84.601%\n",
      "Epoch 15, Batch 754, LR 2.392683 Loss 6.285901, Accuracy 84.603%\n",
      "Epoch 15, Batch 755, LR 2.392629 Loss 6.286689, Accuracy 84.601%\n",
      "Epoch 15, Batch 756, LR 2.392574 Loss 6.287198, Accuracy 84.598%\n",
      "Epoch 15, Batch 757, LR 2.392520 Loss 6.287241, Accuracy 84.599%\n",
      "Epoch 15, Batch 758, LR 2.392466 Loss 6.287698, Accuracy 84.601%\n",
      "Epoch 15, Batch 759, LR 2.392411 Loss 6.287701, Accuracy 84.607%\n",
      "Epoch 15, Batch 760, LR 2.392357 Loss 6.287078, Accuracy 84.613%\n",
      "Epoch 15, Batch 761, LR 2.392303 Loss 6.286573, Accuracy 84.617%\n",
      "Epoch 15, Batch 762, LR 2.392248 Loss 6.287555, Accuracy 84.616%\n",
      "Epoch 15, Batch 763, LR 2.392194 Loss 6.287873, Accuracy 84.613%\n",
      "Epoch 15, Batch 764, LR 2.392139 Loss 6.288616, Accuracy 84.610%\n",
      "Epoch 15, Batch 765, LR 2.392085 Loss 6.289079, Accuracy 84.608%\n",
      "Epoch 15, Batch 766, LR 2.392030 Loss 6.288782, Accuracy 84.610%\n",
      "Epoch 15, Batch 767, LR 2.391976 Loss 6.289507, Accuracy 84.608%\n",
      "Epoch 15, Batch 768, LR 2.391921 Loss 6.289510, Accuracy 84.609%\n",
      "Epoch 15, Batch 769, LR 2.391867 Loss 6.289023, Accuracy 84.614%\n",
      "Epoch 15, Batch 770, LR 2.391812 Loss 6.288329, Accuracy 84.615%\n",
      "Epoch 15, Batch 771, LR 2.391758 Loss 6.288404, Accuracy 84.615%\n",
      "Epoch 15, Batch 772, LR 2.391703 Loss 6.288804, Accuracy 84.617%\n",
      "Epoch 15, Batch 773, LR 2.391649 Loss 6.289665, Accuracy 84.606%\n",
      "Epoch 15, Batch 774, LR 2.391594 Loss 6.290312, Accuracy 84.602%\n",
      "Epoch 15, Batch 775, LR 2.391540 Loss 6.290168, Accuracy 84.608%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 776, LR 2.391485 Loss 6.289635, Accuracy 84.608%\n",
      "Epoch 15, Batch 777, LR 2.391431 Loss 6.289753, Accuracy 84.607%\n",
      "Epoch 15, Batch 778, LR 2.391376 Loss 6.289969, Accuracy 84.607%\n",
      "Epoch 15, Batch 779, LR 2.391321 Loss 6.290607, Accuracy 84.601%\n",
      "Epoch 15, Batch 780, LR 2.391267 Loss 6.291113, Accuracy 84.599%\n",
      "Epoch 15, Batch 781, LR 2.391212 Loss 6.291405, Accuracy 84.599%\n",
      "Epoch 15, Batch 782, LR 2.391157 Loss 6.291599, Accuracy 84.601%\n",
      "Epoch 15, Batch 783, LR 2.391103 Loss 6.290981, Accuracy 84.605%\n",
      "Epoch 15, Batch 784, LR 2.391048 Loss 6.290375, Accuracy 84.606%\n",
      "Epoch 15, Batch 785, LR 2.390993 Loss 6.289541, Accuracy 84.612%\n",
      "Epoch 15, Batch 786, LR 2.390939 Loss 6.289482, Accuracy 84.616%\n",
      "Epoch 15, Batch 787, LR 2.390884 Loss 6.290729, Accuracy 84.605%\n",
      "Epoch 15, Batch 788, LR 2.390829 Loss 6.291351, Accuracy 84.602%\n",
      "Epoch 15, Batch 789, LR 2.390774 Loss 6.290921, Accuracy 84.604%\n",
      "Epoch 15, Batch 790, LR 2.390720 Loss 6.290264, Accuracy 84.606%\n",
      "Epoch 15, Batch 791, LR 2.390665 Loss 6.291112, Accuracy 84.599%\n",
      "Epoch 15, Batch 792, LR 2.390610 Loss 6.291027, Accuracy 84.595%\n",
      "Epoch 15, Batch 793, LR 2.390555 Loss 6.291125, Accuracy 84.591%\n",
      "Epoch 15, Batch 794, LR 2.390500 Loss 6.291540, Accuracy 84.587%\n",
      "Epoch 15, Batch 795, LR 2.390446 Loss 6.291462, Accuracy 84.583%\n",
      "Epoch 15, Batch 796, LR 2.390391 Loss 6.291371, Accuracy 84.584%\n",
      "Epoch 15, Batch 797, LR 2.390336 Loss 6.291210, Accuracy 84.583%\n",
      "Epoch 15, Batch 798, LR 2.390281 Loss 6.291637, Accuracy 84.577%\n",
      "Epoch 15, Batch 799, LR 2.390226 Loss 6.290996, Accuracy 84.578%\n",
      "Epoch 15, Batch 800, LR 2.390171 Loss 6.290919, Accuracy 84.581%\n",
      "Epoch 15, Batch 801, LR 2.390116 Loss 6.290629, Accuracy 84.579%\n",
      "Epoch 15, Batch 802, LR 2.390061 Loss 6.290322, Accuracy 84.575%\n",
      "Epoch 15, Batch 803, LR 2.390006 Loss 6.290061, Accuracy 84.578%\n",
      "Epoch 15, Batch 804, LR 2.389951 Loss 6.290719, Accuracy 84.573%\n",
      "Epoch 15, Batch 805, LR 2.389897 Loss 6.290532, Accuracy 84.574%\n",
      "Epoch 15, Batch 806, LR 2.389842 Loss 6.290222, Accuracy 84.574%\n",
      "Epoch 15, Batch 807, LR 2.389787 Loss 6.289868, Accuracy 84.578%\n",
      "Epoch 15, Batch 808, LR 2.389732 Loss 6.289973, Accuracy 84.574%\n",
      "Epoch 15, Batch 809, LR 2.389677 Loss 6.289492, Accuracy 84.569%\n",
      "Epoch 15, Batch 810, LR 2.389621 Loss 6.289868, Accuracy 84.564%\n",
      "Epoch 15, Batch 811, LR 2.389566 Loss 6.288434, Accuracy 84.573%\n",
      "Epoch 15, Batch 812, LR 2.389511 Loss 6.288632, Accuracy 84.576%\n",
      "Epoch 15, Batch 813, LR 2.389456 Loss 6.288956, Accuracy 84.571%\n",
      "Epoch 15, Batch 814, LR 2.389401 Loss 6.288909, Accuracy 84.576%\n",
      "Epoch 15, Batch 815, LR 2.389346 Loss 6.289279, Accuracy 84.571%\n",
      "Epoch 15, Batch 816, LR 2.389291 Loss 6.289024, Accuracy 84.571%\n",
      "Epoch 15, Batch 817, LR 2.389236 Loss 6.288556, Accuracy 84.575%\n",
      "Epoch 15, Batch 818, LR 2.389181 Loss 6.287935, Accuracy 84.577%\n",
      "Epoch 15, Batch 819, LR 2.389126 Loss 6.287198, Accuracy 84.584%\n",
      "Epoch 15, Batch 820, LR 2.389070 Loss 6.287292, Accuracy 84.579%\n",
      "Epoch 15, Batch 821, LR 2.389015 Loss 6.287661, Accuracy 84.579%\n",
      "Epoch 15, Batch 822, LR 2.388960 Loss 6.287373, Accuracy 84.576%\n",
      "Epoch 15, Batch 823, LR 2.388905 Loss 6.286802, Accuracy 84.579%\n",
      "Epoch 15, Batch 824, LR 2.388850 Loss 6.286162, Accuracy 84.583%\n",
      "Epoch 15, Batch 825, LR 2.388794 Loss 6.286350, Accuracy 84.580%\n",
      "Epoch 15, Batch 826, LR 2.388739 Loss 6.285218, Accuracy 84.583%\n",
      "Epoch 15, Batch 827, LR 2.388684 Loss 6.284507, Accuracy 84.582%\n",
      "Epoch 15, Batch 828, LR 2.388629 Loss 6.285151, Accuracy 84.576%\n",
      "Epoch 15, Batch 829, LR 2.388573 Loss 6.284916, Accuracy 84.574%\n",
      "Epoch 15, Batch 830, LR 2.388518 Loss 6.284795, Accuracy 84.575%\n",
      "Epoch 15, Batch 831, LR 2.388463 Loss 6.284567, Accuracy 84.577%\n",
      "Epoch 15, Batch 832, LR 2.388408 Loss 6.285028, Accuracy 84.574%\n",
      "Epoch 15, Batch 833, LR 2.388352 Loss 6.285321, Accuracy 84.571%\n",
      "Epoch 15, Batch 834, LR 2.388297 Loss 6.284575, Accuracy 84.575%\n",
      "Epoch 15, Batch 835, LR 2.388241 Loss 6.284481, Accuracy 84.571%\n",
      "Epoch 15, Batch 836, LR 2.388186 Loss 6.284510, Accuracy 84.576%\n",
      "Epoch 15, Batch 837, LR 2.388131 Loss 6.283854, Accuracy 84.578%\n",
      "Epoch 15, Batch 838, LR 2.388075 Loss 6.283940, Accuracy 84.576%\n",
      "Epoch 15, Batch 839, LR 2.388020 Loss 6.284045, Accuracy 84.574%\n",
      "Epoch 15, Batch 840, LR 2.387965 Loss 6.284239, Accuracy 84.576%\n",
      "Epoch 15, Batch 841, LR 2.387909 Loss 6.284125, Accuracy 84.577%\n",
      "Epoch 15, Batch 842, LR 2.387854 Loss 6.284384, Accuracy 84.573%\n",
      "Epoch 15, Batch 843, LR 2.387798 Loss 6.284148, Accuracy 84.577%\n",
      "Epoch 15, Batch 844, LR 2.387743 Loss 6.283963, Accuracy 84.576%\n",
      "Epoch 15, Batch 845, LR 2.387687 Loss 6.284127, Accuracy 84.574%\n",
      "Epoch 15, Batch 846, LR 2.387632 Loss 6.284260, Accuracy 84.571%\n",
      "Epoch 15, Batch 847, LR 2.387576 Loss 6.284049, Accuracy 84.574%\n",
      "Epoch 15, Batch 848, LR 2.387521 Loss 6.284101, Accuracy 84.573%\n",
      "Epoch 15, Batch 849, LR 2.387465 Loss 6.284305, Accuracy 84.572%\n",
      "Epoch 15, Batch 850, LR 2.387410 Loss 6.284370, Accuracy 84.569%\n",
      "Epoch 15, Batch 851, LR 2.387354 Loss 6.284404, Accuracy 84.567%\n",
      "Epoch 15, Batch 852, LR 2.387298 Loss 6.283575, Accuracy 84.569%\n",
      "Epoch 15, Batch 853, LR 2.387243 Loss 6.283046, Accuracy 84.570%\n",
      "Epoch 15, Batch 854, LR 2.387187 Loss 6.284121, Accuracy 84.563%\n",
      "Epoch 15, Batch 855, LR 2.387132 Loss 6.282897, Accuracy 84.570%\n",
      "Epoch 15, Batch 856, LR 2.387076 Loss 6.282255, Accuracy 84.575%\n",
      "Epoch 15, Batch 857, LR 2.387020 Loss 6.282893, Accuracy 84.572%\n",
      "Epoch 15, Batch 858, LR 2.386965 Loss 6.281671, Accuracy 84.579%\n",
      "Epoch 15, Batch 859, LR 2.386909 Loss 6.281217, Accuracy 84.582%\n",
      "Epoch 15, Batch 860, LR 2.386853 Loss 6.281093, Accuracy 84.582%\n",
      "Epoch 15, Batch 861, LR 2.386798 Loss 6.281049, Accuracy 84.586%\n",
      "Epoch 15, Batch 862, LR 2.386742 Loss 6.280068, Accuracy 84.592%\n",
      "Epoch 15, Batch 863, LR 2.386686 Loss 6.280382, Accuracy 84.595%\n",
      "Epoch 15, Batch 864, LR 2.386630 Loss 6.280727, Accuracy 84.587%\n",
      "Epoch 15, Batch 865, LR 2.386575 Loss 6.280674, Accuracy 84.588%\n",
      "Epoch 15, Batch 866, LR 2.386519 Loss 6.280530, Accuracy 84.586%\n",
      "Epoch 15, Batch 867, LR 2.386463 Loss 6.280307, Accuracy 84.590%\n",
      "Epoch 15, Batch 868, LR 2.386407 Loss 6.280246, Accuracy 84.587%\n",
      "Epoch 15, Batch 869, LR 2.386352 Loss 6.280115, Accuracy 84.588%\n",
      "Epoch 15, Batch 870, LR 2.386296 Loss 6.280058, Accuracy 84.591%\n",
      "Epoch 15, Batch 871, LR 2.386240 Loss 6.279947, Accuracy 84.591%\n",
      "Epoch 15, Batch 872, LR 2.386184 Loss 6.280328, Accuracy 84.589%\n",
      "Epoch 15, Batch 873, LR 2.386128 Loss 6.280014, Accuracy 84.592%\n",
      "Epoch 15, Batch 874, LR 2.386072 Loss 6.281170, Accuracy 84.582%\n",
      "Epoch 15, Batch 875, LR 2.386016 Loss 6.281747, Accuracy 84.578%\n",
      "Epoch 15, Batch 876, LR 2.385961 Loss 6.281478, Accuracy 84.578%\n",
      "Epoch 15, Batch 877, LR 2.385905 Loss 6.280683, Accuracy 84.581%\n",
      "Epoch 15, Batch 878, LR 2.385849 Loss 6.280927, Accuracy 84.581%\n",
      "Epoch 15, Batch 879, LR 2.385793 Loss 6.281333, Accuracy 84.577%\n",
      "Epoch 15, Batch 880, LR 2.385737 Loss 6.281866, Accuracy 84.574%\n",
      "Epoch 15, Batch 881, LR 2.385681 Loss 6.282441, Accuracy 84.567%\n",
      "Epoch 15, Batch 882, LR 2.385625 Loss 6.282826, Accuracy 84.565%\n",
      "Epoch 15, Batch 883, LR 2.385569 Loss 6.283201, Accuracy 84.570%\n",
      "Epoch 15, Batch 884, LR 2.385513 Loss 6.282751, Accuracy 84.572%\n",
      "Epoch 15, Batch 885, LR 2.385457 Loss 6.283236, Accuracy 84.570%\n",
      "Epoch 15, Batch 886, LR 2.385401 Loss 6.283077, Accuracy 84.572%\n",
      "Epoch 15, Batch 887, LR 2.385345 Loss 6.282916, Accuracy 84.578%\n",
      "Epoch 15, Batch 888, LR 2.385289 Loss 6.283310, Accuracy 84.574%\n",
      "Epoch 15, Batch 889, LR 2.385233 Loss 6.283374, Accuracy 84.575%\n",
      "Epoch 15, Batch 890, LR 2.385177 Loss 6.283432, Accuracy 84.576%\n",
      "Epoch 15, Batch 891, LR 2.385121 Loss 6.283019, Accuracy 84.578%\n",
      "Epoch 15, Batch 892, LR 2.385065 Loss 6.283948, Accuracy 84.573%\n",
      "Epoch 15, Batch 893, LR 2.385008 Loss 6.283945, Accuracy 84.575%\n",
      "Epoch 15, Batch 894, LR 2.384952 Loss 6.283474, Accuracy 84.578%\n",
      "Epoch 15, Batch 895, LR 2.384896 Loss 6.283341, Accuracy 84.580%\n",
      "Epoch 15, Batch 896, LR 2.384840 Loss 6.283315, Accuracy 84.580%\n",
      "Epoch 15, Batch 897, LR 2.384784 Loss 6.283905, Accuracy 84.577%\n",
      "Epoch 15, Batch 898, LR 2.384728 Loss 6.282719, Accuracy 84.580%\n",
      "Epoch 15, Batch 899, LR 2.384671 Loss 6.283239, Accuracy 84.577%\n",
      "Epoch 15, Batch 900, LR 2.384615 Loss 6.283795, Accuracy 84.573%\n",
      "Epoch 15, Batch 901, LR 2.384559 Loss 6.284317, Accuracy 84.569%\n",
      "Epoch 15, Batch 902, LR 2.384503 Loss 6.284501, Accuracy 84.567%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 903, LR 2.384447 Loss 6.284692, Accuracy 84.565%\n",
      "Epoch 15, Batch 904, LR 2.384390 Loss 6.284388, Accuracy 84.571%\n",
      "Epoch 15, Batch 905, LR 2.384334 Loss 6.284234, Accuracy 84.574%\n",
      "Epoch 15, Batch 906, LR 2.384278 Loss 6.283930, Accuracy 84.571%\n",
      "Epoch 15, Batch 907, LR 2.384221 Loss 6.283199, Accuracy 84.574%\n",
      "Epoch 15, Batch 908, LR 2.384165 Loss 6.283289, Accuracy 84.575%\n",
      "Epoch 15, Batch 909, LR 2.384109 Loss 6.283054, Accuracy 84.577%\n",
      "Epoch 15, Batch 910, LR 2.384053 Loss 6.282502, Accuracy 84.577%\n",
      "Epoch 15, Batch 911, LR 2.383996 Loss 6.282024, Accuracy 84.579%\n",
      "Epoch 15, Batch 912, LR 2.383940 Loss 6.281564, Accuracy 84.581%\n",
      "Epoch 15, Batch 913, LR 2.383883 Loss 6.281698, Accuracy 84.580%\n",
      "Epoch 15, Batch 914, LR 2.383827 Loss 6.281352, Accuracy 84.586%\n",
      "Epoch 15, Batch 915, LR 2.383771 Loss 6.281432, Accuracy 84.588%\n",
      "Epoch 15, Batch 916, LR 2.383714 Loss 6.281437, Accuracy 84.587%\n",
      "Epoch 15, Batch 917, LR 2.383658 Loss 6.280901, Accuracy 84.586%\n",
      "Epoch 15, Batch 918, LR 2.383601 Loss 6.280307, Accuracy 84.588%\n",
      "Epoch 15, Batch 919, LR 2.383545 Loss 6.280378, Accuracy 84.588%\n",
      "Epoch 15, Batch 920, LR 2.383488 Loss 6.279590, Accuracy 84.592%\n",
      "Epoch 15, Batch 921, LR 2.383432 Loss 6.279547, Accuracy 84.592%\n",
      "Epoch 15, Batch 922, LR 2.383375 Loss 6.279300, Accuracy 84.594%\n",
      "Epoch 15, Batch 923, LR 2.383319 Loss 6.279769, Accuracy 84.592%\n",
      "Epoch 15, Batch 924, LR 2.383262 Loss 6.280934, Accuracy 84.587%\n",
      "Epoch 15, Batch 925, LR 2.383206 Loss 6.280781, Accuracy 84.590%\n",
      "Epoch 15, Batch 926, LR 2.383149 Loss 6.281354, Accuracy 84.584%\n",
      "Epoch 15, Batch 927, LR 2.383093 Loss 6.281406, Accuracy 84.582%\n",
      "Epoch 15, Batch 928, LR 2.383036 Loss 6.282057, Accuracy 84.575%\n",
      "Epoch 15, Batch 929, LR 2.382980 Loss 6.282640, Accuracy 84.570%\n",
      "Epoch 15, Batch 930, LR 2.382923 Loss 6.281713, Accuracy 84.572%\n",
      "Epoch 15, Batch 931, LR 2.382866 Loss 6.281852, Accuracy 84.571%\n",
      "Epoch 15, Batch 932, LR 2.382810 Loss 6.281631, Accuracy 84.569%\n",
      "Epoch 15, Batch 933, LR 2.382753 Loss 6.281275, Accuracy 84.571%\n",
      "Epoch 15, Batch 934, LR 2.382697 Loss 6.282032, Accuracy 84.565%\n",
      "Epoch 15, Batch 935, LR 2.382640 Loss 6.281523, Accuracy 84.566%\n",
      "Epoch 15, Batch 936, LR 2.382583 Loss 6.282265, Accuracy 84.563%\n",
      "Epoch 15, Batch 937, LR 2.382527 Loss 6.283361, Accuracy 84.558%\n",
      "Epoch 15, Batch 938, LR 2.382470 Loss 6.283562, Accuracy 84.559%\n",
      "Epoch 15, Batch 939, LR 2.382413 Loss 6.283233, Accuracy 84.561%\n",
      "Epoch 15, Batch 940, LR 2.382356 Loss 6.282970, Accuracy 84.561%\n",
      "Epoch 15, Batch 941, LR 2.382300 Loss 6.283263, Accuracy 84.556%\n",
      "Epoch 15, Batch 942, LR 2.382243 Loss 6.283429, Accuracy 84.557%\n",
      "Epoch 15, Batch 943, LR 2.382186 Loss 6.282441, Accuracy 84.561%\n",
      "Epoch 15, Batch 944, LR 2.382129 Loss 6.282297, Accuracy 84.561%\n",
      "Epoch 15, Batch 945, LR 2.382073 Loss 6.282816, Accuracy 84.563%\n",
      "Epoch 15, Batch 946, LR 2.382016 Loss 6.282668, Accuracy 84.558%\n",
      "Epoch 15, Batch 947, LR 2.381959 Loss 6.283320, Accuracy 84.556%\n",
      "Epoch 15, Batch 948, LR 2.381902 Loss 6.282668, Accuracy 84.558%\n",
      "Epoch 15, Batch 949, LR 2.381845 Loss 6.283062, Accuracy 84.554%\n",
      "Epoch 15, Batch 950, LR 2.381788 Loss 6.282682, Accuracy 84.556%\n",
      "Epoch 15, Batch 951, LR 2.381732 Loss 6.283021, Accuracy 84.558%\n",
      "Epoch 15, Batch 952, LR 2.381675 Loss 6.282358, Accuracy 84.559%\n",
      "Epoch 15, Batch 953, LR 2.381618 Loss 6.282334, Accuracy 84.557%\n",
      "Epoch 15, Batch 954, LR 2.381561 Loss 6.283032, Accuracy 84.554%\n",
      "Epoch 15, Batch 955, LR 2.381504 Loss 6.282829, Accuracy 84.553%\n",
      "Epoch 15, Batch 956, LR 2.381447 Loss 6.282964, Accuracy 84.552%\n",
      "Epoch 15, Batch 957, LR 2.381390 Loss 6.283644, Accuracy 84.551%\n",
      "Epoch 15, Batch 958, LR 2.381333 Loss 6.283722, Accuracy 84.550%\n",
      "Epoch 15, Batch 959, LR 2.381276 Loss 6.283614, Accuracy 84.551%\n",
      "Epoch 15, Batch 960, LR 2.381219 Loss 6.283342, Accuracy 84.553%\n",
      "Epoch 15, Batch 961, LR 2.381162 Loss 6.283204, Accuracy 84.552%\n",
      "Epoch 15, Batch 962, LR 2.381105 Loss 6.282437, Accuracy 84.559%\n",
      "Epoch 15, Batch 963, LR 2.381048 Loss 6.282081, Accuracy 84.558%\n",
      "Epoch 15, Batch 964, LR 2.380991 Loss 6.281888, Accuracy 84.557%\n",
      "Epoch 15, Batch 965, LR 2.380934 Loss 6.281752, Accuracy 84.555%\n",
      "Epoch 15, Batch 966, LR 2.380877 Loss 6.281486, Accuracy 84.559%\n",
      "Epoch 15, Batch 967, LR 2.380820 Loss 6.281791, Accuracy 84.555%\n",
      "Epoch 15, Batch 968, LR 2.380763 Loss 6.281608, Accuracy 84.557%\n",
      "Epoch 15, Batch 969, LR 2.380706 Loss 6.281451, Accuracy 84.557%\n",
      "Epoch 15, Batch 970, LR 2.380649 Loss 6.281949, Accuracy 84.554%\n",
      "Epoch 15, Batch 971, LR 2.380591 Loss 6.282520, Accuracy 84.550%\n",
      "Epoch 15, Batch 972, LR 2.380534 Loss 6.282804, Accuracy 84.547%\n",
      "Epoch 15, Batch 973, LR 2.380477 Loss 6.282915, Accuracy 84.548%\n",
      "Epoch 15, Batch 974, LR 2.380420 Loss 6.282301, Accuracy 84.554%\n",
      "Epoch 15, Batch 975, LR 2.380363 Loss 6.283166, Accuracy 84.550%\n",
      "Epoch 15, Batch 976, LR 2.380306 Loss 6.282784, Accuracy 84.553%\n",
      "Epoch 15, Batch 977, LR 2.380248 Loss 6.283028, Accuracy 84.555%\n",
      "Epoch 15, Batch 978, LR 2.380191 Loss 6.283226, Accuracy 84.553%\n",
      "Epoch 15, Batch 979, LR 2.380134 Loss 6.283001, Accuracy 84.555%\n",
      "Epoch 15, Batch 980, LR 2.380077 Loss 6.282594, Accuracy 84.557%\n",
      "Epoch 15, Batch 981, LR 2.380019 Loss 6.282857, Accuracy 84.557%\n",
      "Epoch 15, Batch 982, LR 2.379962 Loss 6.282387, Accuracy 84.560%\n",
      "Epoch 15, Batch 983, LR 2.379905 Loss 6.282494, Accuracy 84.559%\n",
      "Epoch 15, Batch 984, LR 2.379848 Loss 6.282301, Accuracy 84.560%\n",
      "Epoch 15, Batch 985, LR 2.379790 Loss 6.282414, Accuracy 84.562%\n",
      "Epoch 15, Batch 986, LR 2.379733 Loss 6.282398, Accuracy 84.560%\n",
      "Epoch 15, Batch 987, LR 2.379676 Loss 6.282571, Accuracy 84.558%\n",
      "Epoch 15, Batch 988, LR 2.379618 Loss 6.281584, Accuracy 84.563%\n",
      "Epoch 15, Batch 989, LR 2.379561 Loss 6.281756, Accuracy 84.561%\n",
      "Epoch 15, Batch 990, LR 2.379504 Loss 6.282388, Accuracy 84.557%\n",
      "Epoch 15, Batch 991, LR 2.379446 Loss 6.282179, Accuracy 84.555%\n",
      "Epoch 15, Batch 992, LR 2.379389 Loss 6.282021, Accuracy 84.553%\n",
      "Epoch 15, Batch 993, LR 2.379331 Loss 6.281923, Accuracy 84.553%\n",
      "Epoch 15, Batch 994, LR 2.379274 Loss 6.281487, Accuracy 84.558%\n",
      "Epoch 15, Batch 995, LR 2.379216 Loss 6.281049, Accuracy 84.558%\n",
      "Epoch 15, Batch 996, LR 2.379159 Loss 6.280712, Accuracy 84.562%\n",
      "Epoch 15, Batch 997, LR 2.379102 Loss 6.280820, Accuracy 84.561%\n",
      "Epoch 15, Batch 998, LR 2.379044 Loss 6.280781, Accuracy 84.558%\n",
      "Epoch 15, Batch 999, LR 2.378987 Loss 6.280546, Accuracy 84.560%\n",
      "Epoch 15, Batch 1000, LR 2.378929 Loss 6.280608, Accuracy 84.560%\n",
      "Epoch 15, Batch 1001, LR 2.378872 Loss 6.281034, Accuracy 84.557%\n",
      "Epoch 15, Batch 1002, LR 2.378814 Loss 6.280957, Accuracy 84.554%\n",
      "Epoch 15, Batch 1003, LR 2.378756 Loss 6.281169, Accuracy 84.554%\n",
      "Epoch 15, Batch 1004, LR 2.378699 Loss 6.281518, Accuracy 84.549%\n",
      "Epoch 15, Batch 1005, LR 2.378641 Loss 6.281550, Accuracy 84.550%\n",
      "Epoch 15, Batch 1006, LR 2.378584 Loss 6.281407, Accuracy 84.553%\n",
      "Epoch 15, Batch 1007, LR 2.378526 Loss 6.281600, Accuracy 84.547%\n",
      "Epoch 15, Batch 1008, LR 2.378469 Loss 6.281374, Accuracy 84.547%\n",
      "Epoch 15, Batch 1009, LR 2.378411 Loss 6.282025, Accuracy 84.542%\n",
      "Epoch 15, Batch 1010, LR 2.378353 Loss 6.281858, Accuracy 84.543%\n",
      "Epoch 15, Batch 1011, LR 2.378296 Loss 6.281526, Accuracy 84.545%\n",
      "Epoch 15, Batch 1012, LR 2.378238 Loss 6.281718, Accuracy 84.539%\n",
      "Epoch 15, Batch 1013, LR 2.378180 Loss 6.281830, Accuracy 84.542%\n",
      "Epoch 15, Batch 1014, LR 2.378123 Loss 6.281897, Accuracy 84.544%\n",
      "Epoch 15, Batch 1015, LR 2.378065 Loss 6.281989, Accuracy 84.540%\n",
      "Epoch 15, Batch 1016, LR 2.378007 Loss 6.281736, Accuracy 84.540%\n",
      "Epoch 15, Batch 1017, LR 2.377949 Loss 6.281239, Accuracy 84.544%\n",
      "Epoch 15, Batch 1018, LR 2.377892 Loss 6.281570, Accuracy 84.545%\n",
      "Epoch 15, Batch 1019, LR 2.377834 Loss 6.281816, Accuracy 84.539%\n",
      "Epoch 15, Batch 1020, LR 2.377776 Loss 6.281982, Accuracy 84.541%\n",
      "Epoch 15, Batch 1021, LR 2.377718 Loss 6.281899, Accuracy 84.540%\n",
      "Epoch 15, Batch 1022, LR 2.377661 Loss 6.282528, Accuracy 84.537%\n",
      "Epoch 15, Batch 1023, LR 2.377603 Loss 6.282206, Accuracy 84.538%\n",
      "Epoch 15, Batch 1024, LR 2.377545 Loss 6.281309, Accuracy 84.544%\n",
      "Epoch 15, Batch 1025, LR 2.377487 Loss 6.281750, Accuracy 84.540%\n",
      "Epoch 15, Batch 1026, LR 2.377429 Loss 6.282022, Accuracy 84.539%\n",
      "Epoch 15, Batch 1027, LR 2.377372 Loss 6.281935, Accuracy 84.533%\n",
      "Epoch 15, Batch 1028, LR 2.377314 Loss 6.280922, Accuracy 84.541%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 1029, LR 2.377256 Loss 6.280676, Accuracy 84.545%\n",
      "Epoch 15, Batch 1030, LR 2.377198 Loss 6.280683, Accuracy 84.547%\n",
      "Epoch 15, Batch 1031, LR 2.377140 Loss 6.280254, Accuracy 84.549%\n",
      "Epoch 15, Batch 1032, LR 2.377082 Loss 6.280101, Accuracy 84.550%\n",
      "Epoch 15, Batch 1033, LR 2.377024 Loss 6.280424, Accuracy 84.546%\n",
      "Epoch 15, Batch 1034, LR 2.376966 Loss 6.280345, Accuracy 84.547%\n",
      "Epoch 15, Batch 1035, LR 2.376908 Loss 6.280778, Accuracy 84.545%\n",
      "Epoch 15, Batch 1036, LR 2.376850 Loss 6.280141, Accuracy 84.548%\n",
      "Epoch 15, Batch 1037, LR 2.376792 Loss 6.280066, Accuracy 84.549%\n",
      "Epoch 15, Batch 1038, LR 2.376734 Loss 6.280248, Accuracy 84.549%\n",
      "Epoch 15, Batch 1039, LR 2.376676 Loss 6.279983, Accuracy 84.553%\n",
      "Epoch 15, Batch 1040, LR 2.376618 Loss 6.280280, Accuracy 84.553%\n",
      "Epoch 15, Batch 1041, LR 2.376560 Loss 6.281191, Accuracy 84.551%\n",
      "Epoch 15, Batch 1042, LR 2.376502 Loss 6.281590, Accuracy 84.548%\n",
      "Epoch 15, Batch 1043, LR 2.376444 Loss 6.281732, Accuracy 84.550%\n",
      "Epoch 15, Batch 1044, LR 2.376386 Loss 6.282827, Accuracy 84.548%\n",
      "Epoch 15, Batch 1045, LR 2.376328 Loss 6.282833, Accuracy 84.550%\n",
      "Epoch 15, Batch 1046, LR 2.376270 Loss 6.283741, Accuracy 84.545%\n",
      "Epoch 15, Batch 1047, LR 2.376212 Loss 6.283926, Accuracy 84.546%\n",
      "Epoch 15, Loss (train set) 6.283926, Accuracy (train set) 84.546%\n",
      "Epoch 16, Batch 1, LR 2.376154 Loss 5.781994, Accuracy 89.062%\n",
      "Epoch 16, Batch 2, LR 2.376095 Loss 5.595132, Accuracy 89.844%\n",
      "Epoch 16, Batch 3, LR 2.376037 Loss 6.035863, Accuracy 86.458%\n",
      "Epoch 16, Batch 4, LR 2.375979 Loss 6.059361, Accuracy 87.109%\n",
      "Epoch 16, Batch 5, LR 2.375921 Loss 6.017684, Accuracy 87.031%\n",
      "Epoch 16, Batch 6, LR 2.375863 Loss 5.966886, Accuracy 87.370%\n",
      "Epoch 16, Batch 7, LR 2.375805 Loss 5.958552, Accuracy 87.277%\n",
      "Epoch 16, Batch 8, LR 2.375746 Loss 5.964000, Accuracy 87.109%\n",
      "Epoch 16, Batch 9, LR 2.375688 Loss 5.953762, Accuracy 87.240%\n",
      "Epoch 16, Batch 10, LR 2.375630 Loss 5.997166, Accuracy 86.953%\n",
      "Epoch 16, Batch 11, LR 2.375572 Loss 6.010278, Accuracy 86.719%\n",
      "Epoch 16, Batch 12, LR 2.375513 Loss 6.017650, Accuracy 86.458%\n",
      "Epoch 16, Batch 13, LR 2.375455 Loss 6.022190, Accuracy 86.238%\n",
      "Epoch 16, Batch 14, LR 2.375397 Loss 6.008459, Accuracy 86.217%\n",
      "Epoch 16, Batch 15, LR 2.375338 Loss 6.005755, Accuracy 86.250%\n",
      "Epoch 16, Batch 16, LR 2.375280 Loss 6.020907, Accuracy 86.084%\n",
      "Epoch 16, Batch 17, LR 2.375222 Loss 5.987651, Accuracy 86.397%\n",
      "Epoch 16, Batch 18, LR 2.375163 Loss 5.957282, Accuracy 86.458%\n",
      "Epoch 16, Batch 19, LR 2.375105 Loss 5.972454, Accuracy 86.308%\n",
      "Epoch 16, Batch 20, LR 2.375047 Loss 5.992385, Accuracy 86.250%\n",
      "Epoch 16, Batch 21, LR 2.374988 Loss 5.993278, Accuracy 86.198%\n",
      "Epoch 16, Batch 22, LR 2.374930 Loss 5.990600, Accuracy 86.222%\n",
      "Epoch 16, Batch 23, LR 2.374872 Loss 6.025934, Accuracy 85.870%\n",
      "Epoch 16, Batch 24, LR 2.374813 Loss 6.021818, Accuracy 85.938%\n",
      "Epoch 16, Batch 25, LR 2.374755 Loss 6.005182, Accuracy 86.094%\n",
      "Epoch 16, Batch 26, LR 2.374696 Loss 6.011230, Accuracy 86.058%\n",
      "Epoch 16, Batch 27, LR 2.374638 Loss 6.019238, Accuracy 85.966%\n",
      "Epoch 16, Batch 28, LR 2.374579 Loss 6.013483, Accuracy 85.910%\n",
      "Epoch 16, Batch 29, LR 2.374521 Loss 6.005071, Accuracy 85.964%\n",
      "Epoch 16, Batch 30, LR 2.374462 Loss 6.026224, Accuracy 85.859%\n",
      "Epoch 16, Batch 31, LR 2.374404 Loss 6.017603, Accuracy 85.761%\n",
      "Epoch 16, Batch 32, LR 2.374345 Loss 5.988749, Accuracy 85.864%\n",
      "Epoch 16, Batch 33, LR 2.374287 Loss 5.995939, Accuracy 85.866%\n",
      "Epoch 16, Batch 34, LR 2.374228 Loss 6.005608, Accuracy 85.754%\n",
      "Epoch 16, Batch 35, LR 2.374170 Loss 6.033116, Accuracy 85.647%\n",
      "Epoch 16, Batch 36, LR 2.374111 Loss 6.021254, Accuracy 85.655%\n",
      "Epoch 16, Batch 37, LR 2.374052 Loss 6.030752, Accuracy 85.663%\n",
      "Epoch 16, Batch 38, LR 2.373994 Loss 6.031560, Accuracy 85.794%\n",
      "Epoch 16, Batch 39, LR 2.373935 Loss 6.031816, Accuracy 85.677%\n",
      "Epoch 16, Batch 40, LR 2.373877 Loss 6.023867, Accuracy 85.762%\n",
      "Epoch 16, Batch 41, LR 2.373818 Loss 6.025960, Accuracy 85.690%\n",
      "Epoch 16, Batch 42, LR 2.373759 Loss 6.025398, Accuracy 85.621%\n",
      "Epoch 16, Batch 43, LR 2.373701 Loss 6.012094, Accuracy 85.665%\n",
      "Epoch 16, Batch 44, LR 2.373642 Loss 6.018607, Accuracy 85.671%\n",
      "Epoch 16, Batch 45, LR 2.373583 Loss 6.022787, Accuracy 85.660%\n",
      "Epoch 16, Batch 46, LR 2.373525 Loss 6.026082, Accuracy 85.700%\n",
      "Epoch 16, Batch 47, LR 2.373466 Loss 6.036314, Accuracy 85.572%\n",
      "Epoch 16, Batch 48, LR 2.373407 Loss 6.033588, Accuracy 85.612%\n",
      "Epoch 16, Batch 49, LR 2.373348 Loss 6.029026, Accuracy 85.635%\n",
      "Epoch 16, Batch 50, LR 2.373290 Loss 6.037981, Accuracy 85.578%\n",
      "Epoch 16, Batch 51, LR 2.373231 Loss 6.045697, Accuracy 85.570%\n",
      "Epoch 16, Batch 52, LR 2.373172 Loss 6.048047, Accuracy 85.637%\n",
      "Epoch 16, Batch 53, LR 2.373113 Loss 6.051546, Accuracy 85.628%\n",
      "Epoch 16, Batch 54, LR 2.373054 Loss 6.054824, Accuracy 85.590%\n",
      "Epoch 16, Batch 55, LR 2.372996 Loss 6.055932, Accuracy 85.582%\n",
      "Epoch 16, Batch 56, LR 2.372937 Loss 6.052706, Accuracy 85.631%\n",
      "Epoch 16, Batch 57, LR 2.372878 Loss 6.058656, Accuracy 85.609%\n",
      "Epoch 16, Batch 58, LR 2.372819 Loss 6.055462, Accuracy 85.628%\n",
      "Epoch 16, Batch 59, LR 2.372760 Loss 6.058385, Accuracy 85.633%\n",
      "Epoch 16, Batch 60, LR 2.372701 Loss 6.070284, Accuracy 85.612%\n",
      "Epoch 16, Batch 61, LR 2.372642 Loss 6.073125, Accuracy 85.643%\n",
      "Epoch 16, Batch 62, LR 2.372583 Loss 6.074995, Accuracy 85.610%\n",
      "Epoch 16, Batch 63, LR 2.372524 Loss 6.070986, Accuracy 85.615%\n",
      "Epoch 16, Batch 64, LR 2.372466 Loss 6.063749, Accuracy 85.632%\n",
      "Epoch 16, Batch 65, LR 2.372407 Loss 6.055383, Accuracy 85.661%\n",
      "Epoch 16, Batch 66, LR 2.372348 Loss 6.060443, Accuracy 85.653%\n",
      "Epoch 16, Batch 67, LR 2.372289 Loss 6.076244, Accuracy 85.564%\n",
      "Epoch 16, Batch 68, LR 2.372230 Loss 6.072430, Accuracy 85.650%\n",
      "Epoch 16, Batch 69, LR 2.372171 Loss 6.071320, Accuracy 85.677%\n",
      "Epoch 16, Batch 70, LR 2.372112 Loss 6.071767, Accuracy 85.737%\n",
      "Epoch 16, Batch 71, LR 2.372053 Loss 6.079306, Accuracy 85.739%\n",
      "Epoch 16, Batch 72, LR 2.371994 Loss 6.079535, Accuracy 85.710%\n",
      "Epoch 16, Batch 73, LR 2.371935 Loss 6.072070, Accuracy 85.788%\n",
      "Epoch 16, Batch 74, LR 2.371875 Loss 6.088065, Accuracy 85.716%\n",
      "Epoch 16, Batch 75, LR 2.371816 Loss 6.079464, Accuracy 85.792%\n",
      "Epoch 16, Batch 76, LR 2.371757 Loss 6.072604, Accuracy 85.814%\n",
      "Epoch 16, Batch 77, LR 2.371698 Loss 6.077894, Accuracy 85.775%\n",
      "Epoch 16, Batch 78, LR 2.371639 Loss 6.077309, Accuracy 85.817%\n",
      "Epoch 16, Batch 79, LR 2.371580 Loss 6.091690, Accuracy 85.730%\n",
      "Epoch 16, Batch 80, LR 2.371521 Loss 6.092589, Accuracy 85.713%\n",
      "Epoch 16, Batch 81, LR 2.371462 Loss 6.104662, Accuracy 85.677%\n",
      "Epoch 16, Batch 82, LR 2.371402 Loss 6.110552, Accuracy 85.652%\n",
      "Epoch 16, Batch 83, LR 2.371343 Loss 6.117484, Accuracy 85.570%\n",
      "Epoch 16, Batch 84, LR 2.371284 Loss 6.115999, Accuracy 85.603%\n",
      "Epoch 16, Batch 85, LR 2.371225 Loss 6.125133, Accuracy 85.515%\n",
      "Epoch 16, Batch 86, LR 2.371166 Loss 6.121205, Accuracy 85.583%\n",
      "Epoch 16, Batch 87, LR 2.371106 Loss 6.120486, Accuracy 85.578%\n",
      "Epoch 16, Batch 88, LR 2.371047 Loss 6.123677, Accuracy 85.538%\n",
      "Epoch 16, Batch 89, LR 2.370988 Loss 6.122754, Accuracy 85.534%\n",
      "Epoch 16, Batch 90, LR 2.370929 Loss 6.129616, Accuracy 85.486%\n",
      "Epoch 16, Batch 91, LR 2.370869 Loss 6.126417, Accuracy 85.491%\n",
      "Epoch 16, Batch 92, LR 2.370810 Loss 6.130294, Accuracy 85.453%\n",
      "Epoch 16, Batch 93, LR 2.370751 Loss 6.131796, Accuracy 85.442%\n",
      "Epoch 16, Batch 94, LR 2.370691 Loss 6.135368, Accuracy 85.406%\n",
      "Epoch 16, Batch 95, LR 2.370632 Loss 6.127702, Accuracy 85.477%\n",
      "Epoch 16, Batch 96, LR 2.370573 Loss 6.119767, Accuracy 85.498%\n",
      "Epoch 16, Batch 97, LR 2.370513 Loss 6.134460, Accuracy 85.382%\n",
      "Epoch 16, Batch 98, LR 2.370454 Loss 6.133198, Accuracy 85.379%\n",
      "Epoch 16, Batch 99, LR 2.370395 Loss 6.135184, Accuracy 85.346%\n",
      "Epoch 16, Batch 100, LR 2.370335 Loss 6.135790, Accuracy 85.367%\n",
      "Epoch 16, Batch 101, LR 2.370276 Loss 6.133169, Accuracy 85.373%\n",
      "Epoch 16, Batch 102, LR 2.370216 Loss 6.130572, Accuracy 85.371%\n",
      "Epoch 16, Batch 103, LR 2.370157 Loss 6.133398, Accuracy 85.353%\n",
      "Epoch 16, Batch 104, LR 2.370097 Loss 6.132748, Accuracy 85.374%\n",
      "Epoch 16, Batch 105, LR 2.370038 Loss 6.129885, Accuracy 85.357%\n",
      "Epoch 16, Batch 106, LR 2.369978 Loss 6.132038, Accuracy 85.341%\n",
      "Epoch 16, Batch 107, LR 2.369919 Loss 6.131694, Accuracy 85.331%\n",
      "Epoch 16, Batch 108, LR 2.369859 Loss 6.133886, Accuracy 85.352%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 109, LR 2.369800 Loss 6.133303, Accuracy 85.393%\n",
      "Epoch 16, Batch 110, LR 2.369740 Loss 6.129863, Accuracy 85.433%\n",
      "Epoch 16, Batch 111, LR 2.369681 Loss 6.124852, Accuracy 85.487%\n",
      "Epoch 16, Batch 112, LR 2.369621 Loss 6.128770, Accuracy 85.470%\n",
      "Epoch 16, Batch 113, LR 2.369562 Loss 6.120744, Accuracy 85.516%\n",
      "Epoch 16, Batch 114, LR 2.369502 Loss 6.126893, Accuracy 85.485%\n",
      "Epoch 16, Batch 115, LR 2.369443 Loss 6.123960, Accuracy 85.496%\n",
      "Epoch 16, Batch 116, LR 2.369383 Loss 6.130109, Accuracy 85.459%\n",
      "Epoch 16, Batch 117, LR 2.369323 Loss 6.129589, Accuracy 85.477%\n",
      "Epoch 16, Batch 118, LR 2.369264 Loss 6.132129, Accuracy 85.454%\n",
      "Epoch 16, Batch 119, LR 2.369204 Loss 6.130704, Accuracy 85.458%\n",
      "Epoch 16, Batch 120, LR 2.369144 Loss 6.129292, Accuracy 85.462%\n",
      "Epoch 16, Batch 121, LR 2.369085 Loss 6.130983, Accuracy 85.421%\n",
      "Epoch 16, Batch 122, LR 2.369025 Loss 6.128970, Accuracy 85.457%\n",
      "Epoch 16, Batch 123, LR 2.368965 Loss 6.128574, Accuracy 85.455%\n",
      "Epoch 16, Batch 124, LR 2.368906 Loss 6.124191, Accuracy 85.465%\n",
      "Epoch 16, Batch 125, LR 2.368846 Loss 6.118629, Accuracy 85.500%\n",
      "Epoch 16, Batch 126, LR 2.368786 Loss 6.121377, Accuracy 85.497%\n",
      "Epoch 16, Batch 127, LR 2.368726 Loss 6.121701, Accuracy 85.495%\n",
      "Epoch 16, Batch 128, LR 2.368667 Loss 6.125490, Accuracy 85.455%\n",
      "Epoch 16, Batch 129, LR 2.368607 Loss 6.124960, Accuracy 85.441%\n",
      "Epoch 16, Batch 130, LR 2.368547 Loss 6.126588, Accuracy 85.409%\n",
      "Epoch 16, Batch 131, LR 2.368487 Loss 6.119935, Accuracy 85.448%\n",
      "Epoch 16, Batch 132, LR 2.368427 Loss 6.122711, Accuracy 85.440%\n",
      "Epoch 16, Batch 133, LR 2.368368 Loss 6.122412, Accuracy 85.432%\n",
      "Epoch 16, Batch 134, LR 2.368308 Loss 6.124634, Accuracy 85.395%\n",
      "Epoch 16, Batch 135, LR 2.368248 Loss 6.123583, Accuracy 85.388%\n",
      "Epoch 16, Batch 136, LR 2.368188 Loss 6.124193, Accuracy 85.392%\n",
      "Epoch 16, Batch 137, LR 2.368128 Loss 6.122472, Accuracy 85.419%\n",
      "Epoch 16, Batch 138, LR 2.368068 Loss 6.119129, Accuracy 85.439%\n",
      "Epoch 16, Batch 139, LR 2.368008 Loss 6.116440, Accuracy 85.477%\n",
      "Epoch 16, Batch 140, LR 2.367948 Loss 6.113129, Accuracy 85.485%\n",
      "Epoch 16, Batch 141, LR 2.367889 Loss 6.110619, Accuracy 85.516%\n",
      "Epoch 16, Batch 142, LR 2.367829 Loss 6.112850, Accuracy 85.497%\n",
      "Epoch 16, Batch 143, LR 2.367769 Loss 6.115002, Accuracy 85.500%\n",
      "Epoch 16, Batch 144, LR 2.367709 Loss 6.115006, Accuracy 85.509%\n",
      "Epoch 16, Batch 145, LR 2.367649 Loss 6.117960, Accuracy 85.485%\n",
      "Epoch 16, Batch 146, LR 2.367589 Loss 6.115685, Accuracy 85.509%\n",
      "Epoch 16, Batch 147, LR 2.367529 Loss 6.116863, Accuracy 85.502%\n",
      "Epoch 16, Batch 148, LR 2.367469 Loss 6.116023, Accuracy 85.494%\n",
      "Epoch 16, Batch 149, LR 2.367409 Loss 6.115655, Accuracy 85.508%\n",
      "Epoch 16, Batch 150, LR 2.367349 Loss 6.111805, Accuracy 85.526%\n",
      "Epoch 16, Batch 151, LR 2.367289 Loss 6.111338, Accuracy 85.524%\n",
      "Epoch 16, Batch 152, LR 2.367228 Loss 6.115542, Accuracy 85.506%\n",
      "Epoch 16, Batch 153, LR 2.367168 Loss 6.111258, Accuracy 85.544%\n",
      "Epoch 16, Batch 154, LR 2.367108 Loss 6.106777, Accuracy 85.572%\n",
      "Epoch 16, Batch 155, LR 2.367048 Loss 6.110916, Accuracy 85.575%\n",
      "Epoch 16, Batch 156, LR 2.366988 Loss 6.113276, Accuracy 85.562%\n",
      "Epoch 16, Batch 157, LR 2.366928 Loss 6.116480, Accuracy 85.524%\n",
      "Epoch 16, Batch 158, LR 2.366868 Loss 6.112076, Accuracy 85.567%\n",
      "Epoch 16, Batch 159, LR 2.366808 Loss 6.109717, Accuracy 85.574%\n",
      "Epoch 16, Batch 160, LR 2.366747 Loss 6.106348, Accuracy 85.605%\n",
      "Epoch 16, Batch 161, LR 2.366687 Loss 6.108575, Accuracy 85.617%\n",
      "Epoch 16, Batch 162, LR 2.366627 Loss 6.108732, Accuracy 85.610%\n",
      "Epoch 16, Batch 163, LR 2.366567 Loss 6.107721, Accuracy 85.626%\n",
      "Epoch 16, Batch 164, LR 2.366507 Loss 6.103289, Accuracy 85.637%\n",
      "Epoch 16, Batch 165, LR 2.366446 Loss 6.105334, Accuracy 85.630%\n",
      "Epoch 16, Batch 166, LR 2.366386 Loss 6.105356, Accuracy 85.627%\n",
      "Epoch 16, Batch 167, LR 2.366326 Loss 6.108097, Accuracy 85.601%\n",
      "Epoch 16, Batch 168, LR 2.366266 Loss 6.107483, Accuracy 85.603%\n",
      "Epoch 16, Batch 169, LR 2.366205 Loss 6.112865, Accuracy 85.563%\n",
      "Epoch 16, Batch 170, LR 2.366145 Loss 6.113303, Accuracy 85.570%\n",
      "Epoch 16, Batch 171, LR 2.366085 Loss 6.113757, Accuracy 85.586%\n",
      "Epoch 16, Batch 172, LR 2.366024 Loss 6.110003, Accuracy 85.592%\n",
      "Epoch 16, Batch 173, LR 2.365964 Loss 6.111103, Accuracy 85.567%\n",
      "Epoch 16, Batch 174, LR 2.365904 Loss 6.109929, Accuracy 85.565%\n",
      "Epoch 16, Batch 175, LR 2.365843 Loss 6.110866, Accuracy 85.545%\n",
      "Epoch 16, Batch 176, LR 2.365783 Loss 6.113010, Accuracy 85.529%\n",
      "Epoch 16, Batch 177, LR 2.365722 Loss 6.107904, Accuracy 85.567%\n",
      "Epoch 16, Batch 178, LR 2.365662 Loss 6.110866, Accuracy 85.569%\n",
      "Epoch 16, Batch 179, LR 2.365602 Loss 6.111424, Accuracy 85.580%\n",
      "Epoch 16, Batch 180, LR 2.365541 Loss 6.114814, Accuracy 85.560%\n",
      "Epoch 16, Batch 181, LR 2.365481 Loss 6.111904, Accuracy 85.566%\n",
      "Epoch 16, Batch 182, LR 2.365420 Loss 6.109761, Accuracy 85.573%\n",
      "Epoch 16, Batch 183, LR 2.365360 Loss 6.108251, Accuracy 85.553%\n",
      "Epoch 16, Batch 184, LR 2.365299 Loss 6.111075, Accuracy 85.517%\n",
      "Epoch 16, Batch 185, LR 2.365239 Loss 6.111269, Accuracy 85.507%\n",
      "Epoch 16, Batch 186, LR 2.365178 Loss 6.110105, Accuracy 85.517%\n",
      "Epoch 16, Batch 187, LR 2.365118 Loss 6.109542, Accuracy 85.507%\n",
      "Epoch 16, Batch 188, LR 2.365057 Loss 6.109822, Accuracy 85.514%\n",
      "Epoch 16, Batch 189, LR 2.364997 Loss 6.109151, Accuracy 85.532%\n",
      "Epoch 16, Batch 190, LR 2.364936 Loss 6.106365, Accuracy 85.559%\n",
      "Epoch 16, Batch 191, LR 2.364876 Loss 6.109408, Accuracy 85.549%\n",
      "Epoch 16, Batch 192, LR 2.364815 Loss 6.106749, Accuracy 85.563%\n",
      "Epoch 16, Batch 193, LR 2.364754 Loss 6.102769, Accuracy 85.585%\n",
      "Epoch 16, Batch 194, LR 2.364694 Loss 6.101104, Accuracy 85.587%\n",
      "Epoch 16, Batch 195, LR 2.364633 Loss 6.103105, Accuracy 85.577%\n",
      "Epoch 16, Batch 196, LR 2.364573 Loss 6.105084, Accuracy 85.571%\n",
      "Epoch 16, Batch 197, LR 2.364512 Loss 6.105813, Accuracy 85.545%\n",
      "Epoch 16, Batch 198, LR 2.364451 Loss 6.103314, Accuracy 85.551%\n",
      "Epoch 16, Batch 199, LR 2.364391 Loss 6.105845, Accuracy 85.537%\n",
      "Epoch 16, Batch 200, LR 2.364330 Loss 6.104567, Accuracy 85.551%\n",
      "Epoch 16, Batch 201, LR 2.364269 Loss 6.108951, Accuracy 85.541%\n",
      "Epoch 16, Batch 202, LR 2.364209 Loss 6.109367, Accuracy 85.516%\n",
      "Epoch 16, Batch 203, LR 2.364148 Loss 6.109721, Accuracy 85.506%\n",
      "Epoch 16, Batch 204, LR 2.364087 Loss 6.107627, Accuracy 85.528%\n",
      "Epoch 16, Batch 205, LR 2.364026 Loss 6.106535, Accuracy 85.530%\n",
      "Epoch 16, Batch 206, LR 2.363966 Loss 6.108699, Accuracy 85.509%\n",
      "Epoch 16, Batch 207, LR 2.363905 Loss 6.108405, Accuracy 85.515%\n",
      "Epoch 16, Batch 208, LR 2.363844 Loss 6.109057, Accuracy 85.498%\n",
      "Epoch 16, Batch 209, LR 2.363783 Loss 6.108809, Accuracy 85.500%\n",
      "Epoch 16, Batch 210, LR 2.363722 Loss 6.108212, Accuracy 85.525%\n",
      "Epoch 16, Batch 211, LR 2.363662 Loss 6.107543, Accuracy 85.530%\n",
      "Epoch 16, Batch 212, LR 2.363601 Loss 6.109649, Accuracy 85.514%\n",
      "Epoch 16, Batch 213, LR 2.363540 Loss 6.110663, Accuracy 85.508%\n",
      "Epoch 16, Batch 214, LR 2.363479 Loss 6.109898, Accuracy 85.510%\n",
      "Epoch 16, Batch 215, LR 2.363418 Loss 6.110950, Accuracy 85.498%\n",
      "Epoch 16, Batch 216, LR 2.363357 Loss 6.105492, Accuracy 85.547%\n",
      "Epoch 16, Batch 217, LR 2.363296 Loss 6.103875, Accuracy 85.552%\n",
      "Epoch 16, Batch 218, LR 2.363235 Loss 6.102344, Accuracy 85.558%\n",
      "Epoch 16, Batch 219, LR 2.363174 Loss 6.102943, Accuracy 85.559%\n",
      "Epoch 16, Batch 220, LR 2.363113 Loss 6.101358, Accuracy 85.572%\n",
      "Epoch 16, Batch 221, LR 2.363053 Loss 6.100727, Accuracy 85.588%\n",
      "Epoch 16, Batch 222, LR 2.362992 Loss 6.103093, Accuracy 85.568%\n",
      "Epoch 16, Batch 223, LR 2.362931 Loss 6.102344, Accuracy 85.573%\n",
      "Epoch 16, Batch 224, LR 2.362870 Loss 6.103242, Accuracy 85.568%\n",
      "Epoch 16, Batch 225, LR 2.362809 Loss 6.103850, Accuracy 85.559%\n",
      "Epoch 16, Batch 226, LR 2.362748 Loss 6.104677, Accuracy 85.550%\n",
      "Epoch 16, Batch 227, LR 2.362687 Loss 6.103270, Accuracy 85.555%\n",
      "Epoch 16, Batch 228, LR 2.362625 Loss 6.103559, Accuracy 85.554%\n",
      "Epoch 16, Batch 229, LR 2.362564 Loss 6.098937, Accuracy 85.576%\n",
      "Epoch 16, Batch 230, LR 2.362503 Loss 6.095261, Accuracy 85.598%\n",
      "Epoch 16, Batch 231, LR 2.362442 Loss 6.097078, Accuracy 85.593%\n",
      "Epoch 16, Batch 232, LR 2.362381 Loss 6.096345, Accuracy 85.591%\n",
      "Epoch 16, Batch 233, LR 2.362320 Loss 6.096212, Accuracy 85.585%\n",
      "Epoch 16, Batch 234, LR 2.362259 Loss 6.095081, Accuracy 85.600%\n",
      "Epoch 16, Batch 235, LR 2.362198 Loss 6.094549, Accuracy 85.602%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 236, LR 2.362137 Loss 6.094370, Accuracy 85.593%\n",
      "Epoch 16, Batch 237, LR 2.362076 Loss 6.094865, Accuracy 85.591%\n",
      "Epoch 16, Batch 238, LR 2.362014 Loss 6.093793, Accuracy 85.590%\n",
      "Epoch 16, Batch 239, LR 2.361953 Loss 6.095750, Accuracy 85.575%\n",
      "Epoch 16, Batch 240, LR 2.361892 Loss 6.095534, Accuracy 85.583%\n",
      "Epoch 16, Batch 241, LR 2.361831 Loss 6.094009, Accuracy 85.591%\n",
      "Epoch 16, Batch 242, LR 2.361770 Loss 6.093682, Accuracy 85.589%\n",
      "Epoch 16, Batch 243, LR 2.361708 Loss 6.094919, Accuracy 85.600%\n",
      "Epoch 16, Batch 244, LR 2.361647 Loss 6.098151, Accuracy 85.588%\n",
      "Epoch 16, Batch 245, LR 2.361586 Loss 6.097931, Accuracy 85.584%\n",
      "Epoch 16, Batch 246, LR 2.361525 Loss 6.100344, Accuracy 85.582%\n",
      "Epoch 16, Batch 247, LR 2.361463 Loss 6.102764, Accuracy 85.567%\n",
      "Epoch 16, Batch 248, LR 2.361402 Loss 6.102814, Accuracy 85.582%\n",
      "Epoch 16, Batch 249, LR 2.361341 Loss 6.101884, Accuracy 85.592%\n",
      "Epoch 16, Batch 250, LR 2.361279 Loss 6.105497, Accuracy 85.562%\n",
      "Epoch 16, Batch 251, LR 2.361218 Loss 6.105614, Accuracy 85.561%\n",
      "Epoch 16, Batch 252, LR 2.361157 Loss 6.107559, Accuracy 85.553%\n",
      "Epoch 16, Batch 253, LR 2.361095 Loss 6.111963, Accuracy 85.527%\n",
      "Epoch 16, Batch 254, LR 2.361034 Loss 6.112677, Accuracy 85.522%\n",
      "Epoch 16, Batch 255, LR 2.360972 Loss 6.114429, Accuracy 85.515%\n",
      "Epoch 16, Batch 256, LR 2.360911 Loss 6.114893, Accuracy 85.513%\n",
      "Epoch 16, Batch 257, LR 2.360850 Loss 6.115082, Accuracy 85.515%\n",
      "Epoch 16, Batch 258, LR 2.360788 Loss 6.113511, Accuracy 85.538%\n",
      "Epoch 16, Batch 259, LR 2.360727 Loss 6.114497, Accuracy 85.530%\n",
      "Epoch 16, Batch 260, LR 2.360665 Loss 6.112461, Accuracy 85.535%\n",
      "Epoch 16, Batch 261, LR 2.360604 Loss 6.113365, Accuracy 85.536%\n",
      "Epoch 16, Batch 262, LR 2.360542 Loss 6.114718, Accuracy 85.532%\n",
      "Epoch 16, Batch 263, LR 2.360481 Loss 6.114027, Accuracy 85.519%\n",
      "Epoch 16, Batch 264, LR 2.360419 Loss 6.112075, Accuracy 85.532%\n",
      "Epoch 16, Batch 265, LR 2.360358 Loss 6.111728, Accuracy 85.534%\n",
      "Epoch 16, Batch 266, LR 2.360296 Loss 6.112291, Accuracy 85.532%\n",
      "Epoch 16, Batch 267, LR 2.360235 Loss 6.112821, Accuracy 85.513%\n",
      "Epoch 16, Batch 268, LR 2.360173 Loss 6.112331, Accuracy 85.512%\n",
      "Epoch 16, Batch 269, LR 2.360112 Loss 6.111402, Accuracy 85.502%\n",
      "Epoch 16, Batch 270, LR 2.360050 Loss 6.114687, Accuracy 85.489%\n",
      "Epoch 16, Batch 271, LR 2.359988 Loss 6.116187, Accuracy 85.491%\n",
      "Epoch 16, Batch 272, LR 2.359927 Loss 6.116518, Accuracy 85.495%\n",
      "Epoch 16, Batch 273, LR 2.359865 Loss 6.113095, Accuracy 85.517%\n",
      "Epoch 16, Batch 274, LR 2.359804 Loss 6.113401, Accuracy 85.513%\n",
      "Epoch 16, Batch 275, LR 2.359742 Loss 6.111664, Accuracy 85.520%\n",
      "Epoch 16, Batch 276, LR 2.359680 Loss 6.112482, Accuracy 85.519%\n",
      "Epoch 16, Batch 277, LR 2.359619 Loss 6.112567, Accuracy 85.523%\n",
      "Epoch 16, Batch 278, LR 2.359557 Loss 6.113273, Accuracy 85.519%\n",
      "Epoch 16, Batch 279, LR 2.359495 Loss 6.114135, Accuracy 85.509%\n",
      "Epoch 16, Batch 280, LR 2.359434 Loss 6.114227, Accuracy 85.513%\n",
      "Epoch 16, Batch 281, LR 2.359372 Loss 6.114678, Accuracy 85.523%\n",
      "Epoch 16, Batch 282, LR 2.359310 Loss 6.117576, Accuracy 85.497%\n",
      "Epoch 16, Batch 283, LR 2.359248 Loss 6.114313, Accuracy 85.504%\n",
      "Epoch 16, Batch 284, LR 2.359187 Loss 6.112663, Accuracy 85.522%\n",
      "Epoch 16, Batch 285, LR 2.359125 Loss 6.113826, Accuracy 85.504%\n",
      "Epoch 16, Batch 286, LR 2.359063 Loss 6.112640, Accuracy 85.520%\n",
      "Epoch 16, Batch 287, LR 2.359001 Loss 6.113527, Accuracy 85.521%\n",
      "Epoch 16, Batch 288, LR 2.358939 Loss 6.113571, Accuracy 85.520%\n",
      "Epoch 16, Batch 289, LR 2.358878 Loss 6.113564, Accuracy 85.524%\n",
      "Epoch 16, Batch 290, LR 2.358816 Loss 6.114567, Accuracy 85.517%\n",
      "Epoch 16, Batch 291, LR 2.358754 Loss 6.114756, Accuracy 85.513%\n",
      "Epoch 16, Batch 292, LR 2.358692 Loss 6.113775, Accuracy 85.520%\n",
      "Epoch 16, Batch 293, LR 2.358630 Loss 6.112094, Accuracy 85.511%\n",
      "Epoch 16, Batch 294, LR 2.358568 Loss 6.111364, Accuracy 85.507%\n",
      "Epoch 16, Batch 295, LR 2.358506 Loss 6.113517, Accuracy 85.503%\n",
      "Epoch 16, Batch 296, LR 2.358445 Loss 6.114670, Accuracy 85.502%\n",
      "Epoch 16, Batch 297, LR 2.358383 Loss 6.114256, Accuracy 85.501%\n",
      "Epoch 16, Batch 298, LR 2.358321 Loss 6.112150, Accuracy 85.502%\n",
      "Epoch 16, Batch 299, LR 2.358259 Loss 6.111505, Accuracy 85.517%\n",
      "Epoch 16, Batch 300, LR 2.358197 Loss 6.113533, Accuracy 85.508%\n",
      "Epoch 16, Batch 301, LR 2.358135 Loss 6.113489, Accuracy 85.507%\n",
      "Epoch 16, Batch 302, LR 2.358073 Loss 6.115250, Accuracy 85.505%\n",
      "Epoch 16, Batch 303, LR 2.358011 Loss 6.119364, Accuracy 85.479%\n",
      "Epoch 16, Batch 304, LR 2.357949 Loss 6.118816, Accuracy 85.483%\n",
      "Epoch 16, Batch 305, LR 2.357887 Loss 6.119284, Accuracy 85.479%\n",
      "Epoch 16, Batch 306, LR 2.357825 Loss 6.119924, Accuracy 85.486%\n",
      "Epoch 16, Batch 307, LR 2.357763 Loss 6.118831, Accuracy 85.490%\n",
      "Epoch 16, Batch 308, LR 2.357701 Loss 6.117536, Accuracy 85.494%\n",
      "Epoch 16, Batch 309, LR 2.357638 Loss 6.115578, Accuracy 85.500%\n",
      "Epoch 16, Batch 310, LR 2.357576 Loss 6.114110, Accuracy 85.509%\n",
      "Epoch 16, Batch 311, LR 2.357514 Loss 6.115088, Accuracy 85.518%\n",
      "Epoch 16, Batch 312, LR 2.357452 Loss 6.117031, Accuracy 85.507%\n",
      "Epoch 16, Batch 313, LR 2.357390 Loss 6.116582, Accuracy 85.506%\n",
      "Epoch 16, Batch 314, LR 2.357328 Loss 6.115219, Accuracy 85.510%\n",
      "Epoch 16, Batch 315, LR 2.357266 Loss 6.116982, Accuracy 85.501%\n",
      "Epoch 16, Batch 316, LR 2.357204 Loss 6.118175, Accuracy 85.492%\n",
      "Epoch 16, Batch 317, LR 2.357141 Loss 6.118843, Accuracy 85.482%\n",
      "Epoch 16, Batch 318, LR 2.357079 Loss 6.117665, Accuracy 85.483%\n",
      "Epoch 16, Batch 319, LR 2.357017 Loss 6.119026, Accuracy 85.470%\n",
      "Epoch 16, Batch 320, LR 2.356955 Loss 6.117967, Accuracy 85.481%\n",
      "Epoch 16, Batch 321, LR 2.356893 Loss 6.115912, Accuracy 85.487%\n",
      "Epoch 16, Batch 322, LR 2.356830 Loss 6.115918, Accuracy 85.477%\n",
      "Epoch 16, Batch 323, LR 2.356768 Loss 6.115856, Accuracy 85.478%\n",
      "Epoch 16, Batch 324, LR 2.356706 Loss 6.117195, Accuracy 85.484%\n",
      "Epoch 16, Batch 325, LR 2.356644 Loss 6.120256, Accuracy 85.476%\n",
      "Epoch 16, Batch 326, LR 2.356581 Loss 6.120295, Accuracy 85.473%\n",
      "Epoch 16, Batch 327, LR 2.356519 Loss 6.120942, Accuracy 85.462%\n",
      "Epoch 16, Batch 328, LR 2.356457 Loss 6.119972, Accuracy 85.461%\n",
      "Epoch 16, Batch 329, LR 2.356394 Loss 6.121599, Accuracy 85.460%\n",
      "Epoch 16, Batch 330, LR 2.356332 Loss 6.121572, Accuracy 85.459%\n",
      "Epoch 16, Batch 331, LR 2.356270 Loss 6.123189, Accuracy 85.449%\n",
      "Epoch 16, Batch 332, LR 2.356207 Loss 6.121997, Accuracy 85.446%\n",
      "Epoch 16, Batch 333, LR 2.356145 Loss 6.120537, Accuracy 85.459%\n",
      "Epoch 16, Batch 334, LR 2.356082 Loss 6.119747, Accuracy 85.470%\n",
      "Epoch 16, Batch 335, LR 2.356020 Loss 6.118005, Accuracy 85.473%\n",
      "Epoch 16, Batch 336, LR 2.355958 Loss 6.117555, Accuracy 85.472%\n",
      "Epoch 16, Batch 337, LR 2.355895 Loss 6.118254, Accuracy 85.474%\n",
      "Epoch 16, Batch 338, LR 2.355833 Loss 6.117931, Accuracy 85.480%\n",
      "Epoch 16, Batch 339, LR 2.355770 Loss 6.116080, Accuracy 85.493%\n",
      "Epoch 16, Batch 340, LR 2.355708 Loss 6.117378, Accuracy 85.492%\n",
      "Epoch 16, Batch 341, LR 2.355645 Loss 6.115608, Accuracy 85.507%\n",
      "Epoch 16, Batch 342, LR 2.355583 Loss 6.114901, Accuracy 85.497%\n",
      "Epoch 16, Batch 343, LR 2.355520 Loss 6.115174, Accuracy 85.487%\n",
      "Epoch 16, Batch 344, LR 2.355458 Loss 6.115572, Accuracy 85.486%\n",
      "Epoch 16, Batch 345, LR 2.355395 Loss 6.116483, Accuracy 85.471%\n",
      "Epoch 16, Batch 346, LR 2.355333 Loss 6.118978, Accuracy 85.459%\n",
      "Epoch 16, Batch 347, LR 2.355270 Loss 6.116523, Accuracy 85.465%\n",
      "Epoch 16, Batch 348, LR 2.355208 Loss 6.116826, Accuracy 85.455%\n",
      "Epoch 16, Batch 349, LR 2.355145 Loss 6.115732, Accuracy 85.463%\n",
      "Epoch 16, Batch 350, LR 2.355082 Loss 6.118826, Accuracy 85.444%\n",
      "Epoch 16, Batch 351, LR 2.355020 Loss 6.117206, Accuracy 85.448%\n",
      "Epoch 16, Batch 352, LR 2.354957 Loss 6.119437, Accuracy 85.438%\n",
      "Epoch 16, Batch 353, LR 2.354894 Loss 6.117323, Accuracy 85.448%\n",
      "Epoch 16, Batch 354, LR 2.354832 Loss 6.116251, Accuracy 85.452%\n",
      "Epoch 16, Batch 355, LR 2.354769 Loss 6.114954, Accuracy 85.464%\n",
      "Epoch 16, Batch 356, LR 2.354707 Loss 6.114338, Accuracy 85.466%\n",
      "Epoch 16, Batch 357, LR 2.354644 Loss 6.114874, Accuracy 85.463%\n",
      "Epoch 16, Batch 358, LR 2.354581 Loss 6.112209, Accuracy 85.475%\n",
      "Epoch 16, Batch 359, LR 2.354518 Loss 6.112138, Accuracy 85.476%\n",
      "Epoch 16, Batch 360, LR 2.354456 Loss 6.113046, Accuracy 85.467%\n",
      "Epoch 16, Batch 361, LR 2.354393 Loss 6.113824, Accuracy 85.468%\n",
      "Epoch 16, Batch 362, LR 2.354330 Loss 6.115719, Accuracy 85.458%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 363, LR 2.354267 Loss 6.115143, Accuracy 85.466%\n",
      "Epoch 16, Batch 364, LR 2.354205 Loss 6.115155, Accuracy 85.457%\n",
      "Epoch 16, Batch 365, LR 2.354142 Loss 6.113963, Accuracy 85.469%\n",
      "Epoch 16, Batch 366, LR 2.354079 Loss 6.116325, Accuracy 85.453%\n",
      "Epoch 16, Batch 367, LR 2.354016 Loss 6.115249, Accuracy 85.461%\n",
      "Epoch 16, Batch 368, LR 2.353953 Loss 6.116049, Accuracy 85.451%\n",
      "Epoch 16, Batch 369, LR 2.353891 Loss 6.115998, Accuracy 85.448%\n",
      "Epoch 16, Batch 370, LR 2.353828 Loss 6.114947, Accuracy 85.460%\n",
      "Epoch 16, Batch 371, LR 2.353765 Loss 6.114262, Accuracy 85.457%\n",
      "Epoch 16, Batch 372, LR 2.353702 Loss 6.112761, Accuracy 85.463%\n",
      "Epoch 16, Batch 373, LR 2.353639 Loss 6.113698, Accuracy 85.458%\n",
      "Epoch 16, Batch 374, LR 2.353576 Loss 6.114650, Accuracy 85.455%\n",
      "Epoch 16, Batch 375, LR 2.353513 Loss 6.117140, Accuracy 85.433%\n",
      "Epoch 16, Batch 376, LR 2.353450 Loss 6.117132, Accuracy 85.433%\n",
      "Epoch 16, Batch 377, LR 2.353387 Loss 6.117608, Accuracy 85.436%\n",
      "Epoch 16, Batch 378, LR 2.353324 Loss 6.117568, Accuracy 85.427%\n",
      "Epoch 16, Batch 379, LR 2.353261 Loss 6.117798, Accuracy 85.422%\n",
      "Epoch 16, Batch 380, LR 2.353199 Loss 6.117995, Accuracy 85.411%\n",
      "Epoch 16, Batch 381, LR 2.353136 Loss 6.119367, Accuracy 85.406%\n",
      "Epoch 16, Batch 382, LR 2.353073 Loss 6.119025, Accuracy 85.412%\n",
      "Epoch 16, Batch 383, LR 2.353010 Loss 6.120557, Accuracy 85.407%\n",
      "Epoch 16, Batch 384, LR 2.352946 Loss 6.120140, Accuracy 85.402%\n",
      "Epoch 16, Batch 385, LR 2.352883 Loss 6.119279, Accuracy 85.406%\n",
      "Epoch 16, Batch 386, LR 2.352820 Loss 6.120744, Accuracy 85.393%\n",
      "Epoch 16, Batch 387, LR 2.352757 Loss 6.120252, Accuracy 85.382%\n",
      "Epoch 16, Batch 388, LR 2.352694 Loss 6.120656, Accuracy 85.388%\n",
      "Epoch 16, Batch 389, LR 2.352631 Loss 6.119384, Accuracy 85.393%\n",
      "Epoch 16, Batch 390, LR 2.352568 Loss 6.119298, Accuracy 85.391%\n",
      "Epoch 16, Batch 391, LR 2.352505 Loss 6.119695, Accuracy 85.378%\n",
      "Epoch 16, Batch 392, LR 2.352442 Loss 6.120423, Accuracy 85.371%\n",
      "Epoch 16, Batch 393, LR 2.352379 Loss 6.119723, Accuracy 85.363%\n",
      "Epoch 16, Batch 394, LR 2.352315 Loss 6.118586, Accuracy 85.368%\n",
      "Epoch 16, Batch 395, LR 2.352252 Loss 6.118170, Accuracy 85.376%\n",
      "Epoch 16, Batch 396, LR 2.352189 Loss 6.117158, Accuracy 85.375%\n",
      "Epoch 16, Batch 397, LR 2.352126 Loss 6.117192, Accuracy 85.369%\n",
      "Epoch 16, Batch 398, LR 2.352063 Loss 6.117681, Accuracy 85.360%\n",
      "Epoch 16, Batch 399, LR 2.352000 Loss 6.118523, Accuracy 85.354%\n",
      "Epoch 16, Batch 400, LR 2.351936 Loss 6.117019, Accuracy 85.365%\n",
      "Epoch 16, Batch 401, LR 2.351873 Loss 6.117196, Accuracy 85.361%\n",
      "Epoch 16, Batch 402, LR 2.351810 Loss 6.116708, Accuracy 85.366%\n",
      "Epoch 16, Batch 403, LR 2.351747 Loss 6.116916, Accuracy 85.362%\n",
      "Epoch 16, Batch 404, LR 2.351683 Loss 6.115634, Accuracy 85.363%\n",
      "Epoch 16, Batch 405, LR 2.351620 Loss 6.116410, Accuracy 85.357%\n",
      "Epoch 16, Batch 406, LR 2.351557 Loss 6.115673, Accuracy 85.368%\n",
      "Epoch 16, Batch 407, LR 2.351493 Loss 6.115231, Accuracy 85.369%\n",
      "Epoch 16, Batch 408, LR 2.351430 Loss 6.114405, Accuracy 85.373%\n",
      "Epoch 16, Batch 409, LR 2.351367 Loss 6.115304, Accuracy 85.361%\n",
      "Epoch 16, Batch 410, LR 2.351303 Loss 6.115485, Accuracy 85.354%\n",
      "Epoch 16, Batch 411, LR 2.351240 Loss 6.113592, Accuracy 85.363%\n",
      "Epoch 16, Batch 412, LR 2.351177 Loss 6.114001, Accuracy 85.372%\n",
      "Epoch 16, Batch 413, LR 2.351113 Loss 6.114474, Accuracy 85.378%\n",
      "Epoch 16, Batch 414, LR 2.351050 Loss 6.115672, Accuracy 85.369%\n",
      "Epoch 16, Batch 415, LR 2.350986 Loss 6.114775, Accuracy 85.377%\n",
      "Epoch 16, Batch 416, LR 2.350923 Loss 6.114222, Accuracy 85.385%\n",
      "Epoch 16, Batch 417, LR 2.350859 Loss 6.115219, Accuracy 85.387%\n",
      "Epoch 16, Batch 418, LR 2.350796 Loss 6.114596, Accuracy 85.382%\n",
      "Epoch 16, Batch 419, LR 2.350732 Loss 6.113714, Accuracy 85.387%\n",
      "Epoch 16, Batch 420, LR 2.350669 Loss 6.114382, Accuracy 85.381%\n",
      "Epoch 16, Batch 421, LR 2.350605 Loss 6.114647, Accuracy 85.375%\n",
      "Epoch 16, Batch 422, LR 2.350542 Loss 6.113823, Accuracy 85.377%\n",
      "Epoch 16, Batch 423, LR 2.350478 Loss 6.113301, Accuracy 85.372%\n",
      "Epoch 16, Batch 424, LR 2.350415 Loss 6.114540, Accuracy 85.368%\n",
      "Epoch 16, Batch 425, LR 2.350351 Loss 6.115874, Accuracy 85.364%\n",
      "Epoch 16, Batch 426, LR 2.350288 Loss 6.117132, Accuracy 85.354%\n",
      "Epoch 16, Batch 427, LR 2.350224 Loss 6.117919, Accuracy 85.358%\n",
      "Epoch 16, Batch 428, LR 2.350161 Loss 6.118197, Accuracy 85.357%\n",
      "Epoch 16, Batch 429, LR 2.350097 Loss 6.118801, Accuracy 85.362%\n",
      "Epoch 16, Batch 430, LR 2.350033 Loss 6.117544, Accuracy 85.371%\n",
      "Epoch 16, Batch 431, LR 2.349970 Loss 6.116524, Accuracy 85.374%\n",
      "Epoch 16, Batch 432, LR 2.349906 Loss 6.116613, Accuracy 85.382%\n",
      "Epoch 16, Batch 433, LR 2.349843 Loss 6.117371, Accuracy 85.373%\n",
      "Epoch 16, Batch 434, LR 2.349779 Loss 6.118862, Accuracy 85.369%\n",
      "Epoch 16, Batch 435, LR 2.349715 Loss 6.119223, Accuracy 85.359%\n",
      "Epoch 16, Batch 436, LR 2.349652 Loss 6.119453, Accuracy 85.362%\n",
      "Epoch 16, Batch 437, LR 2.349588 Loss 6.118843, Accuracy 85.369%\n",
      "Epoch 16, Batch 438, LR 2.349524 Loss 6.117652, Accuracy 85.372%\n",
      "Epoch 16, Batch 439, LR 2.349460 Loss 6.118116, Accuracy 85.375%\n",
      "Epoch 16, Batch 440, LR 2.349397 Loss 6.117168, Accuracy 85.378%\n",
      "Epoch 16, Batch 441, LR 2.349333 Loss 6.118400, Accuracy 85.374%\n",
      "Epoch 16, Batch 442, LR 2.349269 Loss 6.116739, Accuracy 85.386%\n",
      "Epoch 16, Batch 443, LR 2.349205 Loss 6.117183, Accuracy 85.378%\n",
      "Epoch 16, Batch 444, LR 2.349142 Loss 6.116340, Accuracy 85.389%\n",
      "Epoch 16, Batch 445, LR 2.349078 Loss 6.114738, Accuracy 85.392%\n",
      "Epoch 16, Batch 446, LR 2.349014 Loss 6.114991, Accuracy 85.386%\n",
      "Epoch 16, Batch 447, LR 2.348950 Loss 6.116957, Accuracy 85.371%\n",
      "Epoch 16, Batch 448, LR 2.348886 Loss 6.118153, Accuracy 85.367%\n",
      "Epoch 16, Batch 449, LR 2.348822 Loss 6.117742, Accuracy 85.363%\n",
      "Epoch 16, Batch 450, LR 2.348759 Loss 6.115725, Accuracy 85.375%\n",
      "Epoch 16, Batch 451, LR 2.348695 Loss 6.114694, Accuracy 85.385%\n",
      "Epoch 16, Batch 452, LR 2.348631 Loss 6.114571, Accuracy 85.377%\n",
      "Epoch 16, Batch 453, LR 2.348567 Loss 6.112886, Accuracy 85.386%\n",
      "Epoch 16, Batch 454, LR 2.348503 Loss 6.114599, Accuracy 85.370%\n",
      "Epoch 16, Batch 455, LR 2.348439 Loss 6.112798, Accuracy 85.378%\n",
      "Epoch 16, Batch 456, LR 2.348375 Loss 6.114251, Accuracy 85.374%\n",
      "Epoch 16, Batch 457, LR 2.348311 Loss 6.113968, Accuracy 85.380%\n",
      "Epoch 16, Batch 458, LR 2.348247 Loss 6.113345, Accuracy 85.381%\n",
      "Epoch 16, Batch 459, LR 2.348183 Loss 6.114268, Accuracy 85.376%\n",
      "Epoch 16, Batch 460, LR 2.348119 Loss 6.113030, Accuracy 85.374%\n",
      "Epoch 16, Batch 461, LR 2.348055 Loss 6.113056, Accuracy 85.377%\n",
      "Epoch 16, Batch 462, LR 2.347991 Loss 6.112239, Accuracy 85.378%\n",
      "Epoch 16, Batch 463, LR 2.347927 Loss 6.112415, Accuracy 85.371%\n",
      "Epoch 16, Batch 464, LR 2.347863 Loss 6.113290, Accuracy 85.365%\n",
      "Epoch 16, Batch 465, LR 2.347799 Loss 6.113209, Accuracy 85.361%\n",
      "Epoch 16, Batch 466, LR 2.347735 Loss 6.112556, Accuracy 85.364%\n",
      "Epoch 16, Batch 467, LR 2.347671 Loss 6.113781, Accuracy 85.350%\n",
      "Epoch 16, Batch 468, LR 2.347607 Loss 6.114012, Accuracy 85.355%\n",
      "Epoch 16, Batch 469, LR 2.347543 Loss 6.114573, Accuracy 85.359%\n",
      "Epoch 16, Batch 470, LR 2.347479 Loss 6.113370, Accuracy 85.371%\n",
      "Epoch 16, Batch 471, LR 2.347414 Loss 6.113471, Accuracy 85.370%\n",
      "Epoch 16, Batch 472, LR 2.347350 Loss 6.113890, Accuracy 85.365%\n",
      "Epoch 16, Batch 473, LR 2.347286 Loss 6.112632, Accuracy 85.368%\n",
      "Epoch 16, Batch 474, LR 2.347222 Loss 6.113042, Accuracy 85.361%\n",
      "Epoch 16, Batch 475, LR 2.347158 Loss 6.112893, Accuracy 85.360%\n",
      "Epoch 16, Batch 476, LR 2.347094 Loss 6.113286, Accuracy 85.363%\n",
      "Epoch 16, Batch 477, LR 2.347029 Loss 6.114273, Accuracy 85.353%\n",
      "Epoch 16, Batch 478, LR 2.346965 Loss 6.113131, Accuracy 85.362%\n",
      "Epoch 16, Batch 479, LR 2.346901 Loss 6.113179, Accuracy 85.360%\n",
      "Epoch 16, Batch 480, LR 2.346837 Loss 6.112986, Accuracy 85.363%\n",
      "Epoch 16, Batch 481, LR 2.346773 Loss 6.113042, Accuracy 85.361%\n",
      "Epoch 16, Batch 482, LR 2.346708 Loss 6.111916, Accuracy 85.365%\n",
      "Epoch 16, Batch 483, LR 2.346644 Loss 6.111545, Accuracy 85.365%\n",
      "Epoch 16, Batch 484, LR 2.346580 Loss 6.112159, Accuracy 85.356%\n",
      "Epoch 16, Batch 485, LR 2.346515 Loss 6.111779, Accuracy 85.353%\n",
      "Epoch 16, Batch 486, LR 2.346451 Loss 6.111637, Accuracy 85.343%\n",
      "Epoch 16, Batch 487, LR 2.346387 Loss 6.112228, Accuracy 85.333%\n",
      "Epoch 16, Batch 488, LR 2.346322 Loss 6.112301, Accuracy 85.329%\n",
      "Epoch 16, Batch 489, LR 2.346258 Loss 6.113791, Accuracy 85.316%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 490, LR 2.346194 Loss 6.113633, Accuracy 85.312%\n",
      "Epoch 16, Batch 491, LR 2.346129 Loss 6.113318, Accuracy 85.325%\n",
      "Epoch 16, Batch 492, LR 2.346065 Loss 6.113537, Accuracy 85.328%\n",
      "Epoch 16, Batch 493, LR 2.346000 Loss 6.112486, Accuracy 85.331%\n",
      "Epoch 16, Batch 494, LR 2.345936 Loss 6.112607, Accuracy 85.332%\n",
      "Epoch 16, Batch 495, LR 2.345872 Loss 6.114355, Accuracy 85.322%\n",
      "Epoch 16, Batch 496, LR 2.345807 Loss 6.114803, Accuracy 85.325%\n",
      "Epoch 16, Batch 497, LR 2.345743 Loss 6.116060, Accuracy 85.321%\n",
      "Epoch 16, Batch 498, LR 2.345678 Loss 6.116294, Accuracy 85.315%\n",
      "Epoch 16, Batch 499, LR 2.345614 Loss 6.116170, Accuracy 85.322%\n",
      "Epoch 16, Batch 500, LR 2.345549 Loss 6.117463, Accuracy 85.314%\n",
      "Epoch 16, Batch 501, LR 2.345485 Loss 6.117661, Accuracy 85.309%\n",
      "Epoch 16, Batch 502, LR 2.345420 Loss 6.117981, Accuracy 85.299%\n",
      "Epoch 16, Batch 503, LR 2.345356 Loss 6.117803, Accuracy 85.305%\n",
      "Epoch 16, Batch 504, LR 2.345291 Loss 6.118147, Accuracy 85.302%\n",
      "Epoch 16, Batch 505, LR 2.345227 Loss 6.117603, Accuracy 85.309%\n",
      "Epoch 16, Batch 506, LR 2.345162 Loss 6.118277, Accuracy 85.308%\n",
      "Epoch 16, Batch 507, LR 2.345097 Loss 6.117748, Accuracy 85.306%\n",
      "Epoch 16, Batch 508, LR 2.345033 Loss 6.118875, Accuracy 85.299%\n",
      "Epoch 16, Batch 509, LR 2.344968 Loss 6.118333, Accuracy 85.304%\n",
      "Epoch 16, Batch 510, LR 2.344904 Loss 6.117024, Accuracy 85.311%\n",
      "Epoch 16, Batch 511, LR 2.344839 Loss 6.117093, Accuracy 85.315%\n",
      "Epoch 16, Batch 512, LR 2.344774 Loss 6.115854, Accuracy 85.324%\n",
      "Epoch 16, Batch 513, LR 2.344710 Loss 6.114570, Accuracy 85.330%\n",
      "Epoch 16, Batch 514, LR 2.344645 Loss 6.112919, Accuracy 85.342%\n",
      "Epoch 16, Batch 515, LR 2.344580 Loss 6.112032, Accuracy 85.349%\n",
      "Epoch 16, Batch 516, LR 2.344516 Loss 6.114065, Accuracy 85.333%\n",
      "Epoch 16, Batch 517, LR 2.344451 Loss 6.114548, Accuracy 85.330%\n",
      "Epoch 16, Batch 518, LR 2.344386 Loss 6.114713, Accuracy 85.327%\n",
      "Epoch 16, Batch 519, LR 2.344322 Loss 6.114060, Accuracy 85.329%\n",
      "Epoch 16, Batch 520, LR 2.344257 Loss 6.116031, Accuracy 85.312%\n",
      "Epoch 16, Batch 521, LR 2.344192 Loss 6.115607, Accuracy 85.312%\n",
      "Epoch 16, Batch 522, LR 2.344127 Loss 6.114657, Accuracy 85.318%\n",
      "Epoch 16, Batch 523, LR 2.344062 Loss 6.115786, Accuracy 85.319%\n",
      "Epoch 16, Batch 524, LR 2.343998 Loss 6.116199, Accuracy 85.316%\n",
      "Epoch 16, Batch 525, LR 2.343933 Loss 6.116607, Accuracy 85.311%\n",
      "Epoch 16, Batch 526, LR 2.343868 Loss 6.116703, Accuracy 85.312%\n",
      "Epoch 16, Batch 527, LR 2.343803 Loss 6.115916, Accuracy 85.319%\n",
      "Epoch 16, Batch 528, LR 2.343738 Loss 6.116732, Accuracy 85.312%\n",
      "Epoch 16, Batch 529, LR 2.343673 Loss 6.116417, Accuracy 85.314%\n",
      "Epoch 16, Batch 530, LR 2.343609 Loss 6.116465, Accuracy 85.320%\n",
      "Epoch 16, Batch 531, LR 2.343544 Loss 6.115291, Accuracy 85.331%\n",
      "Epoch 16, Batch 532, LR 2.343479 Loss 6.116379, Accuracy 85.331%\n",
      "Epoch 16, Batch 533, LR 2.343414 Loss 6.115713, Accuracy 85.334%\n",
      "Epoch 16, Batch 534, LR 2.343349 Loss 6.115246, Accuracy 85.339%\n",
      "Epoch 16, Batch 535, LR 2.343284 Loss 6.114888, Accuracy 85.346%\n",
      "Epoch 16, Batch 536, LR 2.343219 Loss 6.113230, Accuracy 85.354%\n",
      "Epoch 16, Batch 537, LR 2.343154 Loss 6.112153, Accuracy 85.357%\n",
      "Epoch 16, Batch 538, LR 2.343089 Loss 6.111290, Accuracy 85.357%\n",
      "Epoch 16, Batch 539, LR 2.343024 Loss 6.110364, Accuracy 85.365%\n",
      "Epoch 16, Batch 540, LR 2.342959 Loss 6.110539, Accuracy 85.356%\n",
      "Epoch 16, Batch 541, LR 2.342894 Loss 6.110609, Accuracy 85.351%\n",
      "Epoch 16, Batch 542, LR 2.342829 Loss 6.110371, Accuracy 85.355%\n",
      "Epoch 16, Batch 543, LR 2.342764 Loss 6.109993, Accuracy 85.362%\n",
      "Epoch 16, Batch 544, LR 2.342699 Loss 6.110494, Accuracy 85.360%\n",
      "Epoch 16, Batch 545, LR 2.342634 Loss 6.110420, Accuracy 85.364%\n",
      "Epoch 16, Batch 546, LR 2.342569 Loss 6.110451, Accuracy 85.362%\n",
      "Epoch 16, Batch 547, LR 2.342504 Loss 6.110730, Accuracy 85.360%\n",
      "Epoch 16, Batch 548, LR 2.342439 Loss 6.111780, Accuracy 85.356%\n",
      "Epoch 16, Batch 549, LR 2.342374 Loss 6.111046, Accuracy 85.364%\n",
      "Epoch 16, Batch 550, LR 2.342309 Loss 6.112601, Accuracy 85.347%\n",
      "Epoch 16, Batch 551, LR 2.342243 Loss 6.112711, Accuracy 85.353%\n",
      "Epoch 16, Batch 552, LR 2.342178 Loss 6.112375, Accuracy 85.352%\n",
      "Epoch 16, Batch 553, LR 2.342113 Loss 6.110892, Accuracy 85.358%\n",
      "Epoch 16, Batch 554, LR 2.342048 Loss 6.110282, Accuracy 85.361%\n",
      "Epoch 16, Batch 555, LR 2.341983 Loss 6.109334, Accuracy 85.366%\n",
      "Epoch 16, Batch 556, LR 2.341918 Loss 6.109663, Accuracy 85.360%\n",
      "Epoch 16, Batch 557, LR 2.341852 Loss 6.109404, Accuracy 85.354%\n",
      "Epoch 16, Batch 558, LR 2.341787 Loss 6.112182, Accuracy 85.333%\n",
      "Epoch 16, Batch 559, LR 2.341722 Loss 6.112705, Accuracy 85.334%\n",
      "Epoch 16, Batch 560, LR 2.341657 Loss 6.112036, Accuracy 85.340%\n",
      "Epoch 16, Batch 561, LR 2.341591 Loss 6.111203, Accuracy 85.347%\n",
      "Epoch 16, Batch 562, LR 2.341526 Loss 6.110231, Accuracy 85.348%\n",
      "Epoch 16, Batch 563, LR 2.341461 Loss 6.109806, Accuracy 85.351%\n",
      "Epoch 16, Batch 564, LR 2.341396 Loss 6.109181, Accuracy 85.354%\n",
      "Epoch 16, Batch 565, LR 2.341330 Loss 6.109878, Accuracy 85.351%\n",
      "Epoch 16, Batch 566, LR 2.341265 Loss 6.110386, Accuracy 85.352%\n",
      "Epoch 16, Batch 567, LR 2.341200 Loss 6.110374, Accuracy 85.348%\n",
      "Epoch 16, Batch 568, LR 2.341134 Loss 6.110298, Accuracy 85.352%\n",
      "Epoch 16, Batch 569, LR 2.341069 Loss 6.110771, Accuracy 85.346%\n",
      "Epoch 16, Batch 570, LR 2.341003 Loss 6.111466, Accuracy 85.343%\n",
      "Epoch 16, Batch 571, LR 2.340938 Loss 6.111727, Accuracy 85.340%\n",
      "Epoch 16, Batch 572, LR 2.340873 Loss 6.112695, Accuracy 85.332%\n",
      "Epoch 16, Batch 573, LR 2.340807 Loss 6.113189, Accuracy 85.325%\n",
      "Epoch 16, Batch 574, LR 2.340742 Loss 6.112386, Accuracy 85.326%\n",
      "Epoch 16, Batch 575, LR 2.340676 Loss 6.112207, Accuracy 85.329%\n",
      "Epoch 16, Batch 576, LR 2.340611 Loss 6.111804, Accuracy 85.330%\n",
      "Epoch 16, Batch 577, LR 2.340546 Loss 6.111456, Accuracy 85.328%\n",
      "Epoch 16, Batch 578, LR 2.340480 Loss 6.110752, Accuracy 85.331%\n",
      "Epoch 16, Batch 579, LR 2.340415 Loss 6.111391, Accuracy 85.322%\n",
      "Epoch 16, Batch 580, LR 2.340349 Loss 6.112592, Accuracy 85.318%\n",
      "Epoch 16, Batch 581, LR 2.340284 Loss 6.111638, Accuracy 85.327%\n",
      "Epoch 16, Batch 582, LR 2.340218 Loss 6.112136, Accuracy 85.325%\n",
      "Epoch 16, Batch 583, LR 2.340153 Loss 6.111236, Accuracy 85.334%\n",
      "Epoch 16, Batch 584, LR 2.340087 Loss 6.112034, Accuracy 85.329%\n",
      "Epoch 16, Batch 585, LR 2.340021 Loss 6.111255, Accuracy 85.339%\n",
      "Epoch 16, Batch 586, LR 2.339956 Loss 6.112486, Accuracy 85.330%\n",
      "Epoch 16, Batch 587, LR 2.339890 Loss 6.112728, Accuracy 85.331%\n",
      "Epoch 16, Batch 588, LR 2.339825 Loss 6.112872, Accuracy 85.337%\n",
      "Epoch 16, Batch 589, LR 2.339759 Loss 6.112792, Accuracy 85.334%\n",
      "Epoch 16, Batch 590, LR 2.339693 Loss 6.112698, Accuracy 85.328%\n",
      "Epoch 16, Batch 591, LR 2.339628 Loss 6.112622, Accuracy 85.327%\n",
      "Epoch 16, Batch 592, LR 2.339562 Loss 6.111492, Accuracy 85.336%\n",
      "Epoch 16, Batch 593, LR 2.339496 Loss 6.111258, Accuracy 85.342%\n",
      "Epoch 16, Batch 594, LR 2.339431 Loss 6.110797, Accuracy 85.346%\n",
      "Epoch 16, Batch 595, LR 2.339365 Loss 6.110183, Accuracy 85.356%\n",
      "Epoch 16, Batch 596, LR 2.339299 Loss 6.111156, Accuracy 85.348%\n",
      "Epoch 16, Batch 597, LR 2.339234 Loss 6.110366, Accuracy 85.354%\n",
      "Epoch 16, Batch 598, LR 2.339168 Loss 6.110625, Accuracy 85.352%\n",
      "Epoch 16, Batch 599, LR 2.339102 Loss 6.110853, Accuracy 85.351%\n",
      "Epoch 16, Batch 600, LR 2.339036 Loss 6.111075, Accuracy 85.353%\n",
      "Epoch 16, Batch 601, LR 2.338971 Loss 6.110411, Accuracy 85.353%\n",
      "Epoch 16, Batch 602, LR 2.338905 Loss 6.110197, Accuracy 85.347%\n",
      "Epoch 16, Batch 603, LR 2.338839 Loss 6.111010, Accuracy 85.342%\n",
      "Epoch 16, Batch 604, LR 2.338773 Loss 6.111610, Accuracy 85.343%\n",
      "Epoch 16, Batch 605, LR 2.338708 Loss 6.111454, Accuracy 85.342%\n",
      "Epoch 16, Batch 606, LR 2.338642 Loss 6.111928, Accuracy 85.337%\n",
      "Epoch 16, Batch 607, LR 2.338576 Loss 6.112774, Accuracy 85.335%\n",
      "Epoch 16, Batch 608, LR 2.338510 Loss 6.112703, Accuracy 85.340%\n",
      "Epoch 16, Batch 609, LR 2.338444 Loss 6.111300, Accuracy 85.347%\n",
      "Epoch 16, Batch 610, LR 2.338378 Loss 6.111188, Accuracy 85.342%\n",
      "Epoch 16, Batch 611, LR 2.338312 Loss 6.110077, Accuracy 85.347%\n",
      "Epoch 16, Batch 612, LR 2.338247 Loss 6.110066, Accuracy 85.345%\n",
      "Epoch 16, Batch 613, LR 2.338181 Loss 6.110194, Accuracy 85.337%\n",
      "Epoch 16, Batch 614, LR 2.338115 Loss 6.109064, Accuracy 85.347%\n",
      "Epoch 16, Batch 615, LR 2.338049 Loss 6.109747, Accuracy 85.347%\n",
      "Epoch 16, Batch 616, LR 2.337983 Loss 6.109615, Accuracy 85.348%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 617, LR 2.337917 Loss 6.110124, Accuracy 85.345%\n",
      "Epoch 16, Batch 618, LR 2.337851 Loss 6.110401, Accuracy 85.341%\n",
      "Epoch 16, Batch 619, LR 2.337785 Loss 6.109232, Accuracy 85.349%\n",
      "Epoch 16, Batch 620, LR 2.337719 Loss 6.109291, Accuracy 85.339%\n",
      "Epoch 16, Batch 621, LR 2.337653 Loss 6.107996, Accuracy 85.347%\n",
      "Epoch 16, Batch 622, LR 2.337587 Loss 6.107958, Accuracy 85.345%\n",
      "Epoch 16, Batch 623, LR 2.337521 Loss 6.107978, Accuracy 85.344%\n",
      "Epoch 16, Batch 624, LR 2.337455 Loss 6.107463, Accuracy 85.349%\n",
      "Epoch 16, Batch 625, LR 2.337389 Loss 6.106742, Accuracy 85.358%\n",
      "Epoch 16, Batch 626, LR 2.337323 Loss 6.106840, Accuracy 85.351%\n",
      "Epoch 16, Batch 627, LR 2.337257 Loss 6.106888, Accuracy 85.346%\n",
      "Epoch 16, Batch 628, LR 2.337190 Loss 6.106812, Accuracy 85.344%\n",
      "Epoch 16, Batch 629, LR 2.337124 Loss 6.107008, Accuracy 85.346%\n",
      "Epoch 16, Batch 630, LR 2.337058 Loss 6.106508, Accuracy 85.355%\n",
      "Epoch 16, Batch 631, LR 2.336992 Loss 6.105962, Accuracy 85.357%\n",
      "Epoch 16, Batch 632, LR 2.336926 Loss 6.106091, Accuracy 85.358%\n",
      "Epoch 16, Batch 633, LR 2.336860 Loss 6.105959, Accuracy 85.360%\n",
      "Epoch 16, Batch 634, LR 2.336794 Loss 6.105773, Accuracy 85.353%\n",
      "Epoch 16, Batch 635, LR 2.336727 Loss 6.104476, Accuracy 85.357%\n",
      "Epoch 16, Batch 636, LR 2.336661 Loss 6.103268, Accuracy 85.365%\n",
      "Epoch 16, Batch 637, LR 2.336595 Loss 6.103134, Accuracy 85.366%\n",
      "Epoch 16, Batch 638, LR 2.336529 Loss 6.104304, Accuracy 85.361%\n",
      "Epoch 16, Batch 639, LR 2.336463 Loss 6.105224, Accuracy 85.351%\n",
      "Epoch 16, Batch 640, LR 2.336396 Loss 6.103635, Accuracy 85.365%\n",
      "Epoch 16, Batch 641, LR 2.336330 Loss 6.102033, Accuracy 85.366%\n",
      "Epoch 16, Batch 642, LR 2.336264 Loss 6.101589, Accuracy 85.368%\n",
      "Epoch 16, Batch 643, LR 2.336197 Loss 6.102802, Accuracy 85.360%\n",
      "Epoch 16, Batch 644, LR 2.336131 Loss 6.102360, Accuracy 85.354%\n",
      "Epoch 16, Batch 645, LR 2.336065 Loss 6.102617, Accuracy 85.356%\n",
      "Epoch 16, Batch 646, LR 2.335999 Loss 6.103395, Accuracy 85.346%\n",
      "Epoch 16, Batch 647, LR 2.335932 Loss 6.102625, Accuracy 85.347%\n",
      "Epoch 16, Batch 648, LR 2.335866 Loss 6.103360, Accuracy 85.342%\n",
      "Epoch 16, Batch 649, LR 2.335800 Loss 6.104409, Accuracy 85.337%\n",
      "Epoch 16, Batch 650, LR 2.335733 Loss 6.104656, Accuracy 85.339%\n",
      "Epoch 16, Batch 651, LR 2.335667 Loss 6.104592, Accuracy 85.346%\n",
      "Epoch 16, Batch 652, LR 2.335600 Loss 6.104248, Accuracy 85.344%\n",
      "Epoch 16, Batch 653, LR 2.335534 Loss 6.104495, Accuracy 85.340%\n",
      "Epoch 16, Batch 654, LR 2.335468 Loss 6.104626, Accuracy 85.341%\n",
      "Epoch 16, Batch 655, LR 2.335401 Loss 6.104380, Accuracy 85.342%\n",
      "Epoch 16, Batch 656, LR 2.335335 Loss 6.104976, Accuracy 85.338%\n",
      "Epoch 16, Batch 657, LR 2.335268 Loss 6.106072, Accuracy 85.329%\n",
      "Epoch 16, Batch 658, LR 2.335202 Loss 6.107394, Accuracy 85.321%\n",
      "Epoch 16, Batch 659, LR 2.335135 Loss 6.107021, Accuracy 85.331%\n",
      "Epoch 16, Batch 660, LR 2.335069 Loss 6.107413, Accuracy 85.329%\n",
      "Epoch 16, Batch 661, LR 2.335002 Loss 6.106556, Accuracy 85.334%\n",
      "Epoch 16, Batch 662, LR 2.334936 Loss 6.106424, Accuracy 85.337%\n",
      "Epoch 16, Batch 663, LR 2.334869 Loss 6.105630, Accuracy 85.342%\n",
      "Epoch 16, Batch 664, LR 2.334803 Loss 6.104258, Accuracy 85.347%\n",
      "Epoch 16, Batch 665, LR 2.334736 Loss 6.103704, Accuracy 85.351%\n",
      "Epoch 16, Batch 666, LR 2.334669 Loss 6.104444, Accuracy 85.345%\n",
      "Epoch 16, Batch 667, LR 2.334603 Loss 6.103689, Accuracy 85.352%\n",
      "Epoch 16, Batch 668, LR 2.334536 Loss 6.102947, Accuracy 85.356%\n",
      "Epoch 16, Batch 669, LR 2.334470 Loss 6.103364, Accuracy 85.351%\n",
      "Epoch 16, Batch 670, LR 2.334403 Loss 6.102742, Accuracy 85.354%\n",
      "Epoch 16, Batch 671, LR 2.334336 Loss 6.102534, Accuracy 85.355%\n",
      "Epoch 16, Batch 672, LR 2.334270 Loss 6.102520, Accuracy 85.357%\n",
      "Epoch 16, Batch 673, LR 2.334203 Loss 6.103237, Accuracy 85.358%\n",
      "Epoch 16, Batch 674, LR 2.334136 Loss 6.102260, Accuracy 85.364%\n",
      "Epoch 16, Batch 675, LR 2.334070 Loss 6.102053, Accuracy 85.372%\n",
      "Epoch 16, Batch 676, LR 2.334003 Loss 6.101455, Accuracy 85.374%\n",
      "Epoch 16, Batch 677, LR 2.333936 Loss 6.101863, Accuracy 85.371%\n",
      "Epoch 16, Batch 678, LR 2.333870 Loss 6.102446, Accuracy 85.364%\n",
      "Epoch 16, Batch 679, LR 2.333803 Loss 6.104009, Accuracy 85.355%\n",
      "Epoch 16, Batch 680, LR 2.333736 Loss 6.104730, Accuracy 85.353%\n",
      "Epoch 16, Batch 681, LR 2.333669 Loss 6.105224, Accuracy 85.356%\n",
      "Epoch 16, Batch 682, LR 2.333603 Loss 6.105275, Accuracy 85.352%\n",
      "Epoch 16, Batch 683, LR 2.333536 Loss 6.105218, Accuracy 85.352%\n",
      "Epoch 16, Batch 684, LR 2.333469 Loss 6.105449, Accuracy 85.350%\n",
      "Epoch 16, Batch 685, LR 2.333402 Loss 6.104692, Accuracy 85.355%\n",
      "Epoch 16, Batch 686, LR 2.333335 Loss 6.105407, Accuracy 85.354%\n",
      "Epoch 16, Batch 687, LR 2.333269 Loss 6.105600, Accuracy 85.354%\n",
      "Epoch 16, Batch 688, LR 2.333202 Loss 6.106067, Accuracy 85.352%\n",
      "Epoch 16, Batch 689, LR 2.333135 Loss 6.106282, Accuracy 85.352%\n",
      "Epoch 16, Batch 690, LR 2.333068 Loss 6.105236, Accuracy 85.357%\n",
      "Epoch 16, Batch 691, LR 2.333001 Loss 6.104979, Accuracy 85.361%\n",
      "Epoch 16, Batch 692, LR 2.332934 Loss 6.105082, Accuracy 85.362%\n",
      "Epoch 16, Batch 693, LR 2.332867 Loss 6.105268, Accuracy 85.363%\n",
      "Epoch 16, Batch 694, LR 2.332800 Loss 6.104720, Accuracy 85.365%\n",
      "Epoch 16, Batch 695, LR 2.332733 Loss 6.105585, Accuracy 85.357%\n",
      "Epoch 16, Batch 696, LR 2.332667 Loss 6.105613, Accuracy 85.359%\n",
      "Epoch 16, Batch 697, LR 2.332600 Loss 6.106087, Accuracy 85.357%\n",
      "Epoch 16, Batch 698, LR 2.332533 Loss 6.105294, Accuracy 85.363%\n",
      "Epoch 16, Batch 699, LR 2.332466 Loss 6.105918, Accuracy 85.356%\n",
      "Epoch 16, Batch 700, LR 2.332399 Loss 6.104958, Accuracy 85.364%\n",
      "Epoch 16, Batch 701, LR 2.332332 Loss 6.104982, Accuracy 85.366%\n",
      "Epoch 16, Batch 702, LR 2.332265 Loss 6.104712, Accuracy 85.364%\n",
      "Epoch 16, Batch 703, LR 2.332198 Loss 6.105580, Accuracy 85.359%\n",
      "Epoch 16, Batch 704, LR 2.332131 Loss 6.106195, Accuracy 85.350%\n",
      "Epoch 16, Batch 705, LR 2.332063 Loss 6.105676, Accuracy 85.351%\n",
      "Epoch 16, Batch 706, LR 2.331996 Loss 6.105301, Accuracy 85.354%\n",
      "Epoch 16, Batch 707, LR 2.331929 Loss 6.104895, Accuracy 85.357%\n",
      "Epoch 16, Batch 708, LR 2.331862 Loss 6.105393, Accuracy 85.352%\n",
      "Epoch 16, Batch 709, LR 2.331795 Loss 6.105520, Accuracy 85.353%\n",
      "Epoch 16, Batch 710, LR 2.331728 Loss 6.105687, Accuracy 85.351%\n",
      "Epoch 16, Batch 711, LR 2.331661 Loss 6.105830, Accuracy 85.354%\n",
      "Epoch 16, Batch 712, LR 2.331594 Loss 6.106750, Accuracy 85.352%\n",
      "Epoch 16, Batch 713, LR 2.331527 Loss 6.106866, Accuracy 85.350%\n",
      "Epoch 16, Batch 714, LR 2.331459 Loss 6.106036, Accuracy 85.358%\n",
      "Epoch 16, Batch 715, LR 2.331392 Loss 6.106169, Accuracy 85.361%\n",
      "Epoch 16, Batch 716, LR 2.331325 Loss 6.105850, Accuracy 85.358%\n",
      "Epoch 16, Batch 717, LR 2.331258 Loss 6.105638, Accuracy 85.359%\n",
      "Epoch 16, Batch 718, LR 2.331191 Loss 6.105679, Accuracy 85.356%\n",
      "Epoch 16, Batch 719, LR 2.331123 Loss 6.105233, Accuracy 85.365%\n",
      "Epoch 16, Batch 720, LR 2.331056 Loss 6.104738, Accuracy 85.371%\n",
      "Epoch 16, Batch 721, LR 2.330989 Loss 6.105021, Accuracy 85.371%\n",
      "Epoch 16, Batch 722, LR 2.330922 Loss 6.104891, Accuracy 85.374%\n",
      "Epoch 16, Batch 723, LR 2.330854 Loss 6.105115, Accuracy 85.379%\n",
      "Epoch 16, Batch 724, LR 2.330787 Loss 6.105696, Accuracy 85.371%\n",
      "Epoch 16, Batch 725, LR 2.330720 Loss 6.105805, Accuracy 85.369%\n",
      "Epoch 16, Batch 726, LR 2.330652 Loss 6.105288, Accuracy 85.371%\n",
      "Epoch 16, Batch 727, LR 2.330585 Loss 6.105983, Accuracy 85.372%\n",
      "Epoch 16, Batch 728, LR 2.330518 Loss 6.105323, Accuracy 85.372%\n",
      "Epoch 16, Batch 729, LR 2.330450 Loss 6.105648, Accuracy 85.367%\n",
      "Epoch 16, Batch 730, LR 2.330383 Loss 6.105277, Accuracy 85.372%\n",
      "Epoch 16, Batch 731, LR 2.330316 Loss 6.105085, Accuracy 85.374%\n",
      "Epoch 16, Batch 732, LR 2.330248 Loss 6.105465, Accuracy 85.372%\n",
      "Epoch 16, Batch 733, LR 2.330181 Loss 6.105398, Accuracy 85.374%\n",
      "Epoch 16, Batch 734, LR 2.330113 Loss 6.104755, Accuracy 85.380%\n",
      "Epoch 16, Batch 735, LR 2.330046 Loss 6.104268, Accuracy 85.384%\n",
      "Epoch 16, Batch 736, LR 2.329979 Loss 6.103639, Accuracy 85.383%\n",
      "Epoch 16, Batch 737, LR 2.329911 Loss 6.104064, Accuracy 85.378%\n",
      "Epoch 16, Batch 738, LR 2.329844 Loss 6.104216, Accuracy 85.376%\n",
      "Epoch 16, Batch 739, LR 2.329776 Loss 6.103759, Accuracy 85.382%\n",
      "Epoch 16, Batch 740, LR 2.329709 Loss 6.103591, Accuracy 85.380%\n",
      "Epoch 16, Batch 741, LR 2.329641 Loss 6.103581, Accuracy 85.379%\n",
      "Epoch 16, Batch 742, LR 2.329574 Loss 6.104218, Accuracy 85.374%\n",
      "Epoch 16, Batch 743, LR 2.329506 Loss 6.103369, Accuracy 85.379%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 744, LR 2.329439 Loss 6.102449, Accuracy 85.382%\n",
      "Epoch 16, Batch 745, LR 2.329371 Loss 6.102542, Accuracy 85.384%\n",
      "Epoch 16, Batch 746, LR 2.329303 Loss 6.103084, Accuracy 85.380%\n",
      "Epoch 16, Batch 747, LR 2.329236 Loss 6.104057, Accuracy 85.370%\n",
      "Epoch 16, Batch 748, LR 2.329168 Loss 6.102988, Accuracy 85.377%\n",
      "Epoch 16, Batch 749, LR 2.329101 Loss 6.103214, Accuracy 85.372%\n",
      "Epoch 16, Batch 750, LR 2.329033 Loss 6.102546, Accuracy 85.378%\n",
      "Epoch 16, Batch 751, LR 2.328965 Loss 6.103263, Accuracy 85.374%\n",
      "Epoch 16, Batch 752, LR 2.328898 Loss 6.103341, Accuracy 85.372%\n",
      "Epoch 16, Batch 753, LR 2.328830 Loss 6.104270, Accuracy 85.366%\n",
      "Epoch 16, Batch 754, LR 2.328762 Loss 6.105149, Accuracy 85.360%\n",
      "Epoch 16, Batch 755, LR 2.328695 Loss 6.105169, Accuracy 85.357%\n",
      "Epoch 16, Batch 756, LR 2.328627 Loss 6.105497, Accuracy 85.359%\n",
      "Epoch 16, Batch 757, LR 2.328559 Loss 6.106686, Accuracy 85.351%\n",
      "Epoch 16, Batch 758, LR 2.328492 Loss 6.106771, Accuracy 85.349%\n",
      "Epoch 16, Batch 759, LR 2.328424 Loss 6.107703, Accuracy 85.342%\n",
      "Epoch 16, Batch 760, LR 2.328356 Loss 6.107953, Accuracy 85.337%\n",
      "Epoch 16, Batch 761, LR 2.328288 Loss 6.107492, Accuracy 85.340%\n",
      "Epoch 16, Batch 762, LR 2.328221 Loss 6.107894, Accuracy 85.340%\n",
      "Epoch 16, Batch 763, LR 2.328153 Loss 6.107669, Accuracy 85.337%\n",
      "Epoch 16, Batch 764, LR 2.328085 Loss 6.108215, Accuracy 85.334%\n",
      "Epoch 16, Batch 765, LR 2.328017 Loss 6.107912, Accuracy 85.334%\n",
      "Epoch 16, Batch 766, LR 2.327949 Loss 6.107567, Accuracy 85.336%\n",
      "Epoch 16, Batch 767, LR 2.327882 Loss 6.107367, Accuracy 85.338%\n",
      "Epoch 16, Batch 768, LR 2.327814 Loss 6.107134, Accuracy 85.342%\n",
      "Epoch 16, Batch 769, LR 2.327746 Loss 6.107639, Accuracy 85.342%\n",
      "Epoch 16, Batch 770, LR 2.327678 Loss 6.108402, Accuracy 85.342%\n",
      "Epoch 16, Batch 771, LR 2.327610 Loss 6.107952, Accuracy 85.340%\n",
      "Epoch 16, Batch 772, LR 2.327542 Loss 6.108197, Accuracy 85.339%\n",
      "Epoch 16, Batch 773, LR 2.327474 Loss 6.108607, Accuracy 85.342%\n",
      "Epoch 16, Batch 774, LR 2.327407 Loss 6.107972, Accuracy 85.348%\n",
      "Epoch 16, Batch 775, LR 2.327339 Loss 6.107951, Accuracy 85.348%\n",
      "Epoch 16, Batch 776, LR 2.327271 Loss 6.107569, Accuracy 85.351%\n",
      "Epoch 16, Batch 777, LR 2.327203 Loss 6.106606, Accuracy 85.353%\n",
      "Epoch 16, Batch 778, LR 2.327135 Loss 6.107136, Accuracy 85.347%\n",
      "Epoch 16, Batch 779, LR 2.327067 Loss 6.107586, Accuracy 85.344%\n",
      "Epoch 16, Batch 780, LR 2.326999 Loss 6.107207, Accuracy 85.345%\n",
      "Epoch 16, Batch 781, LR 2.326931 Loss 6.106721, Accuracy 85.346%\n",
      "Epoch 16, Batch 782, LR 2.326863 Loss 6.106917, Accuracy 85.349%\n",
      "Epoch 16, Batch 783, LR 2.326795 Loss 6.105899, Accuracy 85.349%\n",
      "Epoch 16, Batch 784, LR 2.326727 Loss 6.105493, Accuracy 85.352%\n",
      "Epoch 16, Batch 785, LR 2.326659 Loss 6.105588, Accuracy 85.353%\n",
      "Epoch 16, Batch 786, LR 2.326591 Loss 6.105147, Accuracy 85.354%\n",
      "Epoch 16, Batch 787, LR 2.326523 Loss 6.106317, Accuracy 85.347%\n",
      "Epoch 16, Batch 788, LR 2.326454 Loss 6.105346, Accuracy 85.351%\n",
      "Epoch 16, Batch 789, LR 2.326386 Loss 6.105736, Accuracy 85.350%\n",
      "Epoch 16, Batch 790, LR 2.326318 Loss 6.106370, Accuracy 85.346%\n",
      "Epoch 16, Batch 791, LR 2.326250 Loss 6.106457, Accuracy 85.347%\n",
      "Epoch 16, Batch 792, LR 2.326182 Loss 6.107088, Accuracy 85.341%\n",
      "Epoch 16, Batch 793, LR 2.326114 Loss 6.106604, Accuracy 85.340%\n",
      "Epoch 16, Batch 794, LR 2.326046 Loss 6.106706, Accuracy 85.342%\n",
      "Epoch 16, Batch 795, LR 2.325977 Loss 6.106314, Accuracy 85.343%\n",
      "Epoch 16, Batch 796, LR 2.325909 Loss 6.106181, Accuracy 85.341%\n",
      "Epoch 16, Batch 797, LR 2.325841 Loss 6.106505, Accuracy 85.337%\n",
      "Epoch 16, Batch 798, LR 2.325773 Loss 6.106275, Accuracy 85.337%\n",
      "Epoch 16, Batch 799, LR 2.325705 Loss 6.106265, Accuracy 85.337%\n",
      "Epoch 16, Batch 800, LR 2.325636 Loss 6.106252, Accuracy 85.335%\n",
      "Epoch 16, Batch 801, LR 2.325568 Loss 6.106233, Accuracy 85.333%\n",
      "Epoch 16, Batch 802, LR 2.325500 Loss 6.106637, Accuracy 85.334%\n",
      "Epoch 16, Batch 803, LR 2.325432 Loss 6.106868, Accuracy 85.331%\n",
      "Epoch 16, Batch 804, LR 2.325363 Loss 6.106525, Accuracy 85.333%\n",
      "Epoch 16, Batch 805, LR 2.325295 Loss 6.105254, Accuracy 85.341%\n",
      "Epoch 16, Batch 806, LR 2.325227 Loss 6.105217, Accuracy 85.344%\n",
      "Epoch 16, Batch 807, LR 2.325158 Loss 6.106237, Accuracy 85.338%\n",
      "Epoch 16, Batch 808, LR 2.325090 Loss 6.107035, Accuracy 85.335%\n",
      "Epoch 16, Batch 809, LR 2.325022 Loss 6.107691, Accuracy 85.334%\n",
      "Epoch 16, Batch 810, LR 2.324953 Loss 6.107099, Accuracy 85.337%\n",
      "Epoch 16, Batch 811, LR 2.324885 Loss 6.106476, Accuracy 85.340%\n",
      "Epoch 16, Batch 812, LR 2.324817 Loss 6.106647, Accuracy 85.339%\n",
      "Epoch 16, Batch 813, LR 2.324748 Loss 6.106397, Accuracy 85.343%\n",
      "Epoch 16, Batch 814, LR 2.324680 Loss 6.106460, Accuracy 85.339%\n",
      "Epoch 16, Batch 815, LR 2.324611 Loss 6.105826, Accuracy 85.339%\n",
      "Epoch 16, Batch 816, LR 2.324543 Loss 6.105913, Accuracy 85.338%\n",
      "Epoch 16, Batch 817, LR 2.324475 Loss 6.106237, Accuracy 85.339%\n",
      "Epoch 16, Batch 818, LR 2.324406 Loss 6.106390, Accuracy 85.343%\n",
      "Epoch 16, Batch 819, LR 2.324338 Loss 6.106431, Accuracy 85.341%\n",
      "Epoch 16, Batch 820, LR 2.324269 Loss 6.105589, Accuracy 85.346%\n",
      "Epoch 16, Batch 821, LR 2.324201 Loss 6.104693, Accuracy 85.350%\n",
      "Epoch 16, Batch 822, LR 2.324132 Loss 6.103735, Accuracy 85.357%\n",
      "Epoch 16, Batch 823, LR 2.324064 Loss 6.104770, Accuracy 85.349%\n",
      "Epoch 16, Batch 824, LR 2.323995 Loss 6.104589, Accuracy 85.345%\n",
      "Epoch 16, Batch 825, LR 2.323927 Loss 6.104445, Accuracy 85.342%\n",
      "Epoch 16, Batch 826, LR 2.323858 Loss 6.104149, Accuracy 85.347%\n",
      "Epoch 16, Batch 827, LR 2.323789 Loss 6.103833, Accuracy 85.352%\n",
      "Epoch 16, Batch 828, LR 2.323721 Loss 6.103984, Accuracy 85.351%\n",
      "Epoch 16, Batch 829, LR 2.323652 Loss 6.103734, Accuracy 85.353%\n",
      "Epoch 16, Batch 830, LR 2.323584 Loss 6.103648, Accuracy 85.355%\n",
      "Epoch 16, Batch 831, LR 2.323515 Loss 6.104089, Accuracy 85.354%\n",
      "Epoch 16, Batch 832, LR 2.323446 Loss 6.104510, Accuracy 85.349%\n",
      "Epoch 16, Batch 833, LR 2.323378 Loss 6.104576, Accuracy 85.348%\n",
      "Epoch 16, Batch 834, LR 2.323309 Loss 6.104018, Accuracy 85.348%\n",
      "Epoch 16, Batch 835, LR 2.323240 Loss 6.104139, Accuracy 85.345%\n",
      "Epoch 16, Batch 836, LR 2.323172 Loss 6.104508, Accuracy 85.348%\n",
      "Epoch 16, Batch 837, LR 2.323103 Loss 6.104349, Accuracy 85.352%\n",
      "Epoch 16, Batch 838, LR 2.323034 Loss 6.104810, Accuracy 85.351%\n",
      "Epoch 16, Batch 839, LR 2.322966 Loss 6.104115, Accuracy 85.360%\n",
      "Epoch 16, Batch 840, LR 2.322897 Loss 6.103442, Accuracy 85.362%\n",
      "Epoch 16, Batch 841, LR 2.322828 Loss 6.103903, Accuracy 85.359%\n",
      "Epoch 16, Batch 842, LR 2.322759 Loss 6.103667, Accuracy 85.358%\n",
      "Epoch 16, Batch 843, LR 2.322691 Loss 6.103629, Accuracy 85.363%\n",
      "Epoch 16, Batch 844, LR 2.322622 Loss 6.103523, Accuracy 85.359%\n",
      "Epoch 16, Batch 845, LR 2.322553 Loss 6.103925, Accuracy 85.358%\n",
      "Epoch 16, Batch 846, LR 2.322484 Loss 6.103839, Accuracy 85.360%\n",
      "Epoch 16, Batch 847, LR 2.322416 Loss 6.104031, Accuracy 85.359%\n",
      "Epoch 16, Batch 848, LR 2.322347 Loss 6.103807, Accuracy 85.363%\n",
      "Epoch 16, Batch 849, LR 2.322278 Loss 6.102721, Accuracy 85.367%\n",
      "Epoch 16, Batch 850, LR 2.322209 Loss 6.102146, Accuracy 85.368%\n",
      "Epoch 16, Batch 851, LR 2.322140 Loss 6.102109, Accuracy 85.366%\n",
      "Epoch 16, Batch 852, LR 2.322071 Loss 6.102522, Accuracy 85.366%\n",
      "Epoch 16, Batch 853, LR 2.322002 Loss 6.102900, Accuracy 85.362%\n",
      "Epoch 16, Batch 854, LR 2.321933 Loss 6.102315, Accuracy 85.365%\n",
      "Epoch 16, Batch 855, LR 2.321865 Loss 6.102226, Accuracy 85.365%\n",
      "Epoch 16, Batch 856, LR 2.321796 Loss 6.102793, Accuracy 85.363%\n",
      "Epoch 16, Batch 857, LR 2.321727 Loss 6.102137, Accuracy 85.369%\n",
      "Epoch 16, Batch 858, LR 2.321658 Loss 6.102302, Accuracy 85.367%\n",
      "Epoch 16, Batch 859, LR 2.321589 Loss 6.102518, Accuracy 85.363%\n",
      "Epoch 16, Batch 860, LR 2.321520 Loss 6.101375, Accuracy 85.365%\n",
      "Epoch 16, Batch 861, LR 2.321451 Loss 6.101613, Accuracy 85.364%\n",
      "Epoch 16, Batch 862, LR 2.321382 Loss 6.102036, Accuracy 85.359%\n",
      "Epoch 16, Batch 863, LR 2.321313 Loss 6.102816, Accuracy 85.355%\n",
      "Epoch 16, Batch 864, LR 2.321244 Loss 6.101993, Accuracy 85.359%\n",
      "Epoch 16, Batch 865, LR 2.321175 Loss 6.102246, Accuracy 85.359%\n",
      "Epoch 16, Batch 866, LR 2.321106 Loss 6.101979, Accuracy 85.361%\n",
      "Epoch 16, Batch 867, LR 2.321037 Loss 6.101325, Accuracy 85.364%\n",
      "Epoch 16, Batch 868, LR 2.320968 Loss 6.100911, Accuracy 85.365%\n",
      "Epoch 16, Batch 869, LR 2.320898 Loss 6.101299, Accuracy 85.363%\n",
      "Epoch 16, Batch 870, LR 2.320829 Loss 6.100816, Accuracy 85.365%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 871, LR 2.320760 Loss 6.099827, Accuracy 85.370%\n",
      "Epoch 16, Batch 872, LR 2.320691 Loss 6.099721, Accuracy 85.367%\n",
      "Epoch 16, Batch 873, LR 2.320622 Loss 6.099588, Accuracy 85.370%\n",
      "Epoch 16, Batch 874, LR 2.320553 Loss 6.100326, Accuracy 85.365%\n",
      "Epoch 16, Batch 875, LR 2.320484 Loss 6.099826, Accuracy 85.369%\n",
      "Epoch 16, Batch 876, LR 2.320415 Loss 6.099095, Accuracy 85.371%\n",
      "Epoch 16, Batch 877, LR 2.320345 Loss 6.098745, Accuracy 85.373%\n",
      "Epoch 16, Batch 878, LR 2.320276 Loss 6.098765, Accuracy 85.370%\n",
      "Epoch 16, Batch 879, LR 2.320207 Loss 6.099620, Accuracy 85.363%\n",
      "Epoch 16, Batch 880, LR 2.320138 Loss 6.099791, Accuracy 85.363%\n",
      "Epoch 16, Batch 881, LR 2.320069 Loss 6.099200, Accuracy 85.365%\n",
      "Epoch 16, Batch 882, LR 2.319999 Loss 6.099054, Accuracy 85.367%\n",
      "Epoch 16, Batch 883, LR 2.319930 Loss 6.099663, Accuracy 85.366%\n",
      "Epoch 16, Batch 884, LR 2.319861 Loss 6.099876, Accuracy 85.370%\n",
      "Epoch 16, Batch 885, LR 2.319791 Loss 6.100371, Accuracy 85.369%\n",
      "Epoch 16, Batch 886, LR 2.319722 Loss 6.100889, Accuracy 85.368%\n",
      "Epoch 16, Batch 887, LR 2.319653 Loss 6.101557, Accuracy 85.364%\n",
      "Epoch 16, Batch 888, LR 2.319584 Loss 6.101156, Accuracy 85.367%\n",
      "Epoch 16, Batch 889, LR 2.319514 Loss 6.100819, Accuracy 85.368%\n",
      "Epoch 16, Batch 890, LR 2.319445 Loss 6.100410, Accuracy 85.371%\n",
      "Epoch 16, Batch 891, LR 2.319376 Loss 6.100448, Accuracy 85.372%\n",
      "Epoch 16, Batch 892, LR 2.319306 Loss 6.100245, Accuracy 85.372%\n",
      "Epoch 16, Batch 893, LR 2.319237 Loss 6.100357, Accuracy 85.371%\n",
      "Epoch 16, Batch 894, LR 2.319167 Loss 6.099997, Accuracy 85.374%\n",
      "Epoch 16, Batch 895, LR 2.319098 Loss 6.099848, Accuracy 85.376%\n",
      "Epoch 16, Batch 896, LR 2.319029 Loss 6.099432, Accuracy 85.385%\n",
      "Epoch 16, Batch 897, LR 2.318959 Loss 6.098278, Accuracy 85.391%\n",
      "Epoch 16, Batch 898, LR 2.318890 Loss 6.098262, Accuracy 85.389%\n",
      "Epoch 16, Batch 899, LR 2.318820 Loss 6.098183, Accuracy 85.387%\n",
      "Epoch 16, Batch 900, LR 2.318751 Loss 6.098515, Accuracy 85.387%\n",
      "Epoch 16, Batch 901, LR 2.318681 Loss 6.098464, Accuracy 85.386%\n",
      "Epoch 16, Batch 902, LR 2.318612 Loss 6.098355, Accuracy 85.384%\n",
      "Epoch 16, Batch 903, LR 2.318542 Loss 6.098186, Accuracy 85.383%\n",
      "Epoch 16, Batch 904, LR 2.318473 Loss 6.097741, Accuracy 85.382%\n",
      "Epoch 16, Batch 905, LR 2.318403 Loss 6.097191, Accuracy 85.384%\n",
      "Epoch 16, Batch 906, LR 2.318334 Loss 6.097163, Accuracy 85.384%\n",
      "Epoch 16, Batch 907, LR 2.318264 Loss 6.097354, Accuracy 85.382%\n",
      "Epoch 16, Batch 908, LR 2.318195 Loss 6.097685, Accuracy 85.377%\n",
      "Epoch 16, Batch 909, LR 2.318125 Loss 6.097572, Accuracy 85.381%\n",
      "Epoch 16, Batch 910, LR 2.318055 Loss 6.096950, Accuracy 85.385%\n",
      "Epoch 16, Batch 911, LR 2.317986 Loss 6.096518, Accuracy 85.386%\n",
      "Epoch 16, Batch 912, LR 2.317916 Loss 6.097392, Accuracy 85.380%\n",
      "Epoch 16, Batch 913, LR 2.317847 Loss 6.097621, Accuracy 85.380%\n",
      "Epoch 16, Batch 914, LR 2.317777 Loss 6.097700, Accuracy 85.379%\n",
      "Epoch 16, Batch 915, LR 2.317707 Loss 6.098619, Accuracy 85.379%\n",
      "Epoch 16, Batch 916, LR 2.317638 Loss 6.098721, Accuracy 85.375%\n",
      "Epoch 16, Batch 917, LR 2.317568 Loss 6.098888, Accuracy 85.378%\n",
      "Epoch 16, Batch 918, LR 2.317498 Loss 6.098875, Accuracy 85.374%\n",
      "Epoch 16, Batch 919, LR 2.317429 Loss 6.098678, Accuracy 85.376%\n",
      "Epoch 16, Batch 920, LR 2.317359 Loss 6.098092, Accuracy 85.383%\n",
      "Epoch 16, Batch 921, LR 2.317289 Loss 6.098039, Accuracy 85.381%\n",
      "Epoch 16, Batch 922, LR 2.317219 Loss 6.098809, Accuracy 85.376%\n",
      "Epoch 16, Batch 923, LR 2.317150 Loss 6.098726, Accuracy 85.376%\n",
      "Epoch 16, Batch 924, LR 2.317080 Loss 6.098552, Accuracy 85.380%\n",
      "Epoch 16, Batch 925, LR 2.317010 Loss 6.098604, Accuracy 85.378%\n",
      "Epoch 16, Batch 926, LR 2.316940 Loss 6.098612, Accuracy 85.384%\n",
      "Epoch 16, Batch 927, LR 2.316871 Loss 6.098786, Accuracy 85.382%\n",
      "Epoch 16, Batch 928, LR 2.316801 Loss 6.097926, Accuracy 85.385%\n",
      "Epoch 16, Batch 929, LR 2.316731 Loss 6.097964, Accuracy 85.384%\n",
      "Epoch 16, Batch 930, LR 2.316661 Loss 6.097993, Accuracy 85.386%\n",
      "Epoch 16, Batch 931, LR 2.316591 Loss 6.098110, Accuracy 85.389%\n",
      "Epoch 16, Batch 932, LR 2.316521 Loss 6.097702, Accuracy 85.393%\n",
      "Epoch 16, Batch 933, LR 2.316452 Loss 6.096936, Accuracy 85.393%\n",
      "Epoch 16, Batch 934, LR 2.316382 Loss 6.097186, Accuracy 85.390%\n",
      "Epoch 16, Batch 935, LR 2.316312 Loss 6.097500, Accuracy 85.389%\n",
      "Epoch 16, Batch 936, LR 2.316242 Loss 6.097864, Accuracy 85.386%\n",
      "Epoch 16, Batch 937, LR 2.316172 Loss 6.097390, Accuracy 85.389%\n",
      "Epoch 16, Batch 938, LR 2.316102 Loss 6.097736, Accuracy 85.385%\n",
      "Epoch 16, Batch 939, LR 2.316032 Loss 6.098038, Accuracy 85.387%\n",
      "Epoch 16, Batch 940, LR 2.315962 Loss 6.098613, Accuracy 85.381%\n",
      "Epoch 16, Batch 941, LR 2.315892 Loss 6.098478, Accuracy 85.384%\n",
      "Epoch 16, Batch 942, LR 2.315822 Loss 6.099220, Accuracy 85.375%\n",
      "Epoch 16, Batch 943, LR 2.315752 Loss 6.098977, Accuracy 85.377%\n",
      "Epoch 16, Batch 944, LR 2.315682 Loss 6.099581, Accuracy 85.374%\n",
      "Epoch 16, Batch 945, LR 2.315612 Loss 6.098820, Accuracy 85.377%\n",
      "Epoch 16, Batch 946, LR 2.315542 Loss 6.099009, Accuracy 85.371%\n",
      "Epoch 16, Batch 947, LR 2.315472 Loss 6.099183, Accuracy 85.371%\n",
      "Epoch 16, Batch 948, LR 2.315402 Loss 6.099924, Accuracy 85.363%\n",
      "Epoch 16, Batch 949, LR 2.315332 Loss 6.100076, Accuracy 85.360%\n",
      "Epoch 16, Batch 950, LR 2.315262 Loss 6.100865, Accuracy 85.354%\n",
      "Epoch 16, Batch 951, LR 2.315192 Loss 6.101695, Accuracy 85.353%\n",
      "Epoch 16, Batch 952, LR 2.315122 Loss 6.102498, Accuracy 85.350%\n",
      "Epoch 16, Batch 953, LR 2.315052 Loss 6.102644, Accuracy 85.353%\n",
      "Epoch 16, Batch 954, LR 2.314981 Loss 6.103293, Accuracy 85.349%\n",
      "Epoch 16, Batch 955, LR 2.314911 Loss 6.103275, Accuracy 85.349%\n",
      "Epoch 16, Batch 956, LR 2.314841 Loss 6.103388, Accuracy 85.348%\n",
      "Epoch 16, Batch 957, LR 2.314771 Loss 6.103746, Accuracy 85.345%\n",
      "Epoch 16, Batch 958, LR 2.314701 Loss 6.103429, Accuracy 85.342%\n",
      "Epoch 16, Batch 959, LR 2.314631 Loss 6.103549, Accuracy 85.340%\n",
      "Epoch 16, Batch 960, LR 2.314560 Loss 6.103718, Accuracy 85.337%\n",
      "Epoch 16, Batch 961, LR 2.314490 Loss 6.103867, Accuracy 85.337%\n",
      "Epoch 16, Batch 962, LR 2.314420 Loss 6.103402, Accuracy 85.337%\n",
      "Epoch 16, Batch 963, LR 2.314350 Loss 6.103757, Accuracy 85.335%\n",
      "Epoch 16, Batch 964, LR 2.314279 Loss 6.103725, Accuracy 85.330%\n",
      "Epoch 16, Batch 965, LR 2.314209 Loss 6.103974, Accuracy 85.331%\n",
      "Epoch 16, Batch 966, LR 2.314139 Loss 6.102996, Accuracy 85.335%\n",
      "Epoch 16, Batch 967, LR 2.314069 Loss 6.103171, Accuracy 85.335%\n",
      "Epoch 16, Batch 968, LR 2.313998 Loss 6.103457, Accuracy 85.336%\n",
      "Epoch 16, Batch 969, LR 2.313928 Loss 6.104236, Accuracy 85.330%\n",
      "Epoch 16, Batch 970, LR 2.313858 Loss 6.104031, Accuracy 85.334%\n",
      "Epoch 16, Batch 971, LR 2.313787 Loss 6.103712, Accuracy 85.335%\n",
      "Epoch 16, Batch 972, LR 2.313717 Loss 6.103669, Accuracy 85.335%\n",
      "Epoch 16, Batch 973, LR 2.313647 Loss 6.102900, Accuracy 85.338%\n",
      "Epoch 16, Batch 974, LR 2.313576 Loss 6.103564, Accuracy 85.334%\n",
      "Epoch 16, Batch 975, LR 2.313506 Loss 6.102905, Accuracy 85.336%\n",
      "Epoch 16, Batch 976, LR 2.313436 Loss 6.102632, Accuracy 85.338%\n",
      "Epoch 16, Batch 977, LR 2.313365 Loss 6.103224, Accuracy 85.335%\n",
      "Epoch 16, Batch 978, LR 2.313295 Loss 6.102487, Accuracy 85.341%\n",
      "Epoch 16, Batch 979, LR 2.313224 Loss 6.101650, Accuracy 85.342%\n",
      "Epoch 16, Batch 980, LR 2.313154 Loss 6.101563, Accuracy 85.344%\n",
      "Epoch 16, Batch 981, LR 2.313083 Loss 6.101476, Accuracy 85.343%\n",
      "Epoch 16, Batch 982, LR 2.313013 Loss 6.100882, Accuracy 85.345%\n",
      "Epoch 16, Batch 983, LR 2.312942 Loss 6.100390, Accuracy 85.345%\n",
      "Epoch 16, Batch 984, LR 2.312872 Loss 6.100583, Accuracy 85.342%\n",
      "Epoch 16, Batch 985, LR 2.312801 Loss 6.100201, Accuracy 85.348%\n",
      "Epoch 16, Batch 986, LR 2.312731 Loss 6.100185, Accuracy 85.345%\n",
      "Epoch 16, Batch 987, LR 2.312660 Loss 6.100501, Accuracy 85.345%\n",
      "Epoch 16, Batch 988, LR 2.312590 Loss 6.099805, Accuracy 85.348%\n",
      "Epoch 16, Batch 989, LR 2.312519 Loss 6.099332, Accuracy 85.351%\n",
      "Epoch 16, Batch 990, LR 2.312449 Loss 6.099197, Accuracy 85.352%\n",
      "Epoch 16, Batch 991, LR 2.312378 Loss 6.098348, Accuracy 85.356%\n",
      "Epoch 16, Batch 992, LR 2.312308 Loss 6.098727, Accuracy 85.355%\n",
      "Epoch 16, Batch 993, LR 2.312237 Loss 6.098639, Accuracy 85.357%\n",
      "Epoch 16, Batch 994, LR 2.312166 Loss 6.098141, Accuracy 85.360%\n",
      "Epoch 16, Batch 995, LR 2.312096 Loss 6.098272, Accuracy 85.357%\n",
      "Epoch 16, Batch 996, LR 2.312025 Loss 6.098224, Accuracy 85.353%\n",
      "Epoch 16, Batch 997, LR 2.311954 Loss 6.098153, Accuracy 85.351%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 998, LR 2.311884 Loss 6.098410, Accuracy 85.350%\n",
      "Epoch 16, Batch 999, LR 2.311813 Loss 6.099104, Accuracy 85.344%\n",
      "Epoch 16, Batch 1000, LR 2.311742 Loss 6.098587, Accuracy 85.348%\n",
      "Epoch 16, Batch 1001, LR 2.311672 Loss 6.098632, Accuracy 85.346%\n",
      "Epoch 16, Batch 1002, LR 2.311601 Loss 6.098602, Accuracy 85.348%\n",
      "Epoch 16, Batch 1003, LR 2.311530 Loss 6.098047, Accuracy 85.351%\n",
      "Epoch 16, Batch 1004, LR 2.311459 Loss 6.098034, Accuracy 85.350%\n",
      "Epoch 16, Batch 1005, LR 2.311389 Loss 6.098082, Accuracy 85.351%\n",
      "Epoch 16, Batch 1006, LR 2.311318 Loss 6.099016, Accuracy 85.346%\n",
      "Epoch 16, Batch 1007, LR 2.311247 Loss 6.098805, Accuracy 85.347%\n",
      "Epoch 16, Batch 1008, LR 2.311176 Loss 6.098782, Accuracy 85.348%\n",
      "Epoch 16, Batch 1009, LR 2.311106 Loss 6.098821, Accuracy 85.351%\n",
      "Epoch 16, Batch 1010, LR 2.311035 Loss 6.099608, Accuracy 85.347%\n",
      "Epoch 16, Batch 1011, LR 2.310964 Loss 6.099242, Accuracy 85.345%\n",
      "Epoch 16, Batch 1012, LR 2.310893 Loss 6.099110, Accuracy 85.345%\n",
      "Epoch 16, Batch 1013, LR 2.310822 Loss 6.099845, Accuracy 85.342%\n",
      "Epoch 16, Batch 1014, LR 2.310751 Loss 6.100405, Accuracy 85.340%\n",
      "Epoch 16, Batch 1015, LR 2.310681 Loss 6.100543, Accuracy 85.341%\n",
      "Epoch 16, Batch 1016, LR 2.310610 Loss 6.100507, Accuracy 85.338%\n",
      "Epoch 16, Batch 1017, LR 2.310539 Loss 6.100938, Accuracy 85.340%\n",
      "Epoch 16, Batch 1018, LR 2.310468 Loss 6.101262, Accuracy 85.340%\n",
      "Epoch 16, Batch 1019, LR 2.310397 Loss 6.101056, Accuracy 85.343%\n",
      "Epoch 16, Batch 1020, LR 2.310326 Loss 6.100899, Accuracy 85.345%\n",
      "Epoch 16, Batch 1021, LR 2.310255 Loss 6.101636, Accuracy 85.342%\n",
      "Epoch 16, Batch 1022, LR 2.310184 Loss 6.101299, Accuracy 85.344%\n",
      "Epoch 16, Batch 1023, LR 2.310113 Loss 6.101089, Accuracy 85.347%\n",
      "Epoch 16, Batch 1024, LR 2.310042 Loss 6.101832, Accuracy 85.342%\n",
      "Epoch 16, Batch 1025, LR 2.309971 Loss 6.102659, Accuracy 85.339%\n",
      "Epoch 16, Batch 1026, LR 2.309900 Loss 6.102911, Accuracy 85.341%\n",
      "Epoch 16, Batch 1027, LR 2.309829 Loss 6.103694, Accuracy 85.333%\n",
      "Epoch 16, Batch 1028, LR 2.309758 Loss 6.103692, Accuracy 85.331%\n",
      "Epoch 16, Batch 1029, LR 2.309687 Loss 6.104052, Accuracy 85.331%\n",
      "Epoch 16, Batch 1030, LR 2.309616 Loss 6.103055, Accuracy 85.334%\n",
      "Epoch 16, Batch 1031, LR 2.309545 Loss 6.102696, Accuracy 85.336%\n",
      "Epoch 16, Batch 1032, LR 2.309474 Loss 6.102848, Accuracy 85.337%\n",
      "Epoch 16, Batch 1033, LR 2.309403 Loss 6.102796, Accuracy 85.338%\n",
      "Epoch 16, Batch 1034, LR 2.309332 Loss 6.102625, Accuracy 85.341%\n",
      "Epoch 16, Batch 1035, LR 2.309261 Loss 6.102720, Accuracy 85.340%\n",
      "Epoch 16, Batch 1036, LR 2.309189 Loss 6.102724, Accuracy 85.339%\n",
      "Epoch 16, Batch 1037, LR 2.309118 Loss 6.102795, Accuracy 85.339%\n",
      "Epoch 16, Batch 1038, LR 2.309047 Loss 6.102418, Accuracy 85.341%\n",
      "Epoch 16, Batch 1039, LR 2.308976 Loss 6.101792, Accuracy 85.343%\n",
      "Epoch 16, Batch 1040, LR 2.308905 Loss 6.101096, Accuracy 85.346%\n",
      "Epoch 16, Batch 1041, LR 2.308834 Loss 6.101008, Accuracy 85.349%\n",
      "Epoch 16, Batch 1042, LR 2.308762 Loss 6.100809, Accuracy 85.348%\n",
      "Epoch 16, Batch 1043, LR 2.308691 Loss 6.100549, Accuracy 85.347%\n",
      "Epoch 16, Batch 1044, LR 2.308620 Loss 6.100639, Accuracy 85.347%\n",
      "Epoch 16, Batch 1045, LR 2.308549 Loss 6.101164, Accuracy 85.348%\n",
      "Epoch 16, Batch 1046, LR 2.308477 Loss 6.101384, Accuracy 85.349%\n",
      "Epoch 16, Batch 1047, LR 2.308406 Loss 6.101337, Accuracy 85.350%\n",
      "Epoch 16, Loss (train set) 6.101337, Accuracy (train set) 85.350%\n",
      "Epoch 17, Batch 1, LR 2.308335 Loss 6.007136, Accuracy 88.281%\n",
      "Epoch 17, Batch 2, LR 2.308264 Loss 6.105718, Accuracy 85.156%\n",
      "Epoch 17, Batch 3, LR 2.308192 Loss 5.901872, Accuracy 85.417%\n",
      "Epoch 17, Batch 4, LR 2.308121 Loss 5.945483, Accuracy 85.156%\n",
      "Epoch 17, Batch 5, LR 2.308050 Loss 6.036553, Accuracy 85.156%\n",
      "Epoch 17, Batch 6, LR 2.307978 Loss 5.918610, Accuracy 85.417%\n",
      "Epoch 17, Batch 7, LR 2.307907 Loss 5.995149, Accuracy 85.268%\n",
      "Epoch 17, Batch 8, LR 2.307836 Loss 6.055363, Accuracy 84.668%\n",
      "Epoch 17, Batch 9, LR 2.307764 Loss 6.080280, Accuracy 84.375%\n",
      "Epoch 17, Batch 10, LR 2.307693 Loss 6.119059, Accuracy 84.297%\n",
      "Epoch 17, Batch 11, LR 2.307622 Loss 6.062370, Accuracy 84.375%\n",
      "Epoch 17, Batch 12, LR 2.307550 Loss 6.045326, Accuracy 84.831%\n",
      "Epoch 17, Batch 13, LR 2.307479 Loss 6.036983, Accuracy 84.916%\n",
      "Epoch 17, Batch 14, LR 2.307407 Loss 6.057364, Accuracy 85.045%\n",
      "Epoch 17, Batch 15, LR 2.307336 Loss 6.026571, Accuracy 85.312%\n",
      "Epoch 17, Batch 16, LR 2.307264 Loss 6.002157, Accuracy 85.352%\n",
      "Epoch 17, Batch 17, LR 2.307193 Loss 5.965446, Accuracy 85.432%\n",
      "Epoch 17, Batch 18, LR 2.307121 Loss 5.963663, Accuracy 85.503%\n",
      "Epoch 17, Batch 19, LR 2.307050 Loss 5.977263, Accuracy 85.444%\n",
      "Epoch 17, Batch 20, LR 2.306978 Loss 5.983919, Accuracy 85.586%\n",
      "Epoch 17, Batch 21, LR 2.306907 Loss 5.947315, Accuracy 85.789%\n",
      "Epoch 17, Batch 22, LR 2.306835 Loss 5.938253, Accuracy 85.866%\n",
      "Epoch 17, Batch 23, LR 2.306764 Loss 5.940833, Accuracy 85.904%\n",
      "Epoch 17, Batch 24, LR 2.306692 Loss 5.960600, Accuracy 85.645%\n",
      "Epoch 17, Batch 25, LR 2.306621 Loss 5.952083, Accuracy 85.594%\n",
      "Epoch 17, Batch 26, LR 2.306549 Loss 5.947031, Accuracy 85.697%\n",
      "Epoch 17, Batch 27, LR 2.306478 Loss 5.921916, Accuracy 85.880%\n",
      "Epoch 17, Batch 28, LR 2.306406 Loss 5.912068, Accuracy 85.826%\n",
      "Epoch 17, Batch 29, LR 2.306334 Loss 5.902863, Accuracy 85.857%\n",
      "Epoch 17, Batch 30, LR 2.306263 Loss 5.912017, Accuracy 85.859%\n",
      "Epoch 17, Batch 31, LR 2.306191 Loss 5.915786, Accuracy 85.938%\n",
      "Epoch 17, Batch 32, LR 2.306119 Loss 5.916283, Accuracy 85.889%\n",
      "Epoch 17, Batch 33, LR 2.306048 Loss 5.910336, Accuracy 85.938%\n",
      "Epoch 17, Batch 34, LR 2.305976 Loss 5.900280, Accuracy 86.121%\n",
      "Epoch 17, Batch 35, LR 2.305904 Loss 5.908760, Accuracy 86.049%\n",
      "Epoch 17, Batch 36, LR 2.305833 Loss 5.912313, Accuracy 85.938%\n",
      "Epoch 17, Batch 37, LR 2.305761 Loss 5.913169, Accuracy 85.959%\n",
      "Epoch 17, Batch 38, LR 2.305689 Loss 5.918294, Accuracy 85.979%\n",
      "Epoch 17, Batch 39, LR 2.305618 Loss 5.915776, Accuracy 85.958%\n",
      "Epoch 17, Batch 40, LR 2.305546 Loss 5.893933, Accuracy 86.055%\n",
      "Epoch 17, Batch 41, LR 2.305474 Loss 5.889234, Accuracy 86.109%\n",
      "Epoch 17, Batch 42, LR 2.305402 Loss 5.881246, Accuracy 86.217%\n",
      "Epoch 17, Batch 43, LR 2.305330 Loss 5.884685, Accuracy 86.210%\n",
      "Epoch 17, Batch 44, LR 2.305259 Loss 5.871374, Accuracy 86.275%\n",
      "Epoch 17, Batch 45, LR 2.305187 Loss 5.859199, Accuracy 86.319%\n",
      "Epoch 17, Batch 46, LR 2.305115 Loss 5.857928, Accuracy 86.311%\n",
      "Epoch 17, Batch 47, LR 2.305043 Loss 5.855087, Accuracy 86.237%\n",
      "Epoch 17, Batch 48, LR 2.304971 Loss 5.855597, Accuracy 86.296%\n",
      "Epoch 17, Batch 49, LR 2.304900 Loss 5.842749, Accuracy 86.288%\n",
      "Epoch 17, Batch 50, LR 2.304828 Loss 5.837967, Accuracy 86.281%\n",
      "Epoch 17, Batch 51, LR 2.304756 Loss 5.835726, Accuracy 86.305%\n",
      "Epoch 17, Batch 52, LR 2.304684 Loss 5.837041, Accuracy 86.313%\n",
      "Epoch 17, Batch 53, LR 2.304612 Loss 5.839903, Accuracy 86.306%\n",
      "Epoch 17, Batch 54, LR 2.304540 Loss 5.836340, Accuracy 86.372%\n",
      "Epoch 17, Batch 55, LR 2.304468 Loss 5.846762, Accuracy 86.335%\n",
      "Epoch 17, Batch 56, LR 2.304396 Loss 5.856238, Accuracy 86.244%\n",
      "Epoch 17, Batch 57, LR 2.304324 Loss 5.861957, Accuracy 86.225%\n",
      "Epoch 17, Batch 58, LR 2.304252 Loss 5.863437, Accuracy 86.234%\n",
      "Epoch 17, Batch 59, LR 2.304180 Loss 5.863670, Accuracy 86.216%\n",
      "Epoch 17, Batch 60, LR 2.304108 Loss 5.865357, Accuracy 86.159%\n",
      "Epoch 17, Batch 61, LR 2.304036 Loss 5.874150, Accuracy 86.117%\n",
      "Epoch 17, Batch 62, LR 2.303964 Loss 5.866608, Accuracy 86.152%\n",
      "Epoch 17, Batch 63, LR 2.303892 Loss 5.871231, Accuracy 86.062%\n",
      "Epoch 17, Batch 64, LR 2.303820 Loss 5.873378, Accuracy 86.035%\n",
      "Epoch 17, Batch 65, LR 2.303748 Loss 5.874863, Accuracy 86.058%\n",
      "Epoch 17, Batch 66, LR 2.303676 Loss 5.871062, Accuracy 86.127%\n",
      "Epoch 17, Batch 67, LR 2.303604 Loss 5.866936, Accuracy 86.159%\n",
      "Epoch 17, Batch 68, LR 2.303532 Loss 5.878040, Accuracy 86.098%\n",
      "Epoch 17, Batch 69, LR 2.303460 Loss 5.878542, Accuracy 86.085%\n",
      "Epoch 17, Batch 70, LR 2.303388 Loss 5.878622, Accuracy 86.127%\n",
      "Epoch 17, Batch 71, LR 2.303316 Loss 5.882893, Accuracy 86.114%\n",
      "Epoch 17, Batch 72, LR 2.303244 Loss 5.880390, Accuracy 86.165%\n",
      "Epoch 17, Batch 73, LR 2.303171 Loss 5.893697, Accuracy 86.066%\n",
      "Epoch 17, Batch 74, LR 2.303099 Loss 5.896697, Accuracy 86.085%\n",
      "Epoch 17, Batch 75, LR 2.303027 Loss 5.887475, Accuracy 86.135%\n",
      "Epoch 17, Batch 76, LR 2.302955 Loss 5.880715, Accuracy 86.194%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 77, LR 2.302883 Loss 5.879791, Accuracy 86.232%\n",
      "Epoch 17, Batch 78, LR 2.302810 Loss 5.882810, Accuracy 86.208%\n",
      "Epoch 17, Batch 79, LR 2.302738 Loss 5.882368, Accuracy 86.214%\n",
      "Epoch 17, Batch 80, LR 2.302666 Loss 5.887308, Accuracy 86.191%\n",
      "Epoch 17, Batch 81, LR 2.302594 Loss 5.885321, Accuracy 86.198%\n",
      "Epoch 17, Batch 82, LR 2.302521 Loss 5.879656, Accuracy 86.252%\n",
      "Epoch 17, Batch 83, LR 2.302449 Loss 5.888275, Accuracy 86.210%\n",
      "Epoch 17, Batch 84, LR 2.302377 Loss 5.886395, Accuracy 86.226%\n",
      "Epoch 17, Batch 85, LR 2.302305 Loss 5.884735, Accuracy 86.250%\n",
      "Epoch 17, Batch 86, LR 2.302232 Loss 5.885840, Accuracy 86.237%\n",
      "Epoch 17, Batch 87, LR 2.302160 Loss 5.886794, Accuracy 86.207%\n",
      "Epoch 17, Batch 88, LR 2.302088 Loss 5.886907, Accuracy 86.213%\n",
      "Epoch 17, Batch 89, LR 2.302015 Loss 5.887799, Accuracy 86.183%\n",
      "Epoch 17, Batch 90, LR 2.301943 Loss 5.882533, Accuracy 86.198%\n",
      "Epoch 17, Batch 91, LR 2.301871 Loss 5.887130, Accuracy 86.169%\n",
      "Epoch 17, Batch 92, LR 2.301798 Loss 5.886050, Accuracy 86.150%\n",
      "Epoch 17, Batch 93, LR 2.301726 Loss 5.882179, Accuracy 86.164%\n",
      "Epoch 17, Batch 94, LR 2.301653 Loss 5.895799, Accuracy 86.062%\n",
      "Epoch 17, Batch 95, LR 2.301581 Loss 5.893486, Accuracy 86.077%\n",
      "Epoch 17, Batch 96, LR 2.301509 Loss 5.888334, Accuracy 86.108%\n",
      "Epoch 17, Batch 97, LR 2.301436 Loss 5.884295, Accuracy 86.131%\n",
      "Epoch 17, Batch 98, LR 2.301364 Loss 5.895949, Accuracy 86.057%\n",
      "Epoch 17, Batch 99, LR 2.301291 Loss 5.901709, Accuracy 86.040%\n",
      "Epoch 17, Batch 100, LR 2.301219 Loss 5.909959, Accuracy 86.000%\n",
      "Epoch 17, Batch 101, LR 2.301146 Loss 5.904772, Accuracy 86.054%\n",
      "Epoch 17, Batch 102, LR 2.301074 Loss 5.905244, Accuracy 86.037%\n",
      "Epoch 17, Batch 103, LR 2.301001 Loss 5.900795, Accuracy 86.059%\n",
      "Epoch 17, Batch 104, LR 2.300929 Loss 5.904361, Accuracy 86.043%\n",
      "Epoch 17, Batch 105, LR 2.300856 Loss 5.904018, Accuracy 86.042%\n",
      "Epoch 17, Batch 106, LR 2.300784 Loss 5.898734, Accuracy 86.063%\n",
      "Epoch 17, Batch 107, LR 2.300711 Loss 5.900632, Accuracy 86.011%\n",
      "Epoch 17, Batch 108, LR 2.300639 Loss 5.893816, Accuracy 86.053%\n",
      "Epoch 17, Batch 109, LR 2.300566 Loss 5.893821, Accuracy 86.067%\n",
      "Epoch 17, Batch 110, LR 2.300493 Loss 5.899410, Accuracy 86.051%\n",
      "Epoch 17, Batch 111, LR 2.300421 Loss 5.900934, Accuracy 86.029%\n",
      "Epoch 17, Batch 112, LR 2.300348 Loss 5.904690, Accuracy 85.986%\n",
      "Epoch 17, Batch 113, LR 2.300276 Loss 5.907195, Accuracy 85.938%\n",
      "Epoch 17, Batch 114, LR 2.300203 Loss 5.909539, Accuracy 85.910%\n",
      "Epoch 17, Batch 115, LR 2.300130 Loss 5.912844, Accuracy 85.890%\n",
      "Epoch 17, Batch 116, LR 2.300058 Loss 5.908990, Accuracy 85.944%\n",
      "Epoch 17, Batch 117, LR 2.299985 Loss 5.913976, Accuracy 85.944%\n",
      "Epoch 17, Batch 118, LR 2.299912 Loss 5.912388, Accuracy 85.944%\n",
      "Epoch 17, Batch 119, LR 2.299840 Loss 5.912676, Accuracy 85.938%\n",
      "Epoch 17, Batch 120, LR 2.299767 Loss 5.916045, Accuracy 85.918%\n",
      "Epoch 17, Batch 121, LR 2.299694 Loss 5.918116, Accuracy 85.918%\n",
      "Epoch 17, Batch 122, LR 2.299621 Loss 5.916374, Accuracy 85.931%\n",
      "Epoch 17, Batch 123, LR 2.299549 Loss 5.916652, Accuracy 85.963%\n",
      "Epoch 17, Batch 124, LR 2.299476 Loss 5.915775, Accuracy 85.969%\n",
      "Epoch 17, Batch 125, LR 2.299403 Loss 5.919526, Accuracy 85.919%\n",
      "Epoch 17, Batch 126, LR 2.299330 Loss 5.920328, Accuracy 85.925%\n",
      "Epoch 17, Batch 127, LR 2.299258 Loss 5.913433, Accuracy 85.956%\n",
      "Epoch 17, Batch 128, LR 2.299185 Loss 5.912333, Accuracy 85.968%\n",
      "Epoch 17, Batch 129, LR 2.299112 Loss 5.911017, Accuracy 85.974%\n",
      "Epoch 17, Batch 130, LR 2.299039 Loss 5.909284, Accuracy 85.986%\n",
      "Epoch 17, Batch 131, LR 2.298966 Loss 5.910211, Accuracy 85.997%\n",
      "Epoch 17, Batch 132, LR 2.298893 Loss 5.910491, Accuracy 86.003%\n",
      "Epoch 17, Batch 133, LR 2.298820 Loss 5.910214, Accuracy 86.014%\n",
      "Epoch 17, Batch 134, LR 2.298748 Loss 5.913423, Accuracy 85.990%\n",
      "Epoch 17, Batch 135, LR 2.298675 Loss 5.907012, Accuracy 86.013%\n",
      "Epoch 17, Batch 136, LR 2.298602 Loss 5.904461, Accuracy 86.035%\n",
      "Epoch 17, Batch 137, LR 2.298529 Loss 5.901051, Accuracy 86.046%\n",
      "Epoch 17, Batch 138, LR 2.298456 Loss 5.901625, Accuracy 86.034%\n",
      "Epoch 17, Batch 139, LR 2.298383 Loss 5.900846, Accuracy 86.039%\n",
      "Epoch 17, Batch 140, LR 2.298310 Loss 5.902146, Accuracy 86.049%\n",
      "Epoch 17, Batch 141, LR 2.298237 Loss 5.907085, Accuracy 86.043%\n",
      "Epoch 17, Batch 142, LR 2.298164 Loss 5.906435, Accuracy 86.037%\n",
      "Epoch 17, Batch 143, LR 2.298091 Loss 5.900316, Accuracy 86.080%\n",
      "Epoch 17, Batch 144, LR 2.298018 Loss 5.900380, Accuracy 86.095%\n",
      "Epoch 17, Batch 145, LR 2.297945 Loss 5.898300, Accuracy 86.115%\n",
      "Epoch 17, Batch 146, LR 2.297872 Loss 5.898164, Accuracy 86.098%\n",
      "Epoch 17, Batch 147, LR 2.297799 Loss 5.895240, Accuracy 86.113%\n",
      "Epoch 17, Batch 148, LR 2.297726 Loss 5.890776, Accuracy 86.138%\n",
      "Epoch 17, Batch 149, LR 2.297653 Loss 5.890131, Accuracy 86.116%\n",
      "Epoch 17, Batch 150, LR 2.297580 Loss 5.891590, Accuracy 86.115%\n",
      "Epoch 17, Batch 151, LR 2.297507 Loss 5.894111, Accuracy 86.098%\n",
      "Epoch 17, Batch 152, LR 2.297434 Loss 5.894804, Accuracy 86.087%\n",
      "Epoch 17, Batch 153, LR 2.297361 Loss 5.892874, Accuracy 86.096%\n",
      "Epoch 17, Batch 154, LR 2.297287 Loss 5.891404, Accuracy 86.110%\n",
      "Epoch 17, Batch 155, LR 2.297214 Loss 5.897397, Accuracy 86.089%\n",
      "Epoch 17, Batch 156, LR 2.297141 Loss 5.894420, Accuracy 86.103%\n",
      "Epoch 17, Batch 157, LR 2.297068 Loss 5.898396, Accuracy 86.087%\n",
      "Epoch 17, Batch 158, LR 2.296995 Loss 5.897360, Accuracy 86.106%\n",
      "Epoch 17, Batch 159, LR 2.296922 Loss 5.893889, Accuracy 86.124%\n",
      "Epoch 17, Batch 160, LR 2.296848 Loss 5.895513, Accuracy 86.133%\n",
      "Epoch 17, Batch 161, LR 2.296775 Loss 5.897846, Accuracy 86.127%\n",
      "Epoch 17, Batch 162, LR 2.296702 Loss 5.898982, Accuracy 86.130%\n",
      "Epoch 17, Batch 163, LR 2.296629 Loss 5.900297, Accuracy 86.134%\n",
      "Epoch 17, Batch 164, LR 2.296556 Loss 5.904220, Accuracy 86.109%\n",
      "Epoch 17, Batch 165, LR 2.296482 Loss 5.903665, Accuracy 86.113%\n",
      "Epoch 17, Batch 166, LR 2.296409 Loss 5.910679, Accuracy 86.060%\n",
      "Epoch 17, Batch 167, LR 2.296336 Loss 5.909496, Accuracy 86.078%\n",
      "Epoch 17, Batch 168, LR 2.296262 Loss 5.912583, Accuracy 86.040%\n",
      "Epoch 17, Batch 169, LR 2.296189 Loss 5.912009, Accuracy 86.053%\n",
      "Epoch 17, Batch 170, LR 2.296116 Loss 5.909267, Accuracy 86.071%\n",
      "Epoch 17, Batch 171, LR 2.296042 Loss 5.908601, Accuracy 86.075%\n",
      "Epoch 17, Batch 172, LR 2.295969 Loss 5.912221, Accuracy 86.047%\n",
      "Epoch 17, Batch 173, LR 2.295896 Loss 5.911020, Accuracy 86.064%\n",
      "Epoch 17, Batch 174, LR 2.295822 Loss 5.907842, Accuracy 86.068%\n",
      "Epoch 17, Batch 175, LR 2.295749 Loss 5.906194, Accuracy 86.076%\n",
      "Epoch 17, Batch 176, LR 2.295676 Loss 5.909294, Accuracy 86.062%\n",
      "Epoch 17, Batch 177, LR 2.295602 Loss 5.910961, Accuracy 86.083%\n",
      "Epoch 17, Batch 178, LR 2.295529 Loss 5.912339, Accuracy 86.078%\n",
      "Epoch 17, Batch 179, LR 2.295455 Loss 5.914847, Accuracy 86.068%\n",
      "Epoch 17, Batch 180, LR 2.295382 Loss 5.917990, Accuracy 86.063%\n",
      "Epoch 17, Batch 181, LR 2.295309 Loss 5.917516, Accuracy 86.063%\n",
      "Epoch 17, Batch 182, LR 2.295235 Loss 5.914939, Accuracy 86.079%\n",
      "Epoch 17, Batch 183, LR 2.295162 Loss 5.920257, Accuracy 86.053%\n",
      "Epoch 17, Batch 184, LR 2.295088 Loss 5.918898, Accuracy 86.069%\n",
      "Epoch 17, Batch 185, LR 2.295015 Loss 5.921709, Accuracy 86.043%\n",
      "Epoch 17, Batch 186, LR 2.294941 Loss 5.921051, Accuracy 86.059%\n",
      "Epoch 17, Batch 187, LR 2.294868 Loss 5.919930, Accuracy 86.067%\n",
      "Epoch 17, Batch 188, LR 2.294794 Loss 5.918327, Accuracy 86.054%\n",
      "Epoch 17, Batch 189, LR 2.294721 Loss 5.918823, Accuracy 86.053%\n",
      "Epoch 17, Batch 190, LR 2.294647 Loss 5.917335, Accuracy 86.049%\n",
      "Epoch 17, Batch 191, LR 2.294573 Loss 5.917348, Accuracy 86.056%\n",
      "Epoch 17, Batch 192, LR 2.294500 Loss 5.912960, Accuracy 86.072%\n",
      "Epoch 17, Batch 193, LR 2.294426 Loss 5.912235, Accuracy 86.059%\n",
      "Epoch 17, Batch 194, LR 2.294353 Loss 5.915455, Accuracy 86.050%\n",
      "Epoch 17, Batch 195, LR 2.294279 Loss 5.917469, Accuracy 86.034%\n",
      "Epoch 17, Batch 196, LR 2.294205 Loss 5.914853, Accuracy 86.057%\n",
      "Epoch 17, Batch 197, LR 2.294132 Loss 5.913702, Accuracy 86.064%\n",
      "Epoch 17, Batch 198, LR 2.294058 Loss 5.916041, Accuracy 86.044%\n",
      "Epoch 17, Batch 199, LR 2.293984 Loss 5.914816, Accuracy 86.051%\n",
      "Epoch 17, Batch 200, LR 2.293911 Loss 5.914929, Accuracy 86.062%\n",
      "Epoch 17, Batch 201, LR 2.293837 Loss 5.913787, Accuracy 86.070%\n",
      "Epoch 17, Batch 202, LR 2.293763 Loss 5.915760, Accuracy 86.061%\n",
      "Epoch 17, Batch 203, LR 2.293690 Loss 5.914640, Accuracy 86.072%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 204, LR 2.293616 Loss 5.913555, Accuracy 86.072%\n",
      "Epoch 17, Batch 205, LR 2.293542 Loss 5.912942, Accuracy 86.052%\n",
      "Epoch 17, Batch 206, LR 2.293468 Loss 5.913738, Accuracy 86.070%\n",
      "Epoch 17, Batch 207, LR 2.293395 Loss 5.914977, Accuracy 86.085%\n",
      "Epoch 17, Batch 208, LR 2.293321 Loss 5.913055, Accuracy 86.110%\n",
      "Epoch 17, Batch 209, LR 2.293247 Loss 5.912401, Accuracy 86.121%\n",
      "Epoch 17, Batch 210, LR 2.293173 Loss 5.916458, Accuracy 86.097%\n",
      "Epoch 17, Batch 211, LR 2.293100 Loss 5.918956, Accuracy 86.104%\n",
      "Epoch 17, Batch 212, LR 2.293026 Loss 5.919462, Accuracy 86.096%\n",
      "Epoch 17, Batch 213, LR 2.292952 Loss 5.920390, Accuracy 86.077%\n",
      "Epoch 17, Batch 214, LR 2.292878 Loss 5.918177, Accuracy 86.073%\n",
      "Epoch 17, Batch 215, LR 2.292804 Loss 5.918566, Accuracy 86.083%\n",
      "Epoch 17, Batch 216, LR 2.292730 Loss 5.916609, Accuracy 86.097%\n",
      "Epoch 17, Batch 217, LR 2.292656 Loss 5.918852, Accuracy 86.078%\n",
      "Epoch 17, Batch 218, LR 2.292583 Loss 5.918771, Accuracy 86.092%\n",
      "Epoch 17, Batch 219, LR 2.292509 Loss 5.919394, Accuracy 86.073%\n",
      "Epoch 17, Batch 220, LR 2.292435 Loss 5.920792, Accuracy 86.069%\n",
      "Epoch 17, Batch 221, LR 2.292361 Loss 5.919511, Accuracy 86.061%\n",
      "Epoch 17, Batch 222, LR 2.292287 Loss 5.917215, Accuracy 86.071%\n",
      "Epoch 17, Batch 223, LR 2.292213 Loss 5.918150, Accuracy 86.074%\n",
      "Epoch 17, Batch 224, LR 2.292139 Loss 5.917129, Accuracy 86.077%\n",
      "Epoch 17, Batch 225, LR 2.292065 Loss 5.914307, Accuracy 86.087%\n",
      "Epoch 17, Batch 226, LR 2.291991 Loss 5.915337, Accuracy 86.086%\n",
      "Epoch 17, Batch 227, LR 2.291917 Loss 5.915125, Accuracy 86.072%\n",
      "Epoch 17, Batch 228, LR 2.291843 Loss 5.916025, Accuracy 86.068%\n",
      "Epoch 17, Batch 229, LR 2.291769 Loss 5.915554, Accuracy 86.071%\n",
      "Epoch 17, Batch 230, LR 2.291695 Loss 5.912473, Accuracy 86.080%\n",
      "Epoch 17, Batch 231, LR 2.291621 Loss 5.914648, Accuracy 86.073%\n",
      "Epoch 17, Batch 232, LR 2.291547 Loss 5.918274, Accuracy 86.059%\n",
      "Epoch 17, Batch 233, LR 2.291473 Loss 5.917380, Accuracy 86.068%\n",
      "Epoch 17, Batch 234, LR 2.291399 Loss 5.918571, Accuracy 86.071%\n",
      "Epoch 17, Batch 235, LR 2.291325 Loss 5.917863, Accuracy 86.084%\n",
      "Epoch 17, Batch 236, LR 2.291250 Loss 5.919255, Accuracy 86.080%\n",
      "Epoch 17, Batch 237, LR 2.291176 Loss 5.918259, Accuracy 86.099%\n",
      "Epoch 17, Batch 238, LR 2.291102 Loss 5.916346, Accuracy 86.121%\n",
      "Epoch 17, Batch 239, LR 2.291028 Loss 5.917511, Accuracy 86.114%\n",
      "Epoch 17, Batch 240, LR 2.290954 Loss 5.913038, Accuracy 86.139%\n",
      "Epoch 17, Batch 241, LR 2.290880 Loss 5.912939, Accuracy 86.122%\n",
      "Epoch 17, Batch 242, LR 2.290806 Loss 5.912921, Accuracy 86.112%\n",
      "Epoch 17, Batch 243, LR 2.290731 Loss 5.911442, Accuracy 86.111%\n",
      "Epoch 17, Batch 244, LR 2.290657 Loss 5.913912, Accuracy 86.101%\n",
      "Epoch 17, Batch 245, LR 2.290583 Loss 5.911708, Accuracy 86.100%\n",
      "Epoch 17, Batch 246, LR 2.290509 Loss 5.912319, Accuracy 86.109%\n",
      "Epoch 17, Batch 247, LR 2.290434 Loss 5.911295, Accuracy 86.118%\n",
      "Epoch 17, Batch 248, LR 2.290360 Loss 5.908287, Accuracy 86.139%\n",
      "Epoch 17, Batch 249, LR 2.290286 Loss 5.907205, Accuracy 86.151%\n",
      "Epoch 17, Batch 250, LR 2.290212 Loss 5.905362, Accuracy 86.159%\n",
      "Epoch 17, Batch 251, LR 2.290137 Loss 5.902319, Accuracy 86.171%\n",
      "Epoch 17, Batch 252, LR 2.290063 Loss 5.905136, Accuracy 86.142%\n",
      "Epoch 17, Batch 253, LR 2.289989 Loss 5.904031, Accuracy 86.135%\n",
      "Epoch 17, Batch 254, LR 2.289914 Loss 5.903718, Accuracy 86.128%\n",
      "Epoch 17, Batch 255, LR 2.289840 Loss 5.904929, Accuracy 86.131%\n",
      "Epoch 17, Batch 256, LR 2.289766 Loss 5.910840, Accuracy 86.084%\n",
      "Epoch 17, Batch 257, LR 2.289691 Loss 5.911951, Accuracy 86.083%\n",
      "Epoch 17, Batch 258, LR 2.289617 Loss 5.911533, Accuracy 86.083%\n",
      "Epoch 17, Batch 259, LR 2.289543 Loss 5.912654, Accuracy 86.067%\n",
      "Epoch 17, Batch 260, LR 2.289468 Loss 5.910698, Accuracy 86.076%\n",
      "Epoch 17, Batch 261, LR 2.289394 Loss 5.910398, Accuracy 86.081%\n",
      "Epoch 17, Batch 262, LR 2.289319 Loss 5.910550, Accuracy 86.084%\n",
      "Epoch 17, Batch 263, LR 2.289245 Loss 5.909374, Accuracy 86.101%\n",
      "Epoch 17, Batch 264, LR 2.289171 Loss 5.909828, Accuracy 86.100%\n",
      "Epoch 17, Batch 265, LR 2.289096 Loss 5.910250, Accuracy 86.103%\n",
      "Epoch 17, Batch 266, LR 2.289022 Loss 5.911306, Accuracy 86.093%\n",
      "Epoch 17, Batch 267, LR 2.288947 Loss 5.910631, Accuracy 86.078%\n",
      "Epoch 17, Batch 268, LR 2.288873 Loss 5.910660, Accuracy 86.089%\n",
      "Epoch 17, Batch 269, LR 2.288798 Loss 5.910518, Accuracy 86.097%\n",
      "Epoch 17, Batch 270, LR 2.288724 Loss 5.910778, Accuracy 86.097%\n",
      "Epoch 17, Batch 271, LR 2.288649 Loss 5.910801, Accuracy 86.093%\n",
      "Epoch 17, Batch 272, LR 2.288575 Loss 5.909600, Accuracy 86.107%\n",
      "Epoch 17, Batch 273, LR 2.288500 Loss 5.912099, Accuracy 86.089%\n",
      "Epoch 17, Batch 274, LR 2.288426 Loss 5.913049, Accuracy 86.080%\n",
      "Epoch 17, Batch 275, LR 2.288351 Loss 5.913095, Accuracy 86.082%\n",
      "Epoch 17, Batch 276, LR 2.288276 Loss 5.914117, Accuracy 86.073%\n",
      "Epoch 17, Batch 277, LR 2.288202 Loss 5.913246, Accuracy 86.064%\n",
      "Epoch 17, Batch 278, LR 2.288127 Loss 5.913456, Accuracy 86.075%\n",
      "Epoch 17, Batch 279, LR 2.288053 Loss 5.911160, Accuracy 86.092%\n",
      "Epoch 17, Batch 280, LR 2.287978 Loss 5.910719, Accuracy 86.083%\n",
      "Epoch 17, Batch 281, LR 2.287903 Loss 5.909854, Accuracy 86.096%\n",
      "Epoch 17, Batch 282, LR 2.287829 Loss 5.910082, Accuracy 86.098%\n",
      "Epoch 17, Batch 283, LR 2.287754 Loss 5.908427, Accuracy 86.103%\n",
      "Epoch 17, Batch 284, LR 2.287679 Loss 5.907765, Accuracy 86.105%\n",
      "Epoch 17, Batch 285, LR 2.287605 Loss 5.907044, Accuracy 86.113%\n",
      "Epoch 17, Batch 286, LR 2.287530 Loss 5.905844, Accuracy 86.123%\n",
      "Epoch 17, Batch 287, LR 2.287455 Loss 5.907956, Accuracy 86.117%\n",
      "Epoch 17, Batch 288, LR 2.287380 Loss 5.909362, Accuracy 86.117%\n",
      "Epoch 17, Batch 289, LR 2.287306 Loss 5.910473, Accuracy 86.113%\n",
      "Epoch 17, Batch 290, LR 2.287231 Loss 5.912296, Accuracy 86.110%\n",
      "Epoch 17, Batch 291, LR 2.287156 Loss 5.912645, Accuracy 86.107%\n",
      "Epoch 17, Batch 292, LR 2.287081 Loss 5.914370, Accuracy 86.098%\n",
      "Epoch 17, Batch 293, LR 2.287007 Loss 5.915431, Accuracy 86.092%\n",
      "Epoch 17, Batch 294, LR 2.286932 Loss 5.914513, Accuracy 86.102%\n",
      "Epoch 17, Batch 295, LR 2.286857 Loss 5.914284, Accuracy 86.099%\n",
      "Epoch 17, Batch 296, LR 2.286782 Loss 5.914302, Accuracy 86.104%\n",
      "Epoch 17, Batch 297, LR 2.286707 Loss 5.912849, Accuracy 86.116%\n",
      "Epoch 17, Batch 298, LR 2.286632 Loss 5.913911, Accuracy 86.108%\n",
      "Epoch 17, Batch 299, LR 2.286558 Loss 5.914972, Accuracy 86.105%\n",
      "Epoch 17, Batch 300, LR 2.286483 Loss 5.915059, Accuracy 86.094%\n",
      "Epoch 17, Batch 301, LR 2.286408 Loss 5.913355, Accuracy 86.096%\n",
      "Epoch 17, Batch 302, LR 2.286333 Loss 5.914634, Accuracy 86.082%\n",
      "Epoch 17, Batch 303, LR 2.286258 Loss 5.917632, Accuracy 86.061%\n",
      "Epoch 17, Batch 304, LR 2.286183 Loss 5.917724, Accuracy 86.061%\n",
      "Epoch 17, Batch 305, LR 2.286108 Loss 5.918421, Accuracy 86.045%\n",
      "Epoch 17, Batch 306, LR 2.286033 Loss 5.916411, Accuracy 86.057%\n",
      "Epoch 17, Batch 307, LR 2.285958 Loss 5.915007, Accuracy 86.057%\n",
      "Epoch 17, Batch 308, LR 2.285883 Loss 5.915340, Accuracy 86.049%\n",
      "Epoch 17, Batch 309, LR 2.285808 Loss 5.914323, Accuracy 86.059%\n",
      "Epoch 17, Batch 310, LR 2.285733 Loss 5.915564, Accuracy 86.061%\n",
      "Epoch 17, Batch 311, LR 2.285658 Loss 5.915797, Accuracy 86.056%\n",
      "Epoch 17, Batch 312, LR 2.285583 Loss 5.915023, Accuracy 86.065%\n",
      "Epoch 17, Batch 313, LR 2.285508 Loss 5.914227, Accuracy 86.072%\n",
      "Epoch 17, Batch 314, LR 2.285433 Loss 5.913958, Accuracy 86.079%\n",
      "Epoch 17, Batch 315, LR 2.285358 Loss 5.911988, Accuracy 86.091%\n",
      "Epoch 17, Batch 316, LR 2.285283 Loss 5.913276, Accuracy 86.086%\n",
      "Epoch 17, Batch 317, LR 2.285208 Loss 5.912201, Accuracy 86.088%\n",
      "Epoch 17, Batch 318, LR 2.285133 Loss 5.911850, Accuracy 86.087%\n",
      "Epoch 17, Batch 319, LR 2.285058 Loss 5.910055, Accuracy 86.099%\n",
      "Epoch 17, Batch 320, LR 2.284983 Loss 5.909858, Accuracy 86.096%\n",
      "Epoch 17, Batch 321, LR 2.284908 Loss 5.910340, Accuracy 86.093%\n",
      "Epoch 17, Batch 322, LR 2.284833 Loss 5.908145, Accuracy 86.098%\n",
      "Epoch 17, Batch 323, LR 2.284757 Loss 5.908403, Accuracy 86.090%\n",
      "Epoch 17, Batch 324, LR 2.284682 Loss 5.908183, Accuracy 86.092%\n",
      "Epoch 17, Batch 325, LR 2.284607 Loss 5.909791, Accuracy 86.089%\n",
      "Epoch 17, Batch 326, LR 2.284532 Loss 5.908023, Accuracy 86.103%\n",
      "Epoch 17, Batch 327, LR 2.284457 Loss 5.908041, Accuracy 86.105%\n",
      "Epoch 17, Batch 328, LR 2.284382 Loss 5.905813, Accuracy 86.126%\n",
      "Epoch 17, Batch 329, LR 2.284306 Loss 5.905883, Accuracy 86.120%\n",
      "Epoch 17, Batch 330, LR 2.284231 Loss 5.905820, Accuracy 86.125%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 331, LR 2.284156 Loss 5.905351, Accuracy 86.122%\n",
      "Epoch 17, Batch 332, LR 2.284081 Loss 5.905518, Accuracy 86.123%\n",
      "Epoch 17, Batch 333, LR 2.284005 Loss 5.903969, Accuracy 86.135%\n",
      "Epoch 17, Batch 334, LR 2.283930 Loss 5.902869, Accuracy 86.146%\n",
      "Epoch 17, Batch 335, LR 2.283855 Loss 5.902534, Accuracy 86.147%\n",
      "Epoch 17, Batch 336, LR 2.283780 Loss 5.903263, Accuracy 86.147%\n",
      "Epoch 17, Batch 337, LR 2.283704 Loss 5.901668, Accuracy 86.155%\n",
      "Epoch 17, Batch 338, LR 2.283629 Loss 5.901057, Accuracy 86.148%\n",
      "Epoch 17, Batch 339, LR 2.283554 Loss 5.903117, Accuracy 86.131%\n",
      "Epoch 17, Batch 340, LR 2.283478 Loss 5.903775, Accuracy 86.117%\n",
      "Epoch 17, Batch 341, LR 2.283403 Loss 5.901464, Accuracy 86.132%\n",
      "Epoch 17, Batch 342, LR 2.283327 Loss 5.900506, Accuracy 86.125%\n",
      "Epoch 17, Batch 343, LR 2.283252 Loss 5.903719, Accuracy 86.111%\n",
      "Epoch 17, Batch 344, LR 2.283177 Loss 5.902327, Accuracy 86.103%\n",
      "Epoch 17, Batch 345, LR 2.283101 Loss 5.902100, Accuracy 86.105%\n",
      "Epoch 17, Batch 346, LR 2.283026 Loss 5.902287, Accuracy 86.098%\n",
      "Epoch 17, Batch 347, LR 2.282950 Loss 5.902599, Accuracy 86.095%\n",
      "Epoch 17, Batch 348, LR 2.282875 Loss 5.903129, Accuracy 86.097%\n",
      "Epoch 17, Batch 349, LR 2.282800 Loss 5.904453, Accuracy 86.087%\n",
      "Epoch 17, Batch 350, LR 2.282724 Loss 5.904698, Accuracy 86.087%\n",
      "Epoch 17, Batch 351, LR 2.282649 Loss 5.904652, Accuracy 86.087%\n",
      "Epoch 17, Batch 352, LR 2.282573 Loss 5.904081, Accuracy 86.088%\n",
      "Epoch 17, Batch 353, LR 2.282498 Loss 5.904168, Accuracy 86.084%\n",
      "Epoch 17, Batch 354, LR 2.282422 Loss 5.904986, Accuracy 86.079%\n",
      "Epoch 17, Batch 355, LR 2.282347 Loss 5.904463, Accuracy 86.083%\n",
      "Epoch 17, Batch 356, LR 2.282271 Loss 5.903016, Accuracy 86.087%\n",
      "Epoch 17, Batch 357, LR 2.282196 Loss 5.904312, Accuracy 86.088%\n",
      "Epoch 17, Batch 358, LR 2.282120 Loss 5.901946, Accuracy 86.101%\n",
      "Epoch 17, Batch 359, LR 2.282044 Loss 5.902952, Accuracy 86.085%\n",
      "Epoch 17, Batch 360, LR 2.281969 Loss 5.902669, Accuracy 86.092%\n",
      "Epoch 17, Batch 361, LR 2.281893 Loss 5.904743, Accuracy 86.080%\n",
      "Epoch 17, Batch 362, LR 2.281818 Loss 5.905191, Accuracy 86.078%\n",
      "Epoch 17, Batch 363, LR 2.281742 Loss 5.904166, Accuracy 86.084%\n",
      "Epoch 17, Batch 364, LR 2.281666 Loss 5.902736, Accuracy 86.098%\n",
      "Epoch 17, Batch 365, LR 2.281591 Loss 5.902586, Accuracy 86.107%\n",
      "Epoch 17, Batch 366, LR 2.281515 Loss 5.902523, Accuracy 86.110%\n",
      "Epoch 17, Batch 367, LR 2.281439 Loss 5.901514, Accuracy 86.116%\n",
      "Epoch 17, Batch 368, LR 2.281364 Loss 5.901279, Accuracy 86.124%\n",
      "Epoch 17, Batch 369, LR 2.281288 Loss 5.900902, Accuracy 86.124%\n",
      "Epoch 17, Batch 370, LR 2.281212 Loss 5.900493, Accuracy 86.134%\n",
      "Epoch 17, Batch 371, LR 2.281137 Loss 5.902514, Accuracy 86.133%\n",
      "Epoch 17, Batch 372, LR 2.281061 Loss 5.901857, Accuracy 86.141%\n",
      "Epoch 17, Batch 373, LR 2.280985 Loss 5.901869, Accuracy 86.128%\n",
      "Epoch 17, Batch 374, LR 2.280909 Loss 5.904226, Accuracy 86.100%\n",
      "Epoch 17, Batch 375, LR 2.280834 Loss 5.903887, Accuracy 86.094%\n",
      "Epoch 17, Batch 376, LR 2.280758 Loss 5.905452, Accuracy 86.085%\n",
      "Epoch 17, Batch 377, LR 2.280682 Loss 5.907149, Accuracy 86.074%\n",
      "Epoch 17, Batch 378, LR 2.280606 Loss 5.906894, Accuracy 86.074%\n",
      "Epoch 17, Batch 379, LR 2.280530 Loss 5.908734, Accuracy 86.059%\n",
      "Epoch 17, Batch 380, LR 2.280455 Loss 5.909409, Accuracy 86.059%\n",
      "Epoch 17, Batch 381, LR 2.280379 Loss 5.909822, Accuracy 86.054%\n",
      "Epoch 17, Batch 382, LR 2.280303 Loss 5.909493, Accuracy 86.056%\n",
      "Epoch 17, Batch 383, LR 2.280227 Loss 5.909177, Accuracy 86.066%\n",
      "Epoch 17, Batch 384, LR 2.280151 Loss 5.908301, Accuracy 86.066%\n",
      "Epoch 17, Batch 385, LR 2.280075 Loss 5.907620, Accuracy 86.073%\n",
      "Epoch 17, Batch 386, LR 2.279999 Loss 5.907386, Accuracy 86.081%\n",
      "Epoch 17, Batch 387, LR 2.279924 Loss 5.907649, Accuracy 86.073%\n",
      "Epoch 17, Batch 388, LR 2.279848 Loss 5.907024, Accuracy 86.084%\n",
      "Epoch 17, Batch 389, LR 2.279772 Loss 5.906586, Accuracy 86.080%\n",
      "Epoch 17, Batch 390, LR 2.279696 Loss 5.906258, Accuracy 86.092%\n",
      "Epoch 17, Batch 391, LR 2.279620 Loss 5.905495, Accuracy 86.089%\n",
      "Epoch 17, Batch 392, LR 2.279544 Loss 5.904970, Accuracy 86.093%\n",
      "Epoch 17, Batch 393, LR 2.279468 Loss 5.905397, Accuracy 86.093%\n",
      "Epoch 17, Batch 394, LR 2.279392 Loss 5.904756, Accuracy 86.094%\n",
      "Epoch 17, Batch 395, LR 2.279316 Loss 5.906956, Accuracy 86.078%\n",
      "Epoch 17, Batch 396, LR 2.279240 Loss 5.908129, Accuracy 86.074%\n",
      "Epoch 17, Batch 397, LR 2.279164 Loss 5.908751, Accuracy 86.075%\n",
      "Epoch 17, Batch 398, LR 2.279088 Loss 5.907787, Accuracy 86.077%\n",
      "Epoch 17, Batch 399, LR 2.279012 Loss 5.906613, Accuracy 86.080%\n",
      "Epoch 17, Batch 400, LR 2.278936 Loss 5.906431, Accuracy 86.074%\n",
      "Epoch 17, Batch 401, LR 2.278860 Loss 5.906553, Accuracy 86.068%\n",
      "Epoch 17, Batch 402, LR 2.278784 Loss 5.907915, Accuracy 86.062%\n",
      "Epoch 17, Batch 403, LR 2.278708 Loss 5.906624, Accuracy 86.071%\n",
      "Epoch 17, Batch 404, LR 2.278631 Loss 5.906079, Accuracy 86.075%\n",
      "Epoch 17, Batch 405, LR 2.278555 Loss 5.905503, Accuracy 86.076%\n",
      "Epoch 17, Batch 406, LR 2.278479 Loss 5.903524, Accuracy 86.088%\n",
      "Epoch 17, Batch 407, LR 2.278403 Loss 5.903247, Accuracy 86.087%\n",
      "Epoch 17, Batch 408, LR 2.278327 Loss 5.901712, Accuracy 86.093%\n",
      "Epoch 17, Batch 409, LR 2.278251 Loss 5.903098, Accuracy 86.085%\n",
      "Epoch 17, Batch 410, LR 2.278175 Loss 5.903643, Accuracy 86.079%\n",
      "Epoch 17, Batch 411, LR 2.278098 Loss 5.902538, Accuracy 86.072%\n",
      "Epoch 17, Batch 412, LR 2.278022 Loss 5.902224, Accuracy 86.078%\n",
      "Epoch 17, Batch 413, LR 2.277946 Loss 5.903940, Accuracy 86.074%\n",
      "Epoch 17, Batch 414, LR 2.277870 Loss 5.904127, Accuracy 86.068%\n",
      "Epoch 17, Batch 415, LR 2.277794 Loss 5.905360, Accuracy 86.058%\n",
      "Epoch 17, Batch 416, LR 2.277717 Loss 5.904205, Accuracy 86.067%\n",
      "Epoch 17, Batch 417, LR 2.277641 Loss 5.901852, Accuracy 86.076%\n",
      "Epoch 17, Batch 418, LR 2.277565 Loss 5.901545, Accuracy 86.087%\n",
      "Epoch 17, Batch 419, LR 2.277488 Loss 5.902919, Accuracy 86.077%\n",
      "Epoch 17, Batch 420, LR 2.277412 Loss 5.903193, Accuracy 86.073%\n",
      "Epoch 17, Batch 421, LR 2.277336 Loss 5.904307, Accuracy 86.073%\n",
      "Epoch 17, Batch 422, LR 2.277260 Loss 5.904879, Accuracy 86.074%\n",
      "Epoch 17, Batch 423, LR 2.277183 Loss 5.904037, Accuracy 86.080%\n",
      "Epoch 17, Batch 424, LR 2.277107 Loss 5.904085, Accuracy 86.078%\n",
      "Epoch 17, Batch 425, LR 2.277031 Loss 5.902837, Accuracy 86.085%\n",
      "Epoch 17, Batch 426, LR 2.276954 Loss 5.902240, Accuracy 86.086%\n",
      "Epoch 17, Batch 427, LR 2.276878 Loss 5.902160, Accuracy 86.080%\n",
      "Epoch 17, Batch 428, LR 2.276801 Loss 5.902900, Accuracy 86.074%\n",
      "Epoch 17, Batch 429, LR 2.276725 Loss 5.902751, Accuracy 86.072%\n",
      "Epoch 17, Batch 430, LR 2.276649 Loss 5.901695, Accuracy 86.083%\n",
      "Epoch 17, Batch 431, LR 2.276572 Loss 5.901944, Accuracy 86.077%\n",
      "Epoch 17, Batch 432, LR 2.276496 Loss 5.902179, Accuracy 86.077%\n",
      "Epoch 17, Batch 433, LR 2.276419 Loss 5.903655, Accuracy 86.067%\n",
      "Epoch 17, Batch 434, LR 2.276343 Loss 5.903216, Accuracy 86.065%\n",
      "Epoch 17, Batch 435, LR 2.276266 Loss 5.904126, Accuracy 86.063%\n",
      "Epoch 17, Batch 436, LR 2.276190 Loss 5.904954, Accuracy 86.052%\n",
      "Epoch 17, Batch 437, LR 2.276113 Loss 5.905751, Accuracy 86.048%\n",
      "Epoch 17, Batch 438, LR 2.276037 Loss 5.906801, Accuracy 86.041%\n",
      "Epoch 17, Batch 439, LR 2.275960 Loss 5.907862, Accuracy 86.037%\n",
      "Epoch 17, Batch 440, LR 2.275884 Loss 5.910235, Accuracy 86.030%\n",
      "Epoch 17, Batch 441, LR 2.275807 Loss 5.909563, Accuracy 86.033%\n",
      "Epoch 17, Batch 442, LR 2.275731 Loss 5.908960, Accuracy 86.031%\n",
      "Epoch 17, Batch 443, LR 2.275654 Loss 5.908239, Accuracy 86.033%\n",
      "Epoch 17, Batch 444, LR 2.275578 Loss 5.908443, Accuracy 86.022%\n",
      "Epoch 17, Batch 445, LR 2.275501 Loss 5.909725, Accuracy 86.013%\n",
      "Epoch 17, Batch 446, LR 2.275424 Loss 5.909392, Accuracy 86.013%\n",
      "Epoch 17, Batch 447, LR 2.275348 Loss 5.910174, Accuracy 86.011%\n",
      "Epoch 17, Batch 448, LR 2.275271 Loss 5.909513, Accuracy 86.014%\n",
      "Epoch 17, Batch 449, LR 2.275195 Loss 5.910147, Accuracy 86.018%\n",
      "Epoch 17, Batch 450, LR 2.275118 Loss 5.909454, Accuracy 86.023%\n",
      "Epoch 17, Batch 451, LR 2.275041 Loss 5.910071, Accuracy 86.028%\n",
      "Epoch 17, Batch 452, LR 2.274965 Loss 5.910073, Accuracy 86.033%\n",
      "Epoch 17, Batch 453, LR 2.274888 Loss 5.911056, Accuracy 86.032%\n",
      "Epoch 17, Batch 454, LR 2.274811 Loss 5.911708, Accuracy 86.030%\n",
      "Epoch 17, Batch 455, LR 2.274734 Loss 5.911635, Accuracy 86.032%\n",
      "Epoch 17, Batch 456, LR 2.274658 Loss 5.913435, Accuracy 86.025%\n",
      "Epoch 17, Batch 457, LR 2.274581 Loss 5.913053, Accuracy 86.026%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 458, LR 2.274504 Loss 5.913267, Accuracy 86.019%\n",
      "Epoch 17, Batch 459, LR 2.274428 Loss 5.914377, Accuracy 86.011%\n",
      "Epoch 17, Batch 460, LR 2.274351 Loss 5.914335, Accuracy 86.004%\n",
      "Epoch 17, Batch 461, LR 2.274274 Loss 5.914152, Accuracy 86.004%\n",
      "Epoch 17, Batch 462, LR 2.274197 Loss 5.913903, Accuracy 86.009%\n",
      "Epoch 17, Batch 463, LR 2.274120 Loss 5.912673, Accuracy 86.013%\n",
      "Epoch 17, Batch 464, LR 2.274044 Loss 5.912627, Accuracy 86.013%\n",
      "Epoch 17, Batch 465, LR 2.273967 Loss 5.912260, Accuracy 86.015%\n",
      "Epoch 17, Batch 466, LR 2.273890 Loss 5.914113, Accuracy 86.003%\n",
      "Epoch 17, Batch 467, LR 2.273813 Loss 5.914188, Accuracy 86.006%\n",
      "Epoch 17, Batch 468, LR 2.273736 Loss 5.914243, Accuracy 86.008%\n",
      "Epoch 17, Batch 469, LR 2.273659 Loss 5.912278, Accuracy 86.014%\n",
      "Epoch 17, Batch 470, LR 2.273583 Loss 5.912763, Accuracy 86.019%\n",
      "Epoch 17, Batch 471, LR 2.273506 Loss 5.911992, Accuracy 86.020%\n",
      "Epoch 17, Batch 472, LR 2.273429 Loss 5.910492, Accuracy 86.025%\n",
      "Epoch 17, Batch 473, LR 2.273352 Loss 5.910253, Accuracy 86.032%\n",
      "Epoch 17, Batch 474, LR 2.273275 Loss 5.910984, Accuracy 86.023%\n",
      "Epoch 17, Batch 475, LR 2.273198 Loss 5.911958, Accuracy 86.018%\n",
      "Epoch 17, Batch 476, LR 2.273121 Loss 5.912451, Accuracy 86.015%\n",
      "Epoch 17, Batch 477, LR 2.273044 Loss 5.913163, Accuracy 86.011%\n",
      "Epoch 17, Batch 478, LR 2.272967 Loss 5.912579, Accuracy 86.018%\n",
      "Epoch 17, Batch 479, LR 2.272890 Loss 5.911297, Accuracy 86.017%\n",
      "Epoch 17, Batch 480, LR 2.272813 Loss 5.911086, Accuracy 86.024%\n",
      "Epoch 17, Batch 481, LR 2.272736 Loss 5.909394, Accuracy 86.028%\n",
      "Epoch 17, Batch 482, LR 2.272659 Loss 5.910370, Accuracy 86.028%\n",
      "Epoch 17, Batch 483, LR 2.272582 Loss 5.909099, Accuracy 86.031%\n",
      "Epoch 17, Batch 484, LR 2.272505 Loss 5.909317, Accuracy 86.041%\n",
      "Epoch 17, Batch 485, LR 2.272428 Loss 5.910636, Accuracy 86.034%\n",
      "Epoch 17, Batch 486, LR 2.272351 Loss 5.909992, Accuracy 86.037%\n",
      "Epoch 17, Batch 487, LR 2.272274 Loss 5.908817, Accuracy 86.040%\n",
      "Epoch 17, Batch 488, LR 2.272197 Loss 5.909268, Accuracy 86.037%\n",
      "Epoch 17, Batch 489, LR 2.272120 Loss 5.909293, Accuracy 86.035%\n",
      "Epoch 17, Batch 490, LR 2.272042 Loss 5.910393, Accuracy 86.022%\n",
      "Epoch 17, Batch 491, LR 2.271965 Loss 5.911441, Accuracy 86.023%\n",
      "Epoch 17, Batch 492, LR 2.271888 Loss 5.910746, Accuracy 86.030%\n",
      "Epoch 17, Batch 493, LR 2.271811 Loss 5.911043, Accuracy 86.023%\n",
      "Epoch 17, Batch 494, LR 2.271734 Loss 5.911233, Accuracy 86.028%\n",
      "Epoch 17, Batch 495, LR 2.271657 Loss 5.910009, Accuracy 86.031%\n",
      "Epoch 17, Batch 496, LR 2.271579 Loss 5.910071, Accuracy 86.026%\n",
      "Epoch 17, Batch 497, LR 2.271502 Loss 5.909127, Accuracy 86.030%\n",
      "Epoch 17, Batch 498, LR 2.271425 Loss 5.909644, Accuracy 86.028%\n",
      "Epoch 17, Batch 499, LR 2.271348 Loss 5.909391, Accuracy 86.038%\n",
      "Epoch 17, Batch 500, LR 2.271271 Loss 5.910426, Accuracy 86.027%\n",
      "Epoch 17, Batch 501, LR 2.271193 Loss 5.910866, Accuracy 86.030%\n",
      "Epoch 17, Batch 502, LR 2.271116 Loss 5.911305, Accuracy 86.026%\n",
      "Epoch 17, Batch 503, LR 2.271039 Loss 5.911139, Accuracy 86.020%\n",
      "Epoch 17, Batch 504, LR 2.270962 Loss 5.911930, Accuracy 86.009%\n",
      "Epoch 17, Batch 505, LR 2.270884 Loss 5.911329, Accuracy 86.015%\n",
      "Epoch 17, Batch 506, LR 2.270807 Loss 5.911479, Accuracy 86.016%\n",
      "Epoch 17, Batch 507, LR 2.270730 Loss 5.911465, Accuracy 86.008%\n",
      "Epoch 17, Batch 508, LR 2.270652 Loss 5.911229, Accuracy 86.014%\n",
      "Epoch 17, Batch 509, LR 2.270575 Loss 5.910076, Accuracy 86.020%\n",
      "Epoch 17, Batch 510, LR 2.270498 Loss 5.910466, Accuracy 86.014%\n",
      "Epoch 17, Batch 511, LR 2.270420 Loss 5.910047, Accuracy 86.017%\n",
      "Epoch 17, Batch 512, LR 2.270343 Loss 5.910797, Accuracy 86.012%\n",
      "Epoch 17, Batch 513, LR 2.270265 Loss 5.912456, Accuracy 86.008%\n",
      "Epoch 17, Batch 514, LR 2.270188 Loss 5.912332, Accuracy 86.006%\n",
      "Epoch 17, Batch 515, LR 2.270111 Loss 5.912785, Accuracy 86.010%\n",
      "Epoch 17, Batch 516, LR 2.270033 Loss 5.911705, Accuracy 86.021%\n",
      "Epoch 17, Batch 517, LR 2.269956 Loss 5.911459, Accuracy 86.016%\n",
      "Epoch 17, Batch 518, LR 2.269878 Loss 5.912470, Accuracy 86.010%\n",
      "Epoch 17, Batch 519, LR 2.269801 Loss 5.911872, Accuracy 86.013%\n",
      "Epoch 17, Batch 520, LR 2.269723 Loss 5.911897, Accuracy 86.013%\n",
      "Epoch 17, Batch 521, LR 2.269646 Loss 5.912707, Accuracy 86.003%\n",
      "Epoch 17, Batch 522, LR 2.269568 Loss 5.912762, Accuracy 86.002%\n",
      "Epoch 17, Batch 523, LR 2.269491 Loss 5.912946, Accuracy 86.002%\n",
      "Epoch 17, Batch 524, LR 2.269413 Loss 5.914437, Accuracy 85.991%\n",
      "Epoch 17, Batch 525, LR 2.269336 Loss 5.914249, Accuracy 85.991%\n",
      "Epoch 17, Batch 526, LR 2.269258 Loss 5.913969, Accuracy 85.995%\n",
      "Epoch 17, Batch 527, LR 2.269181 Loss 5.913507, Accuracy 85.997%\n",
      "Epoch 17, Batch 528, LR 2.269103 Loss 5.914465, Accuracy 85.992%\n",
      "Epoch 17, Batch 529, LR 2.269026 Loss 5.915141, Accuracy 85.986%\n",
      "Epoch 17, Batch 530, LR 2.268948 Loss 5.914859, Accuracy 85.988%\n",
      "Epoch 17, Batch 531, LR 2.268871 Loss 5.915951, Accuracy 85.985%\n",
      "Epoch 17, Batch 532, LR 2.268793 Loss 5.915847, Accuracy 85.986%\n",
      "Epoch 17, Batch 533, LR 2.268715 Loss 5.914728, Accuracy 85.989%\n",
      "Epoch 17, Batch 534, LR 2.268638 Loss 5.914930, Accuracy 85.987%\n",
      "Epoch 17, Batch 535, LR 2.268560 Loss 5.914324, Accuracy 85.986%\n",
      "Epoch 17, Batch 536, LR 2.268482 Loss 5.913618, Accuracy 85.989%\n",
      "Epoch 17, Batch 537, LR 2.268405 Loss 5.912456, Accuracy 86.006%\n",
      "Epoch 17, Batch 538, LR 2.268327 Loss 5.913340, Accuracy 85.998%\n",
      "Epoch 17, Batch 539, LR 2.268249 Loss 5.914188, Accuracy 85.997%\n",
      "Epoch 17, Batch 540, LR 2.268172 Loss 5.915534, Accuracy 85.987%\n",
      "Epoch 17, Batch 541, LR 2.268094 Loss 5.915746, Accuracy 85.987%\n",
      "Epoch 17, Batch 542, LR 2.268016 Loss 5.915631, Accuracy 85.984%\n",
      "Epoch 17, Batch 543, LR 2.267938 Loss 5.915034, Accuracy 85.989%\n",
      "Epoch 17, Batch 544, LR 2.267861 Loss 5.915717, Accuracy 85.983%\n",
      "Epoch 17, Batch 545, LR 2.267783 Loss 5.915509, Accuracy 85.989%\n",
      "Epoch 17, Batch 546, LR 2.267705 Loss 5.915496, Accuracy 85.986%\n",
      "Epoch 17, Batch 547, LR 2.267627 Loss 5.915610, Accuracy 85.986%\n",
      "Epoch 17, Batch 548, LR 2.267550 Loss 5.915687, Accuracy 85.979%\n",
      "Epoch 17, Batch 549, LR 2.267472 Loss 5.916335, Accuracy 85.976%\n",
      "Epoch 17, Batch 550, LR 2.267394 Loss 5.915889, Accuracy 85.986%\n",
      "Epoch 17, Batch 551, LR 2.267316 Loss 5.916005, Accuracy 85.974%\n",
      "Epoch 17, Batch 552, LR 2.267238 Loss 5.914980, Accuracy 85.976%\n",
      "Epoch 17, Batch 553, LR 2.267160 Loss 5.915460, Accuracy 85.976%\n",
      "Epoch 17, Batch 554, LR 2.267083 Loss 5.915917, Accuracy 85.976%\n",
      "Epoch 17, Batch 555, LR 2.267005 Loss 5.917500, Accuracy 85.966%\n",
      "Epoch 17, Batch 556, LR 2.266927 Loss 5.917794, Accuracy 85.961%\n",
      "Epoch 17, Batch 557, LR 2.266849 Loss 5.916721, Accuracy 85.964%\n",
      "Epoch 17, Batch 558, LR 2.266771 Loss 5.916016, Accuracy 85.978%\n",
      "Epoch 17, Batch 559, LR 2.266693 Loss 5.916091, Accuracy 85.971%\n",
      "Epoch 17, Batch 560, LR 2.266615 Loss 5.915617, Accuracy 85.979%\n",
      "Epoch 17, Batch 561, LR 2.266537 Loss 5.914992, Accuracy 85.986%\n",
      "Epoch 17, Batch 562, LR 2.266459 Loss 5.913363, Accuracy 85.996%\n",
      "Epoch 17, Batch 563, LR 2.266381 Loss 5.913322, Accuracy 85.996%\n",
      "Epoch 17, Batch 564, LR 2.266303 Loss 5.913139, Accuracy 85.996%\n",
      "Epoch 17, Batch 565, LR 2.266225 Loss 5.914120, Accuracy 85.990%\n",
      "Epoch 17, Batch 566, LR 2.266147 Loss 5.912704, Accuracy 85.998%\n",
      "Epoch 17, Batch 567, LR 2.266069 Loss 5.912771, Accuracy 85.995%\n",
      "Epoch 17, Batch 568, LR 2.265991 Loss 5.911531, Accuracy 86.001%\n",
      "Epoch 17, Batch 569, LR 2.265913 Loss 5.910653, Accuracy 86.014%\n",
      "Epoch 17, Batch 570, LR 2.265835 Loss 5.911791, Accuracy 86.012%\n",
      "Epoch 17, Batch 571, LR 2.265757 Loss 5.911987, Accuracy 86.014%\n",
      "Epoch 17, Batch 572, LR 2.265679 Loss 5.911969, Accuracy 86.014%\n",
      "Epoch 17, Batch 573, LR 2.265601 Loss 5.911225, Accuracy 86.012%\n",
      "Epoch 17, Batch 574, LR 2.265523 Loss 5.911185, Accuracy 86.015%\n",
      "Epoch 17, Batch 575, LR 2.265445 Loss 5.910886, Accuracy 86.018%\n",
      "Epoch 17, Batch 576, LR 2.265367 Loss 5.910820, Accuracy 86.022%\n",
      "Epoch 17, Batch 577, LR 2.265288 Loss 5.911394, Accuracy 86.021%\n",
      "Epoch 17, Batch 578, LR 2.265210 Loss 5.910423, Accuracy 86.028%\n",
      "Epoch 17, Batch 579, LR 2.265132 Loss 5.910167, Accuracy 86.031%\n",
      "Epoch 17, Batch 580, LR 2.265054 Loss 5.910698, Accuracy 86.018%\n",
      "Epoch 17, Batch 581, LR 2.264976 Loss 5.909773, Accuracy 86.022%\n",
      "Epoch 17, Batch 582, LR 2.264898 Loss 5.909611, Accuracy 86.021%\n",
      "Epoch 17, Batch 583, LR 2.264819 Loss 5.909455, Accuracy 86.019%\n",
      "Epoch 17, Batch 584, LR 2.264741 Loss 5.909744, Accuracy 86.023%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 585, LR 2.264663 Loss 5.908799, Accuracy 86.030%\n",
      "Epoch 17, Batch 586, LR 2.264585 Loss 5.908067, Accuracy 86.031%\n",
      "Epoch 17, Batch 587, LR 2.264506 Loss 5.907323, Accuracy 86.036%\n",
      "Epoch 17, Batch 588, LR 2.264428 Loss 5.906918, Accuracy 86.038%\n",
      "Epoch 17, Batch 589, LR 2.264350 Loss 5.906715, Accuracy 86.041%\n",
      "Epoch 17, Batch 590, LR 2.264272 Loss 5.907512, Accuracy 86.039%\n",
      "Epoch 17, Batch 591, LR 2.264193 Loss 5.906366, Accuracy 86.041%\n",
      "Epoch 17, Batch 592, LR 2.264115 Loss 5.906933, Accuracy 86.036%\n",
      "Epoch 17, Batch 593, LR 2.264037 Loss 5.906904, Accuracy 86.034%\n",
      "Epoch 17, Batch 594, LR 2.263958 Loss 5.906831, Accuracy 86.035%\n",
      "Epoch 17, Batch 595, LR 2.263880 Loss 5.906898, Accuracy 86.033%\n",
      "Epoch 17, Batch 596, LR 2.263802 Loss 5.907522, Accuracy 86.032%\n",
      "Epoch 17, Batch 597, LR 2.263723 Loss 5.908063, Accuracy 86.030%\n",
      "Epoch 17, Batch 598, LR 2.263645 Loss 5.908917, Accuracy 86.026%\n",
      "Epoch 17, Batch 599, LR 2.263567 Loss 5.908516, Accuracy 86.027%\n",
      "Epoch 17, Batch 600, LR 2.263488 Loss 5.908823, Accuracy 86.023%\n",
      "Epoch 17, Batch 601, LR 2.263410 Loss 5.908129, Accuracy 86.028%\n",
      "Epoch 17, Batch 602, LR 2.263331 Loss 5.909051, Accuracy 86.019%\n",
      "Epoch 17, Batch 603, LR 2.263253 Loss 5.909426, Accuracy 86.011%\n",
      "Epoch 17, Batch 604, LR 2.263174 Loss 5.909177, Accuracy 86.013%\n",
      "Epoch 17, Batch 605, LR 2.263096 Loss 5.909296, Accuracy 86.007%\n",
      "Epoch 17, Batch 606, LR 2.263018 Loss 5.908061, Accuracy 86.011%\n",
      "Epoch 17, Batch 607, LR 2.262939 Loss 5.908604, Accuracy 86.012%\n",
      "Epoch 17, Batch 608, LR 2.262861 Loss 5.908199, Accuracy 86.013%\n",
      "Epoch 17, Batch 609, LR 2.262782 Loss 5.909000, Accuracy 86.014%\n",
      "Epoch 17, Batch 610, LR 2.262704 Loss 5.908724, Accuracy 86.017%\n",
      "Epoch 17, Batch 611, LR 2.262625 Loss 5.908148, Accuracy 86.013%\n",
      "Epoch 17, Batch 612, LR 2.262546 Loss 5.907836, Accuracy 86.017%\n",
      "Epoch 17, Batch 613, LR 2.262468 Loss 5.907773, Accuracy 86.014%\n",
      "Epoch 17, Batch 614, LR 2.262389 Loss 5.907837, Accuracy 86.013%\n",
      "Epoch 17, Batch 615, LR 2.262311 Loss 5.906760, Accuracy 86.019%\n",
      "Epoch 17, Batch 616, LR 2.262232 Loss 5.906437, Accuracy 86.020%\n",
      "Epoch 17, Batch 617, LR 2.262154 Loss 5.907269, Accuracy 86.016%\n",
      "Epoch 17, Batch 618, LR 2.262075 Loss 5.908444, Accuracy 86.012%\n",
      "Epoch 17, Batch 619, LR 2.261996 Loss 5.907039, Accuracy 86.020%\n",
      "Epoch 17, Batch 620, LR 2.261918 Loss 5.906527, Accuracy 86.016%\n",
      "Epoch 17, Batch 621, LR 2.261839 Loss 5.906418, Accuracy 86.018%\n",
      "Epoch 17, Batch 622, LR 2.261760 Loss 5.907099, Accuracy 86.013%\n",
      "Epoch 17, Batch 623, LR 2.261682 Loss 5.906722, Accuracy 86.010%\n",
      "Epoch 17, Batch 624, LR 2.261603 Loss 5.907375, Accuracy 85.999%\n",
      "Epoch 17, Batch 625, LR 2.261524 Loss 5.906653, Accuracy 86.005%\n",
      "Epoch 17, Batch 626, LR 2.261446 Loss 5.905237, Accuracy 86.010%\n",
      "Epoch 17, Batch 627, LR 2.261367 Loss 5.904114, Accuracy 86.010%\n",
      "Epoch 17, Batch 628, LR 2.261288 Loss 5.904613, Accuracy 86.006%\n",
      "Epoch 17, Batch 629, LR 2.261209 Loss 5.904131, Accuracy 86.006%\n",
      "Epoch 17, Batch 630, LR 2.261131 Loss 5.905290, Accuracy 86.002%\n",
      "Epoch 17, Batch 631, LR 2.261052 Loss 5.905555, Accuracy 86.001%\n",
      "Epoch 17, Batch 632, LR 2.260973 Loss 5.904704, Accuracy 86.003%\n",
      "Epoch 17, Batch 633, LR 2.260894 Loss 5.904974, Accuracy 86.003%\n",
      "Epoch 17, Batch 634, LR 2.260816 Loss 5.904941, Accuracy 86.003%\n",
      "Epoch 17, Batch 635, LR 2.260737 Loss 5.905030, Accuracy 86.005%\n",
      "Epoch 17, Batch 636, LR 2.260658 Loss 5.904848, Accuracy 86.009%\n",
      "Epoch 17, Batch 637, LR 2.260579 Loss 5.905495, Accuracy 86.004%\n",
      "Epoch 17, Batch 638, LR 2.260500 Loss 5.905724, Accuracy 85.996%\n",
      "Epoch 17, Batch 639, LR 2.260421 Loss 5.905245, Accuracy 85.995%\n",
      "Epoch 17, Batch 640, LR 2.260343 Loss 5.906128, Accuracy 85.988%\n",
      "Epoch 17, Batch 641, LR 2.260264 Loss 5.905428, Accuracy 85.989%\n",
      "Epoch 17, Batch 642, LR 2.260185 Loss 5.905008, Accuracy 85.990%\n",
      "Epoch 17, Batch 643, LR 2.260106 Loss 5.905675, Accuracy 85.984%\n",
      "Epoch 17, Batch 644, LR 2.260027 Loss 5.905039, Accuracy 85.985%\n",
      "Epoch 17, Batch 645, LR 2.259948 Loss 5.905251, Accuracy 85.990%\n",
      "Epoch 17, Batch 646, LR 2.259869 Loss 5.905279, Accuracy 85.988%\n",
      "Epoch 17, Batch 647, LR 2.259790 Loss 5.905536, Accuracy 85.988%\n",
      "Epoch 17, Batch 648, LR 2.259711 Loss 5.906221, Accuracy 85.981%\n",
      "Epoch 17, Batch 649, LR 2.259632 Loss 5.906708, Accuracy 85.981%\n",
      "Epoch 17, Batch 650, LR 2.259553 Loss 5.906199, Accuracy 85.982%\n",
      "Epoch 17, Batch 651, LR 2.259474 Loss 5.905710, Accuracy 85.984%\n",
      "Epoch 17, Batch 652, LR 2.259395 Loss 5.906803, Accuracy 85.981%\n",
      "Epoch 17, Batch 653, LR 2.259316 Loss 5.906968, Accuracy 85.981%\n",
      "Epoch 17, Batch 654, LR 2.259237 Loss 5.907966, Accuracy 85.973%\n",
      "Epoch 17, Batch 655, LR 2.259158 Loss 5.907470, Accuracy 85.970%\n",
      "Epoch 17, Batch 656, LR 2.259079 Loss 5.908093, Accuracy 85.966%\n",
      "Epoch 17, Batch 657, LR 2.259000 Loss 5.908506, Accuracy 85.961%\n",
      "Epoch 17, Batch 658, LR 2.258921 Loss 5.906730, Accuracy 85.970%\n",
      "Epoch 17, Batch 659, LR 2.258842 Loss 5.906766, Accuracy 85.970%\n",
      "Epoch 17, Batch 660, LR 2.258763 Loss 5.905843, Accuracy 85.977%\n",
      "Epoch 17, Batch 661, LR 2.258684 Loss 5.905779, Accuracy 85.980%\n",
      "Epoch 17, Batch 662, LR 2.258605 Loss 5.905873, Accuracy 85.985%\n",
      "Epoch 17, Batch 663, LR 2.258525 Loss 5.905338, Accuracy 85.981%\n",
      "Epoch 17, Batch 664, LR 2.258446 Loss 5.905564, Accuracy 85.976%\n",
      "Epoch 17, Batch 665, LR 2.258367 Loss 5.905874, Accuracy 85.972%\n",
      "Epoch 17, Batch 666, LR 2.258288 Loss 5.905733, Accuracy 85.970%\n",
      "Epoch 17, Batch 667, LR 2.258209 Loss 5.905257, Accuracy 85.971%\n",
      "Epoch 17, Batch 668, LR 2.258130 Loss 5.904596, Accuracy 85.976%\n",
      "Epoch 17, Batch 669, LR 2.258050 Loss 5.904756, Accuracy 85.969%\n",
      "Epoch 17, Batch 670, LR 2.257971 Loss 5.903950, Accuracy 85.968%\n",
      "Epoch 17, Batch 671, LR 2.257892 Loss 5.903651, Accuracy 85.974%\n",
      "Epoch 17, Batch 672, LR 2.257813 Loss 5.903284, Accuracy 85.972%\n",
      "Epoch 17, Batch 673, LR 2.257733 Loss 5.903035, Accuracy 85.969%\n",
      "Epoch 17, Batch 674, LR 2.257654 Loss 5.904111, Accuracy 85.969%\n",
      "Epoch 17, Batch 675, LR 2.257575 Loss 5.903776, Accuracy 85.964%\n",
      "Epoch 17, Batch 676, LR 2.257496 Loss 5.903366, Accuracy 85.965%\n",
      "Epoch 17, Batch 677, LR 2.257416 Loss 5.903120, Accuracy 85.966%\n",
      "Epoch 17, Batch 678, LR 2.257337 Loss 5.903650, Accuracy 85.961%\n",
      "Epoch 17, Batch 679, LR 2.257258 Loss 5.902244, Accuracy 85.967%\n",
      "Epoch 17, Batch 680, LR 2.257178 Loss 5.902146, Accuracy 85.966%\n",
      "Epoch 17, Batch 681, LR 2.257099 Loss 5.902948, Accuracy 85.966%\n",
      "Epoch 17, Batch 682, LR 2.257020 Loss 5.902710, Accuracy 85.964%\n",
      "Epoch 17, Batch 683, LR 2.256940 Loss 5.902742, Accuracy 85.962%\n",
      "Epoch 17, Batch 684, LR 2.256861 Loss 5.902463, Accuracy 85.964%\n",
      "Epoch 17, Batch 685, LR 2.256782 Loss 5.901457, Accuracy 85.967%\n",
      "Epoch 17, Batch 686, LR 2.256702 Loss 5.901820, Accuracy 85.963%\n",
      "Epoch 17, Batch 687, LR 2.256623 Loss 5.902268, Accuracy 85.963%\n",
      "Epoch 17, Batch 688, LR 2.256543 Loss 5.902656, Accuracy 85.958%\n",
      "Epoch 17, Batch 689, LR 2.256464 Loss 5.903086, Accuracy 85.953%\n",
      "Epoch 17, Batch 690, LR 2.256384 Loss 5.902768, Accuracy 85.958%\n",
      "Epoch 17, Batch 691, LR 2.256305 Loss 5.902190, Accuracy 85.958%\n",
      "Epoch 17, Batch 692, LR 2.256226 Loss 5.902729, Accuracy 85.953%\n",
      "Epoch 17, Batch 693, LR 2.256146 Loss 5.901965, Accuracy 85.956%\n",
      "Epoch 17, Batch 694, LR 2.256067 Loss 5.901438, Accuracy 85.954%\n",
      "Epoch 17, Batch 695, LR 2.255987 Loss 5.903013, Accuracy 85.942%\n",
      "Epoch 17, Batch 696, LR 2.255908 Loss 5.902863, Accuracy 85.944%\n",
      "Epoch 17, Batch 697, LR 2.255828 Loss 5.901450, Accuracy 85.954%\n",
      "Epoch 17, Batch 698, LR 2.255748 Loss 5.902225, Accuracy 85.945%\n",
      "Epoch 17, Batch 699, LR 2.255669 Loss 5.902464, Accuracy 85.943%\n",
      "Epoch 17, Batch 700, LR 2.255589 Loss 5.902457, Accuracy 85.944%\n",
      "Epoch 17, Batch 701, LR 2.255510 Loss 5.903623, Accuracy 85.933%\n",
      "Epoch 17, Batch 702, LR 2.255430 Loss 5.903453, Accuracy 85.941%\n",
      "Epoch 17, Batch 703, LR 2.255351 Loss 5.903901, Accuracy 85.936%\n",
      "Epoch 17, Batch 704, LR 2.255271 Loss 5.903827, Accuracy 85.941%\n",
      "Epoch 17, Batch 705, LR 2.255191 Loss 5.903793, Accuracy 85.940%\n",
      "Epoch 17, Batch 706, LR 2.255112 Loss 5.904123, Accuracy 85.932%\n",
      "Epoch 17, Batch 707, LR 2.255032 Loss 5.904230, Accuracy 85.935%\n",
      "Epoch 17, Batch 708, LR 2.254952 Loss 5.904272, Accuracy 85.934%\n",
      "Epoch 17, Batch 709, LR 2.254873 Loss 5.905961, Accuracy 85.929%\n",
      "Epoch 17, Batch 710, LR 2.254793 Loss 5.905142, Accuracy 85.935%\n",
      "Epoch 17, Batch 711, LR 2.254713 Loss 5.904211, Accuracy 85.944%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 712, LR 2.254634 Loss 5.905235, Accuracy 85.943%\n",
      "Epoch 17, Batch 713, LR 2.254554 Loss 5.905426, Accuracy 85.941%\n",
      "Epoch 17, Batch 714, LR 2.254474 Loss 5.905746, Accuracy 85.942%\n",
      "Epoch 17, Batch 715, LR 2.254395 Loss 5.906319, Accuracy 85.941%\n",
      "Epoch 17, Batch 716, LR 2.254315 Loss 5.907263, Accuracy 85.936%\n",
      "Epoch 17, Batch 717, LR 2.254235 Loss 5.907732, Accuracy 85.935%\n",
      "Epoch 17, Batch 718, LR 2.254155 Loss 5.907688, Accuracy 85.938%\n",
      "Epoch 17, Batch 719, LR 2.254075 Loss 5.908346, Accuracy 85.939%\n",
      "Epoch 17, Batch 720, LR 2.253996 Loss 5.908889, Accuracy 85.938%\n",
      "Epoch 17, Batch 721, LR 2.253916 Loss 5.909802, Accuracy 85.936%\n",
      "Epoch 17, Batch 722, LR 2.253836 Loss 5.910377, Accuracy 85.935%\n",
      "Epoch 17, Batch 723, LR 2.253756 Loss 5.910191, Accuracy 85.936%\n",
      "Epoch 17, Batch 724, LR 2.253676 Loss 5.910249, Accuracy 85.933%\n",
      "Epoch 17, Batch 725, LR 2.253597 Loss 5.910335, Accuracy 85.930%\n",
      "Epoch 17, Batch 726, LR 2.253517 Loss 5.911132, Accuracy 85.922%\n",
      "Epoch 17, Batch 727, LR 2.253437 Loss 5.911529, Accuracy 85.922%\n",
      "Epoch 17, Batch 728, LR 2.253357 Loss 5.912684, Accuracy 85.914%\n",
      "Epoch 17, Batch 729, LR 2.253277 Loss 5.912813, Accuracy 85.913%\n",
      "Epoch 17, Batch 730, LR 2.253197 Loss 5.913206, Accuracy 85.914%\n",
      "Epoch 17, Batch 731, LR 2.253117 Loss 5.912675, Accuracy 85.917%\n",
      "Epoch 17, Batch 732, LR 2.253037 Loss 5.912162, Accuracy 85.920%\n",
      "Epoch 17, Batch 733, LR 2.252957 Loss 5.912546, Accuracy 85.919%\n",
      "Epoch 17, Batch 734, LR 2.252877 Loss 5.912647, Accuracy 85.922%\n",
      "Epoch 17, Batch 735, LR 2.252797 Loss 5.913238, Accuracy 85.918%\n",
      "Epoch 17, Batch 736, LR 2.252717 Loss 5.913121, Accuracy 85.921%\n",
      "Epoch 17, Batch 737, LR 2.252637 Loss 5.912601, Accuracy 85.922%\n",
      "Epoch 17, Batch 738, LR 2.252557 Loss 5.912151, Accuracy 85.915%\n",
      "Epoch 17, Batch 739, LR 2.252477 Loss 5.912276, Accuracy 85.917%\n",
      "Epoch 17, Batch 740, LR 2.252397 Loss 5.912068, Accuracy 85.914%\n",
      "Epoch 17, Batch 741, LR 2.252317 Loss 5.912353, Accuracy 85.914%\n",
      "Epoch 17, Batch 742, LR 2.252237 Loss 5.911751, Accuracy 85.920%\n",
      "Epoch 17, Batch 743, LR 2.252157 Loss 5.912393, Accuracy 85.922%\n",
      "Epoch 17, Batch 744, LR 2.252077 Loss 5.911829, Accuracy 85.924%\n",
      "Epoch 17, Batch 745, LR 2.251997 Loss 5.912156, Accuracy 85.922%\n",
      "Epoch 17, Batch 746, LR 2.251917 Loss 5.911951, Accuracy 85.922%\n",
      "Epoch 17, Batch 747, LR 2.251837 Loss 5.912176, Accuracy 85.923%\n",
      "Epoch 17, Batch 748, LR 2.251757 Loss 5.911812, Accuracy 85.927%\n",
      "Epoch 17, Batch 749, LR 2.251677 Loss 5.911992, Accuracy 85.923%\n",
      "Epoch 17, Batch 750, LR 2.251597 Loss 5.911863, Accuracy 85.923%\n",
      "Epoch 17, Batch 751, LR 2.251516 Loss 5.911684, Accuracy 85.921%\n",
      "Epoch 17, Batch 752, LR 2.251436 Loss 5.911924, Accuracy 85.917%\n",
      "Epoch 17, Batch 753, LR 2.251356 Loss 5.911776, Accuracy 85.920%\n",
      "Epoch 17, Batch 754, LR 2.251276 Loss 5.911547, Accuracy 85.923%\n",
      "Epoch 17, Batch 755, LR 2.251196 Loss 5.911581, Accuracy 85.922%\n",
      "Epoch 17, Batch 756, LR 2.251115 Loss 5.911875, Accuracy 85.917%\n",
      "Epoch 17, Batch 757, LR 2.251035 Loss 5.911922, Accuracy 85.917%\n",
      "Epoch 17, Batch 758, LR 2.250955 Loss 5.912652, Accuracy 85.911%\n",
      "Epoch 17, Batch 759, LR 2.250875 Loss 5.912128, Accuracy 85.915%\n",
      "Epoch 17, Batch 760, LR 2.250795 Loss 5.911871, Accuracy 85.921%\n",
      "Epoch 17, Batch 761, LR 2.250714 Loss 5.912044, Accuracy 85.920%\n",
      "Epoch 17, Batch 762, LR 2.250634 Loss 5.912606, Accuracy 85.916%\n",
      "Epoch 17, Batch 763, LR 2.250554 Loss 5.913408, Accuracy 85.913%\n",
      "Epoch 17, Batch 764, LR 2.250473 Loss 5.913314, Accuracy 85.912%\n",
      "Epoch 17, Batch 765, LR 2.250393 Loss 5.914173, Accuracy 85.910%\n",
      "Epoch 17, Batch 766, LR 2.250313 Loss 5.914031, Accuracy 85.910%\n",
      "Epoch 17, Batch 767, LR 2.250232 Loss 5.913146, Accuracy 85.919%\n",
      "Epoch 17, Batch 768, LR 2.250152 Loss 5.913066, Accuracy 85.925%\n",
      "Epoch 17, Batch 769, LR 2.250072 Loss 5.913436, Accuracy 85.925%\n",
      "Epoch 17, Batch 770, LR 2.249991 Loss 5.914062, Accuracy 85.920%\n",
      "Epoch 17, Batch 771, LR 2.249911 Loss 5.913960, Accuracy 85.923%\n",
      "Epoch 17, Batch 772, LR 2.249831 Loss 5.913778, Accuracy 85.929%\n",
      "Epoch 17, Batch 773, LR 2.249750 Loss 5.914602, Accuracy 85.922%\n",
      "Epoch 17, Batch 774, LR 2.249670 Loss 5.914268, Accuracy 85.927%\n",
      "Epoch 17, Batch 775, LR 2.249589 Loss 5.914663, Accuracy 85.928%\n",
      "Epoch 17, Batch 776, LR 2.249509 Loss 5.915146, Accuracy 85.926%\n",
      "Epoch 17, Batch 777, LR 2.249428 Loss 5.914814, Accuracy 85.929%\n",
      "Epoch 17, Batch 778, LR 2.249348 Loss 5.914308, Accuracy 85.931%\n",
      "Epoch 17, Batch 779, LR 2.249268 Loss 5.914314, Accuracy 85.928%\n",
      "Epoch 17, Batch 780, LR 2.249187 Loss 5.914864, Accuracy 85.927%\n",
      "Epoch 17, Batch 781, LR 2.249107 Loss 5.915364, Accuracy 85.926%\n",
      "Epoch 17, Batch 782, LR 2.249026 Loss 5.916170, Accuracy 85.921%\n",
      "Epoch 17, Batch 783, LR 2.248946 Loss 5.916078, Accuracy 85.924%\n",
      "Epoch 17, Batch 784, LR 2.248865 Loss 5.915958, Accuracy 85.926%\n",
      "Epoch 17, Batch 785, LR 2.248784 Loss 5.915493, Accuracy 85.931%\n",
      "Epoch 17, Batch 786, LR 2.248704 Loss 5.915481, Accuracy 85.930%\n",
      "Epoch 17, Batch 787, LR 2.248623 Loss 5.915899, Accuracy 85.927%\n",
      "Epoch 17, Batch 788, LR 2.248543 Loss 5.916248, Accuracy 85.923%\n",
      "Epoch 17, Batch 789, LR 2.248462 Loss 5.916623, Accuracy 85.924%\n",
      "Epoch 17, Batch 790, LR 2.248382 Loss 5.915481, Accuracy 85.928%\n",
      "Epoch 17, Batch 791, LR 2.248301 Loss 5.915041, Accuracy 85.932%\n",
      "Epoch 17, Batch 792, LR 2.248220 Loss 5.915331, Accuracy 85.928%\n",
      "Epoch 17, Batch 793, LR 2.248140 Loss 5.915202, Accuracy 85.928%\n",
      "Epoch 17, Batch 794, LR 2.248059 Loss 5.915605, Accuracy 85.919%\n",
      "Epoch 17, Batch 795, LR 2.247978 Loss 5.916023, Accuracy 85.920%\n",
      "Epoch 17, Batch 796, LR 2.247898 Loss 5.915858, Accuracy 85.922%\n",
      "Epoch 17, Batch 797, LR 2.247817 Loss 5.915664, Accuracy 85.923%\n",
      "Epoch 17, Batch 798, LR 2.247736 Loss 5.915562, Accuracy 85.925%\n",
      "Epoch 17, Batch 799, LR 2.247656 Loss 5.916298, Accuracy 85.920%\n",
      "Epoch 17, Batch 800, LR 2.247575 Loss 5.915794, Accuracy 85.926%\n",
      "Epoch 17, Batch 801, LR 2.247494 Loss 5.916090, Accuracy 85.924%\n",
      "Epoch 17, Batch 802, LR 2.247414 Loss 5.916517, Accuracy 85.925%\n",
      "Epoch 17, Batch 803, LR 2.247333 Loss 5.915248, Accuracy 85.935%\n",
      "Epoch 17, Batch 804, LR 2.247252 Loss 5.915662, Accuracy 85.932%\n",
      "Epoch 17, Batch 805, LR 2.247171 Loss 5.916595, Accuracy 85.928%\n",
      "Epoch 17, Batch 806, LR 2.247091 Loss 5.916024, Accuracy 85.930%\n",
      "Epoch 17, Batch 807, LR 2.247010 Loss 5.915085, Accuracy 85.936%\n",
      "Epoch 17, Batch 808, LR 2.246929 Loss 5.914999, Accuracy 85.934%\n",
      "Epoch 17, Batch 809, LR 2.246848 Loss 5.914562, Accuracy 85.938%\n",
      "Epoch 17, Batch 810, LR 2.246767 Loss 5.914749, Accuracy 85.938%\n",
      "Epoch 17, Batch 811, LR 2.246686 Loss 5.915206, Accuracy 85.929%\n",
      "Epoch 17, Batch 812, LR 2.246606 Loss 5.915170, Accuracy 85.927%\n",
      "Epoch 17, Batch 813, LR 2.246525 Loss 5.916061, Accuracy 85.924%\n",
      "Epoch 17, Batch 814, LR 2.246444 Loss 5.915563, Accuracy 85.927%\n",
      "Epoch 17, Batch 815, LR 2.246363 Loss 5.915525, Accuracy 85.929%\n",
      "Epoch 17, Batch 816, LR 2.246282 Loss 5.914956, Accuracy 85.929%\n",
      "Epoch 17, Batch 817, LR 2.246201 Loss 5.914376, Accuracy 85.929%\n",
      "Epoch 17, Batch 818, LR 2.246120 Loss 5.914432, Accuracy 85.930%\n",
      "Epoch 17, Batch 819, LR 2.246039 Loss 5.914003, Accuracy 85.926%\n",
      "Epoch 17, Batch 820, LR 2.245958 Loss 5.913596, Accuracy 85.935%\n",
      "Epoch 17, Batch 821, LR 2.245877 Loss 5.912882, Accuracy 85.936%\n",
      "Epoch 17, Batch 822, LR 2.245796 Loss 5.913046, Accuracy 85.939%\n",
      "Epoch 17, Batch 823, LR 2.245715 Loss 5.913597, Accuracy 85.933%\n",
      "Epoch 17, Batch 824, LR 2.245635 Loss 5.913627, Accuracy 85.934%\n",
      "Epoch 17, Batch 825, LR 2.245554 Loss 5.913472, Accuracy 85.933%\n",
      "Epoch 17, Batch 826, LR 2.245473 Loss 5.913328, Accuracy 85.935%\n",
      "Epoch 17, Batch 827, LR 2.245391 Loss 5.913039, Accuracy 85.938%\n",
      "Epoch 17, Batch 828, LR 2.245310 Loss 5.912623, Accuracy 85.941%\n",
      "Epoch 17, Batch 829, LR 2.245229 Loss 5.912045, Accuracy 85.944%\n",
      "Epoch 17, Batch 830, LR 2.245148 Loss 5.911956, Accuracy 85.945%\n",
      "Epoch 17, Batch 831, LR 2.245067 Loss 5.912110, Accuracy 85.944%\n",
      "Epoch 17, Batch 832, LR 2.244986 Loss 5.912020, Accuracy 85.945%\n",
      "Epoch 17, Batch 833, LR 2.244905 Loss 5.912755, Accuracy 85.940%\n",
      "Epoch 17, Batch 834, LR 2.244824 Loss 5.912931, Accuracy 85.938%\n",
      "Epoch 17, Batch 835, LR 2.244743 Loss 5.912438, Accuracy 85.945%\n",
      "Epoch 17, Batch 836, LR 2.244662 Loss 5.912632, Accuracy 85.947%\n",
      "Epoch 17, Batch 837, LR 2.244581 Loss 5.912646, Accuracy 85.945%\n",
      "Epoch 17, Batch 838, LR 2.244500 Loss 5.913311, Accuracy 85.941%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 839, LR 2.244418 Loss 5.913293, Accuracy 85.940%\n",
      "Epoch 17, Batch 840, LR 2.244337 Loss 5.912988, Accuracy 85.939%\n",
      "Epoch 17, Batch 841, LR 2.244256 Loss 5.913116, Accuracy 85.942%\n",
      "Epoch 17, Batch 842, LR 2.244175 Loss 5.912736, Accuracy 85.945%\n",
      "Epoch 17, Batch 843, LR 2.244094 Loss 5.913392, Accuracy 85.938%\n",
      "Epoch 17, Batch 844, LR 2.244012 Loss 5.912849, Accuracy 85.943%\n",
      "Epoch 17, Batch 845, LR 2.243931 Loss 5.913193, Accuracy 85.945%\n",
      "Epoch 17, Batch 846, LR 2.243850 Loss 5.912449, Accuracy 85.945%\n",
      "Epoch 17, Batch 847, LR 2.243769 Loss 5.911831, Accuracy 85.949%\n",
      "Epoch 17, Batch 848, LR 2.243687 Loss 5.912836, Accuracy 85.938%\n",
      "Epoch 17, Batch 849, LR 2.243606 Loss 5.912367, Accuracy 85.938%\n",
      "Epoch 17, Batch 850, LR 2.243525 Loss 5.912168, Accuracy 85.938%\n",
      "Epoch 17, Batch 851, LR 2.243444 Loss 5.912448, Accuracy 85.935%\n",
      "Epoch 17, Batch 852, LR 2.243362 Loss 5.912709, Accuracy 85.929%\n",
      "Epoch 17, Batch 853, LR 2.243281 Loss 5.912329, Accuracy 85.928%\n",
      "Epoch 17, Batch 854, LR 2.243200 Loss 5.912303, Accuracy 85.933%\n",
      "Epoch 17, Batch 855, LR 2.243118 Loss 5.912111, Accuracy 85.939%\n",
      "Epoch 17, Batch 856, LR 2.243037 Loss 5.911926, Accuracy 85.942%\n",
      "Epoch 17, Batch 857, LR 2.242956 Loss 5.911851, Accuracy 85.946%\n",
      "Epoch 17, Batch 858, LR 2.242874 Loss 5.911625, Accuracy 85.947%\n",
      "Epoch 17, Batch 859, LR 2.242793 Loss 5.911692, Accuracy 85.943%\n",
      "Epoch 17, Batch 860, LR 2.242711 Loss 5.911049, Accuracy 85.947%\n",
      "Epoch 17, Batch 861, LR 2.242630 Loss 5.911745, Accuracy 85.942%\n",
      "Epoch 17, Batch 862, LR 2.242549 Loss 5.911924, Accuracy 85.940%\n",
      "Epoch 17, Batch 863, LR 2.242467 Loss 5.912366, Accuracy 85.939%\n",
      "Epoch 17, Batch 864, LR 2.242386 Loss 5.912548, Accuracy 85.940%\n",
      "Epoch 17, Batch 865, LR 2.242304 Loss 5.912982, Accuracy 85.935%\n",
      "Epoch 17, Batch 866, LR 2.242223 Loss 5.912491, Accuracy 85.937%\n",
      "Epoch 17, Batch 867, LR 2.242141 Loss 5.913559, Accuracy 85.929%\n",
      "Epoch 17, Batch 868, LR 2.242060 Loss 5.913267, Accuracy 85.930%\n",
      "Epoch 17, Batch 869, LR 2.241978 Loss 5.914141, Accuracy 85.926%\n",
      "Epoch 17, Batch 870, LR 2.241897 Loss 5.914403, Accuracy 85.923%\n",
      "Epoch 17, Batch 871, LR 2.241815 Loss 5.914864, Accuracy 85.920%\n",
      "Epoch 17, Batch 872, LR 2.241734 Loss 5.915372, Accuracy 85.917%\n",
      "Epoch 17, Batch 873, LR 2.241652 Loss 5.914909, Accuracy 85.920%\n",
      "Epoch 17, Batch 874, LR 2.241571 Loss 5.914859, Accuracy 85.922%\n",
      "Epoch 17, Batch 875, LR 2.241489 Loss 5.914708, Accuracy 85.921%\n",
      "Epoch 17, Batch 876, LR 2.241407 Loss 5.915270, Accuracy 85.920%\n",
      "Epoch 17, Batch 877, LR 2.241326 Loss 5.915677, Accuracy 85.919%\n",
      "Epoch 17, Batch 878, LR 2.241244 Loss 5.916020, Accuracy 85.917%\n",
      "Epoch 17, Batch 879, LR 2.241163 Loss 5.915765, Accuracy 85.919%\n",
      "Epoch 17, Batch 880, LR 2.241081 Loss 5.915100, Accuracy 85.924%\n",
      "Epoch 17, Batch 881, LR 2.240999 Loss 5.914640, Accuracy 85.927%\n",
      "Epoch 17, Batch 882, LR 2.240918 Loss 5.914689, Accuracy 85.924%\n",
      "Epoch 17, Batch 883, LR 2.240836 Loss 5.914317, Accuracy 85.925%\n",
      "Epoch 17, Batch 884, LR 2.240754 Loss 5.914494, Accuracy 85.927%\n",
      "Epoch 17, Batch 885, LR 2.240673 Loss 5.915053, Accuracy 85.925%\n",
      "Epoch 17, Batch 886, LR 2.240591 Loss 5.915797, Accuracy 85.919%\n",
      "Epoch 17, Batch 887, LR 2.240509 Loss 5.915853, Accuracy 85.918%\n",
      "Epoch 17, Batch 888, LR 2.240428 Loss 5.915514, Accuracy 85.919%\n",
      "Epoch 17, Batch 889, LR 2.240346 Loss 5.916164, Accuracy 85.914%\n",
      "Epoch 17, Batch 890, LR 2.240264 Loss 5.916536, Accuracy 85.916%\n",
      "Epoch 17, Batch 891, LR 2.240182 Loss 5.916942, Accuracy 85.916%\n",
      "Epoch 17, Batch 892, LR 2.240101 Loss 5.917616, Accuracy 85.912%\n",
      "Epoch 17, Batch 893, LR 2.240019 Loss 5.918394, Accuracy 85.907%\n",
      "Epoch 17, Batch 894, LR 2.239937 Loss 5.918832, Accuracy 85.905%\n",
      "Epoch 17, Batch 895, LR 2.239855 Loss 5.918153, Accuracy 85.910%\n",
      "Epoch 17, Batch 896, LR 2.239774 Loss 5.917855, Accuracy 85.914%\n",
      "Epoch 17, Batch 897, LR 2.239692 Loss 5.918147, Accuracy 85.912%\n",
      "Epoch 17, Batch 898, LR 2.239610 Loss 5.917796, Accuracy 85.911%\n",
      "Epoch 17, Batch 899, LR 2.239528 Loss 5.918147, Accuracy 85.911%\n",
      "Epoch 17, Batch 900, LR 2.239446 Loss 5.917475, Accuracy 85.912%\n",
      "Epoch 17, Batch 901, LR 2.239364 Loss 5.917539, Accuracy 85.911%\n",
      "Epoch 17, Batch 902, LR 2.239282 Loss 5.918360, Accuracy 85.904%\n",
      "Epoch 17, Batch 903, LR 2.239201 Loss 5.918398, Accuracy 85.901%\n",
      "Epoch 17, Batch 904, LR 2.239119 Loss 5.918680, Accuracy 85.898%\n",
      "Epoch 17, Batch 905, LR 2.239037 Loss 5.918643, Accuracy 85.902%\n",
      "Epoch 17, Batch 906, LR 2.238955 Loss 5.918780, Accuracy 85.901%\n",
      "Epoch 17, Batch 907, LR 2.238873 Loss 5.918757, Accuracy 85.903%\n",
      "Epoch 17, Batch 908, LR 2.238791 Loss 5.919390, Accuracy 85.896%\n",
      "Epoch 17, Batch 909, LR 2.238709 Loss 5.918872, Accuracy 85.898%\n",
      "Epoch 17, Batch 910, LR 2.238627 Loss 5.919743, Accuracy 85.895%\n",
      "Epoch 17, Batch 911, LR 2.238545 Loss 5.919722, Accuracy 85.895%\n",
      "Epoch 17, Batch 912, LR 2.238463 Loss 5.920429, Accuracy 85.886%\n",
      "Epoch 17, Batch 913, LR 2.238381 Loss 5.919323, Accuracy 85.892%\n",
      "Epoch 17, Batch 914, LR 2.238299 Loss 5.918794, Accuracy 85.896%\n",
      "Epoch 17, Batch 915, LR 2.238217 Loss 5.918715, Accuracy 85.897%\n",
      "Epoch 17, Batch 916, LR 2.238135 Loss 5.918320, Accuracy 85.898%\n",
      "Epoch 17, Batch 917, LR 2.238053 Loss 5.918678, Accuracy 85.893%\n",
      "Epoch 17, Batch 918, LR 2.237971 Loss 5.919234, Accuracy 85.891%\n",
      "Epoch 17, Batch 919, LR 2.237889 Loss 5.919602, Accuracy 85.887%\n",
      "Epoch 17, Batch 920, LR 2.237807 Loss 5.919522, Accuracy 85.888%\n",
      "Epoch 17, Batch 921, LR 2.237725 Loss 5.919573, Accuracy 85.888%\n",
      "Epoch 17, Batch 922, LR 2.237643 Loss 5.919621, Accuracy 85.888%\n",
      "Epoch 17, Batch 923, LR 2.237560 Loss 5.919353, Accuracy 85.888%\n",
      "Epoch 17, Batch 924, LR 2.237478 Loss 5.919805, Accuracy 85.883%\n",
      "Epoch 17, Batch 925, LR 2.237396 Loss 5.920121, Accuracy 85.882%\n",
      "Epoch 17, Batch 926, LR 2.237314 Loss 5.919963, Accuracy 85.881%\n",
      "Epoch 17, Batch 927, LR 2.237232 Loss 5.919865, Accuracy 85.881%\n",
      "Epoch 17, Batch 928, LR 2.237150 Loss 5.920092, Accuracy 85.874%\n",
      "Epoch 17, Batch 929, LR 2.237067 Loss 5.920540, Accuracy 85.869%\n",
      "Epoch 17, Batch 930, LR 2.236985 Loss 5.920367, Accuracy 85.869%\n",
      "Epoch 17, Batch 931, LR 2.236903 Loss 5.920394, Accuracy 85.868%\n",
      "Epoch 17, Batch 932, LR 2.236821 Loss 5.920622, Accuracy 85.866%\n",
      "Epoch 17, Batch 933, LR 2.236739 Loss 5.920209, Accuracy 85.868%\n",
      "Epoch 17, Batch 934, LR 2.236656 Loss 5.919470, Accuracy 85.870%\n",
      "Epoch 17, Batch 935, LR 2.236574 Loss 5.919268, Accuracy 85.871%\n",
      "Epoch 17, Batch 936, LR 2.236492 Loss 5.919244, Accuracy 85.872%\n",
      "Epoch 17, Batch 937, LR 2.236410 Loss 5.919143, Accuracy 85.876%\n",
      "Epoch 17, Batch 938, LR 2.236327 Loss 5.919414, Accuracy 85.872%\n",
      "Epoch 17, Batch 939, LR 2.236245 Loss 5.919922, Accuracy 85.868%\n",
      "Epoch 17, Batch 940, LR 2.236163 Loss 5.919545, Accuracy 85.870%\n",
      "Epoch 17, Batch 941, LR 2.236080 Loss 5.919340, Accuracy 85.872%\n",
      "Epoch 17, Batch 942, LR 2.235998 Loss 5.919168, Accuracy 85.873%\n",
      "Epoch 17, Batch 943, LR 2.235916 Loss 5.918373, Accuracy 85.877%\n",
      "Epoch 17, Batch 944, LR 2.235833 Loss 5.918474, Accuracy 85.875%\n",
      "Epoch 17, Batch 945, LR 2.235751 Loss 5.918033, Accuracy 85.878%\n",
      "Epoch 17, Batch 946, LR 2.235669 Loss 5.917927, Accuracy 85.878%\n",
      "Epoch 17, Batch 947, LR 2.235586 Loss 5.917772, Accuracy 85.881%\n",
      "Epoch 17, Batch 948, LR 2.235504 Loss 5.917994, Accuracy 85.880%\n",
      "Epoch 17, Batch 949, LR 2.235421 Loss 5.918052, Accuracy 85.878%\n",
      "Epoch 17, Batch 950, LR 2.235339 Loss 5.917989, Accuracy 85.876%\n",
      "Epoch 17, Batch 951, LR 2.235257 Loss 5.917327, Accuracy 85.877%\n",
      "Epoch 17, Batch 952, LR 2.235174 Loss 5.917522, Accuracy 85.875%\n",
      "Epoch 17, Batch 953, LR 2.235092 Loss 5.917690, Accuracy 85.878%\n",
      "Epoch 17, Batch 954, LR 2.235009 Loss 5.917076, Accuracy 85.878%\n",
      "Epoch 17, Batch 955, LR 2.234927 Loss 5.917148, Accuracy 85.880%\n",
      "Epoch 17, Batch 956, LR 2.234844 Loss 5.917888, Accuracy 85.875%\n",
      "Epoch 17, Batch 957, LR 2.234762 Loss 5.917304, Accuracy 85.880%\n",
      "Epoch 17, Batch 958, LR 2.234679 Loss 5.917768, Accuracy 85.880%\n",
      "Epoch 17, Batch 959, LR 2.234597 Loss 5.918460, Accuracy 85.876%\n",
      "Epoch 17, Batch 960, LR 2.234514 Loss 5.918317, Accuracy 85.879%\n",
      "Epoch 17, Batch 961, LR 2.234432 Loss 5.918180, Accuracy 85.884%\n",
      "Epoch 17, Batch 962, LR 2.234349 Loss 5.918331, Accuracy 85.884%\n",
      "Epoch 17, Batch 963, LR 2.234267 Loss 5.917837, Accuracy 85.888%\n",
      "Epoch 17, Batch 964, LR 2.234184 Loss 5.917348, Accuracy 85.890%\n",
      "Epoch 17, Batch 965, LR 2.234101 Loss 5.917940, Accuracy 85.886%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 966, LR 2.234019 Loss 5.918482, Accuracy 85.883%\n",
      "Epoch 17, Batch 967, LR 2.233936 Loss 5.918248, Accuracy 85.883%\n",
      "Epoch 17, Batch 968, LR 2.233854 Loss 5.918837, Accuracy 85.880%\n",
      "Epoch 17, Batch 969, LR 2.233771 Loss 5.919003, Accuracy 85.878%\n",
      "Epoch 17, Batch 970, LR 2.233688 Loss 5.918313, Accuracy 85.880%\n",
      "Epoch 17, Batch 971, LR 2.233606 Loss 5.918807, Accuracy 85.879%\n",
      "Epoch 17, Batch 972, LR 2.233523 Loss 5.918835, Accuracy 85.877%\n",
      "Epoch 17, Batch 973, LR 2.233440 Loss 5.919205, Accuracy 85.875%\n",
      "Epoch 17, Batch 974, LR 2.233358 Loss 5.919592, Accuracy 85.877%\n",
      "Epoch 17, Batch 975, LR 2.233275 Loss 5.920455, Accuracy 85.873%\n",
      "Epoch 17, Batch 976, LR 2.233192 Loss 5.920683, Accuracy 85.869%\n",
      "Epoch 17, Batch 977, LR 2.233109 Loss 5.919398, Accuracy 85.874%\n",
      "Epoch 17, Batch 978, LR 2.233027 Loss 5.919249, Accuracy 85.875%\n",
      "Epoch 17, Batch 979, LR 2.232944 Loss 5.919071, Accuracy 85.878%\n",
      "Epoch 17, Batch 980, LR 2.232861 Loss 5.919632, Accuracy 85.875%\n",
      "Epoch 17, Batch 981, LR 2.232778 Loss 5.919298, Accuracy 85.875%\n",
      "Epoch 17, Batch 982, LR 2.232696 Loss 5.919031, Accuracy 85.875%\n",
      "Epoch 17, Batch 983, LR 2.232613 Loss 5.918874, Accuracy 85.873%\n",
      "Epoch 17, Batch 984, LR 2.232530 Loss 5.918435, Accuracy 85.875%\n",
      "Epoch 17, Batch 985, LR 2.232447 Loss 5.918668, Accuracy 85.877%\n",
      "Epoch 17, Batch 986, LR 2.232364 Loss 5.918700, Accuracy 85.877%\n",
      "Epoch 17, Batch 987, LR 2.232282 Loss 5.918120, Accuracy 85.875%\n",
      "Epoch 17, Batch 988, LR 2.232199 Loss 5.917880, Accuracy 85.876%\n",
      "Epoch 17, Batch 989, LR 2.232116 Loss 5.917625, Accuracy 85.877%\n",
      "Epoch 17, Batch 990, LR 2.232033 Loss 5.917592, Accuracy 85.880%\n",
      "Epoch 17, Batch 991, LR 2.231950 Loss 5.917272, Accuracy 85.882%\n",
      "Epoch 17, Batch 992, LR 2.231867 Loss 5.916638, Accuracy 85.882%\n",
      "Epoch 17, Batch 993, LR 2.231784 Loss 5.917021, Accuracy 85.878%\n",
      "Epoch 17, Batch 994, LR 2.231701 Loss 5.917571, Accuracy 85.871%\n",
      "Epoch 17, Batch 995, LR 2.231618 Loss 5.917511, Accuracy 85.872%\n",
      "Epoch 17, Batch 996, LR 2.231536 Loss 5.917544, Accuracy 85.872%\n",
      "Epoch 17, Batch 997, LR 2.231453 Loss 5.917499, Accuracy 85.876%\n",
      "Epoch 17, Batch 998, LR 2.231370 Loss 5.917173, Accuracy 85.876%\n",
      "Epoch 17, Batch 999, LR 2.231287 Loss 5.916693, Accuracy 85.877%\n",
      "Epoch 17, Batch 1000, LR 2.231204 Loss 5.917245, Accuracy 85.873%\n",
      "Epoch 17, Batch 1001, LR 2.231121 Loss 5.917329, Accuracy 85.876%\n",
      "Epoch 17, Batch 1002, LR 2.231038 Loss 5.917068, Accuracy 85.879%\n",
      "Epoch 17, Batch 1003, LR 2.230955 Loss 5.916834, Accuracy 85.882%\n",
      "Epoch 17, Batch 1004, LR 2.230872 Loss 5.917337, Accuracy 85.878%\n",
      "Epoch 17, Batch 1005, LR 2.230789 Loss 5.917932, Accuracy 85.879%\n",
      "Epoch 17, Batch 1006, LR 2.230706 Loss 5.917867, Accuracy 85.882%\n",
      "Epoch 17, Batch 1007, LR 2.230622 Loss 5.917360, Accuracy 85.886%\n",
      "Epoch 17, Batch 1008, LR 2.230539 Loss 5.917638, Accuracy 85.882%\n",
      "Epoch 17, Batch 1009, LR 2.230456 Loss 5.917738, Accuracy 85.884%\n",
      "Epoch 17, Batch 1010, LR 2.230373 Loss 5.918111, Accuracy 85.879%\n",
      "Epoch 17, Batch 1011, LR 2.230290 Loss 5.918124, Accuracy 85.878%\n",
      "Epoch 17, Batch 1012, LR 2.230207 Loss 5.917495, Accuracy 85.880%\n",
      "Epoch 17, Batch 1013, LR 2.230124 Loss 5.917272, Accuracy 85.881%\n",
      "Epoch 17, Batch 1014, LR 2.230041 Loss 5.917306, Accuracy 85.883%\n",
      "Epoch 17, Batch 1015, LR 2.229958 Loss 5.917534, Accuracy 85.881%\n",
      "Epoch 17, Batch 1016, LR 2.229874 Loss 5.917405, Accuracy 85.882%\n",
      "Epoch 17, Batch 1017, LR 2.229791 Loss 5.916842, Accuracy 85.884%\n",
      "Epoch 17, Batch 1018, LR 2.229708 Loss 5.916827, Accuracy 85.882%\n",
      "Epoch 17, Batch 1019, LR 2.229625 Loss 5.917184, Accuracy 85.880%\n",
      "Epoch 17, Batch 1020, LR 2.229542 Loss 5.917340, Accuracy 85.878%\n",
      "Epoch 17, Batch 1021, LR 2.229458 Loss 5.917070, Accuracy 85.877%\n",
      "Epoch 17, Batch 1022, LR 2.229375 Loss 5.917648, Accuracy 85.873%\n",
      "Epoch 17, Batch 1023, LR 2.229292 Loss 5.918063, Accuracy 85.874%\n",
      "Epoch 17, Batch 1024, LR 2.229209 Loss 5.918343, Accuracy 85.872%\n",
      "Epoch 17, Batch 1025, LR 2.229125 Loss 5.918130, Accuracy 85.875%\n",
      "Epoch 17, Batch 1026, LR 2.229042 Loss 5.918417, Accuracy 85.871%\n",
      "Epoch 17, Batch 1027, LR 2.228959 Loss 5.918730, Accuracy 85.871%\n",
      "Epoch 17, Batch 1028, LR 2.228876 Loss 5.918086, Accuracy 85.874%\n",
      "Epoch 17, Batch 1029, LR 2.228792 Loss 5.917781, Accuracy 85.876%\n",
      "Epoch 17, Batch 1030, LR 2.228709 Loss 5.917747, Accuracy 85.876%\n",
      "Epoch 17, Batch 1031, LR 2.228626 Loss 5.917794, Accuracy 85.873%\n",
      "Epoch 17, Batch 1032, LR 2.228542 Loss 5.917774, Accuracy 85.875%\n",
      "Epoch 17, Batch 1033, LR 2.228459 Loss 5.917665, Accuracy 85.875%\n",
      "Epoch 17, Batch 1034, LR 2.228376 Loss 5.917602, Accuracy 85.880%\n",
      "Epoch 17, Batch 1035, LR 2.228292 Loss 5.917914, Accuracy 85.876%\n",
      "Epoch 17, Batch 1036, LR 2.228209 Loss 5.918255, Accuracy 85.872%\n",
      "Epoch 17, Batch 1037, LR 2.228125 Loss 5.918714, Accuracy 85.868%\n",
      "Epoch 17, Batch 1038, LR 2.228042 Loss 5.919172, Accuracy 85.866%\n",
      "Epoch 17, Batch 1039, LR 2.227959 Loss 5.919128, Accuracy 85.862%\n",
      "Epoch 17, Batch 1040, LR 2.227875 Loss 5.919284, Accuracy 85.863%\n",
      "Epoch 17, Batch 1041, LR 2.227792 Loss 5.919101, Accuracy 85.862%\n",
      "Epoch 17, Batch 1042, LR 2.227708 Loss 5.919265, Accuracy 85.866%\n",
      "Epoch 17, Batch 1043, LR 2.227625 Loss 5.919332, Accuracy 85.863%\n",
      "Epoch 17, Batch 1044, LR 2.227541 Loss 5.918704, Accuracy 85.864%\n",
      "Epoch 17, Batch 1045, LR 2.227458 Loss 5.918423, Accuracy 85.867%\n",
      "Epoch 17, Batch 1046, LR 2.227374 Loss 5.917972, Accuracy 85.871%\n",
      "Epoch 17, Batch 1047, LR 2.227291 Loss 5.917776, Accuracy 85.876%\n",
      "Epoch 17, Loss (train set) 5.917776, Accuracy (train set) 85.876%\n",
      "Epoch 18, Batch 1, LR 2.227207 Loss 6.456381, Accuracy 78.906%\n",
      "Epoch 18, Batch 2, LR 2.227124 Loss 6.229432, Accuracy 81.250%\n",
      "Epoch 18, Batch 3, LR 2.227040 Loss 5.932930, Accuracy 84.375%\n",
      "Epoch 18, Batch 4, LR 2.226957 Loss 5.892777, Accuracy 84.570%\n",
      "Epoch 18, Batch 5, LR 2.226873 Loss 5.734676, Accuracy 85.938%\n",
      "Epoch 18, Batch 6, LR 2.226789 Loss 5.846266, Accuracy 85.938%\n",
      "Epoch 18, Batch 7, LR 2.226706 Loss 5.948763, Accuracy 85.603%\n",
      "Epoch 18, Batch 8, LR 2.226622 Loss 5.967983, Accuracy 85.547%\n",
      "Epoch 18, Batch 9, LR 2.226539 Loss 5.867403, Accuracy 86.545%\n",
      "Epoch 18, Batch 10, LR 2.226455 Loss 5.863163, Accuracy 86.562%\n",
      "Epoch 18, Batch 11, LR 2.226371 Loss 5.844816, Accuracy 86.222%\n",
      "Epoch 18, Batch 12, LR 2.226288 Loss 5.798922, Accuracy 86.523%\n",
      "Epoch 18, Batch 13, LR 2.226204 Loss 5.792552, Accuracy 86.358%\n",
      "Epoch 18, Batch 14, LR 2.226120 Loss 5.749895, Accuracy 86.384%\n",
      "Epoch 18, Batch 15, LR 2.226037 Loss 5.781784, Accuracy 86.198%\n",
      "Epoch 18, Batch 16, LR 2.225953 Loss 5.782816, Accuracy 86.084%\n",
      "Epoch 18, Batch 17, LR 2.225869 Loss 5.790121, Accuracy 86.075%\n",
      "Epoch 18, Batch 18, LR 2.225786 Loss 5.792913, Accuracy 85.981%\n",
      "Epoch 18, Batch 19, LR 2.225702 Loss 5.779135, Accuracy 85.979%\n",
      "Epoch 18, Batch 20, LR 2.225618 Loss 5.768203, Accuracy 85.938%\n",
      "Epoch 18, Batch 21, LR 2.225534 Loss 5.758415, Accuracy 85.975%\n",
      "Epoch 18, Batch 22, LR 2.225451 Loss 5.738953, Accuracy 86.044%\n",
      "Epoch 18, Batch 23, LR 2.225367 Loss 5.756054, Accuracy 85.870%\n",
      "Epoch 18, Batch 24, LR 2.225283 Loss 5.754231, Accuracy 85.742%\n",
      "Epoch 18, Batch 25, LR 2.225199 Loss 5.753954, Accuracy 85.688%\n",
      "Epoch 18, Batch 26, LR 2.225115 Loss 5.759733, Accuracy 85.727%\n",
      "Epoch 18, Batch 27, LR 2.225032 Loss 5.746986, Accuracy 85.851%\n",
      "Epoch 18, Batch 28, LR 2.224948 Loss 5.736573, Accuracy 85.910%\n",
      "Epoch 18, Batch 29, LR 2.224864 Loss 5.731411, Accuracy 85.938%\n",
      "Epoch 18, Batch 30, LR 2.224780 Loss 5.740061, Accuracy 85.833%\n",
      "Epoch 18, Batch 31, LR 2.224696 Loss 5.734525, Accuracy 85.887%\n",
      "Epoch 18, Batch 32, LR 2.224612 Loss 5.736115, Accuracy 85.986%\n",
      "Epoch 18, Batch 33, LR 2.224529 Loss 5.726926, Accuracy 86.009%\n",
      "Epoch 18, Batch 34, LR 2.224445 Loss 5.721834, Accuracy 85.983%\n",
      "Epoch 18, Batch 35, LR 2.224361 Loss 5.716307, Accuracy 85.915%\n",
      "Epoch 18, Batch 36, LR 2.224277 Loss 5.693738, Accuracy 86.046%\n",
      "Epoch 18, Batch 37, LR 2.224193 Loss 5.694505, Accuracy 86.022%\n",
      "Epoch 18, Batch 38, LR 2.224109 Loss 5.678244, Accuracy 86.123%\n",
      "Epoch 18, Batch 39, LR 2.224025 Loss 5.680859, Accuracy 86.198%\n",
      "Epoch 18, Batch 40, LR 2.223941 Loss 5.662680, Accuracy 86.172%\n",
      "Epoch 18, Batch 41, LR 2.223857 Loss 5.660861, Accuracy 86.223%\n",
      "Epoch 18, Batch 42, LR 2.223773 Loss 5.662521, Accuracy 86.217%\n",
      "Epoch 18, Batch 43, LR 2.223689 Loss 5.649515, Accuracy 86.283%\n",
      "Epoch 18, Batch 44, LR 2.223605 Loss 5.659148, Accuracy 86.293%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 45, LR 2.223521 Loss 5.661787, Accuracy 86.302%\n",
      "Epoch 18, Batch 46, LR 2.223437 Loss 5.666319, Accuracy 86.311%\n",
      "Epoch 18, Batch 47, LR 2.223353 Loss 5.653983, Accuracy 86.403%\n",
      "Epoch 18, Batch 48, LR 2.223269 Loss 5.655276, Accuracy 86.491%\n",
      "Epoch 18, Batch 49, LR 2.223185 Loss 5.650709, Accuracy 86.527%\n",
      "Epoch 18, Batch 50, LR 2.223101 Loss 5.651444, Accuracy 86.578%\n",
      "Epoch 18, Batch 51, LR 2.223017 Loss 5.641563, Accuracy 86.642%\n",
      "Epoch 18, Batch 52, LR 2.222933 Loss 5.629174, Accuracy 86.704%\n",
      "Epoch 18, Batch 53, LR 2.222849 Loss 5.620334, Accuracy 86.778%\n",
      "Epoch 18, Batch 54, LR 2.222764 Loss 5.616360, Accuracy 86.820%\n",
      "Epoch 18, Batch 55, LR 2.222680 Loss 5.623494, Accuracy 86.818%\n",
      "Epoch 18, Batch 56, LR 2.222596 Loss 5.625283, Accuracy 86.886%\n",
      "Epoch 18, Batch 57, LR 2.222512 Loss 5.622041, Accuracy 86.938%\n",
      "Epoch 18, Batch 58, LR 2.222428 Loss 5.642093, Accuracy 86.853%\n",
      "Epoch 18, Batch 59, LR 2.222344 Loss 5.651688, Accuracy 86.811%\n",
      "Epoch 18, Batch 60, LR 2.222259 Loss 5.651046, Accuracy 86.797%\n",
      "Epoch 18, Batch 61, LR 2.222175 Loss 5.657411, Accuracy 86.693%\n",
      "Epoch 18, Batch 62, LR 2.222091 Loss 5.659852, Accuracy 86.694%\n",
      "Epoch 18, Batch 63, LR 2.222007 Loss 5.662387, Accuracy 86.669%\n",
      "Epoch 18, Batch 64, LR 2.221923 Loss 5.660222, Accuracy 86.694%\n",
      "Epoch 18, Batch 65, LR 2.221838 Loss 5.658660, Accuracy 86.731%\n",
      "Epoch 18, Batch 66, LR 2.221754 Loss 5.658068, Accuracy 86.695%\n",
      "Epoch 18, Batch 67, LR 2.221670 Loss 5.664562, Accuracy 86.672%\n",
      "Epoch 18, Batch 68, LR 2.221586 Loss 5.660964, Accuracy 86.719%\n",
      "Epoch 18, Batch 69, LR 2.221501 Loss 5.659821, Accuracy 86.753%\n",
      "Epoch 18, Batch 70, LR 2.221417 Loss 5.655893, Accuracy 86.808%\n",
      "Epoch 18, Batch 71, LR 2.221333 Loss 5.644142, Accuracy 86.840%\n",
      "Epoch 18, Batch 72, LR 2.221248 Loss 5.644499, Accuracy 86.860%\n",
      "Epoch 18, Batch 73, LR 2.221164 Loss 5.651252, Accuracy 86.794%\n",
      "Epoch 18, Batch 74, LR 2.221080 Loss 5.646608, Accuracy 86.835%\n",
      "Epoch 18, Batch 75, LR 2.220995 Loss 5.648618, Accuracy 86.802%\n",
      "Epoch 18, Batch 76, LR 2.220911 Loss 5.656031, Accuracy 86.729%\n",
      "Epoch 18, Batch 77, LR 2.220827 Loss 5.652690, Accuracy 86.769%\n",
      "Epoch 18, Batch 78, LR 2.220742 Loss 5.657933, Accuracy 86.779%\n",
      "Epoch 18, Batch 79, LR 2.220658 Loss 5.667126, Accuracy 86.719%\n",
      "Epoch 18, Batch 80, LR 2.220573 Loss 5.664176, Accuracy 86.709%\n",
      "Epoch 18, Batch 81, LR 2.220489 Loss 5.667059, Accuracy 86.690%\n",
      "Epoch 18, Batch 82, LR 2.220405 Loss 5.669258, Accuracy 86.671%\n",
      "Epoch 18, Batch 83, LR 2.220320 Loss 5.671085, Accuracy 86.672%\n",
      "Epoch 18, Batch 84, LR 2.220236 Loss 5.676763, Accuracy 86.654%\n",
      "Epoch 18, Batch 85, LR 2.220151 Loss 5.668149, Accuracy 86.737%\n",
      "Epoch 18, Batch 86, LR 2.220067 Loss 5.670110, Accuracy 86.728%\n",
      "Epoch 18, Batch 87, LR 2.219982 Loss 5.667516, Accuracy 86.728%\n",
      "Epoch 18, Batch 88, LR 2.219898 Loss 5.667518, Accuracy 86.728%\n",
      "Epoch 18, Batch 89, LR 2.219813 Loss 5.669091, Accuracy 86.701%\n",
      "Epoch 18, Batch 90, LR 2.219729 Loss 5.666745, Accuracy 86.719%\n",
      "Epoch 18, Batch 91, LR 2.219644 Loss 5.669428, Accuracy 86.693%\n",
      "Epoch 18, Batch 92, LR 2.219560 Loss 5.671946, Accuracy 86.693%\n",
      "Epoch 18, Batch 93, LR 2.219475 Loss 5.671185, Accuracy 86.719%\n",
      "Epoch 18, Batch 94, LR 2.219391 Loss 5.675286, Accuracy 86.710%\n",
      "Epoch 18, Batch 95, LR 2.219306 Loss 5.670963, Accuracy 86.702%\n",
      "Epoch 18, Batch 96, LR 2.219221 Loss 5.664520, Accuracy 86.727%\n",
      "Epoch 18, Batch 97, LR 2.219137 Loss 5.665713, Accuracy 86.735%\n",
      "Epoch 18, Batch 98, LR 2.219052 Loss 5.663091, Accuracy 86.775%\n",
      "Epoch 18, Batch 99, LR 2.218968 Loss 5.664856, Accuracy 86.758%\n",
      "Epoch 18, Batch 100, LR 2.218883 Loss 5.662732, Accuracy 86.805%\n",
      "Epoch 18, Batch 101, LR 2.218798 Loss 5.662731, Accuracy 86.788%\n",
      "Epoch 18, Batch 102, LR 2.218714 Loss 5.668605, Accuracy 86.757%\n",
      "Epoch 18, Batch 103, LR 2.218629 Loss 5.668728, Accuracy 86.772%\n",
      "Epoch 18, Batch 104, LR 2.218544 Loss 5.668223, Accuracy 86.794%\n",
      "Epoch 18, Batch 105, LR 2.218460 Loss 5.671530, Accuracy 86.749%\n",
      "Epoch 18, Batch 106, LR 2.218375 Loss 5.672325, Accuracy 86.748%\n",
      "Epoch 18, Batch 107, LR 2.218290 Loss 5.676936, Accuracy 86.726%\n",
      "Epoch 18, Batch 108, LR 2.218205 Loss 5.676973, Accuracy 86.719%\n",
      "Epoch 18, Batch 109, LR 2.218121 Loss 5.678577, Accuracy 86.697%\n",
      "Epoch 18, Batch 110, LR 2.218036 Loss 5.679163, Accuracy 86.690%\n",
      "Epoch 18, Batch 111, LR 2.217951 Loss 5.675325, Accuracy 86.705%\n",
      "Epoch 18, Batch 112, LR 2.217866 Loss 5.672726, Accuracy 86.719%\n",
      "Epoch 18, Batch 113, LR 2.217782 Loss 5.677947, Accuracy 86.650%\n",
      "Epoch 18, Batch 114, LR 2.217697 Loss 5.683953, Accuracy 86.643%\n",
      "Epoch 18, Batch 115, LR 2.217612 Loss 5.688311, Accuracy 86.630%\n",
      "Epoch 18, Batch 116, LR 2.217527 Loss 5.688526, Accuracy 86.611%\n",
      "Epoch 18, Batch 117, LR 2.217443 Loss 5.689336, Accuracy 86.605%\n",
      "Epoch 18, Batch 118, LR 2.217358 Loss 5.690221, Accuracy 86.593%\n",
      "Epoch 18, Batch 119, LR 2.217273 Loss 5.690518, Accuracy 86.581%\n",
      "Epoch 18, Batch 120, LR 2.217188 Loss 5.687846, Accuracy 86.576%\n",
      "Epoch 18, Batch 121, LR 2.217103 Loss 5.693035, Accuracy 86.551%\n",
      "Epoch 18, Batch 122, LR 2.217018 Loss 5.695065, Accuracy 86.527%\n",
      "Epoch 18, Batch 123, LR 2.216933 Loss 5.694627, Accuracy 86.566%\n",
      "Epoch 18, Batch 124, LR 2.216848 Loss 5.695227, Accuracy 86.574%\n",
      "Epoch 18, Batch 125, LR 2.216764 Loss 5.695882, Accuracy 86.569%\n",
      "Epoch 18, Batch 126, LR 2.216679 Loss 5.692872, Accuracy 86.601%\n",
      "Epoch 18, Batch 127, LR 2.216594 Loss 5.696046, Accuracy 86.596%\n",
      "Epoch 18, Batch 128, LR 2.216509 Loss 5.693272, Accuracy 86.627%\n",
      "Epoch 18, Batch 129, LR 2.216424 Loss 5.690643, Accuracy 86.634%\n",
      "Epoch 18, Batch 130, LR 2.216339 Loss 5.695957, Accuracy 86.581%\n",
      "Epoch 18, Batch 131, LR 2.216254 Loss 5.696330, Accuracy 86.576%\n",
      "Epoch 18, Batch 132, LR 2.216169 Loss 5.694197, Accuracy 86.577%\n",
      "Epoch 18, Batch 133, LR 2.216084 Loss 5.696566, Accuracy 86.572%\n",
      "Epoch 18, Batch 134, LR 2.215999 Loss 5.699803, Accuracy 86.567%\n",
      "Epoch 18, Batch 135, LR 2.215914 Loss 5.703207, Accuracy 86.545%\n",
      "Epoch 18, Batch 136, LR 2.215829 Loss 5.698289, Accuracy 86.575%\n",
      "Epoch 18, Batch 137, LR 2.215744 Loss 5.702557, Accuracy 86.570%\n",
      "Epoch 18, Batch 138, LR 2.215659 Loss 5.710082, Accuracy 86.526%\n",
      "Epoch 18, Batch 139, LR 2.215574 Loss 5.712250, Accuracy 86.494%\n",
      "Epoch 18, Batch 140, LR 2.215489 Loss 5.708326, Accuracy 86.535%\n",
      "Epoch 18, Batch 141, LR 2.215404 Loss 5.708132, Accuracy 86.553%\n",
      "Epoch 18, Batch 142, LR 2.215318 Loss 5.712978, Accuracy 86.510%\n",
      "Epoch 18, Batch 143, LR 2.215233 Loss 5.718521, Accuracy 86.495%\n",
      "Epoch 18, Batch 144, LR 2.215148 Loss 5.719130, Accuracy 86.518%\n",
      "Epoch 18, Batch 145, LR 2.215063 Loss 5.716634, Accuracy 86.509%\n",
      "Epoch 18, Batch 146, LR 2.214978 Loss 5.721379, Accuracy 86.499%\n",
      "Epoch 18, Batch 147, LR 2.214893 Loss 5.725387, Accuracy 86.474%\n",
      "Epoch 18, Batch 148, LR 2.214808 Loss 5.725294, Accuracy 86.486%\n",
      "Epoch 18, Batch 149, LR 2.214722 Loss 5.723738, Accuracy 86.520%\n",
      "Epoch 18, Batch 150, LR 2.214637 Loss 5.723721, Accuracy 86.536%\n",
      "Epoch 18, Batch 151, LR 2.214552 Loss 5.724039, Accuracy 86.527%\n",
      "Epoch 18, Batch 152, LR 2.214467 Loss 5.722433, Accuracy 86.523%\n",
      "Epoch 18, Batch 153, LR 2.214382 Loss 5.723000, Accuracy 86.520%\n",
      "Epoch 18, Batch 154, LR 2.214296 Loss 5.723700, Accuracy 86.511%\n",
      "Epoch 18, Batch 155, LR 2.214211 Loss 5.719959, Accuracy 86.527%\n",
      "Epoch 18, Batch 156, LR 2.214126 Loss 5.720279, Accuracy 86.533%\n",
      "Epoch 18, Batch 157, LR 2.214041 Loss 5.721120, Accuracy 86.515%\n",
      "Epoch 18, Batch 158, LR 2.213955 Loss 5.722124, Accuracy 86.541%\n",
      "Epoch 18, Batch 159, LR 2.213870 Loss 5.721977, Accuracy 86.562%\n",
      "Epoch 18, Batch 160, LR 2.213785 Loss 5.722620, Accuracy 86.558%\n",
      "Epoch 18, Batch 161, LR 2.213699 Loss 5.721787, Accuracy 86.568%\n",
      "Epoch 18, Batch 162, LR 2.213614 Loss 5.724827, Accuracy 86.550%\n",
      "Epoch 18, Batch 163, LR 2.213529 Loss 5.721219, Accuracy 86.570%\n",
      "Epoch 18, Batch 164, LR 2.213443 Loss 5.716514, Accuracy 86.585%\n",
      "Epoch 18, Batch 165, LR 2.213358 Loss 5.719262, Accuracy 86.567%\n",
      "Epoch 18, Batch 166, LR 2.213273 Loss 5.714364, Accuracy 86.596%\n",
      "Epoch 18, Batch 167, LR 2.213187 Loss 5.710384, Accuracy 86.621%\n",
      "Epoch 18, Batch 168, LR 2.213102 Loss 5.703286, Accuracy 86.654%\n",
      "Epoch 18, Batch 169, LR 2.213017 Loss 5.702455, Accuracy 86.654%\n",
      "Epoch 18, Batch 170, LR 2.212931 Loss 5.706635, Accuracy 86.641%\n",
      "Epoch 18, Batch 171, LR 2.212846 Loss 5.707418, Accuracy 86.627%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 172, LR 2.212760 Loss 5.703508, Accuracy 86.660%\n",
      "Epoch 18, Batch 173, LR 2.212675 Loss 5.705730, Accuracy 86.651%\n",
      "Epoch 18, Batch 174, LR 2.212589 Loss 5.711397, Accuracy 86.633%\n",
      "Epoch 18, Batch 175, LR 2.212504 Loss 5.710455, Accuracy 86.656%\n",
      "Epoch 18, Batch 176, LR 2.212419 Loss 5.709744, Accuracy 86.652%\n",
      "Epoch 18, Batch 177, LR 2.212333 Loss 5.710071, Accuracy 86.630%\n",
      "Epoch 18, Batch 178, LR 2.212248 Loss 5.708179, Accuracy 86.644%\n",
      "Epoch 18, Batch 179, LR 2.212162 Loss 5.708772, Accuracy 86.631%\n",
      "Epoch 18, Batch 180, LR 2.212077 Loss 5.711077, Accuracy 86.628%\n",
      "Epoch 18, Batch 181, LR 2.211991 Loss 5.710485, Accuracy 86.615%\n",
      "Epoch 18, Batch 182, LR 2.211905 Loss 5.712863, Accuracy 86.594%\n",
      "Epoch 18, Batch 183, LR 2.211820 Loss 5.716052, Accuracy 86.582%\n",
      "Epoch 18, Batch 184, LR 2.211734 Loss 5.714054, Accuracy 86.591%\n",
      "Epoch 18, Batch 185, LR 2.211649 Loss 5.712875, Accuracy 86.596%\n",
      "Epoch 18, Batch 186, LR 2.211563 Loss 5.712161, Accuracy 86.589%\n",
      "Epoch 18, Batch 187, LR 2.211478 Loss 5.709310, Accuracy 86.573%\n",
      "Epoch 18, Batch 188, LR 2.211392 Loss 5.709740, Accuracy 86.553%\n",
      "Epoch 18, Batch 189, LR 2.211306 Loss 5.710273, Accuracy 86.549%\n",
      "Epoch 18, Batch 190, LR 2.211221 Loss 5.707266, Accuracy 86.558%\n",
      "Epoch 18, Batch 191, LR 2.211135 Loss 5.706595, Accuracy 86.567%\n",
      "Epoch 18, Batch 192, LR 2.211049 Loss 5.704594, Accuracy 86.576%\n",
      "Epoch 18, Batch 193, LR 2.210964 Loss 5.705417, Accuracy 86.569%\n",
      "Epoch 18, Batch 194, LR 2.210878 Loss 5.705798, Accuracy 86.554%\n",
      "Epoch 18, Batch 195, LR 2.210792 Loss 5.705820, Accuracy 86.567%\n",
      "Epoch 18, Batch 196, LR 2.210707 Loss 5.709498, Accuracy 86.547%\n",
      "Epoch 18, Batch 197, LR 2.210621 Loss 5.712012, Accuracy 86.532%\n",
      "Epoch 18, Batch 198, LR 2.210535 Loss 5.710698, Accuracy 86.565%\n",
      "Epoch 18, Batch 199, LR 2.210450 Loss 5.711640, Accuracy 86.546%\n",
      "Epoch 18, Batch 200, LR 2.210364 Loss 5.717749, Accuracy 86.508%\n",
      "Epoch 18, Batch 201, LR 2.210278 Loss 5.718308, Accuracy 86.521%\n",
      "Epoch 18, Batch 202, LR 2.210192 Loss 5.717544, Accuracy 86.537%\n",
      "Epoch 18, Batch 203, LR 2.210107 Loss 5.713692, Accuracy 86.565%\n",
      "Epoch 18, Batch 204, LR 2.210021 Loss 5.714933, Accuracy 86.562%\n",
      "Epoch 18, Batch 205, LR 2.209935 Loss 5.717561, Accuracy 86.528%\n",
      "Epoch 18, Batch 206, LR 2.209849 Loss 5.714498, Accuracy 86.556%\n",
      "Epoch 18, Batch 207, LR 2.209763 Loss 5.715183, Accuracy 86.556%\n",
      "Epoch 18, Batch 208, LR 2.209678 Loss 5.713855, Accuracy 86.565%\n",
      "Epoch 18, Batch 209, LR 2.209592 Loss 5.714377, Accuracy 86.577%\n",
      "Epoch 18, Batch 210, LR 2.209506 Loss 5.714877, Accuracy 86.570%\n",
      "Epoch 18, Batch 211, LR 2.209420 Loss 5.714269, Accuracy 86.593%\n",
      "Epoch 18, Batch 212, LR 2.209334 Loss 5.714581, Accuracy 86.590%\n",
      "Epoch 18, Batch 213, LR 2.209248 Loss 5.714890, Accuracy 86.601%\n",
      "Epoch 18, Batch 214, LR 2.209162 Loss 5.714932, Accuracy 86.602%\n",
      "Epoch 18, Batch 215, LR 2.209076 Loss 5.717699, Accuracy 86.581%\n",
      "Epoch 18, Batch 216, LR 2.208991 Loss 5.716238, Accuracy 86.581%\n",
      "Epoch 18, Batch 217, LR 2.208905 Loss 5.718341, Accuracy 86.586%\n",
      "Epoch 18, Batch 218, LR 2.208819 Loss 5.716923, Accuracy 86.579%\n",
      "Epoch 18, Batch 219, LR 2.208733 Loss 5.716648, Accuracy 86.558%\n",
      "Epoch 18, Batch 220, LR 2.208647 Loss 5.716362, Accuracy 86.552%\n",
      "Epoch 18, Batch 221, LR 2.208561 Loss 5.713594, Accuracy 86.563%\n",
      "Epoch 18, Batch 222, LR 2.208475 Loss 5.714308, Accuracy 86.557%\n",
      "Epoch 18, Batch 223, LR 2.208389 Loss 5.713994, Accuracy 86.544%\n",
      "Epoch 18, Batch 224, LR 2.208303 Loss 5.712408, Accuracy 86.558%\n",
      "Epoch 18, Batch 225, LR 2.208217 Loss 5.713660, Accuracy 86.559%\n",
      "Epoch 18, Batch 226, LR 2.208131 Loss 5.715249, Accuracy 86.574%\n",
      "Epoch 18, Batch 227, LR 2.208045 Loss 5.716284, Accuracy 86.557%\n",
      "Epoch 18, Batch 228, LR 2.207959 Loss 5.713537, Accuracy 86.568%\n",
      "Epoch 18, Batch 229, LR 2.207873 Loss 5.713358, Accuracy 86.569%\n",
      "Epoch 18, Batch 230, LR 2.207787 Loss 5.714196, Accuracy 86.566%\n",
      "Epoch 18, Batch 231, LR 2.207701 Loss 5.713410, Accuracy 86.556%\n",
      "Epoch 18, Batch 232, LR 2.207614 Loss 5.712096, Accuracy 86.560%\n",
      "Epoch 18, Batch 233, LR 2.207528 Loss 5.712118, Accuracy 86.548%\n",
      "Epoch 18, Batch 234, LR 2.207442 Loss 5.711948, Accuracy 86.562%\n",
      "Epoch 18, Batch 235, LR 2.207356 Loss 5.711889, Accuracy 86.562%\n",
      "Epoch 18, Batch 236, LR 2.207270 Loss 5.709713, Accuracy 86.560%\n",
      "Epoch 18, Batch 237, LR 2.207184 Loss 5.708874, Accuracy 86.564%\n",
      "Epoch 18, Batch 238, LR 2.207098 Loss 5.708788, Accuracy 86.551%\n",
      "Epoch 18, Batch 239, LR 2.207011 Loss 5.709439, Accuracy 86.546%\n",
      "Epoch 18, Batch 240, LR 2.206925 Loss 5.711444, Accuracy 86.533%\n",
      "Epoch 18, Batch 241, LR 2.206839 Loss 5.712103, Accuracy 86.508%\n",
      "Epoch 18, Batch 242, LR 2.206753 Loss 5.712724, Accuracy 86.506%\n",
      "Epoch 18, Batch 243, LR 2.206667 Loss 5.710221, Accuracy 86.516%\n",
      "Epoch 18, Batch 244, LR 2.206580 Loss 5.708278, Accuracy 86.530%\n",
      "Epoch 18, Batch 245, LR 2.206494 Loss 5.709036, Accuracy 86.537%\n",
      "Epoch 18, Batch 246, LR 2.206408 Loss 5.711532, Accuracy 86.522%\n",
      "Epoch 18, Batch 247, LR 2.206322 Loss 5.710260, Accuracy 86.523%\n",
      "Epoch 18, Batch 248, LR 2.206235 Loss 5.711327, Accuracy 86.517%\n",
      "Epoch 18, Batch 249, LR 2.206149 Loss 5.712600, Accuracy 86.521%\n",
      "Epoch 18, Batch 250, LR 2.206063 Loss 5.712198, Accuracy 86.525%\n",
      "Epoch 18, Batch 251, LR 2.205977 Loss 5.712877, Accuracy 86.516%\n",
      "Epoch 18, Batch 252, LR 2.205890 Loss 5.711498, Accuracy 86.527%\n",
      "Epoch 18, Batch 253, LR 2.205804 Loss 5.712153, Accuracy 86.503%\n",
      "Epoch 18, Batch 254, LR 2.205718 Loss 5.710701, Accuracy 86.507%\n",
      "Epoch 18, Batch 255, LR 2.205631 Loss 5.708710, Accuracy 86.532%\n",
      "Epoch 18, Batch 256, LR 2.205545 Loss 5.710590, Accuracy 86.517%\n",
      "Epoch 18, Batch 257, LR 2.205459 Loss 5.711857, Accuracy 86.518%\n",
      "Epoch 18, Batch 258, LR 2.205372 Loss 5.714685, Accuracy 86.504%\n",
      "Epoch 18, Batch 259, LR 2.205286 Loss 5.714523, Accuracy 86.493%\n",
      "Epoch 18, Batch 260, LR 2.205199 Loss 5.714299, Accuracy 86.502%\n",
      "Epoch 18, Batch 261, LR 2.205113 Loss 5.709979, Accuracy 86.527%\n",
      "Epoch 18, Batch 262, LR 2.205027 Loss 5.712015, Accuracy 86.510%\n",
      "Epoch 18, Batch 263, LR 2.204940 Loss 5.713172, Accuracy 86.499%\n",
      "Epoch 18, Batch 264, LR 2.204854 Loss 5.711166, Accuracy 86.506%\n",
      "Epoch 18, Batch 265, LR 2.204767 Loss 5.714935, Accuracy 86.486%\n",
      "Epoch 18, Batch 266, LR 2.204681 Loss 5.715559, Accuracy 86.498%\n",
      "Epoch 18, Batch 267, LR 2.204594 Loss 5.717399, Accuracy 86.485%\n",
      "Epoch 18, Batch 268, LR 2.204508 Loss 5.718432, Accuracy 86.483%\n",
      "Epoch 18, Batch 269, LR 2.204421 Loss 5.717647, Accuracy 86.492%\n",
      "Epoch 18, Batch 270, LR 2.204335 Loss 5.716806, Accuracy 86.508%\n",
      "Epoch 18, Batch 271, LR 2.204248 Loss 5.717384, Accuracy 86.503%\n",
      "Epoch 18, Batch 272, LR 2.204162 Loss 5.718235, Accuracy 86.486%\n",
      "Epoch 18, Batch 273, LR 2.204075 Loss 5.717476, Accuracy 86.493%\n",
      "Epoch 18, Batch 274, LR 2.203989 Loss 5.717526, Accuracy 86.496%\n",
      "Epoch 18, Batch 275, LR 2.203902 Loss 5.716733, Accuracy 86.514%\n",
      "Epoch 18, Batch 276, LR 2.203816 Loss 5.716431, Accuracy 86.521%\n",
      "Epoch 18, Batch 277, LR 2.203729 Loss 5.718646, Accuracy 86.533%\n",
      "Epoch 18, Batch 278, LR 2.203642 Loss 5.716799, Accuracy 86.553%\n",
      "Epoch 18, Batch 279, LR 2.203556 Loss 5.717228, Accuracy 86.545%\n",
      "Epoch 18, Batch 280, LR 2.203469 Loss 5.715665, Accuracy 86.557%\n",
      "Epoch 18, Batch 281, LR 2.203383 Loss 5.717313, Accuracy 86.557%\n",
      "Epoch 18, Batch 282, LR 2.203296 Loss 5.716193, Accuracy 86.575%\n",
      "Epoch 18, Batch 283, LR 2.203209 Loss 5.717999, Accuracy 86.550%\n",
      "Epoch 18, Batch 284, LR 2.203123 Loss 5.717574, Accuracy 86.540%\n",
      "Epoch 18, Batch 285, LR 2.203036 Loss 5.720789, Accuracy 86.521%\n",
      "Epoch 18, Batch 286, LR 2.202949 Loss 5.722576, Accuracy 86.511%\n",
      "Epoch 18, Batch 287, LR 2.202863 Loss 5.723951, Accuracy 86.512%\n",
      "Epoch 18, Batch 288, LR 2.202776 Loss 5.722607, Accuracy 86.529%\n",
      "Epoch 18, Batch 289, LR 2.202689 Loss 5.723676, Accuracy 86.532%\n",
      "Epoch 18, Batch 290, LR 2.202602 Loss 5.724771, Accuracy 86.525%\n",
      "Epoch 18, Batch 291, LR 2.202516 Loss 5.724692, Accuracy 86.517%\n",
      "Epoch 18, Batch 292, LR 2.202429 Loss 5.724150, Accuracy 86.529%\n",
      "Epoch 18, Batch 293, LR 2.202342 Loss 5.724904, Accuracy 86.532%\n",
      "Epoch 18, Batch 294, LR 2.202255 Loss 5.725064, Accuracy 86.538%\n",
      "Epoch 18, Batch 295, LR 2.202169 Loss 5.725077, Accuracy 86.539%\n",
      "Epoch 18, Batch 296, LR 2.202082 Loss 5.724247, Accuracy 86.560%\n",
      "Epoch 18, Batch 297, LR 2.201995 Loss 5.724013, Accuracy 86.556%\n",
      "Epoch 18, Batch 298, LR 2.201908 Loss 5.723925, Accuracy 86.551%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 299, LR 2.201821 Loss 5.721908, Accuracy 86.567%\n",
      "Epoch 18, Batch 300, LR 2.201735 Loss 5.722579, Accuracy 86.565%\n",
      "Epoch 18, Batch 301, LR 2.201648 Loss 5.723253, Accuracy 86.571%\n",
      "Epoch 18, Batch 302, LR 2.201561 Loss 5.725288, Accuracy 86.558%\n",
      "Epoch 18, Batch 303, LR 2.201474 Loss 5.723886, Accuracy 86.561%\n",
      "Epoch 18, Batch 304, LR 2.201387 Loss 5.724008, Accuracy 86.580%\n",
      "Epoch 18, Batch 305, LR 2.201300 Loss 5.726022, Accuracy 86.552%\n",
      "Epoch 18, Batch 306, LR 2.201213 Loss 5.725686, Accuracy 86.553%\n",
      "Epoch 18, Batch 307, LR 2.201126 Loss 5.724125, Accuracy 86.576%\n",
      "Epoch 18, Batch 308, LR 2.201039 Loss 5.724601, Accuracy 86.592%\n",
      "Epoch 18, Batch 309, LR 2.200952 Loss 5.724799, Accuracy 86.590%\n",
      "Epoch 18, Batch 310, LR 2.200866 Loss 5.723707, Accuracy 86.600%\n",
      "Epoch 18, Batch 311, LR 2.200779 Loss 5.724469, Accuracy 86.591%\n",
      "Epoch 18, Batch 312, LR 2.200692 Loss 5.725174, Accuracy 86.591%\n",
      "Epoch 18, Batch 313, LR 2.200605 Loss 5.726712, Accuracy 86.591%\n",
      "Epoch 18, Batch 314, LR 2.200518 Loss 5.728405, Accuracy 86.587%\n",
      "Epoch 18, Batch 315, LR 2.200431 Loss 5.727438, Accuracy 86.597%\n",
      "Epoch 18, Batch 316, LR 2.200344 Loss 5.726877, Accuracy 86.612%\n",
      "Epoch 18, Batch 317, LR 2.200257 Loss 5.726169, Accuracy 86.608%\n",
      "Epoch 18, Batch 318, LR 2.200170 Loss 5.726294, Accuracy 86.601%\n",
      "Epoch 18, Batch 319, LR 2.200083 Loss 5.724772, Accuracy 86.611%\n",
      "Epoch 18, Batch 320, LR 2.199995 Loss 5.724620, Accuracy 86.619%\n",
      "Epoch 18, Batch 321, LR 2.199908 Loss 5.726132, Accuracy 86.614%\n",
      "Epoch 18, Batch 322, LR 2.199821 Loss 5.724049, Accuracy 86.631%\n",
      "Epoch 18, Batch 323, LR 2.199734 Loss 5.722632, Accuracy 86.637%\n",
      "Epoch 18, Batch 324, LR 2.199647 Loss 5.726351, Accuracy 86.610%\n",
      "Epoch 18, Batch 325, LR 2.199560 Loss 5.725558, Accuracy 86.625%\n",
      "Epoch 18, Batch 326, LR 2.199473 Loss 5.725513, Accuracy 86.632%\n",
      "Epoch 18, Batch 327, LR 2.199386 Loss 5.724893, Accuracy 86.638%\n",
      "Epoch 18, Batch 328, LR 2.199299 Loss 5.723283, Accuracy 86.654%\n",
      "Epoch 18, Batch 329, LR 2.199212 Loss 5.725012, Accuracy 86.643%\n",
      "Epoch 18, Batch 330, LR 2.199124 Loss 5.724252, Accuracy 86.645%\n",
      "Epoch 18, Batch 331, LR 2.199037 Loss 5.722627, Accuracy 86.653%\n",
      "Epoch 18, Batch 332, LR 2.198950 Loss 5.724036, Accuracy 86.646%\n",
      "Epoch 18, Batch 333, LR 2.198863 Loss 5.723934, Accuracy 86.641%\n",
      "Epoch 18, Batch 334, LR 2.198776 Loss 5.720489, Accuracy 86.670%\n",
      "Epoch 18, Batch 335, LR 2.198688 Loss 5.718974, Accuracy 86.677%\n",
      "Epoch 18, Batch 336, LR 2.198601 Loss 5.717704, Accuracy 86.689%\n",
      "Epoch 18, Batch 337, LR 2.198514 Loss 5.716046, Accuracy 86.689%\n",
      "Epoch 18, Batch 338, LR 2.198427 Loss 5.716408, Accuracy 86.673%\n",
      "Epoch 18, Batch 339, LR 2.198339 Loss 5.716545, Accuracy 86.675%\n",
      "Epoch 18, Batch 340, LR 2.198252 Loss 5.717081, Accuracy 86.677%\n",
      "Epoch 18, Batch 341, LR 2.198165 Loss 5.718274, Accuracy 86.671%\n",
      "Epoch 18, Batch 342, LR 2.198078 Loss 5.716499, Accuracy 86.678%\n",
      "Epoch 18, Batch 343, LR 2.197990 Loss 5.718041, Accuracy 86.671%\n",
      "Epoch 18, Batch 344, LR 2.197903 Loss 5.717863, Accuracy 86.662%\n",
      "Epoch 18, Batch 345, LR 2.197816 Loss 5.715965, Accuracy 86.671%\n",
      "Epoch 18, Batch 346, LR 2.197728 Loss 5.715427, Accuracy 86.683%\n",
      "Epoch 18, Batch 347, LR 2.197641 Loss 5.717838, Accuracy 86.669%\n",
      "Epoch 18, Batch 348, LR 2.197554 Loss 5.718430, Accuracy 86.669%\n",
      "Epoch 18, Batch 349, LR 2.197466 Loss 5.717783, Accuracy 86.670%\n",
      "Epoch 18, Batch 350, LR 2.197379 Loss 5.717225, Accuracy 86.667%\n",
      "Epoch 18, Batch 351, LR 2.197291 Loss 5.717380, Accuracy 86.665%\n",
      "Epoch 18, Batch 352, LR 2.197204 Loss 5.716895, Accuracy 86.661%\n",
      "Epoch 18, Batch 353, LR 2.197117 Loss 5.716291, Accuracy 86.666%\n",
      "Epoch 18, Batch 354, LR 2.197029 Loss 5.716559, Accuracy 86.655%\n",
      "Epoch 18, Batch 355, LR 2.196942 Loss 5.715951, Accuracy 86.659%\n",
      "Epoch 18, Batch 356, LR 2.196854 Loss 5.714242, Accuracy 86.673%\n",
      "Epoch 18, Batch 357, LR 2.196767 Loss 5.713986, Accuracy 86.668%\n",
      "Epoch 18, Batch 358, LR 2.196679 Loss 5.714207, Accuracy 86.658%\n",
      "Epoch 18, Batch 359, LR 2.196592 Loss 5.714150, Accuracy 86.656%\n",
      "Epoch 18, Batch 360, LR 2.196504 Loss 5.712870, Accuracy 86.662%\n",
      "Epoch 18, Batch 361, LR 2.196417 Loss 5.713447, Accuracy 86.669%\n",
      "Epoch 18, Batch 362, LR 2.196329 Loss 5.713054, Accuracy 86.671%\n",
      "Epoch 18, Batch 363, LR 2.196242 Loss 5.711726, Accuracy 86.678%\n",
      "Epoch 18, Batch 364, LR 2.196154 Loss 5.712323, Accuracy 86.663%\n",
      "Epoch 18, Batch 365, LR 2.196067 Loss 5.711759, Accuracy 86.665%\n",
      "Epoch 18, Batch 366, LR 2.195979 Loss 5.712853, Accuracy 86.648%\n",
      "Epoch 18, Batch 367, LR 2.195892 Loss 5.711741, Accuracy 86.649%\n",
      "Epoch 18, Batch 368, LR 2.195804 Loss 5.713220, Accuracy 86.642%\n",
      "Epoch 18, Batch 369, LR 2.195716 Loss 5.712895, Accuracy 86.655%\n",
      "Epoch 18, Batch 370, LR 2.195629 Loss 5.714533, Accuracy 86.651%\n",
      "Epoch 18, Batch 371, LR 2.195541 Loss 5.714982, Accuracy 86.647%\n",
      "Epoch 18, Batch 372, LR 2.195454 Loss 5.715184, Accuracy 86.652%\n",
      "Epoch 18, Batch 373, LR 2.195366 Loss 5.715661, Accuracy 86.645%\n",
      "Epoch 18, Batch 374, LR 2.195278 Loss 5.716304, Accuracy 86.644%\n",
      "Epoch 18, Batch 375, LR 2.195191 Loss 5.715310, Accuracy 86.652%\n",
      "Epoch 18, Batch 376, LR 2.195103 Loss 5.716189, Accuracy 86.652%\n",
      "Epoch 18, Batch 377, LR 2.195015 Loss 5.718464, Accuracy 86.644%\n",
      "Epoch 18, Batch 378, LR 2.194928 Loss 5.717233, Accuracy 86.663%\n",
      "Epoch 18, Batch 379, LR 2.194840 Loss 5.715838, Accuracy 86.675%\n",
      "Epoch 18, Batch 380, LR 2.194752 Loss 5.716051, Accuracy 86.667%\n",
      "Epoch 18, Batch 381, LR 2.194665 Loss 5.715932, Accuracy 86.661%\n",
      "Epoch 18, Batch 382, LR 2.194577 Loss 5.716847, Accuracy 86.645%\n",
      "Epoch 18, Batch 383, LR 2.194489 Loss 5.715619, Accuracy 86.660%\n",
      "Epoch 18, Batch 384, LR 2.194401 Loss 5.715520, Accuracy 86.654%\n",
      "Epoch 18, Batch 385, LR 2.194314 Loss 5.715371, Accuracy 86.664%\n",
      "Epoch 18, Batch 386, LR 2.194226 Loss 5.714977, Accuracy 86.660%\n",
      "Epoch 18, Batch 387, LR 2.194138 Loss 5.714303, Accuracy 86.656%\n",
      "Epoch 18, Batch 388, LR 2.194050 Loss 5.713832, Accuracy 86.658%\n",
      "Epoch 18, Batch 389, LR 2.193962 Loss 5.712614, Accuracy 86.667%\n",
      "Epoch 18, Batch 390, LR 2.193875 Loss 5.711308, Accuracy 86.677%\n",
      "Epoch 18, Batch 391, LR 2.193787 Loss 5.711522, Accuracy 86.681%\n",
      "Epoch 18, Batch 392, LR 2.193699 Loss 5.711565, Accuracy 86.681%\n",
      "Epoch 18, Batch 393, LR 2.193611 Loss 5.711018, Accuracy 86.679%\n",
      "Epoch 18, Batch 394, LR 2.193523 Loss 5.709848, Accuracy 86.685%\n",
      "Epoch 18, Batch 395, LR 2.193435 Loss 5.709763, Accuracy 86.695%\n",
      "Epoch 18, Batch 396, LR 2.193347 Loss 5.708608, Accuracy 86.701%\n",
      "Epoch 18, Batch 397, LR 2.193260 Loss 5.707487, Accuracy 86.697%\n",
      "Epoch 18, Batch 398, LR 2.193172 Loss 5.708138, Accuracy 86.693%\n",
      "Epoch 18, Batch 399, LR 2.193084 Loss 5.707757, Accuracy 86.695%\n",
      "Epoch 18, Batch 400, LR 2.192996 Loss 5.709054, Accuracy 86.688%\n",
      "Epoch 18, Batch 401, LR 2.192908 Loss 5.709842, Accuracy 86.684%\n",
      "Epoch 18, Batch 402, LR 2.192820 Loss 5.711406, Accuracy 86.672%\n",
      "Epoch 18, Batch 403, LR 2.192732 Loss 5.711643, Accuracy 86.666%\n",
      "Epoch 18, Batch 404, LR 2.192644 Loss 5.712060, Accuracy 86.661%\n",
      "Epoch 18, Batch 405, LR 2.192556 Loss 5.712155, Accuracy 86.653%\n",
      "Epoch 18, Batch 406, LR 2.192468 Loss 5.712303, Accuracy 86.651%\n",
      "Epoch 18, Batch 407, LR 2.192380 Loss 5.710710, Accuracy 86.661%\n",
      "Epoch 18, Batch 408, LR 2.192292 Loss 5.709037, Accuracy 86.667%\n",
      "Epoch 18, Batch 409, LR 2.192204 Loss 5.710238, Accuracy 86.663%\n",
      "Epoch 18, Batch 410, LR 2.192116 Loss 5.709595, Accuracy 86.665%\n",
      "Epoch 18, Batch 411, LR 2.192028 Loss 5.710978, Accuracy 86.667%\n",
      "Epoch 18, Batch 412, LR 2.191940 Loss 5.710243, Accuracy 86.669%\n",
      "Epoch 18, Batch 413, LR 2.191852 Loss 5.710091, Accuracy 86.673%\n",
      "Epoch 18, Batch 414, LR 2.191764 Loss 5.710735, Accuracy 86.664%\n",
      "Epoch 18, Batch 415, LR 2.191676 Loss 5.711913, Accuracy 86.657%\n",
      "Epoch 18, Batch 416, LR 2.191588 Loss 5.711417, Accuracy 86.653%\n",
      "Epoch 18, Batch 417, LR 2.191499 Loss 5.711102, Accuracy 86.657%\n",
      "Epoch 18, Batch 418, LR 2.191411 Loss 5.710766, Accuracy 86.665%\n",
      "Epoch 18, Batch 419, LR 2.191323 Loss 5.710236, Accuracy 86.668%\n",
      "Epoch 18, Batch 420, LR 2.191235 Loss 5.709243, Accuracy 86.678%\n",
      "Epoch 18, Batch 421, LR 2.191147 Loss 5.709357, Accuracy 86.680%\n",
      "Epoch 18, Batch 422, LR 2.191059 Loss 5.709070, Accuracy 86.685%\n",
      "Epoch 18, Batch 423, LR 2.190971 Loss 5.709680, Accuracy 86.691%\n",
      "Epoch 18, Batch 424, LR 2.190882 Loss 5.709423, Accuracy 86.695%\n",
      "Epoch 18, Batch 425, LR 2.190794 Loss 5.708466, Accuracy 86.700%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 426, LR 2.190706 Loss 5.708080, Accuracy 86.702%\n",
      "Epoch 18, Batch 427, LR 2.190618 Loss 5.708584, Accuracy 86.702%\n",
      "Epoch 18, Batch 428, LR 2.190530 Loss 5.707897, Accuracy 86.706%\n",
      "Epoch 18, Batch 429, LR 2.190441 Loss 5.707282, Accuracy 86.713%\n",
      "Epoch 18, Batch 430, LR 2.190353 Loss 5.706575, Accuracy 86.710%\n",
      "Epoch 18, Batch 431, LR 2.190265 Loss 5.705450, Accuracy 86.721%\n",
      "Epoch 18, Batch 432, LR 2.190177 Loss 5.706109, Accuracy 86.721%\n",
      "Epoch 18, Batch 433, LR 2.190088 Loss 5.707324, Accuracy 86.715%\n",
      "Epoch 18, Batch 434, LR 2.190000 Loss 5.708318, Accuracy 86.713%\n",
      "Epoch 18, Batch 435, LR 2.189912 Loss 5.708176, Accuracy 86.715%\n",
      "Epoch 18, Batch 436, LR 2.189823 Loss 5.708135, Accuracy 86.717%\n",
      "Epoch 18, Batch 437, LR 2.189735 Loss 5.708947, Accuracy 86.719%\n",
      "Epoch 18, Batch 438, LR 2.189647 Loss 5.709087, Accuracy 86.719%\n",
      "Epoch 18, Batch 439, LR 2.189558 Loss 5.710125, Accuracy 86.719%\n",
      "Epoch 18, Batch 440, LR 2.189470 Loss 5.709912, Accuracy 86.728%\n",
      "Epoch 18, Batch 441, LR 2.189382 Loss 5.710246, Accuracy 86.729%\n",
      "Epoch 18, Batch 442, LR 2.189293 Loss 5.711205, Accuracy 86.721%\n",
      "Epoch 18, Batch 443, LR 2.189205 Loss 5.710771, Accuracy 86.722%\n",
      "Epoch 18, Batch 444, LR 2.189116 Loss 5.711951, Accuracy 86.719%\n",
      "Epoch 18, Batch 445, LR 2.189028 Loss 5.713160, Accuracy 86.719%\n",
      "Epoch 18, Batch 446, LR 2.188940 Loss 5.713412, Accuracy 86.722%\n",
      "Epoch 18, Batch 447, LR 2.188851 Loss 5.714309, Accuracy 86.719%\n",
      "Epoch 18, Batch 448, LR 2.188763 Loss 5.714572, Accuracy 86.720%\n",
      "Epoch 18, Batch 449, LR 2.188674 Loss 5.715453, Accuracy 86.712%\n",
      "Epoch 18, Batch 450, LR 2.188586 Loss 5.715904, Accuracy 86.707%\n",
      "Epoch 18, Batch 451, LR 2.188497 Loss 5.718062, Accuracy 86.703%\n",
      "Epoch 18, Batch 452, LR 2.188409 Loss 5.718810, Accuracy 86.695%\n",
      "Epoch 18, Batch 453, LR 2.188320 Loss 5.718631, Accuracy 86.696%\n",
      "Epoch 18, Batch 454, LR 2.188232 Loss 5.719438, Accuracy 86.700%\n",
      "Epoch 18, Batch 455, LR 2.188143 Loss 5.719507, Accuracy 86.695%\n",
      "Epoch 18, Batch 456, LR 2.188055 Loss 5.721136, Accuracy 86.686%\n",
      "Epoch 18, Batch 457, LR 2.187966 Loss 5.719691, Accuracy 86.702%\n",
      "Epoch 18, Batch 458, LR 2.187878 Loss 5.720730, Accuracy 86.702%\n",
      "Epoch 18, Batch 459, LR 2.187789 Loss 5.723120, Accuracy 86.690%\n",
      "Epoch 18, Batch 460, LR 2.187701 Loss 5.723206, Accuracy 86.686%\n",
      "Epoch 18, Batch 461, LR 2.187612 Loss 5.723472, Accuracy 86.688%\n",
      "Epoch 18, Batch 462, LR 2.187523 Loss 5.724062, Accuracy 86.687%\n",
      "Epoch 18, Batch 463, LR 2.187435 Loss 5.724118, Accuracy 86.692%\n",
      "Epoch 18, Batch 464, LR 2.187346 Loss 5.724818, Accuracy 86.688%\n",
      "Epoch 18, Batch 465, LR 2.187258 Loss 5.726175, Accuracy 86.683%\n",
      "Epoch 18, Batch 466, LR 2.187169 Loss 5.726197, Accuracy 86.684%\n",
      "Epoch 18, Batch 467, LR 2.187080 Loss 5.726306, Accuracy 86.687%\n",
      "Epoch 18, Batch 468, LR 2.186992 Loss 5.726798, Accuracy 86.690%\n",
      "Epoch 18, Batch 469, LR 2.186903 Loss 5.726887, Accuracy 86.687%\n",
      "Epoch 18, Batch 470, LR 2.186814 Loss 5.726452, Accuracy 86.684%\n",
      "Epoch 18, Batch 471, LR 2.186726 Loss 5.726396, Accuracy 86.679%\n",
      "Epoch 18, Batch 472, LR 2.186637 Loss 5.728339, Accuracy 86.671%\n",
      "Epoch 18, Batch 473, LR 2.186548 Loss 5.726709, Accuracy 86.679%\n",
      "Epoch 18, Batch 474, LR 2.186459 Loss 5.726077, Accuracy 86.684%\n",
      "Epoch 18, Batch 475, LR 2.186371 Loss 5.725570, Accuracy 86.679%\n",
      "Epoch 18, Batch 476, LR 2.186282 Loss 5.725880, Accuracy 86.686%\n",
      "Epoch 18, Batch 477, LR 2.186193 Loss 5.724413, Accuracy 86.689%\n",
      "Epoch 18, Batch 478, LR 2.186104 Loss 5.724264, Accuracy 86.688%\n",
      "Epoch 18, Batch 479, LR 2.186016 Loss 5.723734, Accuracy 86.691%\n",
      "Epoch 18, Batch 480, LR 2.185927 Loss 5.725140, Accuracy 86.686%\n",
      "Epoch 18, Batch 481, LR 2.185838 Loss 5.725520, Accuracy 86.688%\n",
      "Epoch 18, Batch 482, LR 2.185749 Loss 5.725517, Accuracy 86.691%\n",
      "Epoch 18, Batch 483, LR 2.185661 Loss 5.725461, Accuracy 86.696%\n",
      "Epoch 18, Batch 484, LR 2.185572 Loss 5.725123, Accuracy 86.695%\n",
      "Epoch 18, Batch 485, LR 2.185483 Loss 5.725605, Accuracy 86.688%\n",
      "Epoch 18, Batch 486, LR 2.185394 Loss 5.726037, Accuracy 86.688%\n",
      "Epoch 18, Batch 487, LR 2.185305 Loss 5.726151, Accuracy 86.688%\n",
      "Epoch 18, Batch 488, LR 2.185216 Loss 5.726654, Accuracy 86.684%\n",
      "Epoch 18, Batch 489, LR 2.185127 Loss 5.726726, Accuracy 86.690%\n",
      "Epoch 18, Batch 490, LR 2.185038 Loss 5.726917, Accuracy 86.687%\n",
      "Epoch 18, Batch 491, LR 2.184950 Loss 5.727260, Accuracy 86.692%\n",
      "Epoch 18, Batch 492, LR 2.184861 Loss 5.726590, Accuracy 86.693%\n",
      "Epoch 18, Batch 493, LR 2.184772 Loss 5.725962, Accuracy 86.693%\n",
      "Epoch 18, Batch 494, LR 2.184683 Loss 5.726248, Accuracy 86.690%\n",
      "Epoch 18, Batch 495, LR 2.184594 Loss 5.726527, Accuracy 86.697%\n",
      "Epoch 18, Batch 496, LR 2.184505 Loss 5.726827, Accuracy 86.700%\n",
      "Epoch 18, Batch 497, LR 2.184416 Loss 5.727282, Accuracy 86.694%\n",
      "Epoch 18, Batch 498, LR 2.184327 Loss 5.727522, Accuracy 86.695%\n",
      "Epoch 18, Batch 499, LR 2.184238 Loss 5.726090, Accuracy 86.700%\n",
      "Epoch 18, Batch 500, LR 2.184149 Loss 5.725776, Accuracy 86.708%\n",
      "Epoch 18, Batch 501, LR 2.184060 Loss 5.727426, Accuracy 86.698%\n",
      "Epoch 18, Batch 502, LR 2.183971 Loss 5.727862, Accuracy 86.692%\n",
      "Epoch 18, Batch 503, LR 2.183882 Loss 5.728103, Accuracy 86.691%\n",
      "Epoch 18, Batch 504, LR 2.183793 Loss 5.727184, Accuracy 86.697%\n",
      "Epoch 18, Batch 505, LR 2.183704 Loss 5.728120, Accuracy 86.699%\n",
      "Epoch 18, Batch 506, LR 2.183615 Loss 5.727264, Accuracy 86.708%\n",
      "Epoch 18, Batch 507, LR 2.183526 Loss 5.726775, Accuracy 86.711%\n",
      "Epoch 18, Batch 508, LR 2.183437 Loss 5.726932, Accuracy 86.717%\n",
      "Epoch 18, Batch 509, LR 2.183347 Loss 5.724950, Accuracy 86.726%\n",
      "Epoch 18, Batch 510, LR 2.183258 Loss 5.723658, Accuracy 86.725%\n",
      "Epoch 18, Batch 511, LR 2.183169 Loss 5.724512, Accuracy 86.720%\n",
      "Epoch 18, Batch 512, LR 2.183080 Loss 5.724707, Accuracy 86.710%\n",
      "Epoch 18, Batch 513, LR 2.182991 Loss 5.724123, Accuracy 86.717%\n",
      "Epoch 18, Batch 514, LR 2.182902 Loss 5.724790, Accuracy 86.713%\n",
      "Epoch 18, Batch 515, LR 2.182813 Loss 5.723393, Accuracy 86.725%\n",
      "Epoch 18, Batch 516, LR 2.182723 Loss 5.723802, Accuracy 86.717%\n",
      "Epoch 18, Batch 517, LR 2.182634 Loss 5.724429, Accuracy 86.716%\n",
      "Epoch 18, Batch 518, LR 2.182545 Loss 5.724700, Accuracy 86.719%\n",
      "Epoch 18, Batch 519, LR 2.182456 Loss 5.724932, Accuracy 86.719%\n",
      "Epoch 18, Batch 520, LR 2.182367 Loss 5.725094, Accuracy 86.713%\n",
      "Epoch 18, Batch 521, LR 2.182277 Loss 5.725589, Accuracy 86.705%\n",
      "Epoch 18, Batch 522, LR 2.182188 Loss 5.726022, Accuracy 86.701%\n",
      "Epoch 18, Batch 523, LR 2.182099 Loss 5.725388, Accuracy 86.702%\n",
      "Epoch 18, Batch 524, LR 2.182010 Loss 5.726006, Accuracy 86.699%\n",
      "Epoch 18, Batch 525, LR 2.181920 Loss 5.726136, Accuracy 86.701%\n",
      "Epoch 18, Batch 526, LR 2.181831 Loss 5.726359, Accuracy 86.696%\n",
      "Epoch 18, Batch 527, LR 2.181742 Loss 5.725410, Accuracy 86.701%\n",
      "Epoch 18, Batch 528, LR 2.181653 Loss 5.725688, Accuracy 86.697%\n",
      "Epoch 18, Batch 529, LR 2.181563 Loss 5.727199, Accuracy 86.692%\n",
      "Epoch 18, Batch 530, LR 2.181474 Loss 5.727549, Accuracy 86.685%\n",
      "Epoch 18, Batch 531, LR 2.181385 Loss 5.727395, Accuracy 86.692%\n",
      "Epoch 18, Batch 532, LR 2.181295 Loss 5.727347, Accuracy 86.700%\n",
      "Epoch 18, Batch 533, LR 2.181206 Loss 5.725604, Accuracy 86.710%\n",
      "Epoch 18, Batch 534, LR 2.181116 Loss 5.725989, Accuracy 86.710%\n",
      "Epoch 18, Batch 535, LR 2.181027 Loss 5.725707, Accuracy 86.709%\n",
      "Epoch 18, Batch 536, LR 2.180938 Loss 5.726172, Accuracy 86.706%\n",
      "Epoch 18, Batch 537, LR 2.180848 Loss 5.726963, Accuracy 86.703%\n",
      "Epoch 18, Batch 538, LR 2.180759 Loss 5.727072, Accuracy 86.704%\n",
      "Epoch 18, Batch 539, LR 2.180669 Loss 5.727420, Accuracy 86.701%\n",
      "Epoch 18, Batch 540, LR 2.180580 Loss 5.726984, Accuracy 86.701%\n",
      "Epoch 18, Batch 541, LR 2.180491 Loss 5.726999, Accuracy 86.699%\n",
      "Epoch 18, Batch 542, LR 2.180401 Loss 5.726830, Accuracy 86.710%\n",
      "Epoch 18, Batch 543, LR 2.180312 Loss 5.726461, Accuracy 86.713%\n",
      "Epoch 18, Batch 544, LR 2.180222 Loss 5.726575, Accuracy 86.713%\n",
      "Epoch 18, Batch 545, LR 2.180133 Loss 5.727105, Accuracy 86.709%\n",
      "Epoch 18, Batch 546, LR 2.180043 Loss 5.726333, Accuracy 86.712%\n",
      "Epoch 18, Batch 547, LR 2.179954 Loss 5.725179, Accuracy 86.719%\n",
      "Epoch 18, Batch 548, LR 2.179864 Loss 5.725042, Accuracy 86.717%\n",
      "Epoch 18, Batch 549, LR 2.179775 Loss 5.724437, Accuracy 86.726%\n",
      "Epoch 18, Batch 550, LR 2.179685 Loss 5.723679, Accuracy 86.734%\n",
      "Epoch 18, Batch 551, LR 2.179596 Loss 5.723495, Accuracy 86.743%\n",
      "Epoch 18, Batch 552, LR 2.179506 Loss 5.723553, Accuracy 86.741%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 553, LR 2.179416 Loss 5.724309, Accuracy 86.733%\n",
      "Epoch 18, Batch 554, LR 2.179327 Loss 5.724815, Accuracy 86.734%\n",
      "Epoch 18, Batch 555, LR 2.179237 Loss 5.725299, Accuracy 86.736%\n",
      "Epoch 18, Batch 556, LR 2.179148 Loss 5.725369, Accuracy 86.731%\n",
      "Epoch 18, Batch 557, LR 2.179058 Loss 5.725755, Accuracy 86.729%\n",
      "Epoch 18, Batch 558, LR 2.178968 Loss 5.724724, Accuracy 86.737%\n",
      "Epoch 18, Batch 559, LR 2.178879 Loss 5.725208, Accuracy 86.734%\n",
      "Epoch 18, Batch 560, LR 2.178789 Loss 5.726140, Accuracy 86.726%\n",
      "Epoch 18, Batch 561, LR 2.178700 Loss 5.725359, Accuracy 86.728%\n",
      "Epoch 18, Batch 562, LR 2.178610 Loss 5.725484, Accuracy 86.730%\n",
      "Epoch 18, Batch 563, LR 2.178520 Loss 5.725255, Accuracy 86.733%\n",
      "Epoch 18, Batch 564, LR 2.178431 Loss 5.725191, Accuracy 86.731%\n",
      "Epoch 18, Batch 565, LR 2.178341 Loss 5.725145, Accuracy 86.734%\n",
      "Epoch 18, Batch 566, LR 2.178251 Loss 5.726159, Accuracy 86.727%\n",
      "Epoch 18, Batch 567, LR 2.178161 Loss 5.725972, Accuracy 86.733%\n",
      "Epoch 18, Batch 568, LR 2.178072 Loss 5.726049, Accuracy 86.735%\n",
      "Epoch 18, Batch 569, LR 2.177982 Loss 5.726691, Accuracy 86.728%\n",
      "Epoch 18, Batch 570, LR 2.177892 Loss 5.726502, Accuracy 86.720%\n",
      "Epoch 18, Batch 571, LR 2.177802 Loss 5.725256, Accuracy 86.727%\n",
      "Epoch 18, Batch 572, LR 2.177713 Loss 5.726474, Accuracy 86.716%\n",
      "Epoch 18, Batch 573, LR 2.177623 Loss 5.726264, Accuracy 86.724%\n",
      "Epoch 18, Batch 574, LR 2.177533 Loss 5.726985, Accuracy 86.712%\n",
      "Epoch 18, Batch 575, LR 2.177443 Loss 5.725680, Accuracy 86.719%\n",
      "Epoch 18, Batch 576, LR 2.177353 Loss 5.726089, Accuracy 86.719%\n",
      "Epoch 18, Batch 577, LR 2.177264 Loss 5.725759, Accuracy 86.717%\n",
      "Epoch 18, Batch 578, LR 2.177174 Loss 5.725885, Accuracy 86.720%\n",
      "Epoch 18, Batch 579, LR 2.177084 Loss 5.726689, Accuracy 86.716%\n",
      "Epoch 18, Batch 580, LR 2.176994 Loss 5.726433, Accuracy 86.713%\n",
      "Epoch 18, Batch 581, LR 2.176904 Loss 5.725011, Accuracy 86.720%\n",
      "Epoch 18, Batch 582, LR 2.176814 Loss 5.725944, Accuracy 86.723%\n",
      "Epoch 18, Batch 583, LR 2.176724 Loss 5.726217, Accuracy 86.720%\n",
      "Epoch 18, Batch 584, LR 2.176635 Loss 5.726979, Accuracy 86.711%\n",
      "Epoch 18, Batch 585, LR 2.176545 Loss 5.727875, Accuracy 86.704%\n",
      "Epoch 18, Batch 586, LR 2.176455 Loss 5.727031, Accuracy 86.711%\n",
      "Epoch 18, Batch 587, LR 2.176365 Loss 5.727564, Accuracy 86.708%\n",
      "Epoch 18, Batch 588, LR 2.176275 Loss 5.728124, Accuracy 86.708%\n",
      "Epoch 18, Batch 589, LR 2.176185 Loss 5.727466, Accuracy 86.717%\n",
      "Epoch 18, Batch 590, LR 2.176095 Loss 5.726758, Accuracy 86.721%\n",
      "Epoch 18, Batch 591, LR 2.176005 Loss 5.726729, Accuracy 86.723%\n",
      "Epoch 18, Batch 592, LR 2.175915 Loss 5.727926, Accuracy 86.723%\n",
      "Epoch 18, Batch 593, LR 2.175825 Loss 5.727546, Accuracy 86.727%\n",
      "Epoch 18, Batch 594, LR 2.175735 Loss 5.728075, Accuracy 86.724%\n",
      "Epoch 18, Batch 595, LR 2.175645 Loss 5.728529, Accuracy 86.723%\n",
      "Epoch 18, Batch 596, LR 2.175555 Loss 5.727601, Accuracy 86.728%\n",
      "Epoch 18, Batch 597, LR 2.175465 Loss 5.728353, Accuracy 86.724%\n",
      "Epoch 18, Batch 598, LR 2.175375 Loss 5.726933, Accuracy 86.728%\n",
      "Epoch 18, Batch 599, LR 2.175285 Loss 5.727232, Accuracy 86.725%\n",
      "Epoch 18, Batch 600, LR 2.175195 Loss 5.727117, Accuracy 86.728%\n",
      "Epoch 18, Batch 601, LR 2.175105 Loss 5.727847, Accuracy 86.719%\n",
      "Epoch 18, Batch 602, LR 2.175015 Loss 5.727022, Accuracy 86.727%\n",
      "Epoch 18, Batch 603, LR 2.174924 Loss 5.726171, Accuracy 86.730%\n",
      "Epoch 18, Batch 604, LR 2.174834 Loss 5.724967, Accuracy 86.734%\n",
      "Epoch 18, Batch 605, LR 2.174744 Loss 5.724718, Accuracy 86.733%\n",
      "Epoch 18, Batch 606, LR 2.174654 Loss 5.724776, Accuracy 86.729%\n",
      "Epoch 18, Batch 607, LR 2.174564 Loss 5.723426, Accuracy 86.735%\n",
      "Epoch 18, Batch 608, LR 2.174474 Loss 5.723302, Accuracy 86.737%\n",
      "Epoch 18, Batch 609, LR 2.174384 Loss 5.722771, Accuracy 86.739%\n",
      "Epoch 18, Batch 610, LR 2.174293 Loss 5.723535, Accuracy 86.742%\n",
      "Epoch 18, Batch 611, LR 2.174203 Loss 5.722640, Accuracy 86.740%\n",
      "Epoch 18, Batch 612, LR 2.174113 Loss 5.722577, Accuracy 86.742%\n",
      "Epoch 18, Batch 613, LR 2.174023 Loss 5.724419, Accuracy 86.726%\n",
      "Epoch 18, Batch 614, LR 2.173933 Loss 5.724654, Accuracy 86.721%\n",
      "Epoch 18, Batch 615, LR 2.173842 Loss 5.724411, Accuracy 86.726%\n",
      "Epoch 18, Batch 616, LR 2.173752 Loss 5.724892, Accuracy 86.723%\n",
      "Epoch 18, Batch 617, LR 2.173662 Loss 5.725018, Accuracy 86.719%\n",
      "Epoch 18, Batch 618, LR 2.173572 Loss 5.724871, Accuracy 86.719%\n",
      "Epoch 18, Batch 619, LR 2.173481 Loss 5.724653, Accuracy 86.719%\n",
      "Epoch 18, Batch 620, LR 2.173391 Loss 5.725087, Accuracy 86.716%\n",
      "Epoch 18, Batch 621, LR 2.173301 Loss 5.725420, Accuracy 86.719%\n",
      "Epoch 18, Batch 622, LR 2.173211 Loss 5.726457, Accuracy 86.716%\n",
      "Epoch 18, Batch 623, LR 2.173120 Loss 5.726092, Accuracy 86.724%\n",
      "Epoch 18, Batch 624, LR 2.173030 Loss 5.726323, Accuracy 86.720%\n",
      "Epoch 18, Batch 625, LR 2.172940 Loss 5.726220, Accuracy 86.720%\n",
      "Epoch 18, Batch 626, LR 2.172849 Loss 5.726141, Accuracy 86.716%\n",
      "Epoch 18, Batch 627, LR 2.172759 Loss 5.726741, Accuracy 86.715%\n",
      "Epoch 18, Batch 628, LR 2.172668 Loss 5.725782, Accuracy 86.721%\n",
      "Epoch 18, Batch 629, LR 2.172578 Loss 5.725575, Accuracy 86.722%\n",
      "Epoch 18, Batch 630, LR 2.172488 Loss 5.725681, Accuracy 86.724%\n",
      "Epoch 18, Batch 631, LR 2.172397 Loss 5.726467, Accuracy 86.718%\n",
      "Epoch 18, Batch 632, LR 2.172307 Loss 5.727252, Accuracy 86.711%\n",
      "Epoch 18, Batch 633, LR 2.172216 Loss 5.727290, Accuracy 86.713%\n",
      "Epoch 18, Batch 634, LR 2.172126 Loss 5.727915, Accuracy 86.711%\n",
      "Epoch 18, Batch 635, LR 2.172036 Loss 5.728957, Accuracy 86.709%\n",
      "Epoch 18, Batch 636, LR 2.171945 Loss 5.729528, Accuracy 86.709%\n",
      "Epoch 18, Batch 637, LR 2.171855 Loss 5.729947, Accuracy 86.706%\n",
      "Epoch 18, Batch 638, LR 2.171764 Loss 5.729020, Accuracy 86.710%\n",
      "Epoch 18, Batch 639, LR 2.171674 Loss 5.728834, Accuracy 86.711%\n",
      "Epoch 18, Batch 640, LR 2.171583 Loss 5.728070, Accuracy 86.714%\n",
      "Epoch 18, Batch 641, LR 2.171493 Loss 5.728156, Accuracy 86.710%\n",
      "Epoch 18, Batch 642, LR 2.171402 Loss 5.728149, Accuracy 86.700%\n",
      "Epoch 18, Batch 643, LR 2.171312 Loss 5.728188, Accuracy 86.699%\n",
      "Epoch 18, Batch 644, LR 2.171221 Loss 5.727531, Accuracy 86.703%\n",
      "Epoch 18, Batch 645, LR 2.171131 Loss 5.726978, Accuracy 86.707%\n",
      "Epoch 18, Batch 646, LR 2.171040 Loss 5.727266, Accuracy 86.707%\n",
      "Epoch 18, Batch 647, LR 2.170949 Loss 5.726895, Accuracy 86.718%\n",
      "Epoch 18, Batch 648, LR 2.170859 Loss 5.727699, Accuracy 86.715%\n",
      "Epoch 18, Batch 649, LR 2.170768 Loss 5.727350, Accuracy 86.719%\n",
      "Epoch 18, Batch 650, LR 2.170678 Loss 5.726605, Accuracy 86.720%\n",
      "Epoch 18, Batch 651, LR 2.170587 Loss 5.727610, Accuracy 86.721%\n",
      "Epoch 18, Batch 652, LR 2.170496 Loss 5.728086, Accuracy 86.718%\n",
      "Epoch 18, Batch 653, LR 2.170406 Loss 5.728410, Accuracy 86.714%\n",
      "Epoch 18, Batch 654, LR 2.170315 Loss 5.728046, Accuracy 86.715%\n",
      "Epoch 18, Batch 655, LR 2.170225 Loss 5.727578, Accuracy 86.714%\n",
      "Epoch 18, Batch 656, LR 2.170134 Loss 5.727590, Accuracy 86.720%\n",
      "Epoch 18, Batch 657, LR 2.170043 Loss 5.726815, Accuracy 86.725%\n",
      "Epoch 18, Batch 658, LR 2.169953 Loss 5.726207, Accuracy 86.727%\n",
      "Epoch 18, Batch 659, LR 2.169862 Loss 5.726617, Accuracy 86.726%\n",
      "Epoch 18, Batch 660, LR 2.169771 Loss 5.727296, Accuracy 86.715%\n",
      "Epoch 18, Batch 661, LR 2.169680 Loss 5.726771, Accuracy 86.718%\n",
      "Epoch 18, Batch 662, LR 2.169590 Loss 5.727870, Accuracy 86.709%\n",
      "Epoch 18, Batch 663, LR 2.169499 Loss 5.727445, Accuracy 86.711%\n",
      "Epoch 18, Batch 664, LR 2.169408 Loss 5.726763, Accuracy 86.713%\n",
      "Epoch 18, Batch 665, LR 2.169317 Loss 5.728046, Accuracy 86.699%\n",
      "Epoch 18, Batch 666, LR 2.169227 Loss 5.729009, Accuracy 86.696%\n",
      "Epoch 18, Batch 667, LR 2.169136 Loss 5.729017, Accuracy 86.700%\n",
      "Epoch 18, Batch 668, LR 2.169045 Loss 5.728822, Accuracy 86.698%\n",
      "Epoch 18, Batch 669, LR 2.168954 Loss 5.729325, Accuracy 86.694%\n",
      "Epoch 18, Batch 670, LR 2.168863 Loss 5.729056, Accuracy 86.699%\n",
      "Epoch 18, Batch 671, LR 2.168773 Loss 5.729302, Accuracy 86.695%\n",
      "Epoch 18, Batch 672, LR 2.168682 Loss 5.730062, Accuracy 86.692%\n",
      "Epoch 18, Batch 673, LR 2.168591 Loss 5.729388, Accuracy 86.699%\n",
      "Epoch 18, Batch 674, LR 2.168500 Loss 5.729555, Accuracy 86.699%\n",
      "Epoch 18, Batch 675, LR 2.168409 Loss 5.728209, Accuracy 86.708%\n",
      "Epoch 18, Batch 676, LR 2.168318 Loss 5.728029, Accuracy 86.711%\n",
      "Epoch 18, Batch 677, LR 2.168228 Loss 5.727909, Accuracy 86.707%\n",
      "Epoch 18, Batch 678, LR 2.168137 Loss 5.728312, Accuracy 86.705%\n",
      "Epoch 18, Batch 679, LR 2.168046 Loss 5.727350, Accuracy 86.710%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 680, LR 2.167955 Loss 5.728432, Accuracy 86.707%\n",
      "Epoch 18, Batch 681, LR 2.167864 Loss 5.727638, Accuracy 86.712%\n",
      "Epoch 18, Batch 682, LR 2.167773 Loss 5.726452, Accuracy 86.720%\n",
      "Epoch 18, Batch 683, LR 2.167682 Loss 5.725912, Accuracy 86.723%\n",
      "Epoch 18, Batch 684, LR 2.167591 Loss 5.726687, Accuracy 86.712%\n",
      "Epoch 18, Batch 685, LR 2.167500 Loss 5.726874, Accuracy 86.713%\n",
      "Epoch 18, Batch 686, LR 2.167409 Loss 5.726590, Accuracy 86.715%\n",
      "Epoch 18, Batch 687, LR 2.167318 Loss 5.727604, Accuracy 86.707%\n",
      "Epoch 18, Batch 688, LR 2.167227 Loss 5.728411, Accuracy 86.702%\n",
      "Epoch 18, Batch 689, LR 2.167136 Loss 5.727977, Accuracy 86.699%\n",
      "Epoch 18, Batch 690, LR 2.167045 Loss 5.727931, Accuracy 86.702%\n",
      "Epoch 18, Batch 691, LR 2.166954 Loss 5.727711, Accuracy 86.700%\n",
      "Epoch 18, Batch 692, LR 2.166863 Loss 5.727196, Accuracy 86.704%\n",
      "Epoch 18, Batch 693, LR 2.166772 Loss 5.726918, Accuracy 86.705%\n",
      "Epoch 18, Batch 694, LR 2.166681 Loss 5.725914, Accuracy 86.711%\n",
      "Epoch 18, Batch 695, LR 2.166590 Loss 5.726158, Accuracy 86.711%\n",
      "Epoch 18, Batch 696, LR 2.166499 Loss 5.726079, Accuracy 86.713%\n",
      "Epoch 18, Batch 697, LR 2.166408 Loss 5.724493, Accuracy 86.721%\n",
      "Epoch 18, Batch 698, LR 2.166317 Loss 5.723885, Accuracy 86.725%\n",
      "Epoch 18, Batch 699, LR 2.166225 Loss 5.722845, Accuracy 86.736%\n",
      "Epoch 18, Batch 700, LR 2.166134 Loss 5.723664, Accuracy 86.731%\n",
      "Epoch 18, Batch 701, LR 2.166043 Loss 5.723566, Accuracy 86.733%\n",
      "Epoch 18, Batch 702, LR 2.165952 Loss 5.723869, Accuracy 86.731%\n",
      "Epoch 18, Batch 703, LR 2.165861 Loss 5.724248, Accuracy 86.731%\n",
      "Epoch 18, Batch 704, LR 2.165770 Loss 5.724033, Accuracy 86.735%\n",
      "Epoch 18, Batch 705, LR 2.165678 Loss 5.724940, Accuracy 86.728%\n",
      "Epoch 18, Batch 706, LR 2.165587 Loss 5.724438, Accuracy 86.725%\n",
      "Epoch 18, Batch 707, LR 2.165496 Loss 5.724998, Accuracy 86.721%\n",
      "Epoch 18, Batch 708, LR 2.165405 Loss 5.725202, Accuracy 86.722%\n",
      "Epoch 18, Batch 709, LR 2.165314 Loss 5.725614, Accuracy 86.721%\n",
      "Epoch 18, Batch 710, LR 2.165222 Loss 5.725613, Accuracy 86.717%\n",
      "Epoch 18, Batch 711, LR 2.165131 Loss 5.724803, Accuracy 86.721%\n",
      "Epoch 18, Batch 712, LR 2.165040 Loss 5.725684, Accuracy 86.709%\n",
      "Epoch 18, Batch 713, LR 2.164949 Loss 5.725123, Accuracy 86.710%\n",
      "Epoch 18, Batch 714, LR 2.164857 Loss 5.725786, Accuracy 86.710%\n",
      "Epoch 18, Batch 715, LR 2.164766 Loss 5.726124, Accuracy 86.706%\n",
      "Epoch 18, Batch 716, LR 2.164675 Loss 5.726541, Accuracy 86.705%\n",
      "Epoch 18, Batch 717, LR 2.164583 Loss 5.726077, Accuracy 86.707%\n",
      "Epoch 18, Batch 718, LR 2.164492 Loss 5.725482, Accuracy 86.713%\n",
      "Epoch 18, Batch 719, LR 2.164401 Loss 5.725986, Accuracy 86.713%\n",
      "Epoch 18, Batch 720, LR 2.164310 Loss 5.725487, Accuracy 86.720%\n",
      "Epoch 18, Batch 721, LR 2.164218 Loss 5.726415, Accuracy 86.719%\n",
      "Epoch 18, Batch 722, LR 2.164127 Loss 5.726694, Accuracy 86.722%\n",
      "Epoch 18, Batch 723, LR 2.164035 Loss 5.727177, Accuracy 86.726%\n",
      "Epoch 18, Batch 724, LR 2.163944 Loss 5.726917, Accuracy 86.731%\n",
      "Epoch 18, Batch 725, LR 2.163853 Loss 5.727561, Accuracy 86.724%\n",
      "Epoch 18, Batch 726, LR 2.163761 Loss 5.728034, Accuracy 86.722%\n",
      "Epoch 18, Batch 727, LR 2.163670 Loss 5.727157, Accuracy 86.727%\n",
      "Epoch 18, Batch 728, LR 2.163578 Loss 5.727990, Accuracy 86.722%\n",
      "Epoch 18, Batch 729, LR 2.163487 Loss 5.727025, Accuracy 86.728%\n",
      "Epoch 18, Batch 730, LR 2.163396 Loss 5.728424, Accuracy 86.719%\n",
      "Epoch 18, Batch 731, LR 2.163304 Loss 5.728108, Accuracy 86.719%\n",
      "Epoch 18, Batch 732, LR 2.163213 Loss 5.728741, Accuracy 86.711%\n",
      "Epoch 18, Batch 733, LR 2.163121 Loss 5.729437, Accuracy 86.706%\n",
      "Epoch 18, Batch 734, LR 2.163030 Loss 5.729313, Accuracy 86.704%\n",
      "Epoch 18, Batch 735, LR 2.162938 Loss 5.728843, Accuracy 86.704%\n",
      "Epoch 18, Batch 736, LR 2.162847 Loss 5.728167, Accuracy 86.708%\n",
      "Epoch 18, Batch 737, LR 2.162755 Loss 5.728718, Accuracy 86.710%\n",
      "Epoch 18, Batch 738, LR 2.162664 Loss 5.728702, Accuracy 86.715%\n",
      "Epoch 18, Batch 739, LR 2.162572 Loss 5.729614, Accuracy 86.713%\n",
      "Epoch 18, Batch 740, LR 2.162481 Loss 5.729966, Accuracy 86.715%\n",
      "Epoch 18, Batch 741, LR 2.162389 Loss 5.729760, Accuracy 86.719%\n",
      "Epoch 18, Batch 742, LR 2.162297 Loss 5.729485, Accuracy 86.721%\n",
      "Epoch 18, Batch 743, LR 2.162206 Loss 5.728957, Accuracy 86.723%\n",
      "Epoch 18, Batch 744, LR 2.162114 Loss 5.728531, Accuracy 86.727%\n",
      "Epoch 18, Batch 745, LR 2.162023 Loss 5.728828, Accuracy 86.729%\n",
      "Epoch 18, Batch 746, LR 2.161931 Loss 5.729290, Accuracy 86.725%\n",
      "Epoch 18, Batch 747, LR 2.161839 Loss 5.729903, Accuracy 86.719%\n",
      "Epoch 18, Batch 748, LR 2.161748 Loss 5.730297, Accuracy 86.719%\n",
      "Epoch 18, Batch 749, LR 2.161656 Loss 5.730999, Accuracy 86.712%\n",
      "Epoch 18, Batch 750, LR 2.161565 Loss 5.732139, Accuracy 86.714%\n",
      "Epoch 18, Batch 751, LR 2.161473 Loss 5.732160, Accuracy 86.711%\n",
      "Epoch 18, Batch 752, LR 2.161381 Loss 5.731299, Accuracy 86.714%\n",
      "Epoch 18, Batch 753, LR 2.161290 Loss 5.731315, Accuracy 86.718%\n",
      "Epoch 18, Batch 754, LR 2.161198 Loss 5.731990, Accuracy 86.714%\n",
      "Epoch 18, Batch 755, LR 2.161106 Loss 5.732549, Accuracy 86.707%\n",
      "Epoch 18, Batch 756, LR 2.161014 Loss 5.733416, Accuracy 86.705%\n",
      "Epoch 18, Batch 757, LR 2.160923 Loss 5.733154, Accuracy 86.706%\n",
      "Epoch 18, Batch 758, LR 2.160831 Loss 5.733463, Accuracy 86.703%\n",
      "Epoch 18, Batch 759, LR 2.160739 Loss 5.733418, Accuracy 86.709%\n",
      "Epoch 18, Batch 760, LR 2.160647 Loss 5.733768, Accuracy 86.706%\n",
      "Epoch 18, Batch 761, LR 2.160556 Loss 5.734836, Accuracy 86.705%\n",
      "Epoch 18, Batch 762, LR 2.160464 Loss 5.735646, Accuracy 86.699%\n",
      "Epoch 18, Batch 763, LR 2.160372 Loss 5.735224, Accuracy 86.706%\n",
      "Epoch 18, Batch 764, LR 2.160280 Loss 5.735317, Accuracy 86.704%\n",
      "Epoch 18, Batch 765, LR 2.160188 Loss 5.736236, Accuracy 86.705%\n",
      "Epoch 18, Batch 766, LR 2.160097 Loss 5.736499, Accuracy 86.702%\n",
      "Epoch 18, Batch 767, LR 2.160005 Loss 5.735713, Accuracy 86.708%\n",
      "Epoch 18, Batch 768, LR 2.159913 Loss 5.735865, Accuracy 86.710%\n",
      "Epoch 18, Batch 769, LR 2.159821 Loss 5.735834, Accuracy 86.704%\n",
      "Epoch 18, Batch 770, LR 2.159729 Loss 5.735786, Accuracy 86.709%\n",
      "Epoch 18, Batch 771, LR 2.159637 Loss 5.734980, Accuracy 86.712%\n",
      "Epoch 18, Batch 772, LR 2.159546 Loss 5.735723, Accuracy 86.707%\n",
      "Epoch 18, Batch 773, LR 2.159454 Loss 5.736207, Accuracy 86.707%\n",
      "Epoch 18, Batch 774, LR 2.159362 Loss 5.735820, Accuracy 86.707%\n",
      "Epoch 18, Batch 775, LR 2.159270 Loss 5.735746, Accuracy 86.710%\n",
      "Epoch 18, Batch 776, LR 2.159178 Loss 5.736127, Accuracy 86.704%\n",
      "Epoch 18, Batch 777, LR 2.159086 Loss 5.735356, Accuracy 86.708%\n",
      "Epoch 18, Batch 778, LR 2.158994 Loss 5.734667, Accuracy 86.711%\n",
      "Epoch 18, Batch 779, LR 2.158902 Loss 5.734179, Accuracy 86.715%\n",
      "Epoch 18, Batch 780, LR 2.158810 Loss 5.734417, Accuracy 86.712%\n",
      "Epoch 18, Batch 781, LR 2.158718 Loss 5.734991, Accuracy 86.710%\n",
      "Epoch 18, Batch 782, LR 2.158626 Loss 5.734291, Accuracy 86.711%\n",
      "Epoch 18, Batch 783, LR 2.158534 Loss 5.734449, Accuracy 86.710%\n",
      "Epoch 18, Batch 784, LR 2.158442 Loss 5.734515, Accuracy 86.710%\n",
      "Epoch 18, Batch 785, LR 2.158350 Loss 5.734081, Accuracy 86.715%\n",
      "Epoch 18, Batch 786, LR 2.158258 Loss 5.734542, Accuracy 86.713%\n",
      "Epoch 18, Batch 787, LR 2.158166 Loss 5.735065, Accuracy 86.705%\n",
      "Epoch 18, Batch 788, LR 2.158074 Loss 5.735666, Accuracy 86.704%\n",
      "Epoch 18, Batch 789, LR 2.157982 Loss 5.735669, Accuracy 86.703%\n",
      "Epoch 18, Batch 790, LR 2.157890 Loss 5.735504, Accuracy 86.705%\n",
      "Epoch 18, Batch 791, LR 2.157798 Loss 5.735740, Accuracy 86.708%\n",
      "Epoch 18, Batch 792, LR 2.157706 Loss 5.736112, Accuracy 86.698%\n",
      "Epoch 18, Batch 793, LR 2.157614 Loss 5.735304, Accuracy 86.702%\n",
      "Epoch 18, Batch 794, LR 2.157521 Loss 5.735074, Accuracy 86.697%\n",
      "Epoch 18, Batch 795, LR 2.157429 Loss 5.734612, Accuracy 86.697%\n",
      "Epoch 18, Batch 796, LR 2.157337 Loss 5.735495, Accuracy 86.695%\n",
      "Epoch 18, Batch 797, LR 2.157245 Loss 5.735505, Accuracy 86.696%\n",
      "Epoch 18, Batch 798, LR 2.157153 Loss 5.735252, Accuracy 86.693%\n",
      "Epoch 18, Batch 799, LR 2.157061 Loss 5.735319, Accuracy 86.689%\n",
      "Epoch 18, Batch 800, LR 2.156969 Loss 5.735243, Accuracy 86.694%\n",
      "Epoch 18, Batch 801, LR 2.156876 Loss 5.735424, Accuracy 86.693%\n",
      "Epoch 18, Batch 802, LR 2.156784 Loss 5.735702, Accuracy 86.692%\n",
      "Epoch 18, Batch 803, LR 2.156692 Loss 5.735805, Accuracy 86.689%\n",
      "Epoch 18, Batch 804, LR 2.156600 Loss 5.736555, Accuracy 86.681%\n",
      "Epoch 18, Batch 805, LR 2.156508 Loss 5.736911, Accuracy 86.678%\n",
      "Epoch 18, Batch 806, LR 2.156415 Loss 5.736934, Accuracy 86.682%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 807, LR 2.156323 Loss 5.736839, Accuracy 86.685%\n",
      "Epoch 18, Batch 808, LR 2.156231 Loss 5.737323, Accuracy 86.679%\n",
      "Epoch 18, Batch 809, LR 2.156139 Loss 5.737716, Accuracy 86.674%\n",
      "Epoch 18, Batch 810, LR 2.156046 Loss 5.737934, Accuracy 86.672%\n",
      "Epoch 18, Batch 811, LR 2.155954 Loss 5.738800, Accuracy 86.668%\n",
      "Epoch 18, Batch 812, LR 2.155862 Loss 5.737677, Accuracy 86.673%\n",
      "Epoch 18, Batch 813, LR 2.155769 Loss 5.738890, Accuracy 86.667%\n",
      "Epoch 18, Batch 814, LR 2.155677 Loss 5.738831, Accuracy 86.669%\n",
      "Epoch 18, Batch 815, LR 2.155585 Loss 5.738093, Accuracy 86.670%\n",
      "Epoch 18, Batch 816, LR 2.155492 Loss 5.737630, Accuracy 86.678%\n",
      "Epoch 18, Batch 817, LR 2.155400 Loss 5.738184, Accuracy 86.679%\n",
      "Epoch 18, Batch 818, LR 2.155308 Loss 5.737592, Accuracy 86.685%\n",
      "Epoch 18, Batch 819, LR 2.155215 Loss 5.737494, Accuracy 86.683%\n",
      "Epoch 18, Batch 820, LR 2.155123 Loss 5.737333, Accuracy 86.685%\n",
      "Epoch 18, Batch 821, LR 2.155031 Loss 5.737458, Accuracy 86.685%\n",
      "Epoch 18, Batch 822, LR 2.154938 Loss 5.737691, Accuracy 86.682%\n",
      "Epoch 18, Batch 823, LR 2.154846 Loss 5.737420, Accuracy 86.684%\n",
      "Epoch 18, Batch 824, LR 2.154753 Loss 5.737732, Accuracy 86.684%\n",
      "Epoch 18, Batch 825, LR 2.154661 Loss 5.738506, Accuracy 86.684%\n",
      "Epoch 18, Batch 826, LR 2.154568 Loss 5.738916, Accuracy 86.683%\n",
      "Epoch 18, Batch 827, LR 2.154476 Loss 5.738718, Accuracy 86.685%\n",
      "Epoch 18, Batch 828, LR 2.154384 Loss 5.738238, Accuracy 86.685%\n",
      "Epoch 18, Batch 829, LR 2.154291 Loss 5.737361, Accuracy 86.690%\n",
      "Epoch 18, Batch 830, LR 2.154199 Loss 5.737731, Accuracy 86.686%\n",
      "Epoch 18, Batch 831, LR 2.154106 Loss 5.738596, Accuracy 86.684%\n",
      "Epoch 18, Batch 832, LR 2.154014 Loss 5.738203, Accuracy 86.684%\n",
      "Epoch 18, Batch 833, LR 2.153921 Loss 5.738322, Accuracy 86.684%\n",
      "Epoch 18, Batch 834, LR 2.153828 Loss 5.738498, Accuracy 86.681%\n",
      "Epoch 18, Batch 835, LR 2.153736 Loss 5.738168, Accuracy 86.682%\n",
      "Epoch 18, Batch 836, LR 2.153643 Loss 5.738285, Accuracy 86.687%\n",
      "Epoch 18, Batch 837, LR 2.153551 Loss 5.738984, Accuracy 86.685%\n",
      "Epoch 18, Batch 838, LR 2.153458 Loss 5.738778, Accuracy 86.684%\n",
      "Epoch 18, Batch 839, LR 2.153366 Loss 5.739169, Accuracy 86.681%\n",
      "Epoch 18, Batch 840, LR 2.153273 Loss 5.738597, Accuracy 86.683%\n",
      "Epoch 18, Batch 841, LR 2.153181 Loss 5.738481, Accuracy 86.686%\n",
      "Epoch 18, Batch 842, LR 2.153088 Loss 5.737778, Accuracy 86.692%\n",
      "Epoch 18, Batch 843, LR 2.152995 Loss 5.737533, Accuracy 86.690%\n",
      "Epoch 18, Batch 844, LR 2.152903 Loss 5.737586, Accuracy 86.694%\n",
      "Epoch 18, Batch 845, LR 2.152810 Loss 5.737718, Accuracy 86.696%\n",
      "Epoch 18, Batch 846, LR 2.152717 Loss 5.738210, Accuracy 86.696%\n",
      "Epoch 18, Batch 847, LR 2.152625 Loss 5.738814, Accuracy 86.699%\n",
      "Epoch 18, Batch 848, LR 2.152532 Loss 5.739348, Accuracy 86.690%\n",
      "Epoch 18, Batch 849, LR 2.152439 Loss 5.739510, Accuracy 86.691%\n",
      "Epoch 18, Batch 850, LR 2.152347 Loss 5.740066, Accuracy 86.690%\n",
      "Epoch 18, Batch 851, LR 2.152254 Loss 5.739688, Accuracy 86.694%\n",
      "Epoch 18, Batch 852, LR 2.152161 Loss 5.738646, Accuracy 86.702%\n",
      "Epoch 18, Batch 853, LR 2.152069 Loss 5.738754, Accuracy 86.703%\n",
      "Epoch 18, Batch 854, LR 2.151976 Loss 5.738836, Accuracy 86.704%\n",
      "Epoch 18, Batch 855, LR 2.151883 Loss 5.738788, Accuracy 86.707%\n",
      "Epoch 18, Batch 856, LR 2.151790 Loss 5.739055, Accuracy 86.707%\n",
      "Epoch 18, Batch 857, LR 2.151698 Loss 5.738331, Accuracy 86.710%\n",
      "Epoch 18, Batch 858, LR 2.151605 Loss 5.738664, Accuracy 86.710%\n",
      "Epoch 18, Batch 859, LR 2.151512 Loss 5.738991, Accuracy 86.709%\n",
      "Epoch 18, Batch 860, LR 2.151419 Loss 5.739226, Accuracy 86.705%\n",
      "Epoch 18, Batch 861, LR 2.151326 Loss 5.738348, Accuracy 86.710%\n",
      "Epoch 18, Batch 862, LR 2.151234 Loss 5.737955, Accuracy 86.711%\n",
      "Epoch 18, Batch 863, LR 2.151141 Loss 5.738094, Accuracy 86.707%\n",
      "Epoch 18, Batch 864, LR 2.151048 Loss 5.738146, Accuracy 86.708%\n",
      "Epoch 18, Batch 865, LR 2.150955 Loss 5.738173, Accuracy 86.713%\n",
      "Epoch 18, Batch 866, LR 2.150862 Loss 5.737924, Accuracy 86.715%\n",
      "Epoch 18, Batch 867, LR 2.150769 Loss 5.737051, Accuracy 86.719%\n",
      "Epoch 18, Batch 868, LR 2.150676 Loss 5.737135, Accuracy 86.716%\n",
      "Epoch 18, Batch 869, LR 2.150584 Loss 5.736877, Accuracy 86.719%\n",
      "Epoch 18, Batch 870, LR 2.150491 Loss 5.737009, Accuracy 86.717%\n",
      "Epoch 18, Batch 871, LR 2.150398 Loss 5.736457, Accuracy 86.718%\n",
      "Epoch 18, Batch 872, LR 2.150305 Loss 5.735746, Accuracy 86.720%\n",
      "Epoch 18, Batch 873, LR 2.150212 Loss 5.735704, Accuracy 86.720%\n",
      "Epoch 18, Batch 874, LR 2.150119 Loss 5.735890, Accuracy 86.721%\n",
      "Epoch 18, Batch 875, LR 2.150026 Loss 5.735429, Accuracy 86.725%\n",
      "Epoch 18, Batch 876, LR 2.149933 Loss 5.734959, Accuracy 86.729%\n",
      "Epoch 18, Batch 877, LR 2.149840 Loss 5.735413, Accuracy 86.728%\n",
      "Epoch 18, Batch 878, LR 2.149747 Loss 5.735921, Accuracy 86.725%\n",
      "Epoch 18, Batch 879, LR 2.149654 Loss 5.735795, Accuracy 86.725%\n",
      "Epoch 18, Batch 880, LR 2.149561 Loss 5.736432, Accuracy 86.719%\n",
      "Epoch 18, Batch 881, LR 2.149468 Loss 5.736180, Accuracy 86.718%\n",
      "Epoch 18, Batch 882, LR 2.149375 Loss 5.736585, Accuracy 86.717%\n",
      "Epoch 18, Batch 883, LR 2.149282 Loss 5.736902, Accuracy 86.714%\n",
      "Epoch 18, Batch 884, LR 2.149189 Loss 5.736692, Accuracy 86.715%\n",
      "Epoch 18, Batch 885, LR 2.149096 Loss 5.736605, Accuracy 86.717%\n",
      "Epoch 18, Batch 886, LR 2.149003 Loss 5.736951, Accuracy 86.713%\n",
      "Epoch 18, Batch 887, LR 2.148910 Loss 5.736097, Accuracy 86.716%\n",
      "Epoch 18, Batch 888, LR 2.148817 Loss 5.735811, Accuracy 86.717%\n",
      "Epoch 18, Batch 889, LR 2.148724 Loss 5.736623, Accuracy 86.712%\n",
      "Epoch 18, Batch 890, LR 2.148630 Loss 5.736122, Accuracy 86.713%\n",
      "Epoch 18, Batch 891, LR 2.148537 Loss 5.736221, Accuracy 86.713%\n",
      "Epoch 18, Batch 892, LR 2.148444 Loss 5.736325, Accuracy 86.708%\n",
      "Epoch 18, Batch 893, LR 2.148351 Loss 5.736036, Accuracy 86.710%\n",
      "Epoch 18, Batch 894, LR 2.148258 Loss 5.736291, Accuracy 86.711%\n",
      "Epoch 18, Batch 895, LR 2.148165 Loss 5.735692, Accuracy 86.714%\n",
      "Epoch 18, Batch 896, LR 2.148072 Loss 5.736469, Accuracy 86.711%\n",
      "Epoch 18, Batch 897, LR 2.147978 Loss 5.737282, Accuracy 86.707%\n",
      "Epoch 18, Batch 898, LR 2.147885 Loss 5.737268, Accuracy 86.705%\n",
      "Epoch 18, Batch 899, LR 2.147792 Loss 5.737827, Accuracy 86.704%\n",
      "Epoch 18, Batch 900, LR 2.147699 Loss 5.738204, Accuracy 86.701%\n",
      "Epoch 18, Batch 901, LR 2.147606 Loss 5.738083, Accuracy 86.705%\n",
      "Epoch 18, Batch 902, LR 2.147512 Loss 5.739123, Accuracy 86.698%\n",
      "Epoch 18, Batch 903, LR 2.147419 Loss 5.739150, Accuracy 86.701%\n",
      "Epoch 18, Batch 904, LR 2.147326 Loss 5.739039, Accuracy 86.704%\n",
      "Epoch 18, Batch 905, LR 2.147233 Loss 5.739056, Accuracy 86.704%\n",
      "Epoch 18, Batch 906, LR 2.147139 Loss 5.738449, Accuracy 86.706%\n",
      "Epoch 18, Batch 907, LR 2.147046 Loss 5.738719, Accuracy 86.697%\n",
      "Epoch 18, Batch 908, LR 2.146953 Loss 5.739145, Accuracy 86.695%\n",
      "Epoch 18, Batch 909, LR 2.146859 Loss 5.739465, Accuracy 86.692%\n",
      "Epoch 18, Batch 910, LR 2.146766 Loss 5.740240, Accuracy 86.688%\n",
      "Epoch 18, Batch 911, LR 2.146673 Loss 5.740002, Accuracy 86.690%\n",
      "Epoch 18, Batch 912, LR 2.146579 Loss 5.739788, Accuracy 86.687%\n",
      "Epoch 18, Batch 913, LR 2.146486 Loss 5.739480, Accuracy 86.689%\n",
      "Epoch 18, Batch 914, LR 2.146393 Loss 5.738734, Accuracy 86.693%\n",
      "Epoch 18, Batch 915, LR 2.146299 Loss 5.738327, Accuracy 86.694%\n",
      "Epoch 18, Batch 916, LR 2.146206 Loss 5.737724, Accuracy 86.694%\n",
      "Epoch 18, Batch 917, LR 2.146113 Loss 5.738013, Accuracy 86.697%\n",
      "Epoch 18, Batch 918, LR 2.146019 Loss 5.737534, Accuracy 86.699%\n",
      "Epoch 18, Batch 919, LR 2.145926 Loss 5.737127, Accuracy 86.699%\n",
      "Epoch 18, Batch 920, LR 2.145832 Loss 5.737638, Accuracy 86.694%\n",
      "Epoch 18, Batch 921, LR 2.145739 Loss 5.736724, Accuracy 86.697%\n",
      "Epoch 18, Batch 922, LR 2.145646 Loss 5.737006, Accuracy 86.692%\n",
      "Epoch 18, Batch 923, LR 2.145552 Loss 5.736576, Accuracy 86.694%\n",
      "Epoch 18, Batch 924, LR 2.145459 Loss 5.736852, Accuracy 86.695%\n",
      "Epoch 18, Batch 925, LR 2.145365 Loss 5.737052, Accuracy 86.693%\n",
      "Epoch 18, Batch 926, LR 2.145272 Loss 5.737693, Accuracy 86.691%\n",
      "Epoch 18, Batch 927, LR 2.145178 Loss 5.737303, Accuracy 86.691%\n",
      "Epoch 18, Batch 928, LR 2.145085 Loss 5.737119, Accuracy 86.692%\n",
      "Epoch 18, Batch 929, LR 2.144991 Loss 5.736147, Accuracy 86.699%\n",
      "Epoch 18, Batch 930, LR 2.144898 Loss 5.736284, Accuracy 86.698%\n",
      "Epoch 18, Batch 931, LR 2.144804 Loss 5.736521, Accuracy 86.694%\n",
      "Epoch 18, Batch 932, LR 2.144711 Loss 5.736463, Accuracy 86.693%\n",
      "Epoch 18, Batch 933, LR 2.144617 Loss 5.735841, Accuracy 86.695%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 934, LR 2.144524 Loss 5.735335, Accuracy 86.694%\n",
      "Epoch 18, Batch 935, LR 2.144430 Loss 5.735037, Accuracy 86.698%\n",
      "Epoch 18, Batch 936, LR 2.144336 Loss 5.734295, Accuracy 86.704%\n",
      "Epoch 18, Batch 937, LR 2.144243 Loss 5.734560, Accuracy 86.701%\n",
      "Epoch 18, Batch 938, LR 2.144149 Loss 5.734760, Accuracy 86.701%\n",
      "Epoch 18, Batch 939, LR 2.144056 Loss 5.735075, Accuracy 86.696%\n",
      "Epoch 18, Batch 940, LR 2.143962 Loss 5.733940, Accuracy 86.705%\n",
      "Epoch 18, Batch 941, LR 2.143868 Loss 5.733814, Accuracy 86.705%\n",
      "Epoch 18, Batch 942, LR 2.143775 Loss 5.733680, Accuracy 86.705%\n",
      "Epoch 18, Batch 943, LR 2.143681 Loss 5.733952, Accuracy 86.701%\n",
      "Epoch 18, Batch 944, LR 2.143587 Loss 5.734079, Accuracy 86.701%\n",
      "Epoch 18, Batch 945, LR 2.143494 Loss 5.734132, Accuracy 86.702%\n",
      "Epoch 18, Batch 946, LR 2.143400 Loss 5.734658, Accuracy 86.693%\n",
      "Epoch 18, Batch 947, LR 2.143306 Loss 5.734970, Accuracy 86.690%\n",
      "Epoch 18, Batch 948, LR 2.143213 Loss 5.735026, Accuracy 86.693%\n",
      "Epoch 18, Batch 949, LR 2.143119 Loss 5.735703, Accuracy 86.687%\n",
      "Epoch 18, Batch 950, LR 2.143025 Loss 5.735186, Accuracy 86.687%\n",
      "Epoch 18, Batch 951, LR 2.142931 Loss 5.735146, Accuracy 86.683%\n",
      "Epoch 18, Batch 952, LR 2.142838 Loss 5.735380, Accuracy 86.680%\n",
      "Epoch 18, Batch 953, LR 2.142744 Loss 5.736324, Accuracy 86.676%\n",
      "Epoch 18, Batch 954, LR 2.142650 Loss 5.735648, Accuracy 86.682%\n",
      "Epoch 18, Batch 955, LR 2.142556 Loss 5.736667, Accuracy 86.678%\n",
      "Epoch 18, Batch 956, LR 2.142463 Loss 5.736514, Accuracy 86.679%\n",
      "Epoch 18, Batch 957, LR 2.142369 Loss 5.736808, Accuracy 86.672%\n",
      "Epoch 18, Batch 958, LR 2.142275 Loss 5.736237, Accuracy 86.672%\n",
      "Epoch 18, Batch 959, LR 2.142181 Loss 5.735623, Accuracy 86.674%\n",
      "Epoch 18, Batch 960, LR 2.142087 Loss 5.735170, Accuracy 86.671%\n",
      "Epoch 18, Batch 961, LR 2.141994 Loss 5.735714, Accuracy 86.673%\n",
      "Epoch 18, Batch 962, LR 2.141900 Loss 5.735914, Accuracy 86.672%\n",
      "Epoch 18, Batch 963, LR 2.141806 Loss 5.735219, Accuracy 86.673%\n",
      "Epoch 18, Batch 964, LR 2.141712 Loss 5.735722, Accuracy 86.669%\n",
      "Epoch 18, Batch 965, LR 2.141618 Loss 5.736408, Accuracy 86.666%\n",
      "Epoch 18, Batch 966, LR 2.141524 Loss 5.735730, Accuracy 86.673%\n",
      "Epoch 18, Batch 967, LR 2.141430 Loss 5.736171, Accuracy 86.670%\n",
      "Epoch 18, Batch 968, LR 2.141336 Loss 5.735697, Accuracy 86.670%\n",
      "Epoch 18, Batch 969, LR 2.141242 Loss 5.735166, Accuracy 86.670%\n",
      "Epoch 18, Batch 970, LR 2.141149 Loss 5.734772, Accuracy 86.670%\n",
      "Epoch 18, Batch 971, LR 2.141055 Loss 5.735053, Accuracy 86.670%\n",
      "Epoch 18, Batch 972, LR 2.140961 Loss 5.735716, Accuracy 86.662%\n",
      "Epoch 18, Batch 973, LR 2.140867 Loss 5.736020, Accuracy 86.660%\n",
      "Epoch 18, Batch 974, LR 2.140773 Loss 5.736232, Accuracy 86.662%\n",
      "Epoch 18, Batch 975, LR 2.140679 Loss 5.737190, Accuracy 86.655%\n",
      "Epoch 18, Batch 976, LR 2.140585 Loss 5.736612, Accuracy 86.654%\n",
      "Epoch 18, Batch 977, LR 2.140491 Loss 5.736202, Accuracy 86.656%\n",
      "Epoch 18, Batch 978, LR 2.140397 Loss 5.736508, Accuracy 86.654%\n",
      "Epoch 18, Batch 979, LR 2.140303 Loss 5.736164, Accuracy 86.660%\n",
      "Epoch 18, Batch 980, LR 2.140209 Loss 5.735934, Accuracy 86.661%\n",
      "Epoch 18, Batch 981, LR 2.140115 Loss 5.736881, Accuracy 86.657%\n",
      "Epoch 18, Batch 982, LR 2.140021 Loss 5.737012, Accuracy 86.653%\n",
      "Epoch 18, Batch 983, LR 2.139927 Loss 5.736967, Accuracy 86.654%\n",
      "Epoch 18, Batch 984, LR 2.139832 Loss 5.736113, Accuracy 86.658%\n",
      "Epoch 18, Batch 985, LR 2.139738 Loss 5.735885, Accuracy 86.662%\n",
      "Epoch 18, Batch 986, LR 2.139644 Loss 5.736442, Accuracy 86.661%\n",
      "Epoch 18, Batch 987, LR 2.139550 Loss 5.736424, Accuracy 86.655%\n",
      "Epoch 18, Batch 988, LR 2.139456 Loss 5.735734, Accuracy 86.659%\n",
      "Epoch 18, Batch 989, LR 2.139362 Loss 5.735135, Accuracy 86.660%\n",
      "Epoch 18, Batch 990, LR 2.139268 Loss 5.735350, Accuracy 86.659%\n",
      "Epoch 18, Batch 991, LR 2.139174 Loss 5.735569, Accuracy 86.659%\n",
      "Epoch 18, Batch 992, LR 2.139079 Loss 5.735857, Accuracy 86.655%\n",
      "Epoch 18, Batch 993, LR 2.138985 Loss 5.735507, Accuracy 86.658%\n",
      "Epoch 18, Batch 994, LR 2.138891 Loss 5.736002, Accuracy 86.650%\n",
      "Epoch 18, Batch 995, LR 2.138797 Loss 5.735382, Accuracy 86.654%\n",
      "Epoch 18, Batch 996, LR 2.138703 Loss 5.735273, Accuracy 86.658%\n",
      "Epoch 18, Batch 997, LR 2.138609 Loss 5.736067, Accuracy 86.651%\n",
      "Epoch 18, Batch 998, LR 2.138514 Loss 5.735347, Accuracy 86.652%\n",
      "Epoch 18, Batch 999, LR 2.138420 Loss 5.735245, Accuracy 86.652%\n",
      "Epoch 18, Batch 1000, LR 2.138326 Loss 5.734747, Accuracy 86.652%\n",
      "Epoch 18, Batch 1001, LR 2.138232 Loss 5.734818, Accuracy 86.652%\n",
      "Epoch 18, Batch 1002, LR 2.138137 Loss 5.734698, Accuracy 86.652%\n",
      "Epoch 18, Batch 1003, LR 2.138043 Loss 5.733525, Accuracy 86.656%\n",
      "Epoch 18, Batch 1004, LR 2.137949 Loss 5.733509, Accuracy 86.657%\n",
      "Epoch 18, Batch 1005, LR 2.137855 Loss 5.733352, Accuracy 86.660%\n",
      "Epoch 18, Batch 1006, LR 2.137760 Loss 5.733187, Accuracy 86.657%\n",
      "Epoch 18, Batch 1007, LR 2.137666 Loss 5.733003, Accuracy 86.657%\n",
      "Epoch 18, Batch 1008, LR 2.137572 Loss 5.733209, Accuracy 86.658%\n",
      "Epoch 18, Batch 1009, LR 2.137477 Loss 5.733013, Accuracy 86.661%\n",
      "Epoch 18, Batch 1010, LR 2.137383 Loss 5.733343, Accuracy 86.658%\n",
      "Epoch 18, Batch 1011, LR 2.137289 Loss 5.733622, Accuracy 86.655%\n",
      "Epoch 18, Batch 1012, LR 2.137194 Loss 5.734489, Accuracy 86.650%\n",
      "Epoch 18, Batch 1013, LR 2.137100 Loss 5.734008, Accuracy 86.655%\n",
      "Epoch 18, Batch 1014, LR 2.137006 Loss 5.734221, Accuracy 86.656%\n",
      "Epoch 18, Batch 1015, LR 2.136911 Loss 5.734824, Accuracy 86.656%\n",
      "Epoch 18, Batch 1016, LR 2.136817 Loss 5.735112, Accuracy 86.654%\n",
      "Epoch 18, Batch 1017, LR 2.136722 Loss 5.734243, Accuracy 86.656%\n",
      "Epoch 18, Batch 1018, LR 2.136628 Loss 5.733962, Accuracy 86.655%\n",
      "Epoch 18, Batch 1019, LR 2.136533 Loss 5.734090, Accuracy 86.657%\n",
      "Epoch 18, Batch 1020, LR 2.136439 Loss 5.733900, Accuracy 86.654%\n",
      "Epoch 18, Batch 1021, LR 2.136345 Loss 5.734139, Accuracy 86.654%\n",
      "Epoch 18, Batch 1022, LR 2.136250 Loss 5.733712, Accuracy 86.658%\n",
      "Epoch 18, Batch 1023, LR 2.136156 Loss 5.734094, Accuracy 86.656%\n",
      "Epoch 18, Batch 1024, LR 2.136061 Loss 5.734952, Accuracy 86.652%\n",
      "Epoch 18, Batch 1025, LR 2.135967 Loss 5.735058, Accuracy 86.653%\n",
      "Epoch 18, Batch 1026, LR 2.135872 Loss 5.735631, Accuracy 86.650%\n",
      "Epoch 18, Batch 1027, LR 2.135778 Loss 5.735894, Accuracy 86.645%\n",
      "Epoch 18, Batch 1028, LR 2.135683 Loss 5.736274, Accuracy 86.644%\n",
      "Epoch 18, Batch 1029, LR 2.135589 Loss 5.737209, Accuracy 86.640%\n",
      "Epoch 18, Batch 1030, LR 2.135494 Loss 5.737328, Accuracy 86.640%\n",
      "Epoch 18, Batch 1031, LR 2.135400 Loss 5.737333, Accuracy 86.641%\n",
      "Epoch 18, Batch 1032, LR 2.135305 Loss 5.737687, Accuracy 86.639%\n",
      "Epoch 18, Batch 1033, LR 2.135210 Loss 5.737753, Accuracy 86.638%\n",
      "Epoch 18, Batch 1034, LR 2.135116 Loss 5.737349, Accuracy 86.639%\n",
      "Epoch 18, Batch 1035, LR 2.135021 Loss 5.737245, Accuracy 86.640%\n",
      "Epoch 18, Batch 1036, LR 2.134927 Loss 5.736980, Accuracy 86.640%\n",
      "Epoch 18, Batch 1037, LR 2.134832 Loss 5.737597, Accuracy 86.637%\n",
      "Epoch 18, Batch 1038, LR 2.134737 Loss 5.738358, Accuracy 86.634%\n",
      "Epoch 18, Batch 1039, LR 2.134643 Loss 5.738181, Accuracy 86.637%\n",
      "Epoch 18, Batch 1040, LR 2.134548 Loss 5.737446, Accuracy 86.644%\n",
      "Epoch 18, Batch 1041, LR 2.134453 Loss 5.737092, Accuracy 86.644%\n",
      "Epoch 18, Batch 1042, LR 2.134359 Loss 5.736345, Accuracy 86.647%\n",
      "Epoch 18, Batch 1043, LR 2.134264 Loss 5.735762, Accuracy 86.649%\n",
      "Epoch 18, Batch 1044, LR 2.134169 Loss 5.736693, Accuracy 86.643%\n",
      "Epoch 18, Batch 1045, LR 2.134075 Loss 5.736726, Accuracy 86.640%\n",
      "Epoch 18, Batch 1046, LR 2.133980 Loss 5.736158, Accuracy 86.643%\n",
      "Epoch 18, Batch 1047, LR 2.133885 Loss 5.735589, Accuracy 86.647%\n",
      "Epoch 18, Loss (train set) 5.735589, Accuracy (train set) 86.647%\n",
      "Epoch 19, Batch 1, LR 2.133791 Loss 5.658181, Accuracy 89.062%\n",
      "Epoch 19, Batch 2, LR 2.133696 Loss 5.271086, Accuracy 90.234%\n",
      "Epoch 19, Batch 3, LR 2.133601 Loss 5.474645, Accuracy 88.021%\n",
      "Epoch 19, Batch 4, LR 2.133506 Loss 5.650708, Accuracy 86.914%\n",
      "Epoch 19, Batch 5, LR 2.133412 Loss 5.532512, Accuracy 87.500%\n",
      "Epoch 19, Batch 6, LR 2.133317 Loss 5.560032, Accuracy 87.109%\n",
      "Epoch 19, Batch 7, LR 2.133222 Loss 5.608698, Accuracy 86.942%\n",
      "Epoch 19, Batch 8, LR 2.133127 Loss 5.584722, Accuracy 87.402%\n",
      "Epoch 19, Batch 9, LR 2.133032 Loss 5.624280, Accuracy 86.892%\n",
      "Epoch 19, Batch 10, LR 2.132938 Loss 5.579226, Accuracy 87.031%\n",
      "Epoch 19, Batch 11, LR 2.132843 Loss 5.557554, Accuracy 87.145%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 12, LR 2.132748 Loss 5.576881, Accuracy 87.044%\n",
      "Epoch 19, Batch 13, LR 2.132653 Loss 5.589110, Accuracy 87.380%\n",
      "Epoch 19, Batch 14, LR 2.132558 Loss 5.570870, Accuracy 87.556%\n",
      "Epoch 19, Batch 15, LR 2.132463 Loss 5.560569, Accuracy 87.448%\n",
      "Epoch 19, Batch 16, LR 2.132369 Loss 5.579181, Accuracy 87.354%\n",
      "Epoch 19, Batch 17, LR 2.132274 Loss 5.587593, Accuracy 87.362%\n",
      "Epoch 19, Batch 18, LR 2.132179 Loss 5.553856, Accuracy 87.674%\n",
      "Epoch 19, Batch 19, LR 2.132084 Loss 5.607575, Accuracy 87.048%\n",
      "Epoch 19, Batch 20, LR 2.131989 Loss 5.636269, Accuracy 86.836%\n",
      "Epoch 19, Batch 21, LR 2.131894 Loss 5.601707, Accuracy 86.942%\n",
      "Epoch 19, Batch 22, LR 2.131799 Loss 5.599686, Accuracy 87.003%\n",
      "Epoch 19, Batch 23, LR 2.131704 Loss 5.578721, Accuracy 87.160%\n",
      "Epoch 19, Batch 24, LR 2.131609 Loss 5.550433, Accuracy 87.272%\n",
      "Epoch 19, Batch 25, LR 2.131514 Loss 5.541864, Accuracy 87.312%\n",
      "Epoch 19, Batch 26, LR 2.131419 Loss 5.539754, Accuracy 87.410%\n",
      "Epoch 19, Batch 27, LR 2.131324 Loss 5.526663, Accuracy 87.471%\n",
      "Epoch 19, Batch 28, LR 2.131229 Loss 5.530222, Accuracy 87.277%\n",
      "Epoch 19, Batch 29, LR 2.131134 Loss 5.540197, Accuracy 87.258%\n",
      "Epoch 19, Batch 30, LR 2.131039 Loss 5.536134, Accuracy 87.292%\n",
      "Epoch 19, Batch 31, LR 2.130944 Loss 5.528731, Accuracy 87.424%\n",
      "Epoch 19, Batch 32, LR 2.130849 Loss 5.512261, Accuracy 87.549%\n",
      "Epoch 19, Batch 33, LR 2.130754 Loss 5.523162, Accuracy 87.500%\n",
      "Epoch 19, Batch 34, LR 2.130659 Loss 5.521602, Accuracy 87.500%\n",
      "Epoch 19, Batch 35, LR 2.130564 Loss 5.511827, Accuracy 87.612%\n",
      "Epoch 19, Batch 36, LR 2.130469 Loss 5.509454, Accuracy 87.630%\n",
      "Epoch 19, Batch 37, LR 2.130374 Loss 5.515580, Accuracy 87.563%\n",
      "Epoch 19, Batch 38, LR 2.130279 Loss 5.525044, Accuracy 87.500%\n",
      "Epoch 19, Batch 39, LR 2.130184 Loss 5.529725, Accuracy 87.380%\n",
      "Epoch 19, Batch 40, LR 2.130088 Loss 5.537005, Accuracy 87.383%\n",
      "Epoch 19, Batch 41, LR 2.129993 Loss 5.556800, Accuracy 87.329%\n",
      "Epoch 19, Batch 42, LR 2.129898 Loss 5.557383, Accuracy 87.351%\n",
      "Epoch 19, Batch 43, LR 2.129803 Loss 5.560876, Accuracy 87.336%\n",
      "Epoch 19, Batch 44, LR 2.129708 Loss 5.551687, Accuracy 87.393%\n",
      "Epoch 19, Batch 45, LR 2.129613 Loss 5.548778, Accuracy 87.483%\n",
      "Epoch 19, Batch 46, LR 2.129518 Loss 5.557817, Accuracy 87.534%\n",
      "Epoch 19, Batch 47, LR 2.129422 Loss 5.565934, Accuracy 87.467%\n",
      "Epoch 19, Batch 48, LR 2.129327 Loss 5.560275, Accuracy 87.500%\n",
      "Epoch 19, Batch 49, LR 2.129232 Loss 5.559998, Accuracy 87.500%\n",
      "Epoch 19, Batch 50, LR 2.129137 Loss 5.559308, Accuracy 87.516%\n",
      "Epoch 19, Batch 51, LR 2.129041 Loss 5.568429, Accuracy 87.500%\n",
      "Epoch 19, Batch 52, LR 2.128946 Loss 5.569945, Accuracy 87.500%\n",
      "Epoch 19, Batch 53, LR 2.128851 Loss 5.551608, Accuracy 87.633%\n",
      "Epoch 19, Batch 54, LR 2.128756 Loss 5.546412, Accuracy 87.645%\n",
      "Epoch 19, Batch 55, LR 2.128660 Loss 5.534277, Accuracy 87.713%\n",
      "Epoch 19, Batch 56, LR 2.128565 Loss 5.530850, Accuracy 87.737%\n",
      "Epoch 19, Batch 57, LR 2.128470 Loss 5.534136, Accuracy 87.678%\n",
      "Epoch 19, Batch 58, LR 2.128375 Loss 5.534973, Accuracy 87.675%\n",
      "Epoch 19, Batch 59, LR 2.128279 Loss 5.533440, Accuracy 87.699%\n",
      "Epoch 19, Batch 60, LR 2.128184 Loss 5.527920, Accuracy 87.721%\n",
      "Epoch 19, Batch 61, LR 2.128089 Loss 5.524634, Accuracy 87.743%\n",
      "Epoch 19, Batch 62, LR 2.127993 Loss 5.519467, Accuracy 87.714%\n",
      "Epoch 19, Batch 63, LR 2.127898 Loss 5.526514, Accuracy 87.711%\n",
      "Epoch 19, Batch 64, LR 2.127803 Loss 5.528582, Accuracy 87.683%\n",
      "Epoch 19, Batch 65, LR 2.127707 Loss 5.528986, Accuracy 87.680%\n",
      "Epoch 19, Batch 66, LR 2.127612 Loss 5.528938, Accuracy 87.678%\n",
      "Epoch 19, Batch 67, LR 2.127516 Loss 5.525705, Accuracy 87.768%\n",
      "Epoch 19, Batch 68, LR 2.127421 Loss 5.523540, Accuracy 87.764%\n",
      "Epoch 19, Batch 69, LR 2.127326 Loss 5.528832, Accuracy 87.715%\n",
      "Epoch 19, Batch 70, LR 2.127230 Loss 5.530199, Accuracy 87.768%\n",
      "Epoch 19, Batch 71, LR 2.127135 Loss 5.527133, Accuracy 87.775%\n",
      "Epoch 19, Batch 72, LR 2.127039 Loss 5.523570, Accuracy 87.804%\n",
      "Epoch 19, Batch 73, LR 2.126944 Loss 5.525968, Accuracy 87.853%\n",
      "Epoch 19, Batch 74, LR 2.126848 Loss 5.530524, Accuracy 87.774%\n",
      "Epoch 19, Batch 75, LR 2.126753 Loss 5.534850, Accuracy 87.729%\n",
      "Epoch 19, Batch 76, LR 2.126657 Loss 5.544546, Accuracy 87.685%\n",
      "Epoch 19, Batch 77, LR 2.126562 Loss 5.541956, Accuracy 87.662%\n",
      "Epoch 19, Batch 78, LR 2.126466 Loss 5.541847, Accuracy 87.660%\n",
      "Epoch 19, Batch 79, LR 2.126371 Loss 5.546092, Accuracy 87.648%\n",
      "Epoch 19, Batch 80, LR 2.126275 Loss 5.548848, Accuracy 87.598%\n",
      "Epoch 19, Batch 81, LR 2.126180 Loss 5.541951, Accuracy 87.635%\n",
      "Epoch 19, Batch 82, LR 2.126084 Loss 5.542183, Accuracy 87.633%\n",
      "Epoch 19, Batch 83, LR 2.125989 Loss 5.551778, Accuracy 87.585%\n",
      "Epoch 19, Batch 84, LR 2.125893 Loss 5.552888, Accuracy 87.574%\n",
      "Epoch 19, Batch 85, LR 2.125798 Loss 5.555533, Accuracy 87.592%\n",
      "Epoch 19, Batch 86, LR 2.125702 Loss 5.555172, Accuracy 87.618%\n",
      "Epoch 19, Batch 87, LR 2.125606 Loss 5.561830, Accuracy 87.590%\n",
      "Epoch 19, Batch 88, LR 2.125511 Loss 5.561791, Accuracy 87.553%\n",
      "Epoch 19, Batch 89, LR 2.125415 Loss 5.559434, Accuracy 87.553%\n",
      "Epoch 19, Batch 90, LR 2.125320 Loss 5.571380, Accuracy 87.483%\n",
      "Epoch 19, Batch 91, LR 2.125224 Loss 5.569397, Accuracy 87.483%\n",
      "Epoch 19, Batch 92, LR 2.125128 Loss 5.563963, Accuracy 87.542%\n",
      "Epoch 19, Batch 93, LR 2.125033 Loss 5.563762, Accuracy 87.508%\n",
      "Epoch 19, Batch 94, LR 2.124937 Loss 5.567090, Accuracy 87.458%\n",
      "Epoch 19, Batch 95, LR 2.124841 Loss 5.564044, Accuracy 87.500%\n",
      "Epoch 19, Batch 96, LR 2.124746 Loss 5.556413, Accuracy 87.573%\n",
      "Epoch 19, Batch 97, LR 2.124650 Loss 5.559406, Accuracy 87.556%\n",
      "Epoch 19, Batch 98, LR 2.124554 Loss 5.565620, Accuracy 87.540%\n",
      "Epoch 19, Batch 99, LR 2.124459 Loss 5.572252, Accuracy 87.516%\n",
      "Epoch 19, Batch 100, LR 2.124363 Loss 5.565823, Accuracy 87.531%\n",
      "Epoch 19, Batch 101, LR 2.124267 Loss 5.567233, Accuracy 87.523%\n",
      "Epoch 19, Batch 102, LR 2.124171 Loss 5.570269, Accuracy 87.477%\n",
      "Epoch 19, Batch 103, LR 2.124076 Loss 5.572658, Accuracy 87.439%\n",
      "Epoch 19, Batch 104, LR 2.123980 Loss 5.570934, Accuracy 87.440%\n",
      "Epoch 19, Batch 105, LR 2.123884 Loss 5.569763, Accuracy 87.426%\n",
      "Epoch 19, Batch 106, LR 2.123788 Loss 5.566046, Accuracy 87.397%\n",
      "Epoch 19, Batch 107, LR 2.123692 Loss 5.570252, Accuracy 87.369%\n",
      "Epoch 19, Batch 108, LR 2.123597 Loss 5.571529, Accuracy 87.341%\n",
      "Epoch 19, Batch 109, LR 2.123501 Loss 5.568582, Accuracy 87.371%\n",
      "Epoch 19, Batch 110, LR 2.123405 Loss 5.569904, Accuracy 87.365%\n",
      "Epoch 19, Batch 111, LR 2.123309 Loss 5.573581, Accuracy 87.338%\n",
      "Epoch 19, Batch 112, LR 2.123213 Loss 5.574766, Accuracy 87.340%\n",
      "Epoch 19, Batch 113, LR 2.123117 Loss 5.570360, Accuracy 87.362%\n",
      "Epoch 19, Batch 114, LR 2.123022 Loss 5.567527, Accuracy 87.418%\n",
      "Epoch 19, Batch 115, LR 2.122926 Loss 5.568930, Accuracy 87.405%\n",
      "Epoch 19, Batch 116, LR 2.122830 Loss 5.565511, Accuracy 87.433%\n",
      "Epoch 19, Batch 117, LR 2.122734 Loss 5.567494, Accuracy 87.467%\n",
      "Epoch 19, Batch 118, LR 2.122638 Loss 5.572070, Accuracy 87.474%\n",
      "Epoch 19, Batch 119, LR 2.122542 Loss 5.574065, Accuracy 87.461%\n",
      "Epoch 19, Batch 120, LR 2.122446 Loss 5.574743, Accuracy 87.428%\n",
      "Epoch 19, Batch 121, LR 2.122350 Loss 5.578005, Accuracy 87.435%\n",
      "Epoch 19, Batch 122, LR 2.122254 Loss 5.570869, Accuracy 87.487%\n",
      "Epoch 19, Batch 123, LR 2.122158 Loss 5.572358, Accuracy 87.487%\n",
      "Epoch 19, Batch 124, LR 2.122062 Loss 5.570475, Accuracy 87.468%\n",
      "Epoch 19, Batch 125, LR 2.121966 Loss 5.561359, Accuracy 87.519%\n",
      "Epoch 19, Batch 126, LR 2.121870 Loss 5.565161, Accuracy 87.519%\n",
      "Epoch 19, Batch 127, LR 2.121774 Loss 5.570626, Accuracy 87.512%\n",
      "Epoch 19, Batch 128, LR 2.121678 Loss 5.565828, Accuracy 87.543%\n",
      "Epoch 19, Batch 129, LR 2.121582 Loss 5.569130, Accuracy 87.512%\n",
      "Epoch 19, Batch 130, LR 2.121486 Loss 5.567102, Accuracy 87.542%\n",
      "Epoch 19, Batch 131, LR 2.121390 Loss 5.563119, Accuracy 87.560%\n",
      "Epoch 19, Batch 132, LR 2.121294 Loss 5.566397, Accuracy 87.536%\n",
      "Epoch 19, Batch 133, LR 2.121198 Loss 5.561993, Accuracy 87.553%\n",
      "Epoch 19, Batch 134, LR 2.121102 Loss 5.560354, Accuracy 87.570%\n",
      "Epoch 19, Batch 135, LR 2.121006 Loss 5.554451, Accuracy 87.593%\n",
      "Epoch 19, Batch 136, LR 2.120910 Loss 5.549687, Accuracy 87.621%\n",
      "Epoch 19, Batch 137, LR 2.120814 Loss 5.545925, Accuracy 87.660%\n",
      "Epoch 19, Batch 138, LR 2.120718 Loss 5.545576, Accuracy 87.687%\n",
      "Epoch 19, Batch 139, LR 2.120622 Loss 5.544322, Accuracy 87.674%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 140, LR 2.120526 Loss 5.542887, Accuracy 87.695%\n",
      "Epoch 19, Batch 141, LR 2.120430 Loss 5.539754, Accuracy 87.711%\n",
      "Epoch 19, Batch 142, LR 2.120333 Loss 5.537694, Accuracy 87.704%\n",
      "Epoch 19, Batch 143, LR 2.120237 Loss 5.536825, Accuracy 87.680%\n",
      "Epoch 19, Batch 144, LR 2.120141 Loss 5.535774, Accuracy 87.690%\n",
      "Epoch 19, Batch 145, LR 2.120045 Loss 5.539245, Accuracy 87.683%\n",
      "Epoch 19, Batch 146, LR 2.119949 Loss 5.538610, Accuracy 87.687%\n",
      "Epoch 19, Batch 147, LR 2.119852 Loss 5.548131, Accuracy 87.638%\n",
      "Epoch 19, Batch 148, LR 2.119756 Loss 5.543864, Accuracy 87.664%\n",
      "Epoch 19, Batch 149, LR 2.119660 Loss 5.548572, Accuracy 87.621%\n",
      "Epoch 19, Batch 150, LR 2.119564 Loss 5.553051, Accuracy 87.583%\n",
      "Epoch 19, Batch 151, LR 2.119468 Loss 5.550049, Accuracy 87.603%\n",
      "Epoch 19, Batch 152, LR 2.119371 Loss 5.550864, Accuracy 87.593%\n",
      "Epoch 19, Batch 153, LR 2.119275 Loss 5.551696, Accuracy 87.602%\n",
      "Epoch 19, Batch 154, LR 2.119179 Loss 5.554789, Accuracy 87.571%\n",
      "Epoch 19, Batch 155, LR 2.119083 Loss 5.556962, Accuracy 87.571%\n",
      "Epoch 19, Batch 156, LR 2.118986 Loss 5.556775, Accuracy 87.545%\n",
      "Epoch 19, Batch 157, LR 2.118890 Loss 5.556920, Accuracy 87.550%\n",
      "Epoch 19, Batch 158, LR 2.118794 Loss 5.555011, Accuracy 87.564%\n",
      "Epoch 19, Batch 159, LR 2.118697 Loss 5.559632, Accuracy 87.520%\n",
      "Epoch 19, Batch 160, LR 2.118601 Loss 5.557179, Accuracy 87.539%\n",
      "Epoch 19, Batch 161, LR 2.118505 Loss 5.553742, Accuracy 87.549%\n",
      "Epoch 19, Batch 162, LR 2.118408 Loss 5.563104, Accuracy 87.510%\n",
      "Epoch 19, Batch 163, LR 2.118312 Loss 5.564137, Accuracy 87.495%\n",
      "Epoch 19, Batch 164, LR 2.118216 Loss 5.567839, Accuracy 87.457%\n",
      "Epoch 19, Batch 165, LR 2.118119 Loss 5.568547, Accuracy 87.457%\n",
      "Epoch 19, Batch 166, LR 2.118023 Loss 5.566510, Accuracy 87.448%\n",
      "Epoch 19, Batch 167, LR 2.117927 Loss 5.562030, Accuracy 87.477%\n",
      "Epoch 19, Batch 168, LR 2.117830 Loss 5.560917, Accuracy 87.453%\n",
      "Epoch 19, Batch 169, LR 2.117734 Loss 5.560017, Accuracy 87.454%\n",
      "Epoch 19, Batch 170, LR 2.117637 Loss 5.562840, Accuracy 87.463%\n",
      "Epoch 19, Batch 171, LR 2.117541 Loss 5.562421, Accuracy 87.468%\n",
      "Epoch 19, Batch 172, LR 2.117444 Loss 5.558520, Accuracy 87.500%\n",
      "Epoch 19, Batch 173, LR 2.117348 Loss 5.561953, Accuracy 87.486%\n",
      "Epoch 19, Batch 174, LR 2.117251 Loss 5.557925, Accuracy 87.504%\n",
      "Epoch 19, Batch 175, LR 2.117155 Loss 5.558766, Accuracy 87.487%\n",
      "Epoch 19, Batch 176, LR 2.117059 Loss 5.559665, Accuracy 87.487%\n",
      "Epoch 19, Batch 177, LR 2.116962 Loss 5.564295, Accuracy 87.469%\n",
      "Epoch 19, Batch 178, LR 2.116866 Loss 5.568822, Accuracy 87.439%\n",
      "Epoch 19, Batch 179, LR 2.116769 Loss 5.563903, Accuracy 87.461%\n",
      "Epoch 19, Batch 180, LR 2.116672 Loss 5.558247, Accuracy 87.491%\n",
      "Epoch 19, Batch 181, LR 2.116576 Loss 5.556913, Accuracy 87.496%\n",
      "Epoch 19, Batch 182, LR 2.116479 Loss 5.557198, Accuracy 87.496%\n",
      "Epoch 19, Batch 183, LR 2.116383 Loss 5.556320, Accuracy 87.500%\n",
      "Epoch 19, Batch 184, LR 2.116286 Loss 5.552975, Accuracy 87.517%\n",
      "Epoch 19, Batch 185, LR 2.116190 Loss 5.555484, Accuracy 87.508%\n",
      "Epoch 19, Batch 186, LR 2.116093 Loss 5.553667, Accuracy 87.517%\n",
      "Epoch 19, Batch 187, LR 2.115997 Loss 5.550995, Accuracy 87.521%\n",
      "Epoch 19, Batch 188, LR 2.115900 Loss 5.552751, Accuracy 87.496%\n",
      "Epoch 19, Batch 189, LR 2.115803 Loss 5.549534, Accuracy 87.512%\n",
      "Epoch 19, Batch 190, LR 2.115707 Loss 5.550093, Accuracy 87.512%\n",
      "Epoch 19, Batch 191, LR 2.115610 Loss 5.554294, Accuracy 87.484%\n",
      "Epoch 19, Batch 192, LR 2.115513 Loss 5.550699, Accuracy 87.492%\n",
      "Epoch 19, Batch 193, LR 2.115417 Loss 5.552336, Accuracy 87.476%\n",
      "Epoch 19, Batch 194, LR 2.115320 Loss 5.551683, Accuracy 87.472%\n",
      "Epoch 19, Batch 195, LR 2.115223 Loss 5.548879, Accuracy 87.488%\n",
      "Epoch 19, Batch 196, LR 2.115127 Loss 5.546622, Accuracy 87.516%\n",
      "Epoch 19, Batch 197, LR 2.115030 Loss 5.546892, Accuracy 87.496%\n",
      "Epoch 19, Batch 198, LR 2.114933 Loss 5.552848, Accuracy 87.457%\n",
      "Epoch 19, Batch 199, LR 2.114837 Loss 5.552166, Accuracy 87.449%\n",
      "Epoch 19, Batch 200, LR 2.114740 Loss 5.551620, Accuracy 87.461%\n",
      "Epoch 19, Batch 201, LR 2.114643 Loss 5.555685, Accuracy 87.446%\n",
      "Epoch 19, Batch 202, LR 2.114546 Loss 5.553533, Accuracy 87.450%\n",
      "Epoch 19, Batch 203, LR 2.114450 Loss 5.556051, Accuracy 87.431%\n",
      "Epoch 19, Batch 204, LR 2.114353 Loss 5.554752, Accuracy 87.446%\n",
      "Epoch 19, Batch 205, LR 2.114256 Loss 5.556056, Accuracy 87.439%\n",
      "Epoch 19, Batch 206, LR 2.114159 Loss 5.553182, Accuracy 87.462%\n",
      "Epoch 19, Batch 207, LR 2.114063 Loss 5.549655, Accuracy 87.485%\n",
      "Epoch 19, Batch 208, LR 2.113966 Loss 5.549577, Accuracy 87.489%\n",
      "Epoch 19, Batch 209, LR 2.113869 Loss 5.548060, Accuracy 87.478%\n",
      "Epoch 19, Batch 210, LR 2.113772 Loss 5.547079, Accuracy 87.496%\n",
      "Epoch 19, Batch 211, LR 2.113675 Loss 5.547117, Accuracy 87.511%\n",
      "Epoch 19, Batch 212, LR 2.113578 Loss 5.551952, Accuracy 87.489%\n",
      "Epoch 19, Batch 213, LR 2.113482 Loss 5.551190, Accuracy 87.500%\n",
      "Epoch 19, Batch 214, LR 2.113385 Loss 5.550171, Accuracy 87.500%\n",
      "Epoch 19, Batch 215, LR 2.113288 Loss 5.550981, Accuracy 87.496%\n",
      "Epoch 19, Batch 216, LR 2.113191 Loss 5.550851, Accuracy 87.500%\n",
      "Epoch 19, Batch 217, LR 2.113094 Loss 5.549838, Accuracy 87.511%\n",
      "Epoch 19, Batch 218, LR 2.112997 Loss 5.550600, Accuracy 87.511%\n",
      "Epoch 19, Batch 219, LR 2.112900 Loss 5.549574, Accuracy 87.518%\n",
      "Epoch 19, Batch 220, LR 2.112803 Loss 5.549979, Accuracy 87.504%\n",
      "Epoch 19, Batch 221, LR 2.112706 Loss 5.550191, Accuracy 87.504%\n",
      "Epoch 19, Batch 222, LR 2.112610 Loss 5.547302, Accuracy 87.518%\n",
      "Epoch 19, Batch 223, LR 2.112513 Loss 5.545792, Accuracy 87.539%\n",
      "Epoch 19, Batch 224, LR 2.112416 Loss 5.545261, Accuracy 87.531%\n",
      "Epoch 19, Batch 225, LR 2.112319 Loss 5.546896, Accuracy 87.528%\n",
      "Epoch 19, Batch 226, LR 2.112222 Loss 5.546283, Accuracy 87.521%\n",
      "Epoch 19, Batch 227, LR 2.112125 Loss 5.544861, Accuracy 87.528%\n",
      "Epoch 19, Batch 228, LR 2.112028 Loss 5.546221, Accuracy 87.510%\n",
      "Epoch 19, Batch 229, LR 2.111931 Loss 5.546896, Accuracy 87.503%\n",
      "Epoch 19, Batch 230, LR 2.111834 Loss 5.545003, Accuracy 87.517%\n",
      "Epoch 19, Batch 231, LR 2.111737 Loss 5.547853, Accuracy 87.500%\n",
      "Epoch 19, Batch 232, LR 2.111640 Loss 5.548386, Accuracy 87.490%\n",
      "Epoch 19, Batch 233, LR 2.111543 Loss 5.548696, Accuracy 87.473%\n",
      "Epoch 19, Batch 234, LR 2.111445 Loss 5.548955, Accuracy 87.467%\n",
      "Epoch 19, Batch 235, LR 2.111348 Loss 5.549104, Accuracy 87.463%\n",
      "Epoch 19, Batch 236, LR 2.111251 Loss 5.548994, Accuracy 87.467%\n",
      "Epoch 19, Batch 237, LR 2.111154 Loss 5.551301, Accuracy 87.460%\n",
      "Epoch 19, Batch 238, LR 2.111057 Loss 5.553492, Accuracy 87.444%\n",
      "Epoch 19, Batch 239, LR 2.110960 Loss 5.548659, Accuracy 87.461%\n",
      "Epoch 19, Batch 240, LR 2.110863 Loss 5.548030, Accuracy 87.467%\n",
      "Epoch 19, Batch 241, LR 2.110766 Loss 5.552355, Accuracy 87.451%\n",
      "Epoch 19, Batch 242, LR 2.110669 Loss 5.552204, Accuracy 87.464%\n",
      "Epoch 19, Batch 243, LR 2.110571 Loss 5.553051, Accuracy 87.445%\n",
      "Epoch 19, Batch 244, LR 2.110474 Loss 5.550548, Accuracy 87.452%\n",
      "Epoch 19, Batch 245, LR 2.110377 Loss 5.552204, Accuracy 87.449%\n",
      "Epoch 19, Batch 246, LR 2.110280 Loss 5.552649, Accuracy 87.449%\n",
      "Epoch 19, Batch 247, LR 2.110183 Loss 5.554872, Accuracy 87.430%\n",
      "Epoch 19, Batch 248, LR 2.110086 Loss 5.554649, Accuracy 87.424%\n",
      "Epoch 19, Batch 249, LR 2.109988 Loss 5.553856, Accuracy 87.431%\n",
      "Epoch 19, Batch 250, LR 2.109891 Loss 5.552695, Accuracy 87.444%\n",
      "Epoch 19, Batch 251, LR 2.109794 Loss 5.551595, Accuracy 87.444%\n",
      "Epoch 19, Batch 252, LR 2.109697 Loss 5.548332, Accuracy 87.450%\n",
      "Epoch 19, Batch 253, LR 2.109599 Loss 5.548162, Accuracy 87.460%\n",
      "Epoch 19, Batch 254, LR 2.109502 Loss 5.550732, Accuracy 87.438%\n",
      "Epoch 19, Batch 255, LR 2.109405 Loss 5.554425, Accuracy 87.420%\n",
      "Epoch 19, Batch 256, LR 2.109308 Loss 5.554135, Accuracy 87.424%\n",
      "Epoch 19, Batch 257, LR 2.109210 Loss 5.553321, Accuracy 87.436%\n",
      "Epoch 19, Batch 258, LR 2.109113 Loss 5.553690, Accuracy 87.439%\n",
      "Epoch 19, Batch 259, LR 2.109016 Loss 5.553893, Accuracy 87.443%\n",
      "Epoch 19, Batch 260, LR 2.108918 Loss 5.554286, Accuracy 87.440%\n",
      "Epoch 19, Batch 261, LR 2.108821 Loss 5.553572, Accuracy 87.449%\n",
      "Epoch 19, Batch 262, LR 2.108724 Loss 5.551314, Accuracy 87.452%\n",
      "Epoch 19, Batch 263, LR 2.108626 Loss 5.552795, Accuracy 87.435%\n",
      "Epoch 19, Batch 264, LR 2.108529 Loss 5.552505, Accuracy 87.444%\n",
      "Epoch 19, Batch 265, LR 2.108432 Loss 5.553300, Accuracy 87.450%\n",
      "Epoch 19, Batch 266, LR 2.108334 Loss 5.551232, Accuracy 87.456%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 267, LR 2.108237 Loss 5.550587, Accuracy 87.465%\n",
      "Epoch 19, Batch 268, LR 2.108140 Loss 5.552090, Accuracy 87.450%\n",
      "Epoch 19, Batch 269, LR 2.108042 Loss 5.553001, Accuracy 87.442%\n",
      "Epoch 19, Batch 270, LR 2.107945 Loss 5.557343, Accuracy 87.410%\n",
      "Epoch 19, Batch 271, LR 2.107847 Loss 5.557679, Accuracy 87.408%\n",
      "Epoch 19, Batch 272, LR 2.107750 Loss 5.557253, Accuracy 87.399%\n",
      "Epoch 19, Batch 273, LR 2.107652 Loss 5.555008, Accuracy 87.406%\n",
      "Epoch 19, Batch 274, LR 2.107555 Loss 5.554517, Accuracy 87.412%\n",
      "Epoch 19, Batch 275, LR 2.107458 Loss 5.552514, Accuracy 87.418%\n",
      "Epoch 19, Batch 276, LR 2.107360 Loss 5.552784, Accuracy 87.429%\n",
      "Epoch 19, Batch 277, LR 2.107263 Loss 5.552071, Accuracy 87.429%\n",
      "Epoch 19, Batch 278, LR 2.107165 Loss 5.552233, Accuracy 87.427%\n",
      "Epoch 19, Batch 279, LR 2.107068 Loss 5.551224, Accuracy 87.447%\n",
      "Epoch 19, Batch 280, LR 2.106970 Loss 5.552393, Accuracy 87.430%\n",
      "Epoch 19, Batch 281, LR 2.106873 Loss 5.550969, Accuracy 87.436%\n",
      "Epoch 19, Batch 282, LR 2.106775 Loss 5.552502, Accuracy 87.417%\n",
      "Epoch 19, Batch 283, LR 2.106677 Loss 5.551731, Accuracy 87.423%\n",
      "Epoch 19, Batch 284, LR 2.106580 Loss 5.551054, Accuracy 87.423%\n",
      "Epoch 19, Batch 285, LR 2.106482 Loss 5.548762, Accuracy 87.429%\n",
      "Epoch 19, Batch 286, LR 2.106385 Loss 5.549747, Accuracy 87.434%\n",
      "Epoch 19, Batch 287, LR 2.106287 Loss 5.546933, Accuracy 87.451%\n",
      "Epoch 19, Batch 288, LR 2.106190 Loss 5.547830, Accuracy 87.454%\n",
      "Epoch 19, Batch 289, LR 2.106092 Loss 5.549781, Accuracy 87.446%\n",
      "Epoch 19, Batch 290, LR 2.105994 Loss 5.549433, Accuracy 87.452%\n",
      "Epoch 19, Batch 291, LR 2.105897 Loss 5.547377, Accuracy 87.465%\n",
      "Epoch 19, Batch 292, LR 2.105799 Loss 5.544825, Accuracy 87.481%\n",
      "Epoch 19, Batch 293, LR 2.105701 Loss 5.545652, Accuracy 87.479%\n",
      "Epoch 19, Batch 294, LR 2.105604 Loss 5.544848, Accuracy 87.489%\n",
      "Epoch 19, Batch 295, LR 2.105506 Loss 5.545830, Accuracy 87.481%\n",
      "Epoch 19, Batch 296, LR 2.105408 Loss 5.547351, Accuracy 87.476%\n",
      "Epoch 19, Batch 297, LR 2.105311 Loss 5.547359, Accuracy 87.474%\n",
      "Epoch 19, Batch 298, LR 2.105213 Loss 5.546463, Accuracy 87.479%\n",
      "Epoch 19, Batch 299, LR 2.105115 Loss 5.545599, Accuracy 87.492%\n",
      "Epoch 19, Batch 300, LR 2.105018 Loss 5.547017, Accuracy 87.469%\n",
      "Epoch 19, Batch 301, LR 2.104920 Loss 5.545745, Accuracy 87.471%\n",
      "Epoch 19, Batch 302, LR 2.104822 Loss 5.542959, Accuracy 87.484%\n",
      "Epoch 19, Batch 303, LR 2.104725 Loss 5.541452, Accuracy 87.487%\n",
      "Epoch 19, Batch 304, LR 2.104627 Loss 5.540769, Accuracy 87.474%\n",
      "Epoch 19, Batch 305, LR 2.104529 Loss 5.541307, Accuracy 87.474%\n",
      "Epoch 19, Batch 306, LR 2.104431 Loss 5.540993, Accuracy 87.467%\n",
      "Epoch 19, Batch 307, LR 2.104333 Loss 5.538358, Accuracy 87.472%\n",
      "Epoch 19, Batch 308, LR 2.104236 Loss 5.537790, Accuracy 87.482%\n",
      "Epoch 19, Batch 309, LR 2.104138 Loss 5.536805, Accuracy 87.485%\n",
      "Epoch 19, Batch 310, LR 2.104040 Loss 5.536994, Accuracy 87.477%\n",
      "Epoch 19, Batch 311, LR 2.103942 Loss 5.535104, Accuracy 87.487%\n",
      "Epoch 19, Batch 312, LR 2.103844 Loss 5.533885, Accuracy 87.495%\n",
      "Epoch 19, Batch 313, LR 2.103747 Loss 5.534263, Accuracy 87.493%\n",
      "Epoch 19, Batch 314, LR 2.103649 Loss 5.535227, Accuracy 87.480%\n",
      "Epoch 19, Batch 315, LR 2.103551 Loss 5.535567, Accuracy 87.470%\n",
      "Epoch 19, Batch 316, LR 2.103453 Loss 5.533820, Accuracy 87.478%\n",
      "Epoch 19, Batch 317, LR 2.103355 Loss 5.534193, Accuracy 87.468%\n",
      "Epoch 19, Batch 318, LR 2.103257 Loss 5.535283, Accuracy 87.458%\n",
      "Epoch 19, Batch 319, LR 2.103159 Loss 5.535259, Accuracy 87.456%\n",
      "Epoch 19, Batch 320, LR 2.103061 Loss 5.533882, Accuracy 87.463%\n",
      "Epoch 19, Batch 321, LR 2.102964 Loss 5.534479, Accuracy 87.456%\n",
      "Epoch 19, Batch 322, LR 2.102866 Loss 5.532539, Accuracy 87.466%\n",
      "Epoch 19, Batch 323, LR 2.102768 Loss 5.532459, Accuracy 87.469%\n",
      "Epoch 19, Batch 324, LR 2.102670 Loss 5.532076, Accuracy 87.469%\n",
      "Epoch 19, Batch 325, LR 2.102572 Loss 5.531966, Accuracy 87.478%\n",
      "Epoch 19, Batch 326, LR 2.102474 Loss 5.532876, Accuracy 87.474%\n",
      "Epoch 19, Batch 327, LR 2.102376 Loss 5.530511, Accuracy 87.481%\n",
      "Epoch 19, Batch 328, LR 2.102278 Loss 5.531285, Accuracy 87.481%\n",
      "Epoch 19, Batch 329, LR 2.102180 Loss 5.532453, Accuracy 87.474%\n",
      "Epoch 19, Batch 330, LR 2.102082 Loss 5.533993, Accuracy 87.469%\n",
      "Epoch 19, Batch 331, LR 2.101984 Loss 5.533679, Accuracy 87.474%\n",
      "Epoch 19, Batch 332, LR 2.101886 Loss 5.534916, Accuracy 87.458%\n",
      "Epoch 19, Batch 333, LR 2.101788 Loss 5.536578, Accuracy 87.446%\n",
      "Epoch 19, Batch 334, LR 2.101690 Loss 5.536250, Accuracy 87.456%\n",
      "Epoch 19, Batch 335, LR 2.101592 Loss 5.534181, Accuracy 87.458%\n",
      "Epoch 19, Batch 336, LR 2.101494 Loss 5.532013, Accuracy 87.484%\n",
      "Epoch 19, Batch 337, LR 2.101396 Loss 5.532128, Accuracy 87.493%\n",
      "Epoch 19, Batch 338, LR 2.101297 Loss 5.531050, Accuracy 87.498%\n",
      "Epoch 19, Batch 339, LR 2.101199 Loss 5.531053, Accuracy 87.500%\n",
      "Epoch 19, Batch 340, LR 2.101101 Loss 5.531804, Accuracy 87.493%\n",
      "Epoch 19, Batch 341, LR 2.101003 Loss 5.532461, Accuracy 87.482%\n",
      "Epoch 19, Batch 342, LR 2.100905 Loss 5.533155, Accuracy 87.473%\n",
      "Epoch 19, Batch 343, LR 2.100807 Loss 5.533137, Accuracy 87.466%\n",
      "Epoch 19, Batch 344, LR 2.100709 Loss 5.532325, Accuracy 87.468%\n",
      "Epoch 19, Batch 345, LR 2.100611 Loss 5.532656, Accuracy 87.462%\n",
      "Epoch 19, Batch 346, LR 2.100512 Loss 5.532203, Accuracy 87.464%\n",
      "Epoch 19, Batch 347, LR 2.100414 Loss 5.534773, Accuracy 87.450%\n",
      "Epoch 19, Batch 348, LR 2.100316 Loss 5.533347, Accuracy 87.462%\n",
      "Epoch 19, Batch 349, LR 2.100218 Loss 5.536140, Accuracy 87.435%\n",
      "Epoch 19, Batch 350, LR 2.100120 Loss 5.534705, Accuracy 87.446%\n",
      "Epoch 19, Batch 351, LR 2.100022 Loss 5.534407, Accuracy 87.449%\n",
      "Epoch 19, Batch 352, LR 2.099923 Loss 5.534979, Accuracy 87.438%\n",
      "Epoch 19, Batch 353, LR 2.099825 Loss 5.535733, Accuracy 87.438%\n",
      "Epoch 19, Batch 354, LR 2.099727 Loss 5.535214, Accuracy 87.443%\n",
      "Epoch 19, Batch 355, LR 2.099629 Loss 5.533401, Accuracy 87.458%\n",
      "Epoch 19, Batch 356, LR 2.099530 Loss 5.532825, Accuracy 87.460%\n",
      "Epoch 19, Batch 357, LR 2.099432 Loss 5.533348, Accuracy 87.452%\n",
      "Epoch 19, Batch 358, LR 2.099334 Loss 5.533287, Accuracy 87.441%\n",
      "Epoch 19, Batch 359, LR 2.099235 Loss 5.532887, Accuracy 87.441%\n",
      "Epoch 19, Batch 360, LR 2.099137 Loss 5.533146, Accuracy 87.446%\n",
      "Epoch 19, Batch 361, LR 2.099039 Loss 5.533874, Accuracy 87.431%\n",
      "Epoch 19, Batch 362, LR 2.098941 Loss 5.535122, Accuracy 87.418%\n",
      "Epoch 19, Batch 363, LR 2.098842 Loss 5.533839, Accuracy 87.416%\n",
      "Epoch 19, Batch 364, LR 2.098744 Loss 5.535203, Accuracy 87.408%\n",
      "Epoch 19, Batch 365, LR 2.098646 Loss 5.534186, Accuracy 87.427%\n",
      "Epoch 19, Batch 366, LR 2.098547 Loss 5.535779, Accuracy 87.419%\n",
      "Epoch 19, Batch 367, LR 2.098449 Loss 5.534371, Accuracy 87.421%\n",
      "Epoch 19, Batch 368, LR 2.098350 Loss 5.534432, Accuracy 87.432%\n",
      "Epoch 19, Batch 369, LR 2.098252 Loss 5.534586, Accuracy 87.443%\n",
      "Epoch 19, Batch 370, LR 2.098154 Loss 5.535348, Accuracy 87.441%\n",
      "Epoch 19, Batch 371, LR 2.098055 Loss 5.536024, Accuracy 87.435%\n",
      "Epoch 19, Batch 372, LR 2.097957 Loss 5.535859, Accuracy 87.441%\n",
      "Epoch 19, Batch 373, LR 2.097858 Loss 5.536437, Accuracy 87.427%\n",
      "Epoch 19, Batch 374, LR 2.097760 Loss 5.537111, Accuracy 87.410%\n",
      "Epoch 19, Batch 375, LR 2.097662 Loss 5.537867, Accuracy 87.410%\n",
      "Epoch 19, Batch 376, LR 2.097563 Loss 5.537085, Accuracy 87.406%\n",
      "Epoch 19, Batch 377, LR 2.097465 Loss 5.540079, Accuracy 87.392%\n",
      "Epoch 19, Batch 378, LR 2.097366 Loss 5.539857, Accuracy 87.397%\n",
      "Epoch 19, Batch 379, LR 2.097268 Loss 5.539138, Accuracy 87.401%\n",
      "Epoch 19, Batch 380, LR 2.097169 Loss 5.539841, Accuracy 87.389%\n",
      "Epoch 19, Batch 381, LR 2.097071 Loss 5.539441, Accuracy 87.383%\n",
      "Epoch 19, Batch 382, LR 2.096972 Loss 5.539818, Accuracy 87.375%\n",
      "Epoch 19, Batch 383, LR 2.096874 Loss 5.538537, Accuracy 87.390%\n",
      "Epoch 19, Batch 384, LR 2.096775 Loss 5.538417, Accuracy 87.384%\n",
      "Epoch 19, Batch 385, LR 2.096677 Loss 5.539588, Accuracy 87.376%\n",
      "Epoch 19, Batch 386, LR 2.096578 Loss 5.542623, Accuracy 87.354%\n",
      "Epoch 19, Batch 387, LR 2.096479 Loss 5.543008, Accuracy 87.353%\n",
      "Epoch 19, Batch 388, LR 2.096381 Loss 5.543267, Accuracy 87.359%\n",
      "Epoch 19, Batch 389, LR 2.096282 Loss 5.543196, Accuracy 87.357%\n",
      "Epoch 19, Batch 390, LR 2.096184 Loss 5.541770, Accuracy 87.370%\n",
      "Epoch 19, Batch 391, LR 2.096085 Loss 5.541392, Accuracy 87.374%\n",
      "Epoch 19, Batch 392, LR 2.095987 Loss 5.542211, Accuracy 87.374%\n",
      "Epoch 19, Batch 393, LR 2.095888 Loss 5.542356, Accuracy 87.375%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 394, LR 2.095789 Loss 5.543570, Accuracy 87.369%\n",
      "Epoch 19, Batch 395, LR 2.095691 Loss 5.542919, Accuracy 87.379%\n",
      "Epoch 19, Batch 396, LR 2.095592 Loss 5.544518, Accuracy 87.368%\n",
      "Epoch 19, Batch 397, LR 2.095493 Loss 5.544285, Accuracy 87.360%\n",
      "Epoch 19, Batch 398, LR 2.095395 Loss 5.545285, Accuracy 87.361%\n",
      "Epoch 19, Batch 399, LR 2.095296 Loss 5.544910, Accuracy 87.361%\n",
      "Epoch 19, Batch 400, LR 2.095197 Loss 5.543010, Accuracy 87.367%\n",
      "Epoch 19, Batch 401, LR 2.095099 Loss 5.542019, Accuracy 87.379%\n",
      "Epoch 19, Batch 402, LR 2.095000 Loss 5.542721, Accuracy 87.378%\n",
      "Epoch 19, Batch 403, LR 2.094901 Loss 5.544679, Accuracy 87.364%\n",
      "Epoch 19, Batch 404, LR 2.094802 Loss 5.546927, Accuracy 87.355%\n",
      "Epoch 19, Batch 405, LR 2.094704 Loss 5.545773, Accuracy 87.365%\n",
      "Epoch 19, Batch 406, LR 2.094605 Loss 5.543778, Accuracy 87.365%\n",
      "Epoch 19, Batch 407, LR 2.094506 Loss 5.545495, Accuracy 87.356%\n",
      "Epoch 19, Batch 408, LR 2.094407 Loss 5.547029, Accuracy 87.356%\n",
      "Epoch 19, Batch 409, LR 2.094309 Loss 5.545941, Accuracy 87.362%\n",
      "Epoch 19, Batch 410, LR 2.094210 Loss 5.546047, Accuracy 87.363%\n",
      "Epoch 19, Batch 411, LR 2.094111 Loss 5.546544, Accuracy 87.359%\n",
      "Epoch 19, Batch 412, LR 2.094012 Loss 5.546556, Accuracy 87.358%\n",
      "Epoch 19, Batch 413, LR 2.093914 Loss 5.545818, Accuracy 87.360%\n",
      "Epoch 19, Batch 414, LR 2.093815 Loss 5.546404, Accuracy 87.345%\n",
      "Epoch 19, Batch 415, LR 2.093716 Loss 5.544305, Accuracy 87.355%\n",
      "Epoch 19, Batch 416, LR 2.093617 Loss 5.544335, Accuracy 87.352%\n",
      "Epoch 19, Batch 417, LR 2.093518 Loss 5.542785, Accuracy 87.359%\n",
      "Epoch 19, Batch 418, LR 2.093419 Loss 5.544479, Accuracy 87.360%\n",
      "Epoch 19, Batch 419, LR 2.093320 Loss 5.544857, Accuracy 87.353%\n",
      "Epoch 19, Batch 420, LR 2.093222 Loss 5.545409, Accuracy 87.353%\n",
      "Epoch 19, Batch 421, LR 2.093123 Loss 5.546000, Accuracy 87.348%\n",
      "Epoch 19, Batch 422, LR 2.093024 Loss 5.547125, Accuracy 87.343%\n",
      "Epoch 19, Batch 423, LR 2.092925 Loss 5.546176, Accuracy 87.343%\n",
      "Epoch 19, Batch 424, LR 2.092826 Loss 5.546631, Accuracy 87.338%\n",
      "Epoch 19, Batch 425, LR 2.092727 Loss 5.545896, Accuracy 87.338%\n",
      "Epoch 19, Batch 426, LR 2.092628 Loss 5.546647, Accuracy 87.337%\n",
      "Epoch 19, Batch 427, LR 2.092529 Loss 5.546929, Accuracy 87.339%\n",
      "Epoch 19, Batch 428, LR 2.092430 Loss 5.548585, Accuracy 87.343%\n",
      "Epoch 19, Batch 429, LR 2.092331 Loss 5.548185, Accuracy 87.347%\n",
      "Epoch 19, Batch 430, LR 2.092232 Loss 5.547218, Accuracy 87.347%\n",
      "Epoch 19, Batch 431, LR 2.092133 Loss 5.547758, Accuracy 87.350%\n",
      "Epoch 19, Batch 432, LR 2.092034 Loss 5.547620, Accuracy 87.357%\n",
      "Epoch 19, Batch 433, LR 2.091935 Loss 5.548850, Accuracy 87.350%\n",
      "Epoch 19, Batch 434, LR 2.091836 Loss 5.549482, Accuracy 87.342%\n",
      "Epoch 19, Batch 435, LR 2.091737 Loss 5.549436, Accuracy 87.344%\n",
      "Epoch 19, Batch 436, LR 2.091638 Loss 5.549546, Accuracy 87.341%\n",
      "Epoch 19, Batch 437, LR 2.091539 Loss 5.549522, Accuracy 87.341%\n",
      "Epoch 19, Batch 438, LR 2.091440 Loss 5.550770, Accuracy 87.331%\n",
      "Epoch 19, Batch 439, LR 2.091341 Loss 5.551772, Accuracy 87.333%\n",
      "Epoch 19, Batch 440, LR 2.091242 Loss 5.551978, Accuracy 87.328%\n",
      "Epoch 19, Batch 441, LR 2.091143 Loss 5.551690, Accuracy 87.332%\n",
      "Epoch 19, Batch 442, LR 2.091044 Loss 5.551386, Accuracy 87.332%\n",
      "Epoch 19, Batch 443, LR 2.090945 Loss 5.551719, Accuracy 87.331%\n",
      "Epoch 19, Batch 444, LR 2.090846 Loss 5.550295, Accuracy 87.333%\n",
      "Epoch 19, Batch 445, LR 2.090746 Loss 5.549511, Accuracy 87.340%\n",
      "Epoch 19, Batch 446, LR 2.090647 Loss 5.548930, Accuracy 87.351%\n",
      "Epoch 19, Batch 447, LR 2.090548 Loss 5.548607, Accuracy 87.348%\n",
      "Epoch 19, Batch 448, LR 2.090449 Loss 5.547672, Accuracy 87.362%\n",
      "Epoch 19, Batch 449, LR 2.090350 Loss 5.546699, Accuracy 87.371%\n",
      "Epoch 19, Batch 450, LR 2.090251 Loss 5.547078, Accuracy 87.363%\n",
      "Epoch 19, Batch 451, LR 2.090151 Loss 5.547623, Accuracy 87.361%\n",
      "Epoch 19, Batch 452, LR 2.090052 Loss 5.546953, Accuracy 87.372%\n",
      "Epoch 19, Batch 453, LR 2.089953 Loss 5.549923, Accuracy 87.353%\n",
      "Epoch 19, Batch 454, LR 2.089854 Loss 5.548678, Accuracy 87.357%\n",
      "Epoch 19, Batch 455, LR 2.089755 Loss 5.547742, Accuracy 87.368%\n",
      "Epoch 19, Batch 456, LR 2.089655 Loss 5.547212, Accuracy 87.370%\n",
      "Epoch 19, Batch 457, LR 2.089556 Loss 5.547825, Accuracy 87.368%\n",
      "Epoch 19, Batch 458, LR 2.089457 Loss 5.545988, Accuracy 87.379%\n",
      "Epoch 19, Batch 459, LR 2.089358 Loss 5.545632, Accuracy 87.376%\n",
      "Epoch 19, Batch 460, LR 2.089258 Loss 5.546253, Accuracy 87.371%\n",
      "Epoch 19, Batch 461, LR 2.089159 Loss 5.545871, Accuracy 87.383%\n",
      "Epoch 19, Batch 462, LR 2.089060 Loss 5.547379, Accuracy 87.370%\n",
      "Epoch 19, Batch 463, LR 2.088961 Loss 5.547902, Accuracy 87.368%\n",
      "Epoch 19, Batch 464, LR 2.088861 Loss 5.547321, Accuracy 87.365%\n",
      "Epoch 19, Batch 465, LR 2.088762 Loss 5.546494, Accuracy 87.357%\n",
      "Epoch 19, Batch 466, LR 2.088663 Loss 5.545511, Accuracy 87.363%\n",
      "Epoch 19, Batch 467, LR 2.088563 Loss 5.545895, Accuracy 87.359%\n",
      "Epoch 19, Batch 468, LR 2.088464 Loss 5.545431, Accuracy 87.356%\n",
      "Epoch 19, Batch 469, LR 2.088365 Loss 5.544864, Accuracy 87.353%\n",
      "Epoch 19, Batch 470, LR 2.088265 Loss 5.545802, Accuracy 87.357%\n",
      "Epoch 19, Batch 471, LR 2.088166 Loss 5.545592, Accuracy 87.364%\n",
      "Epoch 19, Batch 472, LR 2.088066 Loss 5.546080, Accuracy 87.359%\n",
      "Epoch 19, Batch 473, LR 2.087967 Loss 5.546696, Accuracy 87.350%\n",
      "Epoch 19, Batch 474, LR 2.087868 Loss 5.546432, Accuracy 87.343%\n",
      "Epoch 19, Batch 475, LR 2.087768 Loss 5.546826, Accuracy 87.337%\n",
      "Epoch 19, Batch 476, LR 2.087669 Loss 5.547753, Accuracy 87.324%\n",
      "Epoch 19, Batch 477, LR 2.087569 Loss 5.547755, Accuracy 87.323%\n",
      "Epoch 19, Batch 478, LR 2.087470 Loss 5.547748, Accuracy 87.317%\n",
      "Epoch 19, Batch 479, LR 2.087371 Loss 5.548714, Accuracy 87.312%\n",
      "Epoch 19, Batch 480, LR 2.087271 Loss 5.550726, Accuracy 87.305%\n",
      "Epoch 19, Batch 481, LR 2.087172 Loss 5.550869, Accuracy 87.308%\n",
      "Epoch 19, Batch 482, LR 2.087072 Loss 5.551378, Accuracy 87.302%\n",
      "Epoch 19, Batch 483, LR 2.086973 Loss 5.551225, Accuracy 87.299%\n",
      "Epoch 19, Batch 484, LR 2.086873 Loss 5.551732, Accuracy 87.300%\n",
      "Epoch 19, Batch 485, LR 2.086774 Loss 5.552828, Accuracy 87.294%\n",
      "Epoch 19, Batch 486, LR 2.086674 Loss 5.551532, Accuracy 87.296%\n",
      "Epoch 19, Batch 487, LR 2.086575 Loss 5.550004, Accuracy 87.301%\n",
      "Epoch 19, Batch 488, LR 2.086475 Loss 5.549811, Accuracy 87.305%\n",
      "Epoch 19, Batch 489, LR 2.086376 Loss 5.550963, Accuracy 87.296%\n",
      "Epoch 19, Batch 490, LR 2.086276 Loss 5.551016, Accuracy 87.299%\n",
      "Epoch 19, Batch 491, LR 2.086176 Loss 5.549947, Accuracy 87.303%\n",
      "Epoch 19, Batch 492, LR 2.086077 Loss 5.549725, Accuracy 87.303%\n",
      "Epoch 19, Batch 493, LR 2.085977 Loss 5.550709, Accuracy 87.303%\n",
      "Epoch 19, Batch 494, LR 2.085878 Loss 5.551098, Accuracy 87.301%\n",
      "Epoch 19, Batch 495, LR 2.085778 Loss 5.551351, Accuracy 87.309%\n",
      "Epoch 19, Batch 496, LR 2.085678 Loss 5.550519, Accuracy 87.308%\n",
      "Epoch 19, Batch 497, LR 2.085579 Loss 5.552188, Accuracy 87.302%\n",
      "Epoch 19, Batch 498, LR 2.085479 Loss 5.553303, Accuracy 87.287%\n",
      "Epoch 19, Batch 499, LR 2.085380 Loss 5.554581, Accuracy 87.284%\n",
      "Epoch 19, Batch 500, LR 2.085280 Loss 5.554648, Accuracy 87.278%\n",
      "Epoch 19, Batch 501, LR 2.085180 Loss 5.552765, Accuracy 87.293%\n",
      "Epoch 19, Batch 502, LR 2.085081 Loss 5.552605, Accuracy 87.291%\n",
      "Epoch 19, Batch 503, LR 2.084981 Loss 5.549832, Accuracy 87.307%\n",
      "Epoch 19, Batch 504, LR 2.084881 Loss 5.549777, Accuracy 87.316%\n",
      "Epoch 19, Batch 505, LR 2.084781 Loss 5.550293, Accuracy 87.314%\n",
      "Epoch 19, Batch 506, LR 2.084682 Loss 5.549824, Accuracy 87.319%\n",
      "Epoch 19, Batch 507, LR 2.084582 Loss 5.550065, Accuracy 87.310%\n",
      "Epoch 19, Batch 508, LR 2.084482 Loss 5.550987, Accuracy 87.305%\n",
      "Epoch 19, Batch 509, LR 2.084383 Loss 5.548793, Accuracy 87.310%\n",
      "Epoch 19, Batch 510, LR 2.084283 Loss 5.549047, Accuracy 87.310%\n",
      "Epoch 19, Batch 511, LR 2.084183 Loss 5.549084, Accuracy 87.309%\n",
      "Epoch 19, Batch 512, LR 2.084083 Loss 5.550173, Accuracy 87.311%\n",
      "Epoch 19, Batch 513, LR 2.083984 Loss 5.550493, Accuracy 87.317%\n",
      "Epoch 19, Batch 514, LR 2.083884 Loss 5.551430, Accuracy 87.313%\n",
      "Epoch 19, Batch 515, LR 2.083784 Loss 5.552252, Accuracy 87.313%\n",
      "Epoch 19, Batch 516, LR 2.083684 Loss 5.552508, Accuracy 87.308%\n",
      "Epoch 19, Batch 517, LR 2.083584 Loss 5.551840, Accuracy 87.310%\n",
      "Epoch 19, Batch 518, LR 2.083485 Loss 5.551930, Accuracy 87.314%\n",
      "Epoch 19, Batch 519, LR 2.083385 Loss 5.551511, Accuracy 87.313%\n",
      "Epoch 19, Batch 520, LR 2.083285 Loss 5.552144, Accuracy 87.308%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 521, LR 2.083185 Loss 5.551225, Accuracy 87.317%\n",
      "Epoch 19, Batch 522, LR 2.083085 Loss 5.550705, Accuracy 87.320%\n",
      "Epoch 19, Batch 523, LR 2.082985 Loss 5.549961, Accuracy 87.325%\n",
      "Epoch 19, Batch 524, LR 2.082885 Loss 5.549965, Accuracy 87.336%\n",
      "Epoch 19, Batch 525, LR 2.082785 Loss 5.549759, Accuracy 87.336%\n",
      "Epoch 19, Batch 526, LR 2.082686 Loss 5.548609, Accuracy 87.341%\n",
      "Epoch 19, Batch 527, LR 2.082586 Loss 5.549334, Accuracy 87.330%\n",
      "Epoch 19, Batch 528, LR 2.082486 Loss 5.549782, Accuracy 87.330%\n",
      "Epoch 19, Batch 529, LR 2.082386 Loss 5.550898, Accuracy 87.324%\n",
      "Epoch 19, Batch 530, LR 2.082286 Loss 5.550756, Accuracy 87.325%\n",
      "Epoch 19, Batch 531, LR 2.082186 Loss 5.552517, Accuracy 87.313%\n",
      "Epoch 19, Batch 532, LR 2.082086 Loss 5.551641, Accuracy 87.313%\n",
      "Epoch 19, Batch 533, LR 2.081986 Loss 5.550364, Accuracy 87.320%\n",
      "Epoch 19, Batch 534, LR 2.081886 Loss 5.549722, Accuracy 87.324%\n",
      "Epoch 19, Batch 535, LR 2.081786 Loss 5.548621, Accuracy 87.323%\n",
      "Epoch 19, Batch 536, LR 2.081686 Loss 5.549508, Accuracy 87.312%\n",
      "Epoch 19, Batch 537, LR 2.081586 Loss 5.548225, Accuracy 87.312%\n",
      "Epoch 19, Batch 538, LR 2.081486 Loss 5.548694, Accuracy 87.307%\n",
      "Epoch 19, Batch 539, LR 2.081386 Loss 5.549348, Accuracy 87.304%\n",
      "Epoch 19, Batch 540, LR 2.081286 Loss 5.550101, Accuracy 87.302%\n",
      "Epoch 19, Batch 541, LR 2.081186 Loss 5.550645, Accuracy 87.295%\n",
      "Epoch 19, Batch 542, LR 2.081086 Loss 5.551390, Accuracy 87.290%\n",
      "Epoch 19, Batch 543, LR 2.080986 Loss 5.550884, Accuracy 87.283%\n",
      "Epoch 19, Batch 544, LR 2.080886 Loss 5.549320, Accuracy 87.292%\n",
      "Epoch 19, Batch 545, LR 2.080786 Loss 5.548406, Accuracy 87.299%\n",
      "Epoch 19, Batch 546, LR 2.080686 Loss 5.549632, Accuracy 87.287%\n",
      "Epoch 19, Batch 547, LR 2.080585 Loss 5.549831, Accuracy 87.287%\n",
      "Epoch 19, Batch 548, LR 2.080485 Loss 5.549413, Accuracy 87.290%\n",
      "Epoch 19, Batch 549, LR 2.080385 Loss 5.548768, Accuracy 87.294%\n",
      "Epoch 19, Batch 550, LR 2.080285 Loss 5.547543, Accuracy 87.300%\n",
      "Epoch 19, Batch 551, LR 2.080185 Loss 5.548128, Accuracy 87.304%\n",
      "Epoch 19, Batch 552, LR 2.080085 Loss 5.548115, Accuracy 87.302%\n",
      "Epoch 19, Batch 553, LR 2.079985 Loss 5.548491, Accuracy 87.306%\n",
      "Epoch 19, Batch 554, LR 2.079884 Loss 5.548456, Accuracy 87.304%\n",
      "Epoch 19, Batch 555, LR 2.079784 Loss 5.548502, Accuracy 87.299%\n",
      "Epoch 19, Batch 556, LR 2.079684 Loss 5.548623, Accuracy 87.302%\n",
      "Epoch 19, Batch 557, LR 2.079584 Loss 5.548678, Accuracy 87.306%\n",
      "Epoch 19, Batch 558, LR 2.079484 Loss 5.548094, Accuracy 87.304%\n",
      "Epoch 19, Batch 559, LR 2.079384 Loss 5.547859, Accuracy 87.304%\n",
      "Epoch 19, Batch 560, LR 2.079283 Loss 5.546558, Accuracy 87.310%\n",
      "Epoch 19, Batch 561, LR 2.079183 Loss 5.547725, Accuracy 87.309%\n",
      "Epoch 19, Batch 562, LR 2.079083 Loss 5.547918, Accuracy 87.312%\n",
      "Epoch 19, Batch 563, LR 2.078983 Loss 5.547176, Accuracy 87.317%\n",
      "Epoch 19, Batch 564, LR 2.078882 Loss 5.548306, Accuracy 87.305%\n",
      "Epoch 19, Batch 565, LR 2.078782 Loss 5.548486, Accuracy 87.304%\n",
      "Epoch 19, Batch 566, LR 2.078682 Loss 5.548615, Accuracy 87.297%\n",
      "Epoch 19, Batch 567, LR 2.078581 Loss 5.548693, Accuracy 87.293%\n",
      "Epoch 19, Batch 568, LR 2.078481 Loss 5.548074, Accuracy 87.296%\n",
      "Epoch 19, Batch 569, LR 2.078381 Loss 5.548108, Accuracy 87.301%\n",
      "Epoch 19, Batch 570, LR 2.078281 Loss 5.548739, Accuracy 87.301%\n",
      "Epoch 19, Batch 571, LR 2.078180 Loss 5.548337, Accuracy 87.306%\n",
      "Epoch 19, Batch 572, LR 2.078080 Loss 5.548714, Accuracy 87.305%\n",
      "Epoch 19, Batch 573, LR 2.077979 Loss 5.548291, Accuracy 87.306%\n",
      "Epoch 19, Batch 574, LR 2.077879 Loss 5.547718, Accuracy 87.311%\n",
      "Epoch 19, Batch 575, LR 2.077779 Loss 5.548636, Accuracy 87.307%\n",
      "Epoch 19, Batch 576, LR 2.077678 Loss 5.548939, Accuracy 87.314%\n",
      "Epoch 19, Batch 577, LR 2.077578 Loss 5.549375, Accuracy 87.309%\n",
      "Epoch 19, Batch 578, LR 2.077478 Loss 5.549202, Accuracy 87.305%\n",
      "Epoch 19, Batch 579, LR 2.077377 Loss 5.549283, Accuracy 87.303%\n",
      "Epoch 19, Batch 580, LR 2.077277 Loss 5.550438, Accuracy 87.299%\n",
      "Epoch 19, Batch 581, LR 2.077176 Loss 5.552289, Accuracy 87.279%\n",
      "Epoch 19, Batch 582, LR 2.077076 Loss 5.552828, Accuracy 87.280%\n",
      "Epoch 19, Batch 583, LR 2.076975 Loss 5.552965, Accuracy 87.283%\n",
      "Epoch 19, Batch 584, LR 2.076875 Loss 5.553400, Accuracy 87.287%\n",
      "Epoch 19, Batch 585, LR 2.076775 Loss 5.553640, Accuracy 87.290%\n",
      "Epoch 19, Batch 586, LR 2.076674 Loss 5.553844, Accuracy 87.291%\n",
      "Epoch 19, Batch 587, LR 2.076574 Loss 5.554636, Accuracy 87.296%\n",
      "Epoch 19, Batch 588, LR 2.076473 Loss 5.553472, Accuracy 87.299%\n",
      "Epoch 19, Batch 589, LR 2.076373 Loss 5.554604, Accuracy 87.296%\n",
      "Epoch 19, Batch 590, LR 2.076272 Loss 5.554565, Accuracy 87.301%\n",
      "Epoch 19, Batch 591, LR 2.076172 Loss 5.554158, Accuracy 87.298%\n",
      "Epoch 19, Batch 592, LR 2.076071 Loss 5.554204, Accuracy 87.301%\n",
      "Epoch 19, Batch 593, LR 2.075971 Loss 5.555555, Accuracy 87.293%\n",
      "Epoch 19, Batch 594, LR 2.075870 Loss 5.554854, Accuracy 87.290%\n",
      "Epoch 19, Batch 595, LR 2.075769 Loss 5.554843, Accuracy 87.287%\n",
      "Epoch 19, Batch 596, LR 2.075669 Loss 5.554152, Accuracy 87.289%\n",
      "Epoch 19, Batch 597, LR 2.075568 Loss 5.556051, Accuracy 87.279%\n",
      "Epoch 19, Batch 598, LR 2.075468 Loss 5.557069, Accuracy 87.274%\n",
      "Epoch 19, Batch 599, LR 2.075367 Loss 5.556972, Accuracy 87.278%\n",
      "Epoch 19, Batch 600, LR 2.075266 Loss 5.555989, Accuracy 87.289%\n",
      "Epoch 19, Batch 601, LR 2.075166 Loss 5.556482, Accuracy 87.284%\n",
      "Epoch 19, Batch 602, LR 2.075065 Loss 5.558899, Accuracy 87.272%\n",
      "Epoch 19, Batch 603, LR 2.074965 Loss 5.559706, Accuracy 87.271%\n",
      "Epoch 19, Batch 604, LR 2.074864 Loss 5.559408, Accuracy 87.275%\n",
      "Epoch 19, Batch 605, LR 2.074763 Loss 5.561178, Accuracy 87.264%\n",
      "Epoch 19, Batch 606, LR 2.074663 Loss 5.561972, Accuracy 87.258%\n",
      "Epoch 19, Batch 607, LR 2.074562 Loss 5.562390, Accuracy 87.257%\n",
      "Epoch 19, Batch 608, LR 2.074461 Loss 5.561764, Accuracy 87.255%\n",
      "Epoch 19, Batch 609, LR 2.074361 Loss 5.562960, Accuracy 87.245%\n",
      "Epoch 19, Batch 610, LR 2.074260 Loss 5.562502, Accuracy 87.248%\n",
      "Epoch 19, Batch 611, LR 2.074159 Loss 5.563031, Accuracy 87.244%\n",
      "Epoch 19, Batch 612, LR 2.074059 Loss 5.562061, Accuracy 87.247%\n",
      "Epoch 19, Batch 613, LR 2.073958 Loss 5.562066, Accuracy 87.250%\n",
      "Epoch 19, Batch 614, LR 2.073857 Loss 5.562233, Accuracy 87.248%\n",
      "Epoch 19, Batch 615, LR 2.073756 Loss 5.561182, Accuracy 87.252%\n",
      "Epoch 19, Batch 616, LR 2.073656 Loss 5.561330, Accuracy 87.244%\n",
      "Epoch 19, Batch 617, LR 2.073555 Loss 5.561176, Accuracy 87.242%\n",
      "Epoch 19, Batch 618, LR 2.073454 Loss 5.560980, Accuracy 87.238%\n",
      "Epoch 19, Batch 619, LR 2.073353 Loss 5.560538, Accuracy 87.239%\n",
      "Epoch 19, Batch 620, LR 2.073252 Loss 5.560715, Accuracy 87.238%\n",
      "Epoch 19, Batch 621, LR 2.073152 Loss 5.561056, Accuracy 87.240%\n",
      "Epoch 19, Batch 622, LR 2.073051 Loss 5.561853, Accuracy 87.237%\n",
      "Epoch 19, Batch 623, LR 2.072950 Loss 5.561429, Accuracy 87.239%\n",
      "Epoch 19, Batch 624, LR 2.072849 Loss 5.561309, Accuracy 87.231%\n",
      "Epoch 19, Batch 625, LR 2.072748 Loss 5.560481, Accuracy 87.241%\n",
      "Epoch 19, Batch 626, LR 2.072647 Loss 5.560855, Accuracy 87.238%\n",
      "Epoch 19, Batch 627, LR 2.072547 Loss 5.558983, Accuracy 87.251%\n",
      "Epoch 19, Batch 628, LR 2.072446 Loss 5.558530, Accuracy 87.252%\n",
      "Epoch 19, Batch 629, LR 2.072345 Loss 5.558795, Accuracy 87.250%\n",
      "Epoch 19, Batch 630, LR 2.072244 Loss 5.558076, Accuracy 87.251%\n",
      "Epoch 19, Batch 631, LR 2.072143 Loss 5.559554, Accuracy 87.240%\n",
      "Epoch 19, Batch 632, LR 2.072042 Loss 5.559452, Accuracy 87.239%\n",
      "Epoch 19, Batch 633, LR 2.071941 Loss 5.558401, Accuracy 87.242%\n",
      "Epoch 19, Batch 634, LR 2.071840 Loss 5.558690, Accuracy 87.247%\n",
      "Epoch 19, Batch 635, LR 2.071739 Loss 5.559154, Accuracy 87.242%\n",
      "Epoch 19, Batch 636, LR 2.071638 Loss 5.560405, Accuracy 87.242%\n",
      "Epoch 19, Batch 637, LR 2.071537 Loss 5.560743, Accuracy 87.241%\n",
      "Epoch 19, Batch 638, LR 2.071437 Loss 5.560480, Accuracy 87.239%\n",
      "Epoch 19, Batch 639, LR 2.071336 Loss 5.561192, Accuracy 87.236%\n",
      "Epoch 19, Batch 640, LR 2.071235 Loss 5.562219, Accuracy 87.227%\n",
      "Epoch 19, Batch 641, LR 2.071134 Loss 5.562381, Accuracy 87.222%\n",
      "Epoch 19, Batch 642, LR 2.071033 Loss 5.563582, Accuracy 87.212%\n",
      "Epoch 19, Batch 643, LR 2.070932 Loss 5.563596, Accuracy 87.214%\n",
      "Epoch 19, Batch 644, LR 2.070831 Loss 5.563034, Accuracy 87.221%\n",
      "Epoch 19, Batch 645, LR 2.070729 Loss 5.563539, Accuracy 87.220%\n",
      "Epoch 19, Batch 646, LR 2.070628 Loss 5.562558, Accuracy 87.225%\n",
      "Epoch 19, Batch 647, LR 2.070527 Loss 5.562339, Accuracy 87.230%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 648, LR 2.070426 Loss 5.562507, Accuracy 87.235%\n",
      "Epoch 19, Batch 649, LR 2.070325 Loss 5.562237, Accuracy 87.238%\n",
      "Epoch 19, Batch 650, LR 2.070224 Loss 5.562872, Accuracy 87.230%\n",
      "Epoch 19, Batch 651, LR 2.070123 Loss 5.562819, Accuracy 87.229%\n",
      "Epoch 19, Batch 652, LR 2.070022 Loss 5.562630, Accuracy 87.233%\n",
      "Epoch 19, Batch 653, LR 2.069921 Loss 5.563096, Accuracy 87.231%\n",
      "Epoch 19, Batch 654, LR 2.069820 Loss 5.563347, Accuracy 87.229%\n",
      "Epoch 19, Batch 655, LR 2.069719 Loss 5.562612, Accuracy 87.233%\n",
      "Epoch 19, Batch 656, LR 2.069618 Loss 5.563874, Accuracy 87.230%\n",
      "Epoch 19, Batch 657, LR 2.069516 Loss 5.564087, Accuracy 87.230%\n",
      "Epoch 19, Batch 658, LR 2.069415 Loss 5.564345, Accuracy 87.226%\n",
      "Epoch 19, Batch 659, LR 2.069314 Loss 5.564670, Accuracy 87.224%\n",
      "Epoch 19, Batch 660, LR 2.069213 Loss 5.563602, Accuracy 87.225%\n",
      "Epoch 19, Batch 661, LR 2.069112 Loss 5.563126, Accuracy 87.228%\n",
      "Epoch 19, Batch 662, LR 2.069011 Loss 5.563945, Accuracy 87.226%\n",
      "Epoch 19, Batch 663, LR 2.068909 Loss 5.564811, Accuracy 87.220%\n",
      "Epoch 19, Batch 664, LR 2.068808 Loss 5.564581, Accuracy 87.224%\n",
      "Epoch 19, Batch 665, LR 2.068707 Loss 5.564557, Accuracy 87.222%\n",
      "Epoch 19, Batch 666, LR 2.068606 Loss 5.564745, Accuracy 87.218%\n",
      "Epoch 19, Batch 667, LR 2.068504 Loss 5.564407, Accuracy 87.224%\n",
      "Epoch 19, Batch 668, LR 2.068403 Loss 5.564072, Accuracy 87.226%\n",
      "Epoch 19, Batch 669, LR 2.068302 Loss 5.564258, Accuracy 87.222%\n",
      "Epoch 19, Batch 670, LR 2.068201 Loss 5.564722, Accuracy 87.220%\n",
      "Epoch 19, Batch 671, LR 2.068099 Loss 5.565338, Accuracy 87.221%\n",
      "Epoch 19, Batch 672, LR 2.067998 Loss 5.564832, Accuracy 87.227%\n",
      "Epoch 19, Batch 673, LR 2.067897 Loss 5.564957, Accuracy 87.223%\n",
      "Epoch 19, Batch 674, LR 2.067796 Loss 5.565938, Accuracy 87.217%\n",
      "Epoch 19, Batch 675, LR 2.067694 Loss 5.565342, Accuracy 87.221%\n",
      "Epoch 19, Batch 676, LR 2.067593 Loss 5.566027, Accuracy 87.216%\n",
      "Epoch 19, Batch 677, LR 2.067492 Loss 5.566141, Accuracy 87.214%\n",
      "Epoch 19, Batch 678, LR 2.067390 Loss 5.565839, Accuracy 87.220%\n",
      "Epoch 19, Batch 679, LR 2.067289 Loss 5.565970, Accuracy 87.216%\n",
      "Epoch 19, Batch 680, LR 2.067188 Loss 5.566443, Accuracy 87.210%\n",
      "Epoch 19, Batch 681, LR 2.067086 Loss 5.566616, Accuracy 87.214%\n",
      "Epoch 19, Batch 682, LR 2.066985 Loss 5.567184, Accuracy 87.210%\n",
      "Epoch 19, Batch 683, LR 2.066883 Loss 5.567411, Accuracy 87.206%\n",
      "Epoch 19, Batch 684, LR 2.066782 Loss 5.566919, Accuracy 87.208%\n",
      "Epoch 19, Batch 685, LR 2.066681 Loss 5.566821, Accuracy 87.208%\n",
      "Epoch 19, Batch 686, LR 2.066579 Loss 5.567153, Accuracy 87.207%\n",
      "Epoch 19, Batch 687, LR 2.066478 Loss 5.568051, Accuracy 87.202%\n",
      "Epoch 19, Batch 688, LR 2.066376 Loss 5.568814, Accuracy 87.199%\n",
      "Epoch 19, Batch 689, LR 2.066275 Loss 5.568377, Accuracy 87.206%\n",
      "Epoch 19, Batch 690, LR 2.066173 Loss 5.568431, Accuracy 87.204%\n",
      "Epoch 19, Batch 691, LR 2.066072 Loss 5.567607, Accuracy 87.209%\n",
      "Epoch 19, Batch 692, LR 2.065970 Loss 5.567802, Accuracy 87.210%\n",
      "Epoch 19, Batch 693, LR 2.065869 Loss 5.569291, Accuracy 87.201%\n",
      "Epoch 19, Batch 694, LR 2.065767 Loss 5.569140, Accuracy 87.198%\n",
      "Epoch 19, Batch 695, LR 2.065666 Loss 5.570041, Accuracy 87.193%\n",
      "Epoch 19, Batch 696, LR 2.065564 Loss 5.570218, Accuracy 87.197%\n",
      "Epoch 19, Batch 697, LR 2.065463 Loss 5.570777, Accuracy 87.193%\n",
      "Epoch 19, Batch 698, LR 2.065361 Loss 5.571201, Accuracy 87.190%\n",
      "Epoch 19, Batch 699, LR 2.065260 Loss 5.571562, Accuracy 87.187%\n",
      "Epoch 19, Batch 700, LR 2.065158 Loss 5.571996, Accuracy 87.185%\n",
      "Epoch 19, Batch 701, LR 2.065057 Loss 5.571535, Accuracy 87.187%\n",
      "Epoch 19, Batch 702, LR 2.064955 Loss 5.571223, Accuracy 87.190%\n",
      "Epoch 19, Batch 703, LR 2.064854 Loss 5.570783, Accuracy 87.192%\n",
      "Epoch 19, Batch 704, LR 2.064752 Loss 5.570446, Accuracy 87.196%\n",
      "Epoch 19, Batch 705, LR 2.064650 Loss 5.570854, Accuracy 87.194%\n",
      "Epoch 19, Batch 706, LR 2.064549 Loss 5.570963, Accuracy 87.193%\n",
      "Epoch 19, Batch 707, LR 2.064447 Loss 5.570370, Accuracy 87.199%\n",
      "Epoch 19, Batch 708, LR 2.064346 Loss 5.570027, Accuracy 87.202%\n",
      "Epoch 19, Batch 709, LR 2.064244 Loss 5.570299, Accuracy 87.200%\n",
      "Epoch 19, Batch 710, LR 2.064142 Loss 5.570728, Accuracy 87.199%\n",
      "Epoch 19, Batch 711, LR 2.064041 Loss 5.570513, Accuracy 87.192%\n",
      "Epoch 19, Batch 712, LR 2.063939 Loss 5.571347, Accuracy 87.189%\n",
      "Epoch 19, Batch 713, LR 2.063837 Loss 5.570420, Accuracy 87.195%\n",
      "Epoch 19, Batch 714, LR 2.063736 Loss 5.570578, Accuracy 87.194%\n",
      "Epoch 19, Batch 715, LR 2.063634 Loss 5.570849, Accuracy 87.197%\n",
      "Epoch 19, Batch 716, LR 2.063532 Loss 5.571912, Accuracy 87.191%\n",
      "Epoch 19, Batch 717, LR 2.063431 Loss 5.571523, Accuracy 87.192%\n",
      "Epoch 19, Batch 718, LR 2.063329 Loss 5.571829, Accuracy 87.190%\n",
      "Epoch 19, Batch 719, LR 2.063227 Loss 5.572203, Accuracy 87.190%\n",
      "Epoch 19, Batch 720, LR 2.063125 Loss 5.571562, Accuracy 87.193%\n",
      "Epoch 19, Batch 721, LR 2.063024 Loss 5.571542, Accuracy 87.198%\n",
      "Epoch 19, Batch 722, LR 2.062922 Loss 5.571756, Accuracy 87.199%\n",
      "Epoch 19, Batch 723, LR 2.062820 Loss 5.571483, Accuracy 87.201%\n",
      "Epoch 19, Batch 724, LR 2.062718 Loss 5.571283, Accuracy 87.201%\n",
      "Epoch 19, Batch 725, LR 2.062617 Loss 5.571693, Accuracy 87.195%\n",
      "Epoch 19, Batch 726, LR 2.062515 Loss 5.571126, Accuracy 87.201%\n",
      "Epoch 19, Batch 727, LR 2.062413 Loss 5.570604, Accuracy 87.207%\n",
      "Epoch 19, Batch 728, LR 2.062311 Loss 5.568779, Accuracy 87.215%\n",
      "Epoch 19, Batch 729, LR 2.062209 Loss 5.568684, Accuracy 87.215%\n",
      "Epoch 19, Batch 730, LR 2.062108 Loss 5.568804, Accuracy 87.217%\n",
      "Epoch 19, Batch 731, LR 2.062006 Loss 5.569471, Accuracy 87.213%\n",
      "Epoch 19, Batch 732, LR 2.061904 Loss 5.569216, Accuracy 87.219%\n",
      "Epoch 19, Batch 733, LR 2.061802 Loss 5.569393, Accuracy 87.218%\n",
      "Epoch 19, Batch 734, LR 2.061700 Loss 5.569576, Accuracy 87.216%\n",
      "Epoch 19, Batch 735, LR 2.061598 Loss 5.569307, Accuracy 87.215%\n",
      "Epoch 19, Batch 736, LR 2.061496 Loss 5.568560, Accuracy 87.216%\n",
      "Epoch 19, Batch 737, LR 2.061395 Loss 5.568372, Accuracy 87.220%\n",
      "Epoch 19, Batch 738, LR 2.061293 Loss 5.568464, Accuracy 87.221%\n",
      "Epoch 19, Batch 739, LR 2.061191 Loss 5.568179, Accuracy 87.226%\n",
      "Epoch 19, Batch 740, LR 2.061089 Loss 5.567796, Accuracy 87.230%\n",
      "Epoch 19, Batch 741, LR 2.060987 Loss 5.568401, Accuracy 87.227%\n",
      "Epoch 19, Batch 742, LR 2.060885 Loss 5.567961, Accuracy 87.234%\n",
      "Epoch 19, Batch 743, LR 2.060783 Loss 5.567592, Accuracy 87.237%\n",
      "Epoch 19, Batch 744, LR 2.060681 Loss 5.567836, Accuracy 87.235%\n",
      "Epoch 19, Batch 745, LR 2.060579 Loss 5.566742, Accuracy 87.247%\n",
      "Epoch 19, Batch 746, LR 2.060477 Loss 5.566737, Accuracy 87.249%\n",
      "Epoch 19, Batch 747, LR 2.060375 Loss 5.565261, Accuracy 87.258%\n",
      "Epoch 19, Batch 748, LR 2.060273 Loss 5.564927, Accuracy 87.264%\n",
      "Epoch 19, Batch 749, LR 2.060171 Loss 5.565319, Accuracy 87.259%\n",
      "Epoch 19, Batch 750, LR 2.060069 Loss 5.564800, Accuracy 87.258%\n",
      "Epoch 19, Batch 751, LR 2.059967 Loss 5.564644, Accuracy 87.257%\n",
      "Epoch 19, Batch 752, LR 2.059865 Loss 5.564634, Accuracy 87.255%\n",
      "Epoch 19, Batch 753, LR 2.059763 Loss 5.565839, Accuracy 87.249%\n",
      "Epoch 19, Batch 754, LR 2.059661 Loss 5.565164, Accuracy 87.250%\n",
      "Epoch 19, Batch 755, LR 2.059559 Loss 5.564659, Accuracy 87.256%\n",
      "Epoch 19, Batch 756, LR 2.059457 Loss 5.563757, Accuracy 87.263%\n",
      "Epoch 19, Batch 757, LR 2.059355 Loss 5.563233, Accuracy 87.262%\n",
      "Epoch 19, Batch 758, LR 2.059253 Loss 5.562658, Accuracy 87.258%\n",
      "Epoch 19, Batch 759, LR 2.059151 Loss 5.563208, Accuracy 87.257%\n",
      "Epoch 19, Batch 760, LR 2.059048 Loss 5.563462, Accuracy 87.254%\n",
      "Epoch 19, Batch 761, LR 2.058946 Loss 5.563319, Accuracy 87.256%\n",
      "Epoch 19, Batch 762, LR 2.058844 Loss 5.563537, Accuracy 87.252%\n",
      "Epoch 19, Batch 763, LR 2.058742 Loss 5.563617, Accuracy 87.252%\n",
      "Epoch 19, Batch 764, LR 2.058640 Loss 5.563567, Accuracy 87.255%\n",
      "Epoch 19, Batch 765, LR 2.058538 Loss 5.563860, Accuracy 87.252%\n",
      "Epoch 19, Batch 766, LR 2.058436 Loss 5.564644, Accuracy 87.244%\n",
      "Epoch 19, Batch 767, LR 2.058333 Loss 5.564899, Accuracy 87.243%\n",
      "Epoch 19, Batch 768, LR 2.058231 Loss 5.564188, Accuracy 87.250%\n",
      "Epoch 19, Batch 769, LR 2.058129 Loss 5.564331, Accuracy 87.251%\n",
      "Epoch 19, Batch 770, LR 2.058027 Loss 5.564746, Accuracy 87.249%\n",
      "Epoch 19, Batch 771, LR 2.057925 Loss 5.565057, Accuracy 87.247%\n",
      "Epoch 19, Batch 772, LR 2.057822 Loss 5.564676, Accuracy 87.253%\n",
      "Epoch 19, Batch 773, LR 2.057720 Loss 5.563977, Accuracy 87.260%\n",
      "Epoch 19, Batch 774, LR 2.057618 Loss 5.564276, Accuracy 87.257%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 775, LR 2.057516 Loss 5.564024, Accuracy 87.260%\n",
      "Epoch 19, Batch 776, LR 2.057413 Loss 5.563156, Accuracy 87.265%\n",
      "Epoch 19, Batch 777, LR 2.057311 Loss 5.564333, Accuracy 87.255%\n",
      "Epoch 19, Batch 778, LR 2.057209 Loss 5.564613, Accuracy 87.252%\n",
      "Epoch 19, Batch 779, LR 2.057107 Loss 5.564728, Accuracy 87.251%\n",
      "Epoch 19, Batch 780, LR 2.057004 Loss 5.565357, Accuracy 87.250%\n",
      "Epoch 19, Batch 781, LR 2.056902 Loss 5.565072, Accuracy 87.246%\n",
      "Epoch 19, Batch 782, LR 2.056800 Loss 5.564553, Accuracy 87.247%\n",
      "Epoch 19, Batch 783, LR 2.056697 Loss 5.564391, Accuracy 87.248%\n",
      "Epoch 19, Batch 784, LR 2.056595 Loss 5.564461, Accuracy 87.243%\n",
      "Epoch 19, Batch 785, LR 2.056493 Loss 5.564029, Accuracy 87.244%\n",
      "Epoch 19, Batch 786, LR 2.056390 Loss 5.563439, Accuracy 87.249%\n",
      "Epoch 19, Batch 787, LR 2.056288 Loss 5.563196, Accuracy 87.248%\n",
      "Epoch 19, Batch 788, LR 2.056186 Loss 5.563207, Accuracy 87.247%\n",
      "Epoch 19, Batch 789, LR 2.056083 Loss 5.562797, Accuracy 87.255%\n",
      "Epoch 19, Batch 790, LR 2.055981 Loss 5.563054, Accuracy 87.257%\n",
      "Epoch 19, Batch 791, LR 2.055879 Loss 5.563769, Accuracy 87.257%\n",
      "Epoch 19, Batch 792, LR 2.055776 Loss 5.564194, Accuracy 87.254%\n",
      "Epoch 19, Batch 793, LR 2.055674 Loss 5.564241, Accuracy 87.256%\n",
      "Epoch 19, Batch 794, LR 2.055571 Loss 5.564444, Accuracy 87.258%\n",
      "Epoch 19, Batch 795, LR 2.055469 Loss 5.564647, Accuracy 87.255%\n",
      "Epoch 19, Batch 796, LR 2.055366 Loss 5.564763, Accuracy 87.257%\n",
      "Epoch 19, Batch 797, LR 2.055264 Loss 5.565011, Accuracy 87.250%\n",
      "Epoch 19, Batch 798, LR 2.055162 Loss 5.565193, Accuracy 87.250%\n",
      "Epoch 19, Batch 799, LR 2.055059 Loss 5.566442, Accuracy 87.240%\n",
      "Epoch 19, Batch 800, LR 2.054957 Loss 5.566867, Accuracy 87.241%\n",
      "Epoch 19, Batch 801, LR 2.054854 Loss 5.567338, Accuracy 87.239%\n",
      "Epoch 19, Batch 802, LR 2.054752 Loss 5.567165, Accuracy 87.238%\n",
      "Epoch 19, Batch 803, LR 2.054649 Loss 5.567190, Accuracy 87.238%\n",
      "Epoch 19, Batch 804, LR 2.054547 Loss 5.565521, Accuracy 87.244%\n",
      "Epoch 19, Batch 805, LR 2.054444 Loss 5.565144, Accuracy 87.246%\n",
      "Epoch 19, Batch 806, LR 2.054342 Loss 5.565034, Accuracy 87.249%\n",
      "Epoch 19, Batch 807, LR 2.054239 Loss 5.565771, Accuracy 87.245%\n",
      "Epoch 19, Batch 808, LR 2.054136 Loss 5.566414, Accuracy 87.239%\n",
      "Epoch 19, Batch 809, LR 2.054034 Loss 5.566163, Accuracy 87.238%\n",
      "Epoch 19, Batch 810, LR 2.053931 Loss 5.565777, Accuracy 87.241%\n",
      "Epoch 19, Batch 811, LR 2.053829 Loss 5.565087, Accuracy 87.245%\n",
      "Epoch 19, Batch 812, LR 2.053726 Loss 5.565092, Accuracy 87.247%\n",
      "Epoch 19, Batch 813, LR 2.053624 Loss 5.565323, Accuracy 87.244%\n",
      "Epoch 19, Batch 814, LR 2.053521 Loss 5.564422, Accuracy 87.250%\n",
      "Epoch 19, Batch 815, LR 2.053418 Loss 5.565600, Accuracy 87.243%\n",
      "Epoch 19, Batch 816, LR 2.053316 Loss 5.564460, Accuracy 87.250%\n",
      "Epoch 19, Batch 817, LR 2.053213 Loss 5.564903, Accuracy 87.247%\n",
      "Epoch 19, Batch 818, LR 2.053110 Loss 5.565381, Accuracy 87.243%\n",
      "Epoch 19, Batch 819, LR 2.053008 Loss 5.565763, Accuracy 87.241%\n",
      "Epoch 19, Batch 820, LR 2.052905 Loss 5.566030, Accuracy 87.241%\n",
      "Epoch 19, Batch 821, LR 2.052802 Loss 5.565913, Accuracy 87.241%\n",
      "Epoch 19, Batch 822, LR 2.052700 Loss 5.566492, Accuracy 87.237%\n",
      "Epoch 19, Batch 823, LR 2.052597 Loss 5.566740, Accuracy 87.237%\n",
      "Epoch 19, Batch 824, LR 2.052494 Loss 5.566763, Accuracy 87.235%\n",
      "Epoch 19, Batch 825, LR 2.052392 Loss 5.566866, Accuracy 87.237%\n",
      "Epoch 19, Batch 826, LR 2.052289 Loss 5.567177, Accuracy 87.236%\n",
      "Epoch 19, Batch 827, LR 2.052186 Loss 5.567421, Accuracy 87.237%\n",
      "Epoch 19, Batch 828, LR 2.052083 Loss 5.567011, Accuracy 87.241%\n",
      "Epoch 19, Batch 829, LR 2.051981 Loss 5.567504, Accuracy 87.240%\n",
      "Epoch 19, Batch 830, LR 2.051878 Loss 5.567682, Accuracy 87.242%\n",
      "Epoch 19, Batch 831, LR 2.051775 Loss 5.568443, Accuracy 87.241%\n",
      "Epoch 19, Batch 832, LR 2.051672 Loss 5.568327, Accuracy 87.240%\n",
      "Epoch 19, Batch 833, LR 2.051570 Loss 5.568523, Accuracy 87.240%\n",
      "Epoch 19, Batch 834, LR 2.051467 Loss 5.569120, Accuracy 87.237%\n",
      "Epoch 19, Batch 835, LR 2.051364 Loss 5.568921, Accuracy 87.241%\n",
      "Epoch 19, Batch 836, LR 2.051261 Loss 5.568812, Accuracy 87.241%\n",
      "Epoch 19, Batch 837, LR 2.051158 Loss 5.568841, Accuracy 87.242%\n",
      "Epoch 19, Batch 838, LR 2.051056 Loss 5.568916, Accuracy 87.235%\n",
      "Epoch 19, Batch 839, LR 2.050953 Loss 5.568479, Accuracy 87.236%\n",
      "Epoch 19, Batch 840, LR 2.050850 Loss 5.568439, Accuracy 87.238%\n",
      "Epoch 19, Batch 841, LR 2.050747 Loss 5.568197, Accuracy 87.233%\n",
      "Epoch 19, Batch 842, LR 2.050644 Loss 5.568153, Accuracy 87.230%\n",
      "Epoch 19, Batch 843, LR 2.050541 Loss 5.568002, Accuracy 87.235%\n",
      "Epoch 19, Batch 844, LR 2.050438 Loss 5.568444, Accuracy 87.232%\n",
      "Epoch 19, Batch 845, LR 2.050336 Loss 5.569606, Accuracy 87.225%\n",
      "Epoch 19, Batch 846, LR 2.050233 Loss 5.569557, Accuracy 87.220%\n",
      "Epoch 19, Batch 847, LR 2.050130 Loss 5.569369, Accuracy 87.221%\n",
      "Epoch 19, Batch 848, LR 2.050027 Loss 5.569418, Accuracy 87.222%\n",
      "Epoch 19, Batch 849, LR 2.049924 Loss 5.568997, Accuracy 87.225%\n",
      "Epoch 19, Batch 850, LR 2.049821 Loss 5.569391, Accuracy 87.224%\n",
      "Epoch 19, Batch 851, LR 2.049718 Loss 5.569500, Accuracy 87.228%\n",
      "Epoch 19, Batch 852, LR 2.049615 Loss 5.569734, Accuracy 87.225%\n",
      "Epoch 19, Batch 853, LR 2.049512 Loss 5.570101, Accuracy 87.229%\n",
      "Epoch 19, Batch 854, LR 2.049409 Loss 5.570154, Accuracy 87.230%\n",
      "Epoch 19, Batch 855, LR 2.049306 Loss 5.569699, Accuracy 87.233%\n",
      "Epoch 19, Batch 856, LR 2.049203 Loss 5.569599, Accuracy 87.234%\n",
      "Epoch 19, Batch 857, LR 2.049100 Loss 5.568457, Accuracy 87.241%\n",
      "Epoch 19, Batch 858, LR 2.048997 Loss 5.569122, Accuracy 87.242%\n",
      "Epoch 19, Batch 859, LR 2.048894 Loss 5.568298, Accuracy 87.244%\n",
      "Epoch 19, Batch 860, LR 2.048791 Loss 5.567844, Accuracy 87.247%\n",
      "Epoch 19, Batch 861, LR 2.048688 Loss 5.567840, Accuracy 87.245%\n",
      "Epoch 19, Batch 862, LR 2.048585 Loss 5.567996, Accuracy 87.246%\n",
      "Epoch 19, Batch 863, LR 2.048482 Loss 5.567815, Accuracy 87.251%\n",
      "Epoch 19, Batch 864, LR 2.048379 Loss 5.568109, Accuracy 87.253%\n",
      "Epoch 19, Batch 865, LR 2.048276 Loss 5.567721, Accuracy 87.254%\n",
      "Epoch 19, Batch 866, LR 2.048173 Loss 5.566629, Accuracy 87.257%\n",
      "Epoch 19, Batch 867, LR 2.048070 Loss 5.567056, Accuracy 87.250%\n",
      "Epoch 19, Batch 868, LR 2.047967 Loss 5.566485, Accuracy 87.259%\n",
      "Epoch 19, Batch 869, LR 2.047863 Loss 5.566503, Accuracy 87.260%\n",
      "Epoch 19, Batch 870, LR 2.047760 Loss 5.565969, Accuracy 87.264%\n",
      "Epoch 19, Batch 871, LR 2.047657 Loss 5.567023, Accuracy 87.261%\n",
      "Epoch 19, Batch 872, LR 2.047554 Loss 5.566728, Accuracy 87.263%\n",
      "Epoch 19, Batch 873, LR 2.047451 Loss 5.565536, Accuracy 87.273%\n",
      "Epoch 19, Batch 874, LR 2.047348 Loss 5.565721, Accuracy 87.271%\n",
      "Epoch 19, Batch 875, LR 2.047245 Loss 5.566185, Accuracy 87.266%\n",
      "Epoch 19, Batch 876, LR 2.047141 Loss 5.567143, Accuracy 87.258%\n",
      "Epoch 19, Batch 877, LR 2.047038 Loss 5.566961, Accuracy 87.258%\n",
      "Epoch 19, Batch 878, LR 2.046935 Loss 5.566455, Accuracy 87.263%\n",
      "Epoch 19, Batch 879, LR 2.046832 Loss 5.566590, Accuracy 87.260%\n",
      "Epoch 19, Batch 880, LR 2.046729 Loss 5.567632, Accuracy 87.255%\n",
      "Epoch 19, Batch 881, LR 2.046625 Loss 5.567808, Accuracy 87.253%\n",
      "Epoch 19, Batch 882, LR 2.046522 Loss 5.567335, Accuracy 87.256%\n",
      "Epoch 19, Batch 883, LR 2.046419 Loss 5.566938, Accuracy 87.261%\n",
      "Epoch 19, Batch 884, LR 2.046316 Loss 5.566946, Accuracy 87.261%\n",
      "Epoch 19, Batch 885, LR 2.046212 Loss 5.566093, Accuracy 87.263%\n",
      "Epoch 19, Batch 886, LR 2.046109 Loss 5.565692, Accuracy 87.267%\n",
      "Epoch 19, Batch 887, LR 2.046006 Loss 5.565975, Accuracy 87.263%\n",
      "Epoch 19, Batch 888, LR 2.045903 Loss 5.565588, Accuracy 87.264%\n",
      "Epoch 19, Batch 889, LR 2.045799 Loss 5.565028, Accuracy 87.266%\n",
      "Epoch 19, Batch 890, LR 2.045696 Loss 5.565292, Accuracy 87.263%\n",
      "Epoch 19, Batch 891, LR 2.045593 Loss 5.565213, Accuracy 87.260%\n",
      "Epoch 19, Batch 892, LR 2.045489 Loss 5.565079, Accuracy 87.257%\n",
      "Epoch 19, Batch 893, LR 2.045386 Loss 5.564794, Accuracy 87.252%\n",
      "Epoch 19, Batch 894, LR 2.045283 Loss 5.564847, Accuracy 87.248%\n",
      "Epoch 19, Batch 895, LR 2.045179 Loss 5.564372, Accuracy 87.250%\n",
      "Epoch 19, Batch 896, LR 2.045076 Loss 5.564599, Accuracy 87.245%\n",
      "Epoch 19, Batch 897, LR 2.044973 Loss 5.563727, Accuracy 87.250%\n",
      "Epoch 19, Batch 898, LR 2.044869 Loss 5.563877, Accuracy 87.250%\n",
      "Epoch 19, Batch 899, LR 2.044766 Loss 5.564644, Accuracy 87.247%\n",
      "Epoch 19, Batch 900, LR 2.044662 Loss 5.564463, Accuracy 87.245%\n",
      "Epoch 19, Batch 901, LR 2.044559 Loss 5.564507, Accuracy 87.244%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 902, LR 2.044456 Loss 5.564121, Accuracy 87.249%\n",
      "Epoch 19, Batch 903, LR 2.044352 Loss 5.564706, Accuracy 87.247%\n",
      "Epoch 19, Batch 904, LR 2.044249 Loss 5.564016, Accuracy 87.251%\n",
      "Epoch 19, Batch 905, LR 2.044145 Loss 5.565000, Accuracy 87.247%\n",
      "Epoch 19, Batch 906, LR 2.044042 Loss 5.565375, Accuracy 87.250%\n",
      "Epoch 19, Batch 907, LR 2.043938 Loss 5.564555, Accuracy 87.253%\n",
      "Epoch 19, Batch 908, LR 2.043835 Loss 5.565043, Accuracy 87.250%\n",
      "Epoch 19, Batch 909, LR 2.043731 Loss 5.564915, Accuracy 87.247%\n",
      "Epoch 19, Batch 910, LR 2.043628 Loss 5.563913, Accuracy 87.251%\n",
      "Epoch 19, Batch 911, LR 2.043524 Loss 5.563998, Accuracy 87.256%\n",
      "Epoch 19, Batch 912, LR 2.043421 Loss 5.563554, Accuracy 87.259%\n",
      "Epoch 19, Batch 913, LR 2.043317 Loss 5.564000, Accuracy 87.254%\n",
      "Epoch 19, Batch 914, LR 2.043214 Loss 5.564770, Accuracy 87.251%\n",
      "Epoch 19, Batch 915, LR 2.043110 Loss 5.564867, Accuracy 87.251%\n",
      "Epoch 19, Batch 916, LR 2.043007 Loss 5.564372, Accuracy 87.251%\n",
      "Epoch 19, Batch 917, LR 2.042903 Loss 5.564300, Accuracy 87.251%\n",
      "Epoch 19, Batch 918, LR 2.042800 Loss 5.565246, Accuracy 87.240%\n",
      "Epoch 19, Batch 919, LR 2.042696 Loss 5.564647, Accuracy 87.244%\n",
      "Epoch 19, Batch 920, LR 2.042593 Loss 5.564854, Accuracy 87.243%\n",
      "Epoch 19, Batch 921, LR 2.042489 Loss 5.564824, Accuracy 87.240%\n",
      "Epoch 19, Batch 922, LR 2.042385 Loss 5.564808, Accuracy 87.236%\n",
      "Epoch 19, Batch 923, LR 2.042282 Loss 5.564972, Accuracy 87.239%\n",
      "Epoch 19, Batch 924, LR 2.042178 Loss 5.564442, Accuracy 87.244%\n",
      "Epoch 19, Batch 925, LR 2.042075 Loss 5.564163, Accuracy 87.245%\n",
      "Epoch 19, Batch 926, LR 2.041971 Loss 5.563803, Accuracy 87.248%\n",
      "Epoch 19, Batch 927, LR 2.041867 Loss 5.564155, Accuracy 87.245%\n",
      "Epoch 19, Batch 928, LR 2.041764 Loss 5.563892, Accuracy 87.242%\n",
      "Epoch 19, Batch 929, LR 2.041660 Loss 5.563699, Accuracy 87.247%\n",
      "Epoch 19, Batch 930, LR 2.041556 Loss 5.564159, Accuracy 87.245%\n",
      "Epoch 19, Batch 931, LR 2.041453 Loss 5.564268, Accuracy 87.243%\n",
      "Epoch 19, Batch 932, LR 2.041349 Loss 5.564122, Accuracy 87.246%\n",
      "Epoch 19, Batch 933, LR 2.041245 Loss 5.563987, Accuracy 87.245%\n",
      "Epoch 19, Batch 934, LR 2.041142 Loss 5.564021, Accuracy 87.248%\n",
      "Epoch 19, Batch 935, LR 2.041038 Loss 5.563539, Accuracy 87.250%\n",
      "Epoch 19, Batch 936, LR 2.040934 Loss 5.563383, Accuracy 87.252%\n",
      "Epoch 19, Batch 937, LR 2.040830 Loss 5.563606, Accuracy 87.249%\n",
      "Epoch 19, Batch 938, LR 2.040727 Loss 5.563450, Accuracy 87.251%\n",
      "Epoch 19, Batch 939, LR 2.040623 Loss 5.563240, Accuracy 87.253%\n",
      "Epoch 19, Batch 940, LR 2.040519 Loss 5.563211, Accuracy 87.253%\n",
      "Epoch 19, Batch 941, LR 2.040415 Loss 5.563599, Accuracy 87.253%\n",
      "Epoch 19, Batch 942, LR 2.040312 Loss 5.563083, Accuracy 87.255%\n",
      "Epoch 19, Batch 943, LR 2.040208 Loss 5.563245, Accuracy 87.254%\n",
      "Epoch 19, Batch 944, LR 2.040104 Loss 5.563352, Accuracy 87.258%\n",
      "Epoch 19, Batch 945, LR 2.040000 Loss 5.562598, Accuracy 87.260%\n",
      "Epoch 19, Batch 946, LR 2.039896 Loss 5.563668, Accuracy 87.252%\n",
      "Epoch 19, Batch 947, LR 2.039793 Loss 5.563741, Accuracy 87.251%\n",
      "Epoch 19, Batch 948, LR 2.039689 Loss 5.564094, Accuracy 87.251%\n",
      "Epoch 19, Batch 949, LR 2.039585 Loss 5.563619, Accuracy 87.251%\n",
      "Epoch 19, Batch 950, LR 2.039481 Loss 5.563948, Accuracy 87.252%\n",
      "Epoch 19, Batch 951, LR 2.039377 Loss 5.563936, Accuracy 87.251%\n",
      "Epoch 19, Batch 952, LR 2.039273 Loss 5.563295, Accuracy 87.253%\n",
      "Epoch 19, Batch 953, LR 2.039169 Loss 5.563303, Accuracy 87.248%\n",
      "Epoch 19, Batch 954, LR 2.039066 Loss 5.563255, Accuracy 87.251%\n",
      "Epoch 19, Batch 955, LR 2.038962 Loss 5.562487, Accuracy 87.253%\n",
      "Epoch 19, Batch 956, LR 2.038858 Loss 5.562550, Accuracy 87.253%\n",
      "Epoch 19, Batch 957, LR 2.038754 Loss 5.562845, Accuracy 87.254%\n",
      "Epoch 19, Batch 958, LR 2.038650 Loss 5.562511, Accuracy 87.256%\n",
      "Epoch 19, Batch 959, LR 2.038546 Loss 5.562062, Accuracy 87.260%\n",
      "Epoch 19, Batch 960, LR 2.038442 Loss 5.560812, Accuracy 87.266%\n",
      "Epoch 19, Batch 961, LR 2.038338 Loss 5.560879, Accuracy 87.266%\n",
      "Epoch 19, Batch 962, LR 2.038234 Loss 5.560941, Accuracy 87.269%\n",
      "Epoch 19, Batch 963, LR 2.038130 Loss 5.560462, Accuracy 87.274%\n",
      "Epoch 19, Batch 964, LR 2.038026 Loss 5.560424, Accuracy 87.278%\n",
      "Epoch 19, Batch 965, LR 2.037922 Loss 5.560288, Accuracy 87.280%\n",
      "Epoch 19, Batch 966, LR 2.037818 Loss 5.560248, Accuracy 87.282%\n",
      "Epoch 19, Batch 967, LR 2.037714 Loss 5.560126, Accuracy 87.284%\n",
      "Epoch 19, Batch 968, LR 2.037610 Loss 5.560799, Accuracy 87.278%\n",
      "Epoch 19, Batch 969, LR 2.037506 Loss 5.560854, Accuracy 87.277%\n",
      "Epoch 19, Batch 970, LR 2.037402 Loss 5.561205, Accuracy 87.277%\n",
      "Epoch 19, Batch 971, LR 2.037298 Loss 5.560859, Accuracy 87.276%\n",
      "Epoch 19, Batch 972, LR 2.037194 Loss 5.561067, Accuracy 87.273%\n",
      "Epoch 19, Batch 973, LR 2.037090 Loss 5.560718, Accuracy 87.274%\n",
      "Epoch 19, Batch 974, LR 2.036986 Loss 5.560242, Accuracy 87.277%\n",
      "Epoch 19, Batch 975, LR 2.036882 Loss 5.560304, Accuracy 87.276%\n",
      "Epoch 19, Batch 976, LR 2.036778 Loss 5.560083, Accuracy 87.278%\n",
      "Epoch 19, Batch 977, LR 2.036674 Loss 5.559319, Accuracy 87.284%\n",
      "Epoch 19, Batch 978, LR 2.036570 Loss 5.559349, Accuracy 87.284%\n",
      "Epoch 19, Batch 979, LR 2.036465 Loss 5.559642, Accuracy 87.284%\n",
      "Epoch 19, Batch 980, LR 2.036361 Loss 5.559549, Accuracy 87.282%\n",
      "Epoch 19, Batch 981, LR 2.036257 Loss 5.560388, Accuracy 87.277%\n",
      "Epoch 19, Batch 982, LR 2.036153 Loss 5.561244, Accuracy 87.275%\n",
      "Epoch 19, Batch 983, LR 2.036049 Loss 5.561992, Accuracy 87.274%\n",
      "Epoch 19, Batch 984, LR 2.035945 Loss 5.562331, Accuracy 87.274%\n",
      "Epoch 19, Batch 985, LR 2.035841 Loss 5.562475, Accuracy 87.273%\n",
      "Epoch 19, Batch 986, LR 2.035736 Loss 5.562590, Accuracy 87.273%\n",
      "Epoch 19, Batch 987, LR 2.035632 Loss 5.563044, Accuracy 87.271%\n",
      "Epoch 19, Batch 988, LR 2.035528 Loss 5.563342, Accuracy 87.269%\n",
      "Epoch 19, Batch 989, LR 2.035424 Loss 5.563167, Accuracy 87.271%\n",
      "Epoch 19, Batch 990, LR 2.035320 Loss 5.562905, Accuracy 87.273%\n",
      "Epoch 19, Batch 991, LR 2.035215 Loss 5.562975, Accuracy 87.271%\n",
      "Epoch 19, Batch 992, LR 2.035111 Loss 5.562898, Accuracy 87.268%\n",
      "Epoch 19, Batch 993, LR 2.035007 Loss 5.562037, Accuracy 87.270%\n",
      "Epoch 19, Batch 994, LR 2.034903 Loss 5.561905, Accuracy 87.273%\n",
      "Epoch 19, Batch 995, LR 2.034798 Loss 5.561985, Accuracy 87.273%\n",
      "Epoch 19, Batch 996, LR 2.034694 Loss 5.562780, Accuracy 87.268%\n",
      "Epoch 19, Batch 997, LR 2.034590 Loss 5.563386, Accuracy 87.263%\n",
      "Epoch 19, Batch 998, LR 2.034486 Loss 5.563937, Accuracy 87.257%\n",
      "Epoch 19, Batch 999, LR 2.034381 Loss 5.564664, Accuracy 87.253%\n",
      "Epoch 19, Batch 1000, LR 2.034277 Loss 5.564066, Accuracy 87.259%\n",
      "Epoch 19, Batch 1001, LR 2.034173 Loss 5.563870, Accuracy 87.260%\n",
      "Epoch 19, Batch 1002, LR 2.034068 Loss 5.564315, Accuracy 87.255%\n",
      "Epoch 19, Batch 1003, LR 2.033964 Loss 5.564304, Accuracy 87.254%\n",
      "Epoch 19, Batch 1004, LR 2.033860 Loss 5.564064, Accuracy 87.256%\n",
      "Epoch 19, Batch 1005, LR 2.033755 Loss 5.564267, Accuracy 87.252%\n",
      "Epoch 19, Batch 1006, LR 2.033651 Loss 5.564442, Accuracy 87.253%\n",
      "Epoch 19, Batch 1007, LR 2.033547 Loss 5.564381, Accuracy 87.253%\n",
      "Epoch 19, Batch 1008, LR 2.033442 Loss 5.564258, Accuracy 87.254%\n",
      "Epoch 19, Batch 1009, LR 2.033338 Loss 5.564479, Accuracy 87.250%\n",
      "Epoch 19, Batch 1010, LR 2.033233 Loss 5.564155, Accuracy 87.250%\n",
      "Epoch 19, Batch 1011, LR 2.033129 Loss 5.564267, Accuracy 87.250%\n",
      "Epoch 19, Batch 1012, LR 2.033025 Loss 5.564096, Accuracy 87.250%\n",
      "Epoch 19, Batch 1013, LR 2.032920 Loss 5.563615, Accuracy 87.253%\n",
      "Epoch 19, Batch 1014, LR 2.032816 Loss 5.563694, Accuracy 87.255%\n",
      "Epoch 19, Batch 1015, LR 2.032711 Loss 5.563969, Accuracy 87.255%\n",
      "Epoch 19, Batch 1016, LR 2.032607 Loss 5.564859, Accuracy 87.249%\n",
      "Epoch 19, Batch 1017, LR 2.032502 Loss 5.564016, Accuracy 87.250%\n",
      "Epoch 19, Batch 1018, LR 2.032398 Loss 5.564351, Accuracy 87.248%\n",
      "Epoch 19, Batch 1019, LR 2.032293 Loss 5.565043, Accuracy 87.244%\n",
      "Epoch 19, Batch 1020, LR 2.032189 Loss 5.565342, Accuracy 87.244%\n",
      "Epoch 19, Batch 1021, LR 2.032085 Loss 5.565950, Accuracy 87.239%\n",
      "Epoch 19, Batch 1022, LR 2.031980 Loss 5.566451, Accuracy 87.236%\n",
      "Epoch 19, Batch 1023, LR 2.031876 Loss 5.566709, Accuracy 87.240%\n",
      "Epoch 19, Batch 1024, LR 2.031771 Loss 5.566743, Accuracy 87.242%\n",
      "Epoch 19, Batch 1025, LR 2.031666 Loss 5.566343, Accuracy 87.242%\n",
      "Epoch 19, Batch 1026, LR 2.031562 Loss 5.566484, Accuracy 87.243%\n",
      "Epoch 19, Batch 1027, LR 2.031457 Loss 5.566369, Accuracy 87.247%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 1028, LR 2.031353 Loss 5.566262, Accuracy 87.249%\n",
      "Epoch 19, Batch 1029, LR 2.031248 Loss 5.567084, Accuracy 87.243%\n",
      "Epoch 19, Batch 1030, LR 2.031144 Loss 5.567060, Accuracy 87.249%\n",
      "Epoch 19, Batch 1031, LR 2.031039 Loss 5.566805, Accuracy 87.251%\n",
      "Epoch 19, Batch 1032, LR 2.030935 Loss 5.566599, Accuracy 87.252%\n",
      "Epoch 19, Batch 1033, LR 2.030830 Loss 5.566723, Accuracy 87.251%\n",
      "Epoch 19, Batch 1034, LR 2.030725 Loss 5.567282, Accuracy 87.249%\n",
      "Epoch 19, Batch 1035, LR 2.030621 Loss 5.567829, Accuracy 87.249%\n",
      "Epoch 19, Batch 1036, LR 2.030516 Loss 5.567291, Accuracy 87.250%\n",
      "Epoch 19, Batch 1037, LR 2.030411 Loss 5.567556, Accuracy 87.243%\n",
      "Epoch 19, Batch 1038, LR 2.030307 Loss 5.567529, Accuracy 87.243%\n",
      "Epoch 19, Batch 1039, LR 2.030202 Loss 5.566761, Accuracy 87.244%\n",
      "Epoch 19, Batch 1040, LR 2.030097 Loss 5.566914, Accuracy 87.242%\n",
      "Epoch 19, Batch 1041, LR 2.029993 Loss 5.566261, Accuracy 87.243%\n",
      "Epoch 19, Batch 1042, LR 2.029888 Loss 5.566152, Accuracy 87.242%\n",
      "Epoch 19, Batch 1043, LR 2.029783 Loss 5.565573, Accuracy 87.248%\n",
      "Epoch 19, Batch 1044, LR 2.029679 Loss 5.565243, Accuracy 87.252%\n",
      "Epoch 19, Batch 1045, LR 2.029574 Loss 5.565422, Accuracy 87.254%\n",
      "Epoch 19, Batch 1046, LR 2.029469 Loss 5.565533, Accuracy 87.252%\n",
      "Epoch 19, Batch 1047, LR 2.029365 Loss 5.565348, Accuracy 87.253%\n",
      "Epoch 19, Loss (train set) 5.565348, Accuracy (train set) 87.253%\n",
      "Epoch 19, Accuracy (validation set) 73.090%\n",
      "Epoch 20, Batch 1, LR 2.029260 Loss 5.424060, Accuracy 88.281%\n",
      "Epoch 20, Batch 2, LR 2.029155 Loss 5.369472, Accuracy 89.453%\n",
      "Epoch 20, Batch 3, LR 2.029050 Loss 5.380871, Accuracy 88.542%\n",
      "Epoch 20, Batch 4, LR 2.028946 Loss 5.270225, Accuracy 89.453%\n",
      "Epoch 20, Batch 5, LR 2.028841 Loss 5.291971, Accuracy 89.219%\n",
      "Epoch 20, Batch 6, LR 2.028736 Loss 5.383690, Accuracy 88.411%\n",
      "Epoch 20, Batch 7, LR 2.028631 Loss 5.340009, Accuracy 88.393%\n",
      "Epoch 20, Batch 8, LR 2.028526 Loss 5.322031, Accuracy 88.574%\n",
      "Epoch 20, Batch 9, LR 2.028422 Loss 5.342692, Accuracy 88.281%\n",
      "Epoch 20, Batch 10, LR 2.028317 Loss 5.343023, Accuracy 88.047%\n",
      "Epoch 20, Batch 11, LR 2.028212 Loss 5.350571, Accuracy 88.068%\n",
      "Epoch 20, Batch 12, LR 2.028107 Loss 5.312717, Accuracy 88.346%\n",
      "Epoch 20, Batch 13, LR 2.028002 Loss 5.322993, Accuracy 88.341%\n",
      "Epoch 20, Batch 14, LR 2.027898 Loss 5.332752, Accuracy 88.170%\n",
      "Epoch 20, Batch 15, LR 2.027793 Loss 5.361779, Accuracy 88.021%\n",
      "Epoch 20, Batch 16, LR 2.027688 Loss 5.360027, Accuracy 88.037%\n",
      "Epoch 20, Batch 17, LR 2.027583 Loss 5.345744, Accuracy 88.097%\n",
      "Epoch 20, Batch 18, LR 2.027478 Loss 5.367664, Accuracy 88.021%\n",
      "Epoch 20, Batch 19, LR 2.027373 Loss 5.339252, Accuracy 88.281%\n",
      "Epoch 20, Batch 20, LR 2.027268 Loss 5.380345, Accuracy 88.047%\n",
      "Epoch 20, Batch 21, LR 2.027163 Loss 5.379701, Accuracy 87.946%\n",
      "Epoch 20, Batch 22, LR 2.027058 Loss 5.358712, Accuracy 88.104%\n",
      "Epoch 20, Batch 23, LR 2.026953 Loss 5.355341, Accuracy 88.145%\n",
      "Epoch 20, Batch 24, LR 2.026849 Loss 5.359331, Accuracy 88.151%\n",
      "Epoch 20, Batch 25, LR 2.026744 Loss 5.382217, Accuracy 88.031%\n",
      "Epoch 20, Batch 26, LR 2.026639 Loss 5.364887, Accuracy 88.041%\n",
      "Epoch 20, Batch 27, LR 2.026534 Loss 5.386407, Accuracy 87.818%\n",
      "Epoch 20, Batch 28, LR 2.026429 Loss 5.379074, Accuracy 87.751%\n",
      "Epoch 20, Batch 29, LR 2.026324 Loss 5.379643, Accuracy 87.608%\n",
      "Epoch 20, Batch 30, LR 2.026219 Loss 5.396193, Accuracy 87.474%\n",
      "Epoch 20, Batch 31, LR 2.026114 Loss 5.396812, Accuracy 87.550%\n",
      "Epoch 20, Batch 32, LR 2.026009 Loss 5.418323, Accuracy 87.549%\n",
      "Epoch 20, Batch 33, LR 2.025904 Loss 5.426336, Accuracy 87.618%\n",
      "Epoch 20, Batch 34, LR 2.025799 Loss 5.419617, Accuracy 87.684%\n",
      "Epoch 20, Batch 35, LR 2.025694 Loss 5.432151, Accuracy 87.679%\n",
      "Epoch 20, Batch 36, LR 2.025589 Loss 5.442743, Accuracy 87.609%\n",
      "Epoch 20, Batch 37, LR 2.025484 Loss 5.436856, Accuracy 87.627%\n",
      "Epoch 20, Batch 38, LR 2.025378 Loss 5.425881, Accuracy 87.706%\n",
      "Epoch 20, Batch 39, LR 2.025273 Loss 5.434228, Accuracy 87.640%\n",
      "Epoch 20, Batch 40, LR 2.025168 Loss 5.431166, Accuracy 87.676%\n",
      "Epoch 20, Batch 41, LR 2.025063 Loss 5.443243, Accuracy 87.595%\n",
      "Epoch 20, Batch 42, LR 2.024958 Loss 5.456782, Accuracy 87.370%\n",
      "Epoch 20, Batch 43, LR 2.024853 Loss 5.465603, Accuracy 87.264%\n",
      "Epoch 20, Batch 44, LR 2.024748 Loss 5.468114, Accuracy 87.198%\n",
      "Epoch 20, Batch 45, LR 2.024643 Loss 5.462856, Accuracy 87.170%\n",
      "Epoch 20, Batch 46, LR 2.024538 Loss 5.452617, Accuracy 87.160%\n",
      "Epoch 20, Batch 47, LR 2.024432 Loss 5.449238, Accuracy 87.217%\n",
      "Epoch 20, Batch 48, LR 2.024327 Loss 5.428417, Accuracy 87.337%\n",
      "Epoch 20, Batch 49, LR 2.024222 Loss 5.428241, Accuracy 87.357%\n",
      "Epoch 20, Batch 50, LR 2.024117 Loss 5.424522, Accuracy 87.438%\n",
      "Epoch 20, Batch 51, LR 2.024012 Loss 5.421650, Accuracy 87.393%\n",
      "Epoch 20, Batch 52, LR 2.023907 Loss 5.427488, Accuracy 87.350%\n",
      "Epoch 20, Batch 53, LR 2.023801 Loss 5.424824, Accuracy 87.323%\n",
      "Epoch 20, Batch 54, LR 2.023696 Loss 5.427675, Accuracy 87.355%\n",
      "Epoch 20, Batch 55, LR 2.023591 Loss 5.430953, Accuracy 87.358%\n",
      "Epoch 20, Batch 56, LR 2.023486 Loss 5.432501, Accuracy 87.333%\n",
      "Epoch 20, Batch 57, LR 2.023381 Loss 5.423629, Accuracy 87.363%\n",
      "Epoch 20, Batch 58, LR 2.023275 Loss 5.412874, Accuracy 87.419%\n",
      "Epoch 20, Batch 59, LR 2.023170 Loss 5.405193, Accuracy 87.474%\n",
      "Epoch 20, Batch 60, LR 2.023065 Loss 5.410749, Accuracy 87.487%\n",
      "Epoch 20, Batch 61, LR 2.022960 Loss 5.407314, Accuracy 87.538%\n",
      "Epoch 20, Batch 62, LR 2.022854 Loss 5.405990, Accuracy 87.550%\n",
      "Epoch 20, Batch 63, LR 2.022749 Loss 5.401594, Accuracy 87.550%\n",
      "Epoch 20, Batch 64, LR 2.022644 Loss 5.391326, Accuracy 87.634%\n",
      "Epoch 20, Batch 65, LR 2.022538 Loss 5.390890, Accuracy 87.680%\n",
      "Epoch 20, Batch 66, LR 2.022433 Loss 5.396392, Accuracy 87.654%\n",
      "Epoch 20, Batch 67, LR 2.022328 Loss 5.410191, Accuracy 87.582%\n",
      "Epoch 20, Batch 68, LR 2.022222 Loss 5.407490, Accuracy 87.546%\n",
      "Epoch 20, Batch 69, LR 2.022117 Loss 5.400849, Accuracy 87.557%\n",
      "Epoch 20, Batch 70, LR 2.022012 Loss 5.398915, Accuracy 87.612%\n",
      "Epoch 20, Batch 71, LR 2.021906 Loss 5.392863, Accuracy 87.654%\n",
      "Epoch 20, Batch 72, LR 2.021801 Loss 5.392867, Accuracy 87.641%\n",
      "Epoch 20, Batch 73, LR 2.021696 Loss 5.391183, Accuracy 87.661%\n",
      "Epoch 20, Batch 74, LR 2.021590 Loss 5.393575, Accuracy 87.637%\n",
      "Epoch 20, Batch 75, LR 2.021485 Loss 5.392524, Accuracy 87.625%\n",
      "Epoch 20, Batch 76, LR 2.021379 Loss 5.395914, Accuracy 87.603%\n",
      "Epoch 20, Batch 77, LR 2.021274 Loss 5.394410, Accuracy 87.601%\n",
      "Epoch 20, Batch 78, LR 2.021169 Loss 5.389539, Accuracy 87.620%\n",
      "Epoch 20, Batch 79, LR 2.021063 Loss 5.394942, Accuracy 87.569%\n",
      "Epoch 20, Batch 80, LR 2.020958 Loss 5.401277, Accuracy 87.539%\n",
      "Epoch 20, Batch 81, LR 2.020852 Loss 5.407038, Accuracy 87.500%\n",
      "Epoch 20, Batch 82, LR 2.020747 Loss 5.406535, Accuracy 87.538%\n",
      "Epoch 20, Batch 83, LR 2.020641 Loss 5.412870, Accuracy 87.519%\n",
      "Epoch 20, Batch 84, LR 2.020536 Loss 5.407891, Accuracy 87.547%\n",
      "Epoch 20, Batch 85, LR 2.020430 Loss 5.404584, Accuracy 87.546%\n",
      "Epoch 20, Batch 86, LR 2.020325 Loss 5.406195, Accuracy 87.509%\n",
      "Epoch 20, Batch 87, LR 2.020219 Loss 5.408807, Accuracy 87.455%\n",
      "Epoch 20, Batch 88, LR 2.020114 Loss 5.400097, Accuracy 87.500%\n",
      "Epoch 20, Batch 89, LR 2.020008 Loss 5.391015, Accuracy 87.561%\n",
      "Epoch 20, Batch 90, LR 2.019903 Loss 5.392914, Accuracy 87.569%\n",
      "Epoch 20, Batch 91, LR 2.019797 Loss 5.389947, Accuracy 87.586%\n",
      "Epoch 20, Batch 92, LR 2.019692 Loss 5.387832, Accuracy 87.602%\n",
      "Epoch 20, Batch 93, LR 2.019586 Loss 5.389610, Accuracy 87.576%\n",
      "Epoch 20, Batch 94, LR 2.019481 Loss 5.387315, Accuracy 87.583%\n",
      "Epoch 20, Batch 95, LR 2.019375 Loss 5.393599, Accuracy 87.533%\n",
      "Epoch 20, Batch 96, LR 2.019270 Loss 5.393097, Accuracy 87.541%\n",
      "Epoch 20, Batch 97, LR 2.019164 Loss 5.390758, Accuracy 87.540%\n",
      "Epoch 20, Batch 98, LR 2.019058 Loss 5.386752, Accuracy 87.588%\n",
      "Epoch 20, Batch 99, LR 2.018953 Loss 5.391332, Accuracy 87.555%\n",
      "Epoch 20, Batch 100, LR 2.018847 Loss 5.387410, Accuracy 87.570%\n",
      "Epoch 20, Batch 101, LR 2.018742 Loss 5.386449, Accuracy 87.570%\n",
      "Epoch 20, Batch 102, LR 2.018636 Loss 5.385075, Accuracy 87.546%\n",
      "Epoch 20, Batch 103, LR 2.018530 Loss 5.391518, Accuracy 87.546%\n",
      "Epoch 20, Batch 104, LR 2.018425 Loss 5.394872, Accuracy 87.545%\n",
      "Epoch 20, Batch 105, LR 2.018319 Loss 5.393087, Accuracy 87.597%\n",
      "Epoch 20, Batch 106, LR 2.018213 Loss 5.385585, Accuracy 87.611%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 107, LR 2.018108 Loss 5.386463, Accuracy 87.639%\n",
      "Epoch 20, Batch 108, LR 2.018002 Loss 5.383970, Accuracy 87.645%\n",
      "Epoch 20, Batch 109, LR 2.017896 Loss 5.384506, Accuracy 87.622%\n",
      "Epoch 20, Batch 110, LR 2.017791 Loss 5.385937, Accuracy 87.607%\n",
      "Epoch 20, Batch 111, LR 2.017685 Loss 5.388830, Accuracy 87.591%\n",
      "Epoch 20, Batch 112, LR 2.017579 Loss 5.386144, Accuracy 87.605%\n",
      "Epoch 20, Batch 113, LR 2.017473 Loss 5.384605, Accuracy 87.618%\n",
      "Epoch 20, Batch 114, LR 2.017368 Loss 5.383278, Accuracy 87.617%\n",
      "Epoch 20, Batch 115, LR 2.017262 Loss 5.378049, Accuracy 87.643%\n",
      "Epoch 20, Batch 116, LR 2.017156 Loss 5.376375, Accuracy 87.641%\n",
      "Epoch 20, Batch 117, LR 2.017050 Loss 5.377911, Accuracy 87.654%\n",
      "Epoch 20, Batch 118, LR 2.016945 Loss 5.378720, Accuracy 87.652%\n",
      "Epoch 20, Batch 119, LR 2.016839 Loss 5.372222, Accuracy 87.671%\n",
      "Epoch 20, Batch 120, LR 2.016733 Loss 5.373116, Accuracy 87.669%\n",
      "Epoch 20, Batch 121, LR 2.016627 Loss 5.373856, Accuracy 87.661%\n",
      "Epoch 20, Batch 122, LR 2.016521 Loss 5.365795, Accuracy 87.711%\n",
      "Epoch 20, Batch 123, LR 2.016416 Loss 5.358826, Accuracy 87.741%\n",
      "Epoch 20, Batch 124, LR 2.016310 Loss 5.357835, Accuracy 87.765%\n",
      "Epoch 20, Batch 125, LR 2.016204 Loss 5.360051, Accuracy 87.756%\n",
      "Epoch 20, Batch 126, LR 2.016098 Loss 5.360246, Accuracy 87.742%\n",
      "Epoch 20, Batch 127, LR 2.015992 Loss 5.359588, Accuracy 87.740%\n",
      "Epoch 20, Batch 128, LR 2.015886 Loss 5.359970, Accuracy 87.744%\n",
      "Epoch 20, Batch 129, LR 2.015781 Loss 5.365842, Accuracy 87.700%\n",
      "Epoch 20, Batch 130, LR 2.015675 Loss 5.370871, Accuracy 87.692%\n",
      "Epoch 20, Batch 131, LR 2.015569 Loss 5.364716, Accuracy 87.715%\n",
      "Epoch 20, Batch 132, LR 2.015463 Loss 5.364246, Accuracy 87.713%\n",
      "Epoch 20, Batch 133, LR 2.015357 Loss 5.368494, Accuracy 87.700%\n",
      "Epoch 20, Batch 134, LR 2.015251 Loss 5.372979, Accuracy 87.681%\n",
      "Epoch 20, Batch 135, LR 2.015145 Loss 5.372139, Accuracy 87.685%\n",
      "Epoch 20, Batch 136, LR 2.015039 Loss 5.372385, Accuracy 87.678%\n",
      "Epoch 20, Batch 137, LR 2.014933 Loss 5.368535, Accuracy 87.688%\n",
      "Epoch 20, Batch 138, LR 2.014827 Loss 5.371527, Accuracy 87.675%\n",
      "Epoch 20, Batch 139, LR 2.014721 Loss 5.371289, Accuracy 87.669%\n",
      "Epoch 20, Batch 140, LR 2.014615 Loss 5.373373, Accuracy 87.667%\n",
      "Epoch 20, Batch 141, LR 2.014509 Loss 5.375854, Accuracy 87.650%\n",
      "Epoch 20, Batch 142, LR 2.014403 Loss 5.385332, Accuracy 87.610%\n",
      "Epoch 20, Batch 143, LR 2.014297 Loss 5.382431, Accuracy 87.598%\n",
      "Epoch 20, Batch 144, LR 2.014191 Loss 5.384108, Accuracy 87.587%\n",
      "Epoch 20, Batch 145, LR 2.014085 Loss 5.386793, Accuracy 87.597%\n",
      "Epoch 20, Batch 146, LR 2.013979 Loss 5.387190, Accuracy 87.596%\n",
      "Epoch 20, Batch 147, LR 2.013873 Loss 5.389960, Accuracy 87.569%\n",
      "Epoch 20, Batch 148, LR 2.013767 Loss 5.387276, Accuracy 87.584%\n",
      "Epoch 20, Batch 149, LR 2.013661 Loss 5.387995, Accuracy 87.589%\n",
      "Epoch 20, Batch 150, LR 2.013555 Loss 5.388911, Accuracy 87.573%\n",
      "Epoch 20, Batch 151, LR 2.013449 Loss 5.387979, Accuracy 87.593%\n",
      "Epoch 20, Batch 152, LR 2.013343 Loss 5.388423, Accuracy 87.608%\n",
      "Epoch 20, Batch 153, LR 2.013237 Loss 5.388278, Accuracy 87.612%\n",
      "Epoch 20, Batch 154, LR 2.013131 Loss 5.387999, Accuracy 87.622%\n",
      "Epoch 20, Batch 155, LR 2.013025 Loss 5.384633, Accuracy 87.641%\n",
      "Epoch 20, Batch 156, LR 2.012919 Loss 5.380018, Accuracy 87.675%\n",
      "Epoch 20, Batch 157, LR 2.012813 Loss 5.377991, Accuracy 87.694%\n",
      "Epoch 20, Batch 158, LR 2.012707 Loss 5.377782, Accuracy 87.673%\n",
      "Epoch 20, Batch 159, LR 2.012600 Loss 5.378247, Accuracy 87.652%\n",
      "Epoch 20, Batch 160, LR 2.012494 Loss 5.376358, Accuracy 87.666%\n",
      "Epoch 20, Batch 161, LR 2.012388 Loss 5.376377, Accuracy 87.665%\n",
      "Epoch 20, Batch 162, LR 2.012282 Loss 5.370960, Accuracy 87.703%\n",
      "Epoch 20, Batch 163, LR 2.012176 Loss 5.372281, Accuracy 87.692%\n",
      "Epoch 20, Batch 164, LR 2.012070 Loss 5.374361, Accuracy 87.695%\n",
      "Epoch 20, Batch 165, LR 2.011963 Loss 5.376458, Accuracy 87.685%\n",
      "Epoch 20, Batch 166, LR 2.011857 Loss 5.373461, Accuracy 87.702%\n",
      "Epoch 20, Batch 167, LR 2.011751 Loss 5.373596, Accuracy 87.720%\n",
      "Epoch 20, Batch 168, LR 2.011645 Loss 5.375100, Accuracy 87.705%\n",
      "Epoch 20, Batch 169, LR 2.011539 Loss 5.372406, Accuracy 87.708%\n",
      "Epoch 20, Batch 170, LR 2.011432 Loss 5.370452, Accuracy 87.702%\n",
      "Epoch 20, Batch 171, LR 2.011326 Loss 5.375813, Accuracy 87.683%\n",
      "Epoch 20, Batch 172, LR 2.011220 Loss 5.375187, Accuracy 87.686%\n",
      "Epoch 20, Batch 173, LR 2.011114 Loss 5.376463, Accuracy 87.663%\n",
      "Epoch 20, Batch 174, LR 2.011007 Loss 5.372801, Accuracy 87.680%\n",
      "Epoch 20, Batch 175, LR 2.010901 Loss 5.373139, Accuracy 87.692%\n",
      "Epoch 20, Batch 176, LR 2.010795 Loss 5.374444, Accuracy 87.660%\n",
      "Epoch 20, Batch 177, LR 2.010689 Loss 5.374771, Accuracy 87.668%\n",
      "Epoch 20, Batch 178, LR 2.010582 Loss 5.379679, Accuracy 87.649%\n",
      "Epoch 20, Batch 179, LR 2.010476 Loss 5.382157, Accuracy 87.653%\n",
      "Epoch 20, Batch 180, LR 2.010370 Loss 5.384488, Accuracy 87.630%\n",
      "Epoch 20, Batch 181, LR 2.010263 Loss 5.381859, Accuracy 87.655%\n",
      "Epoch 20, Batch 182, LR 2.010157 Loss 5.383466, Accuracy 87.637%\n",
      "Epoch 20, Batch 183, LR 2.010051 Loss 5.381441, Accuracy 87.637%\n",
      "Epoch 20, Batch 184, LR 2.009944 Loss 5.384902, Accuracy 87.619%\n",
      "Epoch 20, Batch 185, LR 2.009838 Loss 5.384943, Accuracy 87.627%\n",
      "Epoch 20, Batch 186, LR 2.009732 Loss 5.386074, Accuracy 87.597%\n",
      "Epoch 20, Batch 187, LR 2.009625 Loss 5.384743, Accuracy 87.588%\n",
      "Epoch 20, Batch 188, LR 2.009519 Loss 5.387397, Accuracy 87.596%\n",
      "Epoch 20, Batch 189, LR 2.009412 Loss 5.389355, Accuracy 87.583%\n",
      "Epoch 20, Batch 190, LR 2.009306 Loss 5.386378, Accuracy 87.586%\n",
      "Epoch 20, Batch 191, LR 2.009200 Loss 5.384731, Accuracy 87.586%\n",
      "Epoch 20, Batch 192, LR 2.009093 Loss 5.383099, Accuracy 87.610%\n",
      "Epoch 20, Batch 193, LR 2.008987 Loss 5.383157, Accuracy 87.609%\n",
      "Epoch 20, Batch 194, LR 2.008880 Loss 5.382247, Accuracy 87.609%\n",
      "Epoch 20, Batch 195, LR 2.008774 Loss 5.384200, Accuracy 87.596%\n",
      "Epoch 20, Batch 196, LR 2.008667 Loss 5.384901, Accuracy 87.588%\n",
      "Epoch 20, Batch 197, LR 2.008561 Loss 5.384444, Accuracy 87.595%\n",
      "Epoch 20, Batch 198, LR 2.008454 Loss 5.383335, Accuracy 87.618%\n",
      "Epoch 20, Batch 199, LR 2.008348 Loss 5.380670, Accuracy 87.653%\n",
      "Epoch 20, Batch 200, LR 2.008241 Loss 5.379584, Accuracy 87.648%\n",
      "Epoch 20, Batch 201, LR 2.008135 Loss 5.381289, Accuracy 87.644%\n",
      "Epoch 20, Batch 202, LR 2.008028 Loss 5.380396, Accuracy 87.659%\n",
      "Epoch 20, Batch 203, LR 2.007922 Loss 5.379883, Accuracy 87.650%\n",
      "Epoch 20, Batch 204, LR 2.007815 Loss 5.383917, Accuracy 87.634%\n",
      "Epoch 20, Batch 205, LR 2.007709 Loss 5.383491, Accuracy 87.645%\n",
      "Epoch 20, Batch 206, LR 2.007602 Loss 5.382100, Accuracy 87.637%\n",
      "Epoch 20, Batch 207, LR 2.007496 Loss 5.383965, Accuracy 87.625%\n",
      "Epoch 20, Batch 208, LR 2.007389 Loss 5.385026, Accuracy 87.620%\n",
      "Epoch 20, Batch 209, LR 2.007283 Loss 5.388241, Accuracy 87.620%\n",
      "Epoch 20, Batch 210, LR 2.007176 Loss 5.387926, Accuracy 87.615%\n",
      "Epoch 20, Batch 211, LR 2.007069 Loss 5.389323, Accuracy 87.622%\n",
      "Epoch 20, Batch 212, LR 2.006963 Loss 5.392193, Accuracy 87.599%\n",
      "Epoch 20, Batch 213, LR 2.006856 Loss 5.389599, Accuracy 87.617%\n",
      "Epoch 20, Batch 214, LR 2.006750 Loss 5.386428, Accuracy 87.620%\n",
      "Epoch 20, Batch 215, LR 2.006643 Loss 5.383538, Accuracy 87.627%\n",
      "Epoch 20, Batch 216, LR 2.006536 Loss 5.382571, Accuracy 87.634%\n",
      "Epoch 20, Batch 217, LR 2.006430 Loss 5.382361, Accuracy 87.644%\n",
      "Epoch 20, Batch 218, LR 2.006323 Loss 5.384040, Accuracy 87.633%\n",
      "Epoch 20, Batch 219, LR 2.006216 Loss 5.384621, Accuracy 87.625%\n",
      "Epoch 20, Batch 220, LR 2.006110 Loss 5.385078, Accuracy 87.621%\n",
      "Epoch 20, Batch 221, LR 2.006003 Loss 5.386736, Accuracy 87.595%\n",
      "Epoch 20, Batch 222, LR 2.005896 Loss 5.389693, Accuracy 87.567%\n",
      "Epoch 20, Batch 223, LR 2.005790 Loss 5.387095, Accuracy 87.595%\n",
      "Epoch 20, Batch 224, LR 2.005683 Loss 5.385191, Accuracy 87.608%\n",
      "Epoch 20, Batch 225, LR 2.005576 Loss 5.385244, Accuracy 87.611%\n",
      "Epoch 20, Batch 226, LR 2.005470 Loss 5.382161, Accuracy 87.621%\n",
      "Epoch 20, Batch 227, LR 2.005363 Loss 5.384212, Accuracy 87.600%\n",
      "Epoch 20, Batch 228, LR 2.005256 Loss 5.384330, Accuracy 87.613%\n",
      "Epoch 20, Batch 229, LR 2.005149 Loss 5.382651, Accuracy 87.623%\n",
      "Epoch 20, Batch 230, LR 2.005043 Loss 5.380453, Accuracy 87.626%\n",
      "Epoch 20, Batch 231, LR 2.004936 Loss 5.381364, Accuracy 87.612%\n",
      "Epoch 20, Batch 232, LR 2.004829 Loss 5.383955, Accuracy 87.594%\n",
      "Epoch 20, Batch 233, LR 2.004722 Loss 5.384109, Accuracy 87.597%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 234, LR 2.004616 Loss 5.382304, Accuracy 87.617%\n",
      "Epoch 20, Batch 235, LR 2.004509 Loss 5.386003, Accuracy 87.603%\n",
      "Epoch 20, Batch 236, LR 2.004402 Loss 5.383417, Accuracy 87.622%\n",
      "Epoch 20, Batch 237, LR 2.004295 Loss 5.383421, Accuracy 87.625%\n",
      "Epoch 20, Batch 238, LR 2.004188 Loss 5.380192, Accuracy 87.635%\n",
      "Epoch 20, Batch 239, LR 2.004082 Loss 5.381337, Accuracy 87.650%\n",
      "Epoch 20, Batch 240, LR 2.003975 Loss 5.384141, Accuracy 87.637%\n",
      "Epoch 20, Batch 241, LR 2.003868 Loss 5.384343, Accuracy 87.646%\n",
      "Epoch 20, Batch 242, LR 2.003761 Loss 5.384759, Accuracy 87.645%\n",
      "Epoch 20, Batch 243, LR 2.003654 Loss 5.385359, Accuracy 87.651%\n",
      "Epoch 20, Batch 244, LR 2.003547 Loss 5.384496, Accuracy 87.657%\n",
      "Epoch 20, Batch 245, LR 2.003440 Loss 5.382936, Accuracy 87.663%\n",
      "Epoch 20, Batch 246, LR 2.003334 Loss 5.383410, Accuracy 87.665%\n",
      "Epoch 20, Batch 247, LR 2.003227 Loss 5.381539, Accuracy 87.671%\n",
      "Epoch 20, Batch 248, LR 2.003120 Loss 5.381212, Accuracy 87.664%\n",
      "Epoch 20, Batch 249, LR 2.003013 Loss 5.383217, Accuracy 87.651%\n",
      "Epoch 20, Batch 250, LR 2.002906 Loss 5.384700, Accuracy 87.638%\n",
      "Epoch 20, Batch 251, LR 2.002799 Loss 5.382896, Accuracy 87.631%\n",
      "Epoch 20, Batch 252, LR 2.002692 Loss 5.380262, Accuracy 87.646%\n",
      "Epoch 20, Batch 253, LR 2.002585 Loss 5.380701, Accuracy 87.642%\n",
      "Epoch 20, Batch 254, LR 2.002478 Loss 5.379271, Accuracy 87.651%\n",
      "Epoch 20, Batch 255, LR 2.002371 Loss 5.381151, Accuracy 87.641%\n",
      "Epoch 20, Batch 256, LR 2.002264 Loss 5.382005, Accuracy 87.619%\n",
      "Epoch 20, Batch 257, LR 2.002157 Loss 5.381184, Accuracy 87.619%\n",
      "Epoch 20, Batch 258, LR 2.002050 Loss 5.384278, Accuracy 87.588%\n",
      "Epoch 20, Batch 259, LR 2.001943 Loss 5.382409, Accuracy 87.587%\n",
      "Epoch 20, Batch 260, LR 2.001836 Loss 5.382402, Accuracy 87.584%\n",
      "Epoch 20, Batch 261, LR 2.001729 Loss 5.382339, Accuracy 87.584%\n",
      "Epoch 20, Batch 262, LR 2.001622 Loss 5.381541, Accuracy 87.586%\n",
      "Epoch 20, Batch 263, LR 2.001515 Loss 5.380775, Accuracy 87.574%\n",
      "Epoch 20, Batch 264, LR 2.001408 Loss 5.378222, Accuracy 87.580%\n",
      "Epoch 20, Batch 265, LR 2.001301 Loss 5.377372, Accuracy 87.580%\n",
      "Epoch 20, Batch 266, LR 2.001194 Loss 5.375395, Accuracy 87.576%\n",
      "Epoch 20, Batch 267, LR 2.001087 Loss 5.377726, Accuracy 87.564%\n",
      "Epoch 20, Batch 268, LR 2.000980 Loss 5.378061, Accuracy 87.567%\n",
      "Epoch 20, Batch 269, LR 2.000873 Loss 5.377244, Accuracy 87.573%\n",
      "Epoch 20, Batch 270, LR 2.000766 Loss 5.378176, Accuracy 87.569%\n",
      "Epoch 20, Batch 271, LR 2.000658 Loss 5.380861, Accuracy 87.561%\n",
      "Epoch 20, Batch 272, LR 2.000551 Loss 5.380167, Accuracy 87.566%\n",
      "Epoch 20, Batch 273, LR 2.000444 Loss 5.379911, Accuracy 87.563%\n",
      "Epoch 20, Batch 274, LR 2.000337 Loss 5.380179, Accuracy 87.557%\n",
      "Epoch 20, Batch 275, LR 2.000230 Loss 5.380794, Accuracy 87.557%\n",
      "Epoch 20, Batch 276, LR 2.000123 Loss 5.381558, Accuracy 87.554%\n",
      "Epoch 20, Batch 277, LR 2.000016 Loss 5.381962, Accuracy 87.562%\n",
      "Epoch 20, Batch 278, LR 1.999908 Loss 5.382872, Accuracy 87.567%\n",
      "Epoch 20, Batch 279, LR 1.999801 Loss 5.383720, Accuracy 87.564%\n",
      "Epoch 20, Batch 280, LR 1.999694 Loss 5.384812, Accuracy 87.573%\n",
      "Epoch 20, Batch 281, LR 1.999587 Loss 5.382267, Accuracy 87.572%\n",
      "Epoch 20, Batch 282, LR 1.999480 Loss 5.381845, Accuracy 87.575%\n",
      "Epoch 20, Batch 283, LR 1.999373 Loss 5.383784, Accuracy 87.572%\n",
      "Epoch 20, Batch 284, LR 1.999265 Loss 5.385873, Accuracy 87.563%\n",
      "Epoch 20, Batch 285, LR 1.999158 Loss 5.388004, Accuracy 87.552%\n",
      "Epoch 20, Batch 286, LR 1.999051 Loss 5.389707, Accuracy 87.544%\n",
      "Epoch 20, Batch 287, LR 1.998944 Loss 5.388816, Accuracy 87.546%\n",
      "Epoch 20, Batch 288, LR 1.998836 Loss 5.390047, Accuracy 87.546%\n",
      "Epoch 20, Batch 289, LR 1.998729 Loss 5.391774, Accuracy 87.541%\n",
      "Epoch 20, Batch 290, LR 1.998622 Loss 5.392200, Accuracy 87.538%\n",
      "Epoch 20, Batch 291, LR 1.998515 Loss 5.390012, Accuracy 87.546%\n",
      "Epoch 20, Batch 292, LR 1.998407 Loss 5.390692, Accuracy 87.527%\n",
      "Epoch 20, Batch 293, LR 1.998300 Loss 5.390291, Accuracy 87.527%\n",
      "Epoch 20, Batch 294, LR 1.998193 Loss 5.391679, Accuracy 87.519%\n",
      "Epoch 20, Batch 295, LR 1.998085 Loss 5.390949, Accuracy 87.524%\n",
      "Epoch 20, Batch 296, LR 1.997978 Loss 5.391527, Accuracy 87.521%\n",
      "Epoch 20, Batch 297, LR 1.997871 Loss 5.390430, Accuracy 87.529%\n",
      "Epoch 20, Batch 298, LR 1.997763 Loss 5.392693, Accuracy 87.521%\n",
      "Epoch 20, Batch 299, LR 1.997656 Loss 5.390709, Accuracy 87.524%\n",
      "Epoch 20, Batch 300, LR 1.997549 Loss 5.389587, Accuracy 87.536%\n",
      "Epoch 20, Batch 301, LR 1.997441 Loss 5.389954, Accuracy 87.531%\n",
      "Epoch 20, Batch 302, LR 1.997334 Loss 5.390394, Accuracy 87.523%\n",
      "Epoch 20, Batch 303, LR 1.997227 Loss 5.390409, Accuracy 87.534%\n",
      "Epoch 20, Batch 304, LR 1.997119 Loss 5.389060, Accuracy 87.544%\n",
      "Epoch 20, Batch 305, LR 1.997012 Loss 5.388474, Accuracy 87.541%\n",
      "Epoch 20, Batch 306, LR 1.996904 Loss 5.387338, Accuracy 87.554%\n",
      "Epoch 20, Batch 307, LR 1.996797 Loss 5.391113, Accuracy 87.531%\n",
      "Epoch 20, Batch 308, LR 1.996690 Loss 5.392917, Accuracy 87.523%\n",
      "Epoch 20, Batch 309, LR 1.996582 Loss 5.392228, Accuracy 87.523%\n",
      "Epoch 20, Batch 310, LR 1.996475 Loss 5.392225, Accuracy 87.513%\n",
      "Epoch 20, Batch 311, LR 1.996367 Loss 5.390474, Accuracy 87.520%\n",
      "Epoch 20, Batch 312, LR 1.996260 Loss 5.389430, Accuracy 87.530%\n",
      "Epoch 20, Batch 313, LR 1.996152 Loss 5.389319, Accuracy 87.542%\n",
      "Epoch 20, Batch 314, LR 1.996045 Loss 5.388110, Accuracy 87.545%\n",
      "Epoch 20, Batch 315, LR 1.995937 Loss 5.388236, Accuracy 87.537%\n",
      "Epoch 20, Batch 316, LR 1.995830 Loss 5.390093, Accuracy 87.547%\n",
      "Epoch 20, Batch 317, LR 1.995722 Loss 5.389237, Accuracy 87.537%\n",
      "Epoch 20, Batch 318, LR 1.995615 Loss 5.388867, Accuracy 87.539%\n",
      "Epoch 20, Batch 319, LR 1.995507 Loss 5.386888, Accuracy 87.554%\n",
      "Epoch 20, Batch 320, LR 1.995400 Loss 5.387308, Accuracy 87.554%\n",
      "Epoch 20, Batch 321, LR 1.995292 Loss 5.387771, Accuracy 87.554%\n",
      "Epoch 20, Batch 322, LR 1.995185 Loss 5.388634, Accuracy 87.546%\n",
      "Epoch 20, Batch 323, LR 1.995077 Loss 5.389265, Accuracy 87.546%\n",
      "Epoch 20, Batch 324, LR 1.994970 Loss 5.389045, Accuracy 87.543%\n",
      "Epoch 20, Batch 325, LR 1.994862 Loss 5.389160, Accuracy 87.541%\n",
      "Epoch 20, Batch 326, LR 1.994754 Loss 5.390955, Accuracy 87.519%\n",
      "Epoch 20, Batch 327, LR 1.994647 Loss 5.392466, Accuracy 87.514%\n",
      "Epoch 20, Batch 328, LR 1.994539 Loss 5.394745, Accuracy 87.498%\n",
      "Epoch 20, Batch 329, LR 1.994432 Loss 5.394596, Accuracy 87.509%\n",
      "Epoch 20, Batch 330, LR 1.994324 Loss 5.393769, Accuracy 87.514%\n",
      "Epoch 20, Batch 331, LR 1.994216 Loss 5.392520, Accuracy 87.524%\n",
      "Epoch 20, Batch 332, LR 1.994109 Loss 5.392545, Accuracy 87.524%\n",
      "Epoch 20, Batch 333, LR 1.994001 Loss 5.393130, Accuracy 87.523%\n",
      "Epoch 20, Batch 334, LR 1.993893 Loss 5.392475, Accuracy 87.523%\n",
      "Epoch 20, Batch 335, LR 1.993786 Loss 5.392766, Accuracy 87.521%\n",
      "Epoch 20, Batch 336, LR 1.993678 Loss 5.391927, Accuracy 87.533%\n",
      "Epoch 20, Batch 337, LR 1.993570 Loss 5.390979, Accuracy 87.530%\n",
      "Epoch 20, Batch 338, LR 1.993463 Loss 5.391881, Accuracy 87.521%\n",
      "Epoch 20, Batch 339, LR 1.993355 Loss 5.392859, Accuracy 87.514%\n",
      "Epoch 20, Batch 340, LR 1.993247 Loss 5.393392, Accuracy 87.514%\n",
      "Epoch 20, Batch 341, LR 1.993140 Loss 5.392604, Accuracy 87.518%\n",
      "Epoch 20, Batch 342, LR 1.993032 Loss 5.393518, Accuracy 87.516%\n",
      "Epoch 20, Batch 343, LR 1.992924 Loss 5.393721, Accuracy 87.516%\n",
      "Epoch 20, Batch 344, LR 1.992817 Loss 5.390951, Accuracy 87.532%\n",
      "Epoch 20, Batch 345, LR 1.992709 Loss 5.393286, Accuracy 87.514%\n",
      "Epoch 20, Batch 346, LR 1.992601 Loss 5.393109, Accuracy 87.516%\n",
      "Epoch 20, Batch 347, LR 1.992493 Loss 5.392395, Accuracy 87.511%\n",
      "Epoch 20, Batch 348, LR 1.992386 Loss 5.392143, Accuracy 87.509%\n",
      "Epoch 20, Batch 349, LR 1.992278 Loss 5.391436, Accuracy 87.520%\n",
      "Epoch 20, Batch 350, LR 1.992170 Loss 5.391823, Accuracy 87.536%\n",
      "Epoch 20, Batch 351, LR 1.992062 Loss 5.390034, Accuracy 87.536%\n",
      "Epoch 20, Batch 352, LR 1.991954 Loss 5.390944, Accuracy 87.533%\n",
      "Epoch 20, Batch 353, LR 1.991847 Loss 5.390899, Accuracy 87.542%\n",
      "Epoch 20, Batch 354, LR 1.991739 Loss 5.391409, Accuracy 87.531%\n",
      "Epoch 20, Batch 355, LR 1.991631 Loss 5.390806, Accuracy 87.535%\n",
      "Epoch 20, Batch 356, LR 1.991523 Loss 5.390109, Accuracy 87.540%\n",
      "Epoch 20, Batch 357, LR 1.991415 Loss 5.391904, Accuracy 87.524%\n",
      "Epoch 20, Batch 358, LR 1.991307 Loss 5.390256, Accuracy 87.535%\n",
      "Epoch 20, Batch 359, LR 1.991200 Loss 5.389773, Accuracy 87.535%\n",
      "Epoch 20, Batch 360, LR 1.991092 Loss 5.389011, Accuracy 87.548%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 361, LR 1.990984 Loss 5.388954, Accuracy 87.543%\n",
      "Epoch 20, Batch 362, LR 1.990876 Loss 5.388475, Accuracy 87.547%\n",
      "Epoch 20, Batch 363, LR 1.990768 Loss 5.388259, Accuracy 87.554%\n",
      "Epoch 20, Batch 364, LR 1.990660 Loss 5.388521, Accuracy 87.552%\n",
      "Epoch 20, Batch 365, LR 1.990552 Loss 5.389642, Accuracy 87.543%\n",
      "Epoch 20, Batch 366, LR 1.990444 Loss 5.389857, Accuracy 87.534%\n",
      "Epoch 20, Batch 367, LR 1.990336 Loss 5.390462, Accuracy 87.532%\n",
      "Epoch 20, Batch 368, LR 1.990228 Loss 5.391052, Accuracy 87.534%\n",
      "Epoch 20, Batch 369, LR 1.990120 Loss 5.392945, Accuracy 87.523%\n",
      "Epoch 20, Batch 370, LR 1.990013 Loss 5.392657, Accuracy 87.519%\n",
      "Epoch 20, Batch 371, LR 1.989905 Loss 5.391901, Accuracy 87.527%\n",
      "Epoch 20, Batch 372, LR 1.989797 Loss 5.392450, Accuracy 87.519%\n",
      "Epoch 20, Batch 373, LR 1.989689 Loss 5.392646, Accuracy 87.517%\n",
      "Epoch 20, Batch 374, LR 1.989581 Loss 5.392277, Accuracy 87.517%\n",
      "Epoch 20, Batch 375, LR 1.989473 Loss 5.392604, Accuracy 87.525%\n",
      "Epoch 20, Batch 376, LR 1.989365 Loss 5.391637, Accuracy 87.535%\n",
      "Epoch 20, Batch 377, LR 1.989257 Loss 5.390640, Accuracy 87.546%\n",
      "Epoch 20, Batch 378, LR 1.989149 Loss 5.390841, Accuracy 87.533%\n",
      "Epoch 20, Batch 379, LR 1.989041 Loss 5.390775, Accuracy 87.539%\n",
      "Epoch 20, Batch 380, LR 1.988933 Loss 5.391201, Accuracy 87.535%\n",
      "Epoch 20, Batch 381, LR 1.988825 Loss 5.391906, Accuracy 87.535%\n",
      "Epoch 20, Batch 382, LR 1.988716 Loss 5.391718, Accuracy 87.539%\n",
      "Epoch 20, Batch 383, LR 1.988608 Loss 5.389796, Accuracy 87.553%\n",
      "Epoch 20, Batch 384, LR 1.988500 Loss 5.389205, Accuracy 87.553%\n",
      "Epoch 20, Batch 385, LR 1.988392 Loss 5.388103, Accuracy 87.567%\n",
      "Epoch 20, Batch 386, LR 1.988284 Loss 5.388799, Accuracy 87.569%\n",
      "Epoch 20, Batch 387, LR 1.988176 Loss 5.389587, Accuracy 87.565%\n",
      "Epoch 20, Batch 388, LR 1.988068 Loss 5.390197, Accuracy 87.560%\n",
      "Epoch 20, Batch 389, LR 1.987960 Loss 5.388615, Accuracy 87.568%\n",
      "Epoch 20, Batch 390, LR 1.987852 Loss 5.388688, Accuracy 87.570%\n",
      "Epoch 20, Batch 391, LR 1.987744 Loss 5.391127, Accuracy 87.564%\n",
      "Epoch 20, Batch 392, LR 1.987635 Loss 5.393145, Accuracy 87.558%\n",
      "Epoch 20, Batch 393, LR 1.987527 Loss 5.390842, Accuracy 87.568%\n",
      "Epoch 20, Batch 394, LR 1.987419 Loss 5.390613, Accuracy 87.563%\n",
      "Epoch 20, Batch 395, LR 1.987311 Loss 5.390109, Accuracy 87.569%\n",
      "Epoch 20, Batch 396, LR 1.987203 Loss 5.389956, Accuracy 87.579%\n",
      "Epoch 20, Batch 397, LR 1.987095 Loss 5.389235, Accuracy 87.579%\n",
      "Epoch 20, Batch 398, LR 1.986986 Loss 5.388494, Accuracy 87.580%\n",
      "Epoch 20, Batch 399, LR 1.986878 Loss 5.389480, Accuracy 87.574%\n",
      "Epoch 20, Batch 400, LR 1.986770 Loss 5.388411, Accuracy 87.576%\n",
      "Epoch 20, Batch 401, LR 1.986662 Loss 5.388396, Accuracy 87.574%\n",
      "Epoch 20, Batch 402, LR 1.986554 Loss 5.388358, Accuracy 87.582%\n",
      "Epoch 20, Batch 403, LR 1.986445 Loss 5.388418, Accuracy 87.591%\n",
      "Epoch 20, Batch 404, LR 1.986337 Loss 5.390136, Accuracy 87.599%\n",
      "Epoch 20, Batch 405, LR 1.986229 Loss 5.390412, Accuracy 87.602%\n",
      "Epoch 20, Batch 406, LR 1.986121 Loss 5.390443, Accuracy 87.606%\n",
      "Epoch 20, Batch 407, LR 1.986012 Loss 5.389086, Accuracy 87.607%\n",
      "Epoch 20, Batch 408, LR 1.985904 Loss 5.390290, Accuracy 87.605%\n",
      "Epoch 20, Batch 409, LR 1.985796 Loss 5.391374, Accuracy 87.609%\n",
      "Epoch 20, Batch 410, LR 1.985687 Loss 5.391786, Accuracy 87.603%\n",
      "Epoch 20, Batch 411, LR 1.985579 Loss 5.391352, Accuracy 87.597%\n",
      "Epoch 20, Batch 412, LR 1.985471 Loss 5.389596, Accuracy 87.610%\n",
      "Epoch 20, Batch 413, LR 1.985363 Loss 5.389156, Accuracy 87.617%\n",
      "Epoch 20, Batch 414, LR 1.985254 Loss 5.389936, Accuracy 87.608%\n",
      "Epoch 20, Batch 415, LR 1.985146 Loss 5.391045, Accuracy 87.592%\n",
      "Epoch 20, Batch 416, LR 1.985038 Loss 5.392204, Accuracy 87.586%\n",
      "Epoch 20, Batch 417, LR 1.984929 Loss 5.392479, Accuracy 87.588%\n",
      "Epoch 20, Batch 418, LR 1.984821 Loss 5.392462, Accuracy 87.590%\n",
      "Epoch 20, Batch 419, LR 1.984712 Loss 5.392627, Accuracy 87.589%\n",
      "Epoch 20, Batch 420, LR 1.984604 Loss 5.389918, Accuracy 87.597%\n",
      "Epoch 20, Batch 421, LR 1.984496 Loss 5.389720, Accuracy 87.593%\n",
      "Epoch 20, Batch 422, LR 1.984387 Loss 5.390952, Accuracy 87.589%\n",
      "Epoch 20, Batch 423, LR 1.984279 Loss 5.391171, Accuracy 87.589%\n",
      "Epoch 20, Batch 424, LR 1.984171 Loss 5.393261, Accuracy 87.577%\n",
      "Epoch 20, Batch 425, LR 1.984062 Loss 5.393994, Accuracy 87.568%\n",
      "Epoch 20, Batch 426, LR 1.983954 Loss 5.394419, Accuracy 87.566%\n",
      "Epoch 20, Batch 427, LR 1.983845 Loss 5.394393, Accuracy 87.570%\n",
      "Epoch 20, Batch 428, LR 1.983737 Loss 5.394784, Accuracy 87.571%\n",
      "Epoch 20, Batch 429, LR 1.983628 Loss 5.394720, Accuracy 87.567%\n",
      "Epoch 20, Batch 430, LR 1.983520 Loss 5.395798, Accuracy 87.574%\n",
      "Epoch 20, Batch 431, LR 1.983411 Loss 5.396437, Accuracy 87.565%\n",
      "Epoch 20, Batch 432, LR 1.983303 Loss 5.395750, Accuracy 87.563%\n",
      "Epoch 20, Batch 433, LR 1.983194 Loss 5.396407, Accuracy 87.565%\n",
      "Epoch 20, Batch 434, LR 1.983086 Loss 5.397016, Accuracy 87.561%\n",
      "Epoch 20, Batch 435, LR 1.982977 Loss 5.396656, Accuracy 87.559%\n",
      "Epoch 20, Batch 436, LR 1.982869 Loss 5.398127, Accuracy 87.554%\n",
      "Epoch 20, Batch 437, LR 1.982760 Loss 5.398128, Accuracy 87.550%\n",
      "Epoch 20, Batch 438, LR 1.982652 Loss 5.397931, Accuracy 87.554%\n",
      "Epoch 20, Batch 439, LR 1.982543 Loss 5.397676, Accuracy 87.561%\n",
      "Epoch 20, Batch 440, LR 1.982435 Loss 5.398869, Accuracy 87.555%\n",
      "Epoch 20, Batch 441, LR 1.982326 Loss 5.400833, Accuracy 87.544%\n",
      "Epoch 20, Batch 442, LR 1.982218 Loss 5.402704, Accuracy 87.544%\n",
      "Epoch 20, Batch 443, LR 1.982109 Loss 5.403522, Accuracy 87.546%\n",
      "Epoch 20, Batch 444, LR 1.982001 Loss 5.404144, Accuracy 87.542%\n",
      "Epoch 20, Batch 445, LR 1.981892 Loss 5.403736, Accuracy 87.542%\n",
      "Epoch 20, Batch 446, LR 1.981783 Loss 5.403173, Accuracy 87.544%\n",
      "Epoch 20, Batch 447, LR 1.981675 Loss 5.401549, Accuracy 87.552%\n",
      "Epoch 20, Batch 448, LR 1.981566 Loss 5.402806, Accuracy 87.552%\n",
      "Epoch 20, Batch 449, LR 1.981458 Loss 5.403122, Accuracy 87.549%\n",
      "Epoch 20, Batch 450, LR 1.981349 Loss 5.402367, Accuracy 87.557%\n",
      "Epoch 20, Batch 451, LR 1.981240 Loss 5.402942, Accuracy 87.562%\n",
      "Epoch 20, Batch 452, LR 1.981132 Loss 5.402320, Accuracy 87.567%\n",
      "Epoch 20, Batch 453, LR 1.981023 Loss 5.402076, Accuracy 87.571%\n",
      "Epoch 20, Batch 454, LR 1.980914 Loss 5.401769, Accuracy 87.574%\n",
      "Epoch 20, Batch 455, LR 1.980806 Loss 5.403415, Accuracy 87.569%\n",
      "Epoch 20, Batch 456, LR 1.980697 Loss 5.403633, Accuracy 87.569%\n",
      "Epoch 20, Batch 457, LR 1.980588 Loss 5.402299, Accuracy 87.572%\n",
      "Epoch 20, Batch 458, LR 1.980480 Loss 5.402163, Accuracy 87.572%\n",
      "Epoch 20, Batch 459, LR 1.980371 Loss 5.402370, Accuracy 87.573%\n",
      "Epoch 20, Batch 460, LR 1.980262 Loss 5.401246, Accuracy 87.573%\n",
      "Epoch 20, Batch 461, LR 1.980153 Loss 5.401737, Accuracy 87.573%\n",
      "Epoch 20, Batch 462, LR 1.980045 Loss 5.402382, Accuracy 87.568%\n",
      "Epoch 20, Batch 463, LR 1.979936 Loss 5.402403, Accuracy 87.566%\n",
      "Epoch 20, Batch 464, LR 1.979827 Loss 5.401575, Accuracy 87.569%\n",
      "Epoch 20, Batch 465, LR 1.979718 Loss 5.401984, Accuracy 87.569%\n",
      "Epoch 20, Batch 466, LR 1.979610 Loss 5.403254, Accuracy 87.559%\n",
      "Epoch 20, Batch 467, LR 1.979501 Loss 5.403555, Accuracy 87.555%\n",
      "Epoch 20, Batch 468, LR 1.979392 Loss 5.403354, Accuracy 87.553%\n",
      "Epoch 20, Batch 469, LR 1.979283 Loss 5.403974, Accuracy 87.548%\n",
      "Epoch 20, Batch 470, LR 1.979175 Loss 5.405865, Accuracy 87.540%\n",
      "Epoch 20, Batch 471, LR 1.979066 Loss 5.406271, Accuracy 87.533%\n",
      "Epoch 20, Batch 472, LR 1.978957 Loss 5.406628, Accuracy 87.536%\n",
      "Epoch 20, Batch 473, LR 1.978848 Loss 5.405961, Accuracy 87.536%\n",
      "Epoch 20, Batch 474, LR 1.978739 Loss 5.406721, Accuracy 87.541%\n",
      "Epoch 20, Batch 475, LR 1.978630 Loss 5.404567, Accuracy 87.553%\n",
      "Epoch 20, Batch 476, LR 1.978522 Loss 5.405464, Accuracy 87.544%\n",
      "Epoch 20, Batch 477, LR 1.978413 Loss 5.406409, Accuracy 87.534%\n",
      "Epoch 20, Batch 478, LR 1.978304 Loss 5.406680, Accuracy 87.541%\n",
      "Epoch 20, Batch 479, LR 1.978195 Loss 5.406716, Accuracy 87.544%\n",
      "Epoch 20, Batch 480, LR 1.978086 Loss 5.405992, Accuracy 87.555%\n",
      "Epoch 20, Batch 481, LR 1.977977 Loss 5.405111, Accuracy 87.554%\n",
      "Epoch 20, Batch 482, LR 1.977868 Loss 5.406942, Accuracy 87.544%\n",
      "Epoch 20, Batch 483, LR 1.977759 Loss 5.407230, Accuracy 87.545%\n",
      "Epoch 20, Batch 484, LR 1.977651 Loss 5.407870, Accuracy 87.537%\n",
      "Epoch 20, Batch 485, LR 1.977542 Loss 5.407117, Accuracy 87.535%\n",
      "Epoch 20, Batch 486, LR 1.977433 Loss 5.407935, Accuracy 87.531%\n",
      "Epoch 20, Batch 487, LR 1.977324 Loss 5.407189, Accuracy 87.530%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 488, LR 1.977215 Loss 5.406084, Accuracy 87.532%\n",
      "Epoch 20, Batch 489, LR 1.977106 Loss 5.406775, Accuracy 87.530%\n",
      "Epoch 20, Batch 490, LR 1.976997 Loss 5.406699, Accuracy 87.526%\n",
      "Epoch 20, Batch 491, LR 1.976888 Loss 5.406490, Accuracy 87.529%\n",
      "Epoch 20, Batch 492, LR 1.976779 Loss 5.406755, Accuracy 87.530%\n",
      "Epoch 20, Batch 493, LR 1.976670 Loss 5.406409, Accuracy 87.538%\n",
      "Epoch 20, Batch 494, LR 1.976561 Loss 5.407359, Accuracy 87.533%\n",
      "Epoch 20, Batch 495, LR 1.976452 Loss 5.408053, Accuracy 87.530%\n",
      "Epoch 20, Batch 496, LR 1.976343 Loss 5.408602, Accuracy 87.525%\n",
      "Epoch 20, Batch 497, LR 1.976234 Loss 5.409177, Accuracy 87.530%\n",
      "Epoch 20, Batch 498, LR 1.976125 Loss 5.408340, Accuracy 87.533%\n",
      "Epoch 20, Batch 499, LR 1.976016 Loss 5.408188, Accuracy 87.539%\n",
      "Epoch 20, Batch 500, LR 1.975907 Loss 5.409086, Accuracy 87.534%\n",
      "Epoch 20, Batch 501, LR 1.975798 Loss 5.407139, Accuracy 87.537%\n",
      "Epoch 20, Batch 502, LR 1.975689 Loss 5.406987, Accuracy 87.539%\n",
      "Epoch 20, Batch 503, LR 1.975580 Loss 5.407049, Accuracy 87.540%\n",
      "Epoch 20, Batch 504, LR 1.975470 Loss 5.407311, Accuracy 87.534%\n",
      "Epoch 20, Batch 505, LR 1.975361 Loss 5.406266, Accuracy 87.539%\n",
      "Epoch 20, Batch 506, LR 1.975252 Loss 5.405945, Accuracy 87.539%\n",
      "Epoch 20, Batch 507, LR 1.975143 Loss 5.406465, Accuracy 87.537%\n",
      "Epoch 20, Batch 508, LR 1.975034 Loss 5.407455, Accuracy 87.531%\n",
      "Epoch 20, Batch 509, LR 1.974925 Loss 5.406127, Accuracy 87.538%\n",
      "Epoch 20, Batch 510, LR 1.974816 Loss 5.405938, Accuracy 87.537%\n",
      "Epoch 20, Batch 511, LR 1.974707 Loss 5.406299, Accuracy 87.535%\n",
      "Epoch 20, Batch 512, LR 1.974598 Loss 5.405448, Accuracy 87.535%\n",
      "Epoch 20, Batch 513, LR 1.974488 Loss 5.405936, Accuracy 87.534%\n",
      "Epoch 20, Batch 514, LR 1.974379 Loss 5.407776, Accuracy 87.524%\n",
      "Epoch 20, Batch 515, LR 1.974270 Loss 5.406170, Accuracy 87.533%\n",
      "Epoch 20, Batch 516, LR 1.974161 Loss 5.405912, Accuracy 87.530%\n",
      "Epoch 20, Batch 517, LR 1.974052 Loss 5.405422, Accuracy 87.539%\n",
      "Epoch 20, Batch 518, LR 1.973942 Loss 5.405967, Accuracy 87.542%\n",
      "Epoch 20, Batch 519, LR 1.973833 Loss 5.407161, Accuracy 87.538%\n",
      "Epoch 20, Batch 520, LR 1.973724 Loss 5.405095, Accuracy 87.547%\n",
      "Epoch 20, Batch 521, LR 1.973615 Loss 5.404686, Accuracy 87.552%\n",
      "Epoch 20, Batch 522, LR 1.973506 Loss 5.403496, Accuracy 87.555%\n",
      "Epoch 20, Batch 523, LR 1.973396 Loss 5.401801, Accuracy 87.564%\n",
      "Epoch 20, Batch 524, LR 1.973287 Loss 5.401366, Accuracy 87.570%\n",
      "Epoch 20, Batch 525, LR 1.973178 Loss 5.401557, Accuracy 87.568%\n",
      "Epoch 20, Batch 526, LR 1.973069 Loss 5.401889, Accuracy 87.570%\n",
      "Epoch 20, Batch 527, LR 1.972959 Loss 5.402375, Accuracy 87.570%\n",
      "Epoch 20, Batch 528, LR 1.972850 Loss 5.402441, Accuracy 87.567%\n",
      "Epoch 20, Batch 529, LR 1.972741 Loss 5.402115, Accuracy 87.568%\n",
      "Epoch 20, Batch 530, LR 1.972631 Loss 5.402379, Accuracy 87.569%\n",
      "Epoch 20, Batch 531, LR 1.972522 Loss 5.402609, Accuracy 87.574%\n",
      "Epoch 20, Batch 532, LR 1.972413 Loss 5.403029, Accuracy 87.565%\n",
      "Epoch 20, Batch 533, LR 1.972304 Loss 5.403678, Accuracy 87.567%\n",
      "Epoch 20, Batch 534, LR 1.972194 Loss 5.402412, Accuracy 87.576%\n",
      "Epoch 20, Batch 535, LR 1.972085 Loss 5.401816, Accuracy 87.579%\n",
      "Epoch 20, Batch 536, LR 1.971976 Loss 5.402700, Accuracy 87.577%\n",
      "Epoch 20, Batch 537, LR 1.971866 Loss 5.400487, Accuracy 87.587%\n",
      "Epoch 20, Batch 538, LR 1.971757 Loss 5.400850, Accuracy 87.587%\n",
      "Epoch 20, Batch 539, LR 1.971647 Loss 5.400712, Accuracy 87.591%\n",
      "Epoch 20, Batch 540, LR 1.971538 Loss 5.400606, Accuracy 87.591%\n",
      "Epoch 20, Batch 541, LR 1.971429 Loss 5.401463, Accuracy 87.585%\n",
      "Epoch 20, Batch 542, LR 1.971319 Loss 5.400722, Accuracy 87.591%\n",
      "Epoch 20, Batch 543, LR 1.971210 Loss 5.400142, Accuracy 87.589%\n",
      "Epoch 20, Batch 544, LR 1.971100 Loss 5.399746, Accuracy 87.596%\n",
      "Epoch 20, Batch 545, LR 1.970991 Loss 5.398212, Accuracy 87.605%\n",
      "Epoch 20, Batch 546, LR 1.970882 Loss 5.398054, Accuracy 87.606%\n",
      "Epoch 20, Batch 547, LR 1.970772 Loss 5.398528, Accuracy 87.606%\n",
      "Epoch 20, Batch 548, LR 1.970663 Loss 5.398618, Accuracy 87.608%\n",
      "Epoch 20, Batch 549, LR 1.970553 Loss 5.398163, Accuracy 87.614%\n",
      "Epoch 20, Batch 550, LR 1.970444 Loss 5.397825, Accuracy 87.615%\n",
      "Epoch 20, Batch 551, LR 1.970334 Loss 5.398121, Accuracy 87.615%\n",
      "Epoch 20, Batch 552, LR 1.970225 Loss 5.397834, Accuracy 87.617%\n",
      "Epoch 20, Batch 553, LR 1.970115 Loss 5.396907, Accuracy 87.621%\n",
      "Epoch 20, Batch 554, LR 1.970006 Loss 5.395574, Accuracy 87.624%\n",
      "Epoch 20, Batch 555, LR 1.969896 Loss 5.396118, Accuracy 87.624%\n",
      "Epoch 20, Batch 556, LR 1.969787 Loss 5.397016, Accuracy 87.624%\n",
      "Epoch 20, Batch 557, LR 1.969677 Loss 5.397108, Accuracy 87.628%\n",
      "Epoch 20, Batch 558, LR 1.969568 Loss 5.396370, Accuracy 87.629%\n",
      "Epoch 20, Batch 559, LR 1.969458 Loss 5.397601, Accuracy 87.624%\n",
      "Epoch 20, Batch 560, LR 1.969349 Loss 5.398576, Accuracy 87.614%\n",
      "Epoch 20, Batch 561, LR 1.969239 Loss 5.397623, Accuracy 87.620%\n",
      "Epoch 20, Batch 562, LR 1.969130 Loss 5.397060, Accuracy 87.624%\n",
      "Epoch 20, Batch 563, LR 1.969020 Loss 5.396149, Accuracy 87.625%\n",
      "Epoch 20, Batch 564, LR 1.968910 Loss 5.396063, Accuracy 87.621%\n",
      "Epoch 20, Batch 565, LR 1.968801 Loss 5.396051, Accuracy 87.623%\n",
      "Epoch 20, Batch 566, LR 1.968691 Loss 5.395330, Accuracy 87.621%\n",
      "Epoch 20, Batch 567, LR 1.968582 Loss 5.394526, Accuracy 87.625%\n",
      "Epoch 20, Batch 568, LR 1.968472 Loss 5.394655, Accuracy 87.633%\n",
      "Epoch 20, Batch 569, LR 1.968362 Loss 5.394757, Accuracy 87.636%\n",
      "Epoch 20, Batch 570, LR 1.968253 Loss 5.395061, Accuracy 87.637%\n",
      "Epoch 20, Batch 571, LR 1.968143 Loss 5.396318, Accuracy 87.631%\n",
      "Epoch 20, Batch 572, LR 1.968034 Loss 5.397453, Accuracy 87.619%\n",
      "Epoch 20, Batch 573, LR 1.967924 Loss 5.397371, Accuracy 87.627%\n",
      "Epoch 20, Batch 574, LR 1.967814 Loss 5.397858, Accuracy 87.622%\n",
      "Epoch 20, Batch 575, LR 1.967705 Loss 5.396946, Accuracy 87.626%\n",
      "Epoch 20, Batch 576, LR 1.967595 Loss 5.397653, Accuracy 87.621%\n",
      "Epoch 20, Batch 577, LR 1.967485 Loss 5.397032, Accuracy 87.625%\n",
      "Epoch 20, Batch 578, LR 1.967375 Loss 5.397320, Accuracy 87.622%\n",
      "Epoch 20, Batch 579, LR 1.967266 Loss 5.397696, Accuracy 87.617%\n",
      "Epoch 20, Batch 580, LR 1.967156 Loss 5.398187, Accuracy 87.620%\n",
      "Epoch 20, Batch 581, LR 1.967046 Loss 5.399222, Accuracy 87.616%\n",
      "Epoch 20, Batch 582, LR 1.966937 Loss 5.399136, Accuracy 87.618%\n",
      "Epoch 20, Batch 583, LR 1.966827 Loss 5.399385, Accuracy 87.622%\n",
      "Epoch 20, Batch 584, LR 1.966717 Loss 5.398350, Accuracy 87.624%\n",
      "Epoch 20, Batch 585, LR 1.966607 Loss 5.397961, Accuracy 87.627%\n",
      "Epoch 20, Batch 586, LR 1.966498 Loss 5.397682, Accuracy 87.629%\n",
      "Epoch 20, Batch 587, LR 1.966388 Loss 5.397572, Accuracy 87.632%\n",
      "Epoch 20, Batch 588, LR 1.966278 Loss 5.397018, Accuracy 87.628%\n",
      "Epoch 20, Batch 589, LR 1.966168 Loss 5.396305, Accuracy 87.637%\n",
      "Epoch 20, Batch 590, LR 1.966059 Loss 5.397407, Accuracy 87.630%\n",
      "Epoch 20, Batch 591, LR 1.965949 Loss 5.397853, Accuracy 87.626%\n",
      "Epoch 20, Batch 592, LR 1.965839 Loss 5.398074, Accuracy 87.621%\n",
      "Epoch 20, Batch 593, LR 1.965729 Loss 5.398404, Accuracy 87.623%\n",
      "Epoch 20, Batch 594, LR 1.965619 Loss 5.399527, Accuracy 87.625%\n",
      "Epoch 20, Batch 595, LR 1.965509 Loss 5.400224, Accuracy 87.623%\n",
      "Epoch 20, Batch 596, LR 1.965400 Loss 5.398375, Accuracy 87.630%\n",
      "Epoch 20, Batch 597, LR 1.965290 Loss 5.397651, Accuracy 87.632%\n",
      "Epoch 20, Batch 598, LR 1.965180 Loss 5.397968, Accuracy 87.629%\n",
      "Epoch 20, Batch 599, LR 1.965070 Loss 5.397346, Accuracy 87.638%\n",
      "Epoch 20, Batch 600, LR 1.964960 Loss 5.398293, Accuracy 87.633%\n",
      "Epoch 20, Batch 601, LR 1.964850 Loss 5.397522, Accuracy 87.638%\n",
      "Epoch 20, Batch 602, LR 1.964740 Loss 5.397224, Accuracy 87.643%\n",
      "Epoch 20, Batch 603, LR 1.964630 Loss 5.397233, Accuracy 87.644%\n",
      "Epoch 20, Batch 604, LR 1.964521 Loss 5.397108, Accuracy 87.644%\n",
      "Epoch 20, Batch 605, LR 1.964411 Loss 5.396623, Accuracy 87.646%\n",
      "Epoch 20, Batch 606, LR 1.964301 Loss 5.396236, Accuracy 87.644%\n",
      "Epoch 20, Batch 607, LR 1.964191 Loss 5.394964, Accuracy 87.649%\n",
      "Epoch 20, Batch 608, LR 1.964081 Loss 5.395010, Accuracy 87.645%\n",
      "Epoch 20, Batch 609, LR 1.963971 Loss 5.394352, Accuracy 87.648%\n",
      "Epoch 20, Batch 610, LR 1.963861 Loss 5.394213, Accuracy 87.650%\n",
      "Epoch 20, Batch 611, LR 1.963751 Loss 5.394087, Accuracy 87.648%\n",
      "Epoch 20, Batch 612, LR 1.963641 Loss 5.393879, Accuracy 87.646%\n",
      "Epoch 20, Batch 613, LR 1.963531 Loss 5.393363, Accuracy 87.652%\n",
      "Epoch 20, Batch 614, LR 1.963421 Loss 5.393205, Accuracy 87.653%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 615, LR 1.963311 Loss 5.393567, Accuracy 87.658%\n",
      "Epoch 20, Batch 616, LR 1.963201 Loss 5.393630, Accuracy 87.652%\n",
      "Epoch 20, Batch 617, LR 1.963091 Loss 5.392779, Accuracy 87.658%\n",
      "Epoch 20, Batch 618, LR 1.962981 Loss 5.392415, Accuracy 87.657%\n",
      "Epoch 20, Batch 619, LR 1.962871 Loss 5.392655, Accuracy 87.663%\n",
      "Epoch 20, Batch 620, LR 1.962761 Loss 5.392458, Accuracy 87.656%\n",
      "Epoch 20, Batch 621, LR 1.962651 Loss 5.392787, Accuracy 87.647%\n",
      "Epoch 20, Batch 622, LR 1.962541 Loss 5.392061, Accuracy 87.656%\n",
      "Epoch 20, Batch 623, LR 1.962431 Loss 5.391912, Accuracy 87.662%\n",
      "Epoch 20, Batch 624, LR 1.962321 Loss 5.393439, Accuracy 87.657%\n",
      "Epoch 20, Batch 625, LR 1.962211 Loss 5.394832, Accuracy 87.644%\n",
      "Epoch 20, Batch 626, LR 1.962101 Loss 5.394479, Accuracy 87.642%\n",
      "Epoch 20, Batch 627, LR 1.961990 Loss 5.394523, Accuracy 87.647%\n",
      "Epoch 20, Batch 628, LR 1.961880 Loss 5.395173, Accuracy 87.642%\n",
      "Epoch 20, Batch 629, LR 1.961770 Loss 5.396586, Accuracy 87.630%\n",
      "Epoch 20, Batch 630, LR 1.961660 Loss 5.396318, Accuracy 87.629%\n",
      "Epoch 20, Batch 631, LR 1.961550 Loss 5.395387, Accuracy 87.629%\n",
      "Epoch 20, Batch 632, LR 1.961440 Loss 5.396324, Accuracy 87.626%\n",
      "Epoch 20, Batch 633, LR 1.961330 Loss 5.397182, Accuracy 87.621%\n",
      "Epoch 20, Batch 634, LR 1.961220 Loss 5.397880, Accuracy 87.618%\n",
      "Epoch 20, Batch 635, LR 1.961109 Loss 5.398503, Accuracy 87.616%\n",
      "Epoch 20, Batch 636, LR 1.960999 Loss 5.397650, Accuracy 87.614%\n",
      "Epoch 20, Batch 637, LR 1.960889 Loss 5.398333, Accuracy 87.610%\n",
      "Epoch 20, Batch 638, LR 1.960779 Loss 5.397636, Accuracy 87.613%\n",
      "Epoch 20, Batch 639, LR 1.960669 Loss 5.398738, Accuracy 87.612%\n",
      "Epoch 20, Batch 640, LR 1.960558 Loss 5.399193, Accuracy 87.612%\n",
      "Epoch 20, Batch 641, LR 1.960448 Loss 5.398985, Accuracy 87.613%\n",
      "Epoch 20, Batch 642, LR 1.960338 Loss 5.398458, Accuracy 87.610%\n",
      "Epoch 20, Batch 643, LR 1.960228 Loss 5.398196, Accuracy 87.614%\n",
      "Epoch 20, Batch 644, LR 1.960118 Loss 5.399218, Accuracy 87.601%\n",
      "Epoch 20, Batch 645, LR 1.960007 Loss 5.399950, Accuracy 87.593%\n",
      "Epoch 20, Batch 646, LR 1.959897 Loss 5.399993, Accuracy 87.597%\n",
      "Epoch 20, Batch 647, LR 1.959787 Loss 5.399612, Accuracy 87.599%\n",
      "Epoch 20, Batch 648, LR 1.959677 Loss 5.400181, Accuracy 87.600%\n",
      "Epoch 20, Batch 649, LR 1.959566 Loss 5.400089, Accuracy 87.602%\n",
      "Epoch 20, Batch 650, LR 1.959456 Loss 5.401241, Accuracy 87.594%\n",
      "Epoch 20, Batch 651, LR 1.959346 Loss 5.400954, Accuracy 87.591%\n",
      "Epoch 20, Batch 652, LR 1.959235 Loss 5.400451, Accuracy 87.595%\n",
      "Epoch 20, Batch 653, LR 1.959125 Loss 5.399950, Accuracy 87.595%\n",
      "Epoch 20, Batch 654, LR 1.959015 Loss 5.400304, Accuracy 87.591%\n",
      "Epoch 20, Batch 655, LR 1.958904 Loss 5.400393, Accuracy 87.593%\n",
      "Epoch 20, Batch 656, LR 1.958794 Loss 5.399714, Accuracy 87.594%\n",
      "Epoch 20, Batch 657, LR 1.958684 Loss 5.399962, Accuracy 87.595%\n",
      "Epoch 20, Batch 658, LR 1.958573 Loss 5.399725, Accuracy 87.600%\n",
      "Epoch 20, Batch 659, LR 1.958463 Loss 5.400184, Accuracy 87.602%\n",
      "Epoch 20, Batch 660, LR 1.958353 Loss 5.400202, Accuracy 87.607%\n",
      "Epoch 20, Batch 661, LR 1.958242 Loss 5.401154, Accuracy 87.598%\n",
      "Epoch 20, Batch 662, LR 1.958132 Loss 5.401922, Accuracy 87.593%\n",
      "Epoch 20, Batch 663, LR 1.958022 Loss 5.401491, Accuracy 87.598%\n",
      "Epoch 20, Batch 664, LR 1.957911 Loss 5.400972, Accuracy 87.598%\n",
      "Epoch 20, Batch 665, LR 1.957801 Loss 5.402489, Accuracy 87.588%\n",
      "Epoch 20, Batch 666, LR 1.957690 Loss 5.402628, Accuracy 87.588%\n",
      "Epoch 20, Batch 667, LR 1.957580 Loss 5.402804, Accuracy 87.595%\n",
      "Epoch 20, Batch 668, LR 1.957469 Loss 5.403423, Accuracy 87.595%\n",
      "Epoch 20, Batch 669, LR 1.957359 Loss 5.403481, Accuracy 87.590%\n",
      "Epoch 20, Batch 670, LR 1.957249 Loss 5.403916, Accuracy 87.591%\n",
      "Epoch 20, Batch 671, LR 1.957138 Loss 5.404153, Accuracy 87.591%\n",
      "Epoch 20, Batch 672, LR 1.957028 Loss 5.404265, Accuracy 87.591%\n",
      "Epoch 20, Batch 673, LR 1.956917 Loss 5.403372, Accuracy 87.595%\n",
      "Epoch 20, Batch 674, LR 1.956807 Loss 5.402429, Accuracy 87.595%\n",
      "Epoch 20, Batch 675, LR 1.956696 Loss 5.402659, Accuracy 87.591%\n",
      "Epoch 20, Batch 676, LR 1.956586 Loss 5.403467, Accuracy 87.589%\n",
      "Epoch 20, Batch 677, LR 1.956475 Loss 5.403176, Accuracy 87.589%\n",
      "Epoch 20, Batch 678, LR 1.956365 Loss 5.403289, Accuracy 87.585%\n",
      "Epoch 20, Batch 679, LR 1.956254 Loss 5.403522, Accuracy 87.586%\n",
      "Epoch 20, Batch 680, LR 1.956144 Loss 5.403571, Accuracy 87.585%\n",
      "Epoch 20, Batch 681, LR 1.956033 Loss 5.403204, Accuracy 87.586%\n",
      "Epoch 20, Batch 682, LR 1.955923 Loss 5.402602, Accuracy 87.582%\n",
      "Epoch 20, Batch 683, LR 1.955812 Loss 5.401980, Accuracy 87.588%\n",
      "Epoch 20, Batch 684, LR 1.955701 Loss 5.401453, Accuracy 87.595%\n",
      "Epoch 20, Batch 685, LR 1.955591 Loss 5.401101, Accuracy 87.599%\n",
      "Epoch 20, Batch 686, LR 1.955480 Loss 5.400430, Accuracy 87.601%\n",
      "Epoch 20, Batch 687, LR 1.955370 Loss 5.401189, Accuracy 87.601%\n",
      "Epoch 20, Batch 688, LR 1.955259 Loss 5.401408, Accuracy 87.606%\n",
      "Epoch 20, Batch 689, LR 1.955149 Loss 5.402131, Accuracy 87.604%\n",
      "Epoch 20, Batch 690, LR 1.955038 Loss 5.402101, Accuracy 87.605%\n",
      "Epoch 20, Batch 691, LR 1.954927 Loss 5.402576, Accuracy 87.605%\n",
      "Epoch 20, Batch 692, LR 1.954817 Loss 5.401982, Accuracy 87.610%\n",
      "Epoch 20, Batch 693, LR 1.954706 Loss 5.401494, Accuracy 87.616%\n",
      "Epoch 20, Batch 694, LR 1.954595 Loss 5.401149, Accuracy 87.617%\n",
      "Epoch 20, Batch 695, LR 1.954485 Loss 5.401694, Accuracy 87.620%\n",
      "Epoch 20, Batch 696, LR 1.954374 Loss 5.401414, Accuracy 87.625%\n",
      "Epoch 20, Batch 697, LR 1.954263 Loss 5.401193, Accuracy 87.623%\n",
      "Epoch 20, Batch 698, LR 1.954153 Loss 5.400553, Accuracy 87.628%\n",
      "Epoch 20, Batch 699, LR 1.954042 Loss 5.401148, Accuracy 87.625%\n",
      "Epoch 20, Batch 700, LR 1.953931 Loss 5.401413, Accuracy 87.624%\n",
      "Epoch 20, Batch 701, LR 1.953821 Loss 5.401842, Accuracy 87.624%\n",
      "Epoch 20, Batch 702, LR 1.953710 Loss 5.402465, Accuracy 87.617%\n",
      "Epoch 20, Batch 703, LR 1.953599 Loss 5.403031, Accuracy 87.614%\n",
      "Epoch 20, Batch 704, LR 1.953489 Loss 5.401766, Accuracy 87.621%\n",
      "Epoch 20, Batch 705, LR 1.953378 Loss 5.401450, Accuracy 87.624%\n",
      "Epoch 20, Batch 706, LR 1.953267 Loss 5.401698, Accuracy 87.624%\n",
      "Epoch 20, Batch 707, LR 1.953156 Loss 5.402293, Accuracy 87.619%\n",
      "Epoch 20, Batch 708, LR 1.953046 Loss 5.402191, Accuracy 87.625%\n",
      "Epoch 20, Batch 709, LR 1.952935 Loss 5.402144, Accuracy 87.627%\n",
      "Epoch 20, Batch 710, LR 1.952824 Loss 5.402117, Accuracy 87.623%\n",
      "Epoch 20, Batch 711, LR 1.952713 Loss 5.401931, Accuracy 87.625%\n",
      "Epoch 20, Batch 712, LR 1.952603 Loss 5.402042, Accuracy 87.632%\n",
      "Epoch 20, Batch 713, LR 1.952492 Loss 5.402369, Accuracy 87.635%\n",
      "Epoch 20, Batch 714, LR 1.952381 Loss 5.402593, Accuracy 87.631%\n",
      "Epoch 20, Batch 715, LR 1.952270 Loss 5.402403, Accuracy 87.629%\n",
      "Epoch 20, Batch 716, LR 1.952159 Loss 5.401973, Accuracy 87.633%\n",
      "Epoch 20, Batch 717, LR 1.952048 Loss 5.401347, Accuracy 87.641%\n",
      "Epoch 20, Batch 718, LR 1.951938 Loss 5.401740, Accuracy 87.643%\n",
      "Epoch 20, Batch 719, LR 1.951827 Loss 5.401250, Accuracy 87.645%\n",
      "Epoch 20, Batch 720, LR 1.951716 Loss 5.400416, Accuracy 87.651%\n",
      "Epoch 20, Batch 721, LR 1.951605 Loss 5.400116, Accuracy 87.653%\n",
      "Epoch 20, Batch 722, LR 1.951494 Loss 5.399462, Accuracy 87.659%\n",
      "Epoch 20, Batch 723, LR 1.951383 Loss 5.399065, Accuracy 87.661%\n",
      "Epoch 20, Batch 724, LR 1.951272 Loss 5.398322, Accuracy 87.660%\n",
      "Epoch 20, Batch 725, LR 1.951162 Loss 5.399391, Accuracy 87.654%\n",
      "Epoch 20, Batch 726, LR 1.951051 Loss 5.399482, Accuracy 87.653%\n",
      "Epoch 20, Batch 727, LR 1.950940 Loss 5.400470, Accuracy 87.650%\n",
      "Epoch 20, Batch 728, LR 1.950829 Loss 5.400786, Accuracy 87.645%\n",
      "Epoch 20, Batch 729, LR 1.950718 Loss 5.400494, Accuracy 87.645%\n",
      "Epoch 20, Batch 730, LR 1.950607 Loss 5.400830, Accuracy 87.644%\n",
      "Epoch 20, Batch 731, LR 1.950496 Loss 5.400598, Accuracy 87.643%\n",
      "Epoch 20, Batch 732, LR 1.950385 Loss 5.399575, Accuracy 87.649%\n",
      "Epoch 20, Batch 733, LR 1.950274 Loss 5.399766, Accuracy 87.648%\n",
      "Epoch 20, Batch 734, LR 1.950163 Loss 5.400050, Accuracy 87.649%\n",
      "Epoch 20, Batch 735, LR 1.950052 Loss 5.400760, Accuracy 87.643%\n",
      "Epoch 20, Batch 736, LR 1.949941 Loss 5.401544, Accuracy 87.634%\n",
      "Epoch 20, Batch 737, LR 1.949830 Loss 5.401143, Accuracy 87.636%\n",
      "Epoch 20, Batch 738, LR 1.949719 Loss 5.400874, Accuracy 87.633%\n",
      "Epoch 20, Batch 739, LR 1.949608 Loss 5.400998, Accuracy 87.634%\n",
      "Epoch 20, Batch 740, LR 1.949497 Loss 5.400870, Accuracy 87.633%\n",
      "Epoch 20, Batch 741, LR 1.949386 Loss 5.400926, Accuracy 87.629%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 742, LR 1.949275 Loss 5.400849, Accuracy 87.623%\n",
      "Epoch 20, Batch 743, LR 1.949164 Loss 5.400879, Accuracy 87.625%\n",
      "Epoch 20, Batch 744, LR 1.949053 Loss 5.400744, Accuracy 87.627%\n",
      "Epoch 20, Batch 745, LR 1.948942 Loss 5.399659, Accuracy 87.632%\n",
      "Epoch 20, Batch 746, LR 1.948831 Loss 5.400326, Accuracy 87.628%\n",
      "Epoch 20, Batch 747, LR 1.948720 Loss 5.400549, Accuracy 87.628%\n",
      "Epoch 20, Batch 748, LR 1.948609 Loss 5.401361, Accuracy 87.623%\n",
      "Epoch 20, Batch 749, LR 1.948498 Loss 5.401255, Accuracy 87.623%\n",
      "Epoch 20, Batch 750, LR 1.948387 Loss 5.400631, Accuracy 87.630%\n",
      "Epoch 20, Batch 751, LR 1.948276 Loss 5.400478, Accuracy 87.634%\n",
      "Epoch 20, Batch 752, LR 1.948164 Loss 5.399733, Accuracy 87.639%\n",
      "Epoch 20, Batch 753, LR 1.948053 Loss 5.399458, Accuracy 87.643%\n",
      "Epoch 20, Batch 754, LR 1.947942 Loss 5.399948, Accuracy 87.640%\n",
      "Epoch 20, Batch 755, LR 1.947831 Loss 5.400167, Accuracy 87.638%\n",
      "Epoch 20, Batch 756, LR 1.947720 Loss 5.400364, Accuracy 87.637%\n",
      "Epoch 20, Batch 757, LR 1.947609 Loss 5.400635, Accuracy 87.638%\n",
      "Epoch 20, Batch 758, LR 1.947498 Loss 5.401040, Accuracy 87.639%\n",
      "Epoch 20, Batch 759, LR 1.947386 Loss 5.400381, Accuracy 87.646%\n",
      "Epoch 20, Batch 760, LR 1.947275 Loss 5.400068, Accuracy 87.652%\n",
      "Epoch 20, Batch 761, LR 1.947164 Loss 5.399893, Accuracy 87.650%\n",
      "Epoch 20, Batch 762, LR 1.947053 Loss 5.400377, Accuracy 87.649%\n",
      "Epoch 20, Batch 763, LR 1.946942 Loss 5.400249, Accuracy 87.646%\n",
      "Epoch 20, Batch 764, LR 1.946831 Loss 5.399817, Accuracy 87.647%\n",
      "Epoch 20, Batch 765, LR 1.946719 Loss 5.399113, Accuracy 87.653%\n",
      "Epoch 20, Batch 766, LR 1.946608 Loss 5.399025, Accuracy 87.653%\n",
      "Epoch 20, Batch 767, LR 1.946497 Loss 5.400024, Accuracy 87.651%\n",
      "Epoch 20, Batch 768, LR 1.946386 Loss 5.399489, Accuracy 87.654%\n",
      "Epoch 20, Batch 769, LR 1.946274 Loss 5.399825, Accuracy 87.643%\n",
      "Epoch 20, Batch 770, LR 1.946163 Loss 5.400189, Accuracy 87.637%\n",
      "Epoch 20, Batch 771, LR 1.946052 Loss 5.401139, Accuracy 87.633%\n",
      "Epoch 20, Batch 772, LR 1.945941 Loss 5.400973, Accuracy 87.633%\n",
      "Epoch 20, Batch 773, LR 1.945829 Loss 5.400592, Accuracy 87.632%\n",
      "Epoch 20, Batch 774, LR 1.945718 Loss 5.400455, Accuracy 87.632%\n",
      "Epoch 20, Batch 775, LR 1.945607 Loss 5.400455, Accuracy 87.631%\n",
      "Epoch 20, Batch 776, LR 1.945495 Loss 5.400654, Accuracy 87.632%\n",
      "Epoch 20, Batch 777, LR 1.945384 Loss 5.400622, Accuracy 87.631%\n",
      "Epoch 20, Batch 778, LR 1.945273 Loss 5.400364, Accuracy 87.630%\n",
      "Epoch 20, Batch 779, LR 1.945162 Loss 5.400211, Accuracy 87.629%\n",
      "Epoch 20, Batch 780, LR 1.945050 Loss 5.399215, Accuracy 87.638%\n",
      "Epoch 20, Batch 781, LR 1.944939 Loss 5.399369, Accuracy 87.636%\n",
      "Epoch 20, Batch 782, LR 1.944828 Loss 5.399344, Accuracy 87.635%\n",
      "Epoch 20, Batch 783, LR 1.944716 Loss 5.398092, Accuracy 87.641%\n",
      "Epoch 20, Batch 784, LR 1.944605 Loss 5.397445, Accuracy 87.642%\n",
      "Epoch 20, Batch 785, LR 1.944493 Loss 5.397176, Accuracy 87.639%\n",
      "Epoch 20, Batch 786, LR 1.944382 Loss 5.396833, Accuracy 87.640%\n",
      "Epoch 20, Batch 787, LR 1.944271 Loss 5.396766, Accuracy 87.641%\n",
      "Epoch 20, Batch 788, LR 1.944159 Loss 5.396318, Accuracy 87.643%\n",
      "Epoch 20, Batch 789, LR 1.944048 Loss 5.395896, Accuracy 87.648%\n",
      "Epoch 20, Batch 790, LR 1.943936 Loss 5.396111, Accuracy 87.647%\n",
      "Epoch 20, Batch 791, LR 1.943825 Loss 5.396798, Accuracy 87.647%\n",
      "Epoch 20, Batch 792, LR 1.943714 Loss 5.397248, Accuracy 87.648%\n",
      "Epoch 20, Batch 793, LR 1.943602 Loss 5.396729, Accuracy 87.653%\n",
      "Epoch 20, Batch 794, LR 1.943491 Loss 5.397388, Accuracy 87.654%\n",
      "Epoch 20, Batch 795, LR 1.943379 Loss 5.397430, Accuracy 87.654%\n",
      "Epoch 20, Batch 796, LR 1.943268 Loss 5.397291, Accuracy 87.654%\n",
      "Epoch 20, Batch 797, LR 1.943156 Loss 5.397072, Accuracy 87.654%\n",
      "Epoch 20, Batch 798, LR 1.943045 Loss 5.395971, Accuracy 87.657%\n",
      "Epoch 20, Batch 799, LR 1.942933 Loss 5.396032, Accuracy 87.654%\n",
      "Epoch 20, Batch 800, LR 1.942822 Loss 5.395533, Accuracy 87.652%\n",
      "Epoch 20, Batch 801, LR 1.942710 Loss 5.395629, Accuracy 87.655%\n",
      "Epoch 20, Batch 802, LR 1.942599 Loss 5.395607, Accuracy 87.659%\n",
      "Epoch 20, Batch 803, LR 1.942487 Loss 5.395614, Accuracy 87.662%\n",
      "Epoch 20, Batch 804, LR 1.942376 Loss 5.395783, Accuracy 87.661%\n",
      "Epoch 20, Batch 805, LR 1.942264 Loss 5.395787, Accuracy 87.658%\n",
      "Epoch 20, Batch 806, LR 1.942153 Loss 5.395543, Accuracy 87.665%\n",
      "Epoch 20, Batch 807, LR 1.942041 Loss 5.395279, Accuracy 87.664%\n",
      "Epoch 20, Batch 808, LR 1.941930 Loss 5.394954, Accuracy 87.665%\n",
      "Epoch 20, Batch 809, LR 1.941818 Loss 5.394776, Accuracy 87.663%\n",
      "Epoch 20, Batch 810, LR 1.941707 Loss 5.395078, Accuracy 87.662%\n",
      "Epoch 20, Batch 811, LR 1.941595 Loss 5.395790, Accuracy 87.653%\n",
      "Epoch 20, Batch 812, LR 1.941483 Loss 5.395306, Accuracy 87.662%\n",
      "Epoch 20, Batch 813, LR 1.941372 Loss 5.394895, Accuracy 87.660%\n",
      "Epoch 20, Batch 814, LR 1.941260 Loss 5.394779, Accuracy 87.663%\n",
      "Epoch 20, Batch 815, LR 1.941149 Loss 5.395365, Accuracy 87.662%\n",
      "Epoch 20, Batch 816, LR 1.941037 Loss 5.395419, Accuracy 87.667%\n",
      "Epoch 20, Batch 817, LR 1.940925 Loss 5.394570, Accuracy 87.671%\n",
      "Epoch 20, Batch 818, LR 1.940814 Loss 5.394317, Accuracy 87.673%\n",
      "Epoch 20, Batch 819, LR 1.940702 Loss 5.393521, Accuracy 87.675%\n",
      "Epoch 20, Batch 820, LR 1.940590 Loss 5.393324, Accuracy 87.677%\n",
      "Epoch 20, Batch 821, LR 1.940479 Loss 5.393040, Accuracy 87.681%\n",
      "Epoch 20, Batch 822, LR 1.940367 Loss 5.392661, Accuracy 87.685%\n",
      "Epoch 20, Batch 823, LR 1.940255 Loss 5.391356, Accuracy 87.690%\n",
      "Epoch 20, Batch 824, LR 1.940144 Loss 5.391079, Accuracy 87.692%\n",
      "Epoch 20, Batch 825, LR 1.940032 Loss 5.390660, Accuracy 87.699%\n",
      "Epoch 20, Batch 826, LR 1.939920 Loss 5.390305, Accuracy 87.699%\n",
      "Epoch 20, Batch 827, LR 1.939809 Loss 5.390582, Accuracy 87.698%\n",
      "Epoch 20, Batch 828, LR 1.939697 Loss 5.390914, Accuracy 87.700%\n",
      "Epoch 20, Batch 829, LR 1.939585 Loss 5.391807, Accuracy 87.700%\n",
      "Epoch 20, Batch 830, LR 1.939473 Loss 5.392275, Accuracy 87.688%\n",
      "Epoch 20, Batch 831, LR 1.939362 Loss 5.392515, Accuracy 87.689%\n",
      "Epoch 20, Batch 832, LR 1.939250 Loss 5.392996, Accuracy 87.689%\n",
      "Epoch 20, Batch 833, LR 1.939138 Loss 5.393061, Accuracy 87.693%\n",
      "Epoch 20, Batch 834, LR 1.939026 Loss 5.392419, Accuracy 87.699%\n",
      "Epoch 20, Batch 835, LR 1.938915 Loss 5.392175, Accuracy 87.699%\n",
      "Epoch 20, Batch 836, LR 1.938803 Loss 5.392334, Accuracy 87.699%\n",
      "Epoch 20, Batch 837, LR 1.938691 Loss 5.391965, Accuracy 87.703%\n",
      "Epoch 20, Batch 838, LR 1.938579 Loss 5.392281, Accuracy 87.699%\n",
      "Epoch 20, Batch 839, LR 1.938468 Loss 5.392281, Accuracy 87.699%\n",
      "Epoch 20, Batch 840, LR 1.938356 Loss 5.391853, Accuracy 87.697%\n",
      "Epoch 20, Batch 841, LR 1.938244 Loss 5.392006, Accuracy 87.699%\n",
      "Epoch 20, Batch 842, LR 1.938132 Loss 5.392580, Accuracy 87.699%\n",
      "Epoch 20, Batch 843, LR 1.938020 Loss 5.393221, Accuracy 87.693%\n",
      "Epoch 20, Batch 844, LR 1.937908 Loss 5.393192, Accuracy 87.695%\n",
      "Epoch 20, Batch 845, LR 1.937797 Loss 5.393235, Accuracy 87.697%\n",
      "Epoch 20, Batch 846, LR 1.937685 Loss 5.393188, Accuracy 87.698%\n",
      "Epoch 20, Batch 847, LR 1.937573 Loss 5.393496, Accuracy 87.696%\n",
      "Epoch 20, Batch 848, LR 1.937461 Loss 5.393361, Accuracy 87.698%\n",
      "Epoch 20, Batch 849, LR 1.937349 Loss 5.393660, Accuracy 87.698%\n",
      "Epoch 20, Batch 850, LR 1.937237 Loss 5.393299, Accuracy 87.697%\n",
      "Epoch 20, Batch 851, LR 1.937125 Loss 5.392271, Accuracy 87.702%\n",
      "Epoch 20, Batch 852, LR 1.937013 Loss 5.390842, Accuracy 87.704%\n",
      "Epoch 20, Batch 853, LR 1.936901 Loss 5.391525, Accuracy 87.701%\n",
      "Epoch 20, Batch 854, LR 1.936790 Loss 5.390485, Accuracy 87.708%\n",
      "Epoch 20, Batch 855, LR 1.936678 Loss 5.390070, Accuracy 87.710%\n",
      "Epoch 20, Batch 856, LR 1.936566 Loss 5.389773, Accuracy 87.713%\n",
      "Epoch 20, Batch 857, LR 1.936454 Loss 5.390492, Accuracy 87.707%\n",
      "Epoch 20, Batch 858, LR 1.936342 Loss 5.391147, Accuracy 87.707%\n",
      "Epoch 20, Batch 859, LR 1.936230 Loss 5.390850, Accuracy 87.710%\n",
      "Epoch 20, Batch 860, LR 1.936118 Loss 5.390812, Accuracy 87.711%\n",
      "Epoch 20, Batch 861, LR 1.936006 Loss 5.390387, Accuracy 87.712%\n",
      "Epoch 20, Batch 862, LR 1.935894 Loss 5.389778, Accuracy 87.717%\n",
      "Epoch 20, Batch 863, LR 1.935782 Loss 5.390898, Accuracy 87.709%\n",
      "Epoch 20, Batch 864, LR 1.935670 Loss 5.390356, Accuracy 87.716%\n",
      "Epoch 20, Batch 865, LR 1.935558 Loss 5.390676, Accuracy 87.716%\n",
      "Epoch 20, Batch 866, LR 1.935446 Loss 5.391227, Accuracy 87.713%\n",
      "Epoch 20, Batch 867, LR 1.935334 Loss 5.392332, Accuracy 87.709%\n",
      "Epoch 20, Batch 868, LR 1.935222 Loss 5.392666, Accuracy 87.707%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 869, LR 1.935110 Loss 5.392866, Accuracy 87.707%\n",
      "Epoch 20, Batch 870, LR 1.934998 Loss 5.392821, Accuracy 87.707%\n",
      "Epoch 20, Batch 871, LR 1.934886 Loss 5.393072, Accuracy 87.702%\n",
      "Epoch 20, Batch 872, LR 1.934774 Loss 5.393109, Accuracy 87.702%\n",
      "Epoch 20, Batch 873, LR 1.934662 Loss 5.392917, Accuracy 87.703%\n",
      "Epoch 20, Batch 874, LR 1.934550 Loss 5.392960, Accuracy 87.699%\n",
      "Epoch 20, Batch 875, LR 1.934437 Loss 5.393136, Accuracy 87.701%\n",
      "Epoch 20, Batch 876, LR 1.934325 Loss 5.393345, Accuracy 87.702%\n",
      "Epoch 20, Batch 877, LR 1.934213 Loss 5.392965, Accuracy 87.706%\n",
      "Epoch 20, Batch 878, LR 1.934101 Loss 5.393486, Accuracy 87.705%\n",
      "Epoch 20, Batch 879, LR 1.933989 Loss 5.393528, Accuracy 87.702%\n",
      "Epoch 20, Batch 880, LR 1.933877 Loss 5.393502, Accuracy 87.703%\n",
      "Epoch 20, Batch 881, LR 1.933765 Loss 5.392711, Accuracy 87.708%\n",
      "Epoch 20, Batch 882, LR 1.933653 Loss 5.392003, Accuracy 87.710%\n",
      "Epoch 20, Batch 883, LR 1.933540 Loss 5.391749, Accuracy 87.708%\n",
      "Epoch 20, Batch 884, LR 1.933428 Loss 5.391417, Accuracy 87.707%\n",
      "Epoch 20, Batch 885, LR 1.933316 Loss 5.391598, Accuracy 87.702%\n",
      "Epoch 20, Batch 886, LR 1.933204 Loss 5.392159, Accuracy 87.696%\n",
      "Epoch 20, Batch 887, LR 1.933092 Loss 5.391949, Accuracy 87.698%\n",
      "Epoch 20, Batch 888, LR 1.932980 Loss 5.392176, Accuracy 87.697%\n",
      "Epoch 20, Batch 889, LR 1.932867 Loss 5.392186, Accuracy 87.696%\n",
      "Epoch 20, Batch 890, LR 1.932755 Loss 5.392175, Accuracy 87.698%\n",
      "Epoch 20, Batch 891, LR 1.932643 Loss 5.391920, Accuracy 87.695%\n",
      "Epoch 20, Batch 892, LR 1.932531 Loss 5.392668, Accuracy 87.689%\n",
      "Epoch 20, Batch 893, LR 1.932419 Loss 5.392531, Accuracy 87.692%\n",
      "Epoch 20, Batch 894, LR 1.932306 Loss 5.392627, Accuracy 87.693%\n",
      "Epoch 20, Batch 895, LR 1.932194 Loss 5.392349, Accuracy 87.699%\n",
      "Epoch 20, Batch 896, LR 1.932082 Loss 5.392618, Accuracy 87.699%\n",
      "Epoch 20, Batch 897, LR 1.931970 Loss 5.392542, Accuracy 87.700%\n",
      "Epoch 20, Batch 898, LR 1.931857 Loss 5.392876, Accuracy 87.701%\n",
      "Epoch 20, Batch 899, LR 1.931745 Loss 5.392377, Accuracy 87.704%\n",
      "Epoch 20, Batch 900, LR 1.931633 Loss 5.391898, Accuracy 87.706%\n",
      "Epoch 20, Batch 901, LR 1.931520 Loss 5.392971, Accuracy 87.699%\n",
      "Epoch 20, Batch 902, LR 1.931408 Loss 5.393111, Accuracy 87.696%\n",
      "Epoch 20, Batch 903, LR 1.931296 Loss 5.392866, Accuracy 87.696%\n",
      "Epoch 20, Batch 904, LR 1.931184 Loss 5.393037, Accuracy 87.697%\n",
      "Epoch 20, Batch 905, LR 1.931071 Loss 5.393121, Accuracy 87.699%\n",
      "Epoch 20, Batch 906, LR 1.930959 Loss 5.394386, Accuracy 87.696%\n",
      "Epoch 20, Batch 907, LR 1.930847 Loss 5.395435, Accuracy 87.689%\n",
      "Epoch 20, Batch 908, LR 1.930734 Loss 5.394947, Accuracy 87.693%\n",
      "Epoch 20, Batch 909, LR 1.930622 Loss 5.394258, Accuracy 87.699%\n",
      "Epoch 20, Batch 910, LR 1.930510 Loss 5.394419, Accuracy 87.698%\n",
      "Epoch 20, Batch 911, LR 1.930397 Loss 5.394094, Accuracy 87.701%\n",
      "Epoch 20, Batch 912, LR 1.930285 Loss 5.393783, Accuracy 87.701%\n",
      "Epoch 20, Batch 913, LR 1.930172 Loss 5.394246, Accuracy 87.696%\n",
      "Epoch 20, Batch 914, LR 1.930060 Loss 5.394588, Accuracy 87.694%\n",
      "Epoch 20, Batch 915, LR 1.929948 Loss 5.394827, Accuracy 87.692%\n",
      "Epoch 20, Batch 916, LR 1.929835 Loss 5.395282, Accuracy 87.694%\n",
      "Epoch 20, Batch 917, LR 1.929723 Loss 5.394574, Accuracy 87.698%\n",
      "Epoch 20, Batch 918, LR 1.929610 Loss 5.394608, Accuracy 87.696%\n",
      "Epoch 20, Batch 919, LR 1.929498 Loss 5.395022, Accuracy 87.693%\n",
      "Epoch 20, Batch 920, LR 1.929386 Loss 5.394810, Accuracy 87.692%\n",
      "Epoch 20, Batch 921, LR 1.929273 Loss 5.394758, Accuracy 87.691%\n",
      "Epoch 20, Batch 922, LR 1.929161 Loss 5.395143, Accuracy 87.687%\n",
      "Epoch 20, Batch 923, LR 1.929048 Loss 5.394175, Accuracy 87.688%\n",
      "Epoch 20, Batch 924, LR 1.928936 Loss 5.394125, Accuracy 87.684%\n",
      "Epoch 20, Batch 925, LR 1.928823 Loss 5.393878, Accuracy 87.687%\n",
      "Epoch 20, Batch 926, LR 1.928711 Loss 5.393744, Accuracy 87.688%\n",
      "Epoch 20, Batch 927, LR 1.928598 Loss 5.393506, Accuracy 87.686%\n",
      "Epoch 20, Batch 928, LR 1.928486 Loss 5.393803, Accuracy 87.684%\n",
      "Epoch 20, Batch 929, LR 1.928373 Loss 5.394005, Accuracy 87.687%\n",
      "Epoch 20, Batch 930, LR 1.928261 Loss 5.394025, Accuracy 87.687%\n",
      "Epoch 20, Batch 931, LR 1.928148 Loss 5.393872, Accuracy 87.688%\n",
      "Epoch 20, Batch 932, LR 1.928036 Loss 5.394349, Accuracy 87.685%\n",
      "Epoch 20, Batch 933, LR 1.927923 Loss 5.394029, Accuracy 87.688%\n",
      "Epoch 20, Batch 934, LR 1.927811 Loss 5.393717, Accuracy 87.692%\n",
      "Epoch 20, Batch 935, LR 1.927698 Loss 5.393640, Accuracy 87.693%\n",
      "Epoch 20, Batch 936, LR 1.927585 Loss 5.393950, Accuracy 87.692%\n",
      "Epoch 20, Batch 937, LR 1.927473 Loss 5.394711, Accuracy 87.685%\n",
      "Epoch 20, Batch 938, LR 1.927360 Loss 5.395105, Accuracy 87.682%\n",
      "Epoch 20, Batch 939, LR 1.927248 Loss 5.394933, Accuracy 87.682%\n",
      "Epoch 20, Batch 940, LR 1.927135 Loss 5.394911, Accuracy 87.681%\n",
      "Epoch 20, Batch 941, LR 1.927023 Loss 5.394713, Accuracy 87.681%\n",
      "Epoch 20, Batch 942, LR 1.926910 Loss 5.394861, Accuracy 87.680%\n",
      "Epoch 20, Batch 943, LR 1.926797 Loss 5.394625, Accuracy 87.681%\n",
      "Epoch 20, Batch 944, LR 1.926685 Loss 5.395551, Accuracy 87.677%\n",
      "Epoch 20, Batch 945, LR 1.926572 Loss 5.395424, Accuracy 87.674%\n",
      "Epoch 20, Batch 946, LR 1.926459 Loss 5.395372, Accuracy 87.672%\n",
      "Epoch 20, Batch 947, LR 1.926347 Loss 5.395147, Accuracy 87.671%\n",
      "Epoch 20, Batch 948, LR 1.926234 Loss 5.394834, Accuracy 87.671%\n",
      "Epoch 20, Batch 949, LR 1.926121 Loss 5.394891, Accuracy 87.675%\n",
      "Epoch 20, Batch 950, LR 1.926009 Loss 5.394557, Accuracy 87.677%\n",
      "Epoch 20, Batch 951, LR 1.925896 Loss 5.394800, Accuracy 87.677%\n",
      "Epoch 20, Batch 952, LR 1.925783 Loss 5.395180, Accuracy 87.678%\n",
      "Epoch 20, Batch 953, LR 1.925671 Loss 5.395366, Accuracy 87.675%\n",
      "Epoch 20, Batch 954, LR 1.925558 Loss 5.395135, Accuracy 87.674%\n",
      "Epoch 20, Batch 955, LR 1.925445 Loss 5.395138, Accuracy 87.675%\n",
      "Epoch 20, Batch 956, LR 1.925333 Loss 5.394952, Accuracy 87.678%\n",
      "Epoch 20, Batch 957, LR 1.925220 Loss 5.394711, Accuracy 87.678%\n",
      "Epoch 20, Batch 958, LR 1.925107 Loss 5.394329, Accuracy 87.682%\n",
      "Epoch 20, Batch 959, LR 1.924994 Loss 5.394002, Accuracy 87.686%\n",
      "Epoch 20, Batch 960, LR 1.924882 Loss 5.394272, Accuracy 87.683%\n",
      "Epoch 20, Batch 961, LR 1.924769 Loss 5.394135, Accuracy 87.685%\n",
      "Epoch 20, Batch 962, LR 1.924656 Loss 5.394258, Accuracy 87.681%\n",
      "Epoch 20, Batch 963, LR 1.924543 Loss 5.394425, Accuracy 87.681%\n",
      "Epoch 20, Batch 964, LR 1.924431 Loss 5.394284, Accuracy 87.682%\n",
      "Epoch 20, Batch 965, LR 1.924318 Loss 5.394301, Accuracy 87.685%\n",
      "Epoch 20, Batch 966, LR 1.924205 Loss 5.394612, Accuracy 87.688%\n",
      "Epoch 20, Batch 967, LR 1.924092 Loss 5.394201, Accuracy 87.691%\n",
      "Epoch 20, Batch 968, LR 1.923979 Loss 5.394625, Accuracy 87.686%\n",
      "Epoch 20, Batch 969, LR 1.923867 Loss 5.395000, Accuracy 87.690%\n",
      "Epoch 20, Batch 970, LR 1.923754 Loss 5.395538, Accuracy 87.686%\n",
      "Epoch 20, Batch 971, LR 1.923641 Loss 5.395657, Accuracy 87.687%\n",
      "Epoch 20, Batch 972, LR 1.923528 Loss 5.395758, Accuracy 87.686%\n",
      "Epoch 20, Batch 973, LR 1.923415 Loss 5.395782, Accuracy 87.689%\n",
      "Epoch 20, Batch 974, LR 1.923302 Loss 5.395727, Accuracy 87.686%\n",
      "Epoch 20, Batch 975, LR 1.923189 Loss 5.396195, Accuracy 87.683%\n",
      "Epoch 20, Batch 976, LR 1.923077 Loss 5.396848, Accuracy 87.679%\n",
      "Epoch 20, Batch 977, LR 1.922964 Loss 5.397391, Accuracy 87.679%\n",
      "Epoch 20, Batch 978, LR 1.922851 Loss 5.398207, Accuracy 87.678%\n",
      "Epoch 20, Batch 979, LR 1.922738 Loss 5.398415, Accuracy 87.677%\n",
      "Epoch 20, Batch 980, LR 1.922625 Loss 5.397856, Accuracy 87.679%\n",
      "Epoch 20, Batch 981, LR 1.922512 Loss 5.397621, Accuracy 87.686%\n",
      "Epoch 20, Batch 982, LR 1.922399 Loss 5.397845, Accuracy 87.685%\n",
      "Epoch 20, Batch 983, LR 1.922286 Loss 5.397124, Accuracy 87.688%\n",
      "Epoch 20, Batch 984, LR 1.922173 Loss 5.396954, Accuracy 87.686%\n",
      "Epoch 20, Batch 985, LR 1.922060 Loss 5.396619, Accuracy 87.686%\n",
      "Epoch 20, Batch 986, LR 1.921947 Loss 5.397044, Accuracy 87.689%\n",
      "Epoch 20, Batch 987, LR 1.921835 Loss 5.397285, Accuracy 87.688%\n",
      "Epoch 20, Batch 988, LR 1.921722 Loss 5.397168, Accuracy 87.689%\n",
      "Epoch 20, Batch 989, LR 1.921609 Loss 5.396848, Accuracy 87.695%\n",
      "Epoch 20, Batch 990, LR 1.921496 Loss 5.397385, Accuracy 87.693%\n",
      "Epoch 20, Batch 991, LR 1.921383 Loss 5.397939, Accuracy 87.689%\n",
      "Epoch 20, Batch 992, LR 1.921270 Loss 5.398083, Accuracy 87.688%\n",
      "Epoch 20, Batch 993, LR 1.921157 Loss 5.397838, Accuracy 87.690%\n",
      "Epoch 20, Batch 994, LR 1.921044 Loss 5.397445, Accuracy 87.693%\n",
      "Epoch 20, Batch 995, LR 1.920931 Loss 5.397383, Accuracy 87.694%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 996, LR 1.920818 Loss 5.397736, Accuracy 87.691%\n",
      "Epoch 20, Batch 997, LR 1.920705 Loss 5.397420, Accuracy 87.694%\n",
      "Epoch 20, Batch 998, LR 1.920591 Loss 5.397736, Accuracy 87.694%\n",
      "Epoch 20, Batch 999, LR 1.920478 Loss 5.397562, Accuracy 87.696%\n",
      "Epoch 20, Batch 1000, LR 1.920365 Loss 5.397921, Accuracy 87.693%\n",
      "Epoch 20, Batch 1001, LR 1.920252 Loss 5.398512, Accuracy 87.690%\n",
      "Epoch 20, Batch 1002, LR 1.920139 Loss 5.397789, Accuracy 87.695%\n",
      "Epoch 20, Batch 1003, LR 1.920026 Loss 5.397184, Accuracy 87.696%\n",
      "Epoch 20, Batch 1004, LR 1.919913 Loss 5.397140, Accuracy 87.699%\n",
      "Epoch 20, Batch 1005, LR 1.919800 Loss 5.396842, Accuracy 87.704%\n",
      "Epoch 20, Batch 1006, LR 1.919687 Loss 5.396428, Accuracy 87.707%\n",
      "Epoch 20, Batch 1007, LR 1.919574 Loss 5.396421, Accuracy 87.709%\n",
      "Epoch 20, Batch 1008, LR 1.919461 Loss 5.396291, Accuracy 87.712%\n",
      "Epoch 20, Batch 1009, LR 1.919348 Loss 5.397028, Accuracy 87.704%\n",
      "Epoch 20, Batch 1010, LR 1.919234 Loss 5.397571, Accuracy 87.701%\n",
      "Epoch 20, Batch 1011, LR 1.919121 Loss 5.397903, Accuracy 87.702%\n",
      "Epoch 20, Batch 1012, LR 1.919008 Loss 5.397843, Accuracy 87.706%\n",
      "Epoch 20, Batch 1013, LR 1.918895 Loss 5.397908, Accuracy 87.707%\n",
      "Epoch 20, Batch 1014, LR 1.918782 Loss 5.397754, Accuracy 87.704%\n",
      "Epoch 20, Batch 1015, LR 1.918669 Loss 5.396745, Accuracy 87.711%\n",
      "Epoch 20, Batch 1016, LR 1.918555 Loss 5.397048, Accuracy 87.707%\n",
      "Epoch 20, Batch 1017, LR 1.918442 Loss 5.397138, Accuracy 87.708%\n",
      "Epoch 20, Batch 1018, LR 1.918329 Loss 5.397311, Accuracy 87.705%\n",
      "Epoch 20, Batch 1019, LR 1.918216 Loss 5.397988, Accuracy 87.700%\n",
      "Epoch 20, Batch 1020, LR 1.918103 Loss 5.398222, Accuracy 87.701%\n",
      "Epoch 20, Batch 1021, LR 1.917989 Loss 5.399117, Accuracy 87.694%\n",
      "Epoch 20, Batch 1022, LR 1.917876 Loss 5.398973, Accuracy 87.694%\n",
      "Epoch 20, Batch 1023, LR 1.917763 Loss 5.398735, Accuracy 87.698%\n",
      "Epoch 20, Batch 1024, LR 1.917650 Loss 5.398272, Accuracy 87.697%\n",
      "Epoch 20, Batch 1025, LR 1.917536 Loss 5.398426, Accuracy 87.696%\n",
      "Epoch 20, Batch 1026, LR 1.917423 Loss 5.398580, Accuracy 87.694%\n",
      "Epoch 20, Batch 1027, LR 1.917310 Loss 5.398113, Accuracy 87.695%\n",
      "Epoch 20, Batch 1028, LR 1.917197 Loss 5.397937, Accuracy 87.695%\n",
      "Epoch 20, Batch 1029, LR 1.917083 Loss 5.397494, Accuracy 87.694%\n",
      "Epoch 20, Batch 1030, LR 1.916970 Loss 5.397637, Accuracy 87.693%\n",
      "Epoch 20, Batch 1031, LR 1.916857 Loss 5.397748, Accuracy 87.692%\n",
      "Epoch 20, Batch 1032, LR 1.916743 Loss 5.397902, Accuracy 87.692%\n",
      "Epoch 20, Batch 1033, LR 1.916630 Loss 5.397793, Accuracy 87.690%\n",
      "Epoch 20, Batch 1034, LR 1.916517 Loss 5.397799, Accuracy 87.689%\n",
      "Epoch 20, Batch 1035, LR 1.916404 Loss 5.397351, Accuracy 87.689%\n",
      "Epoch 20, Batch 1036, LR 1.916290 Loss 5.397578, Accuracy 87.688%\n",
      "Epoch 20, Batch 1037, LR 1.916177 Loss 5.397781, Accuracy 87.686%\n",
      "Epoch 20, Batch 1038, LR 1.916064 Loss 5.398115, Accuracy 87.681%\n",
      "Epoch 20, Batch 1039, LR 1.915950 Loss 5.398137, Accuracy 87.677%\n",
      "Epoch 20, Batch 1040, LR 1.915837 Loss 5.398451, Accuracy 87.677%\n",
      "Epoch 20, Batch 1041, LR 1.915723 Loss 5.398486, Accuracy 87.676%\n",
      "Epoch 20, Batch 1042, LR 1.915610 Loss 5.398604, Accuracy 87.675%\n",
      "Epoch 20, Batch 1043, LR 1.915497 Loss 5.398473, Accuracy 87.675%\n",
      "Epoch 20, Batch 1044, LR 1.915383 Loss 5.399198, Accuracy 87.668%\n",
      "Epoch 20, Batch 1045, LR 1.915270 Loss 5.399480, Accuracy 87.670%\n",
      "Epoch 20, Batch 1046, LR 1.915156 Loss 5.398803, Accuracy 87.673%\n",
      "Epoch 20, Batch 1047, LR 1.915043 Loss 5.398685, Accuracy 87.674%\n",
      "Epoch 20, Loss (train set) 5.398685, Accuracy (train set) 87.674%\n",
      "Epoch 21, Batch 1, LR 1.914930 Loss 5.073194, Accuracy 90.625%\n",
      "Epoch 21, Batch 2, LR 1.914816 Loss 5.142281, Accuracy 89.844%\n",
      "Epoch 21, Batch 3, LR 1.914703 Loss 5.244287, Accuracy 88.021%\n",
      "Epoch 21, Batch 4, LR 1.914589 Loss 5.391680, Accuracy 87.500%\n",
      "Epoch 21, Batch 5, LR 1.914476 Loss 5.445565, Accuracy 87.344%\n",
      "Epoch 21, Batch 6, LR 1.914362 Loss 5.522022, Accuracy 87.500%\n",
      "Epoch 21, Batch 7, LR 1.914249 Loss 5.523364, Accuracy 87.277%\n",
      "Epoch 21, Batch 8, LR 1.914135 Loss 5.440156, Accuracy 87.402%\n",
      "Epoch 21, Batch 9, LR 1.914022 Loss 5.316038, Accuracy 87.674%\n",
      "Epoch 21, Batch 10, LR 1.913908 Loss 5.305431, Accuracy 87.578%\n",
      "Epoch 21, Batch 11, LR 1.913795 Loss 5.299597, Accuracy 87.784%\n",
      "Epoch 21, Batch 12, LR 1.913681 Loss 5.265281, Accuracy 88.151%\n",
      "Epoch 21, Batch 13, LR 1.913568 Loss 5.294053, Accuracy 88.161%\n",
      "Epoch 21, Batch 14, LR 1.913454 Loss 5.257814, Accuracy 88.616%\n",
      "Epoch 21, Batch 15, LR 1.913341 Loss 5.247855, Accuracy 88.646%\n",
      "Epoch 21, Batch 16, LR 1.913227 Loss 5.170815, Accuracy 89.062%\n",
      "Epoch 21, Batch 17, LR 1.913114 Loss 5.181392, Accuracy 89.246%\n",
      "Epoch 21, Batch 18, LR 1.913000 Loss 5.195799, Accuracy 89.106%\n",
      "Epoch 21, Batch 19, LR 1.912887 Loss 5.178975, Accuracy 89.186%\n",
      "Epoch 21, Batch 20, LR 1.912773 Loss 5.172314, Accuracy 89.258%\n",
      "Epoch 21, Batch 21, LR 1.912659 Loss 5.150512, Accuracy 89.286%\n",
      "Epoch 21, Batch 22, LR 1.912546 Loss 5.157540, Accuracy 89.098%\n",
      "Epoch 21, Batch 23, LR 1.912432 Loss 5.164604, Accuracy 89.130%\n",
      "Epoch 21, Batch 24, LR 1.912319 Loss 5.182055, Accuracy 88.997%\n",
      "Epoch 21, Batch 25, LR 1.912205 Loss 5.191431, Accuracy 88.938%\n",
      "Epoch 21, Batch 26, LR 1.912091 Loss 5.210370, Accuracy 88.762%\n",
      "Epoch 21, Batch 27, LR 1.911978 Loss 5.231255, Accuracy 88.571%\n",
      "Epoch 21, Batch 28, LR 1.911864 Loss 5.247131, Accuracy 88.365%\n",
      "Epoch 21, Batch 29, LR 1.911751 Loss 5.227231, Accuracy 88.470%\n",
      "Epoch 21, Batch 30, LR 1.911637 Loss 5.239764, Accuracy 88.385%\n",
      "Epoch 21, Batch 31, LR 1.911523 Loss 5.213321, Accuracy 88.508%\n",
      "Epoch 21, Batch 32, LR 1.911410 Loss 5.221046, Accuracy 88.525%\n",
      "Epoch 21, Batch 33, LR 1.911296 Loss 5.197268, Accuracy 88.707%\n",
      "Epoch 21, Batch 34, LR 1.911182 Loss 5.195555, Accuracy 88.649%\n",
      "Epoch 21, Batch 35, LR 1.911069 Loss 5.197887, Accuracy 88.638%\n",
      "Epoch 21, Batch 36, LR 1.910955 Loss 5.213110, Accuracy 88.542%\n",
      "Epoch 21, Batch 37, LR 1.910841 Loss 5.205426, Accuracy 88.577%\n",
      "Epoch 21, Batch 38, LR 1.910728 Loss 5.199317, Accuracy 88.590%\n",
      "Epoch 21, Batch 39, LR 1.910614 Loss 5.187347, Accuracy 88.602%\n",
      "Epoch 21, Batch 40, LR 1.910500 Loss 5.184357, Accuracy 88.711%\n",
      "Epoch 21, Batch 41, LR 1.910386 Loss 5.199265, Accuracy 88.700%\n",
      "Epoch 21, Batch 42, LR 1.910273 Loss 5.183951, Accuracy 88.728%\n",
      "Epoch 21, Batch 43, LR 1.910159 Loss 5.170592, Accuracy 88.772%\n",
      "Epoch 21, Batch 44, LR 1.910045 Loss 5.156330, Accuracy 88.849%\n",
      "Epoch 21, Batch 45, LR 1.909931 Loss 5.161139, Accuracy 88.785%\n",
      "Epoch 21, Batch 46, LR 1.909818 Loss 5.154552, Accuracy 88.808%\n",
      "Epoch 21, Batch 47, LR 1.909704 Loss 5.139983, Accuracy 88.863%\n",
      "Epoch 21, Batch 48, LR 1.909590 Loss 5.138520, Accuracy 88.883%\n",
      "Epoch 21, Batch 49, LR 1.909476 Loss 5.158588, Accuracy 88.871%\n",
      "Epoch 21, Batch 50, LR 1.909362 Loss 5.164927, Accuracy 88.844%\n",
      "Epoch 21, Batch 51, LR 1.909249 Loss 5.167428, Accuracy 88.802%\n",
      "Epoch 21, Batch 52, LR 1.909135 Loss 5.172381, Accuracy 88.717%\n",
      "Epoch 21, Batch 53, LR 1.909021 Loss 5.175518, Accuracy 88.709%\n",
      "Epoch 21, Batch 54, LR 1.908907 Loss 5.165388, Accuracy 88.802%\n",
      "Epoch 21, Batch 55, LR 1.908793 Loss 5.156724, Accuracy 88.864%\n",
      "Epoch 21, Batch 56, LR 1.908679 Loss 5.147070, Accuracy 88.895%\n",
      "Epoch 21, Batch 57, LR 1.908566 Loss 5.157659, Accuracy 88.857%\n",
      "Epoch 21, Batch 58, LR 1.908452 Loss 5.161844, Accuracy 88.793%\n",
      "Epoch 21, Batch 59, LR 1.908338 Loss 5.155362, Accuracy 88.837%\n",
      "Epoch 21, Batch 60, LR 1.908224 Loss 5.152521, Accuracy 88.802%\n",
      "Epoch 21, Batch 61, LR 1.908110 Loss 5.159714, Accuracy 88.665%\n",
      "Epoch 21, Batch 62, LR 1.907996 Loss 5.160691, Accuracy 88.659%\n",
      "Epoch 21, Batch 63, LR 1.907882 Loss 5.160864, Accuracy 88.678%\n",
      "Epoch 21, Batch 64, LR 1.907768 Loss 5.164162, Accuracy 88.660%\n",
      "Epoch 21, Batch 65, LR 1.907655 Loss 5.159793, Accuracy 88.702%\n",
      "Epoch 21, Batch 66, LR 1.907541 Loss 5.168115, Accuracy 88.684%\n",
      "Epoch 21, Batch 67, LR 1.907427 Loss 5.172525, Accuracy 88.643%\n",
      "Epoch 21, Batch 68, LR 1.907313 Loss 5.172887, Accuracy 88.672%\n",
      "Epoch 21, Batch 69, LR 1.907199 Loss 5.171738, Accuracy 88.700%\n",
      "Epoch 21, Batch 70, LR 1.907085 Loss 5.178390, Accuracy 88.627%\n",
      "Epoch 21, Batch 71, LR 1.906971 Loss 5.176852, Accuracy 88.655%\n",
      "Epoch 21, Batch 72, LR 1.906857 Loss 5.164036, Accuracy 88.737%\n",
      "Epoch 21, Batch 73, LR 1.906743 Loss 5.174475, Accuracy 88.667%\n",
      "Epoch 21, Batch 74, LR 1.906629 Loss 5.183623, Accuracy 88.651%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 75, LR 1.906515 Loss 5.180035, Accuracy 88.646%\n",
      "Epoch 21, Batch 76, LR 1.906401 Loss 5.183118, Accuracy 88.600%\n",
      "Epoch 21, Batch 77, LR 1.906287 Loss 5.183438, Accuracy 88.586%\n",
      "Epoch 21, Batch 78, LR 1.906173 Loss 5.177321, Accuracy 88.642%\n",
      "Epoch 21, Batch 79, LR 1.906059 Loss 5.183763, Accuracy 88.617%\n",
      "Epoch 21, Batch 80, LR 1.905945 Loss 5.185010, Accuracy 88.594%\n",
      "Epoch 21, Batch 81, LR 1.905831 Loss 5.182145, Accuracy 88.609%\n",
      "Epoch 21, Batch 82, LR 1.905717 Loss 5.180214, Accuracy 88.634%\n",
      "Epoch 21, Batch 83, LR 1.905603 Loss 5.176424, Accuracy 88.648%\n",
      "Epoch 21, Batch 84, LR 1.905489 Loss 5.188473, Accuracy 88.570%\n",
      "Epoch 21, Batch 85, LR 1.905375 Loss 5.191282, Accuracy 88.603%\n",
      "Epoch 21, Batch 86, LR 1.905261 Loss 5.193166, Accuracy 88.599%\n",
      "Epoch 21, Batch 87, LR 1.905147 Loss 5.192575, Accuracy 88.614%\n",
      "Epoch 21, Batch 88, LR 1.905033 Loss 5.193159, Accuracy 88.610%\n",
      "Epoch 21, Batch 89, LR 1.904918 Loss 5.190771, Accuracy 88.632%\n",
      "Epoch 21, Batch 90, LR 1.904804 Loss 5.188816, Accuracy 88.620%\n",
      "Epoch 21, Batch 91, LR 1.904690 Loss 5.190146, Accuracy 88.599%\n",
      "Epoch 21, Batch 92, LR 1.904576 Loss 5.186853, Accuracy 88.604%\n",
      "Epoch 21, Batch 93, LR 1.904462 Loss 5.185679, Accuracy 88.600%\n",
      "Epoch 21, Batch 94, LR 1.904348 Loss 5.189575, Accuracy 88.564%\n",
      "Epoch 21, Batch 95, LR 1.904234 Loss 5.190731, Accuracy 88.577%\n",
      "Epoch 21, Batch 96, LR 1.904120 Loss 5.190054, Accuracy 88.558%\n",
      "Epoch 21, Batch 97, LR 1.904005 Loss 5.180554, Accuracy 88.628%\n",
      "Epoch 21, Batch 98, LR 1.903891 Loss 5.182424, Accuracy 88.592%\n",
      "Epoch 21, Batch 99, LR 1.903777 Loss 5.184142, Accuracy 88.557%\n",
      "Epoch 21, Batch 100, LR 1.903663 Loss 5.182666, Accuracy 88.539%\n",
      "Epoch 21, Batch 101, LR 1.903549 Loss 5.184528, Accuracy 88.544%\n",
      "Epoch 21, Batch 102, LR 1.903435 Loss 5.186668, Accuracy 88.549%\n",
      "Epoch 21, Batch 103, LR 1.903320 Loss 5.186219, Accuracy 88.532%\n",
      "Epoch 21, Batch 104, LR 1.903206 Loss 5.185519, Accuracy 88.552%\n",
      "Epoch 21, Batch 105, LR 1.903092 Loss 5.186094, Accuracy 88.557%\n",
      "Epoch 21, Batch 106, LR 1.902978 Loss 5.191852, Accuracy 88.524%\n",
      "Epoch 21, Batch 107, LR 1.902863 Loss 5.193008, Accuracy 88.500%\n",
      "Epoch 21, Batch 108, LR 1.902749 Loss 5.196438, Accuracy 88.484%\n",
      "Epoch 21, Batch 109, LR 1.902635 Loss 5.195305, Accuracy 88.525%\n",
      "Epoch 21, Batch 110, LR 1.902521 Loss 5.196440, Accuracy 88.509%\n",
      "Epoch 21, Batch 111, LR 1.902406 Loss 5.194762, Accuracy 88.556%\n",
      "Epoch 21, Batch 112, LR 1.902292 Loss 5.195569, Accuracy 88.574%\n",
      "Epoch 21, Batch 113, LR 1.902178 Loss 5.194930, Accuracy 88.572%\n",
      "Epoch 21, Batch 114, LR 1.902064 Loss 5.195165, Accuracy 88.562%\n",
      "Epoch 21, Batch 115, LR 1.901949 Loss 5.195541, Accuracy 88.546%\n",
      "Epoch 21, Batch 116, LR 1.901835 Loss 5.200935, Accuracy 88.537%\n",
      "Epoch 21, Batch 117, LR 1.901721 Loss 5.201040, Accuracy 88.548%\n",
      "Epoch 21, Batch 118, LR 1.901606 Loss 5.193779, Accuracy 88.566%\n",
      "Epoch 21, Batch 119, LR 1.901492 Loss 5.196803, Accuracy 88.557%\n",
      "Epoch 21, Batch 120, LR 1.901378 Loss 5.196385, Accuracy 88.581%\n",
      "Epoch 21, Batch 121, LR 1.901264 Loss 5.202890, Accuracy 88.533%\n",
      "Epoch 21, Batch 122, LR 1.901149 Loss 5.200367, Accuracy 88.544%\n",
      "Epoch 21, Batch 123, LR 1.901035 Loss 5.202323, Accuracy 88.504%\n",
      "Epoch 21, Batch 124, LR 1.900920 Loss 5.193213, Accuracy 88.546%\n",
      "Epoch 21, Batch 125, LR 1.900806 Loss 5.192232, Accuracy 88.562%\n",
      "Epoch 21, Batch 126, LR 1.900692 Loss 5.190708, Accuracy 88.554%\n",
      "Epoch 21, Batch 127, LR 1.900577 Loss 5.192212, Accuracy 88.552%\n",
      "Epoch 21, Batch 128, LR 1.900463 Loss 5.194758, Accuracy 88.544%\n",
      "Epoch 21, Batch 129, LR 1.900349 Loss 5.189798, Accuracy 88.572%\n",
      "Epoch 21, Batch 130, LR 1.900234 Loss 5.186469, Accuracy 88.588%\n",
      "Epoch 21, Batch 131, LR 1.900120 Loss 5.187073, Accuracy 88.591%\n",
      "Epoch 21, Batch 132, LR 1.900005 Loss 5.186779, Accuracy 88.595%\n",
      "Epoch 21, Batch 133, LR 1.899891 Loss 5.178040, Accuracy 88.628%\n",
      "Epoch 21, Batch 134, LR 1.899777 Loss 5.175215, Accuracy 88.649%\n",
      "Epoch 21, Batch 135, LR 1.899662 Loss 5.175721, Accuracy 88.623%\n",
      "Epoch 21, Batch 136, LR 1.899548 Loss 5.182600, Accuracy 88.586%\n",
      "Epoch 21, Batch 137, LR 1.899433 Loss 5.178038, Accuracy 88.589%\n",
      "Epoch 21, Batch 138, LR 1.899319 Loss 5.179274, Accuracy 88.576%\n",
      "Epoch 21, Batch 139, LR 1.899204 Loss 5.187496, Accuracy 88.529%\n",
      "Epoch 21, Batch 140, LR 1.899090 Loss 5.187316, Accuracy 88.544%\n",
      "Epoch 21, Batch 141, LR 1.898975 Loss 5.186330, Accuracy 88.542%\n",
      "Epoch 21, Batch 142, LR 1.898861 Loss 5.184664, Accuracy 88.562%\n",
      "Epoch 21, Batch 143, LR 1.898746 Loss 5.185628, Accuracy 88.527%\n",
      "Epoch 21, Batch 144, LR 1.898632 Loss 5.186789, Accuracy 88.525%\n",
      "Epoch 21, Batch 145, LR 1.898517 Loss 5.189466, Accuracy 88.508%\n",
      "Epoch 21, Batch 146, LR 1.898403 Loss 5.192229, Accuracy 88.495%\n",
      "Epoch 21, Batch 147, LR 1.898288 Loss 5.189841, Accuracy 88.515%\n",
      "Epoch 21, Batch 148, LR 1.898174 Loss 5.189211, Accuracy 88.514%\n",
      "Epoch 21, Batch 149, LR 1.898059 Loss 5.187316, Accuracy 88.512%\n",
      "Epoch 21, Batch 150, LR 1.897945 Loss 5.191493, Accuracy 88.505%\n",
      "Epoch 21, Batch 151, LR 1.897830 Loss 5.188535, Accuracy 88.509%\n",
      "Epoch 21, Batch 152, LR 1.897716 Loss 5.190768, Accuracy 88.513%\n",
      "Epoch 21, Batch 153, LR 1.897601 Loss 5.190216, Accuracy 88.531%\n",
      "Epoch 21, Batch 154, LR 1.897486 Loss 5.189433, Accuracy 88.560%\n",
      "Epoch 21, Batch 155, LR 1.897372 Loss 5.188061, Accuracy 88.584%\n",
      "Epoch 21, Batch 156, LR 1.897257 Loss 5.190904, Accuracy 88.597%\n",
      "Epoch 21, Batch 157, LR 1.897143 Loss 5.194310, Accuracy 88.600%\n",
      "Epoch 21, Batch 158, LR 1.897028 Loss 5.195647, Accuracy 88.603%\n",
      "Epoch 21, Batch 159, LR 1.896913 Loss 5.196988, Accuracy 88.586%\n",
      "Epoch 21, Batch 160, LR 1.896799 Loss 5.195325, Accuracy 88.604%\n",
      "Epoch 21, Batch 161, LR 1.896684 Loss 5.199757, Accuracy 88.606%\n",
      "Epoch 21, Batch 162, LR 1.896569 Loss 5.199533, Accuracy 88.628%\n",
      "Epoch 21, Batch 163, LR 1.896455 Loss 5.198408, Accuracy 88.650%\n",
      "Epoch 21, Batch 164, LR 1.896340 Loss 5.199213, Accuracy 88.677%\n",
      "Epoch 21, Batch 165, LR 1.896226 Loss 5.197141, Accuracy 88.703%\n",
      "Epoch 21, Batch 166, LR 1.896111 Loss 5.201670, Accuracy 88.681%\n",
      "Epoch 21, Batch 167, LR 1.895996 Loss 5.204599, Accuracy 88.660%\n",
      "Epoch 21, Batch 168, LR 1.895881 Loss 5.202367, Accuracy 88.663%\n",
      "Epoch 21, Batch 169, LR 1.895767 Loss 5.202653, Accuracy 88.642%\n",
      "Epoch 21, Batch 170, LR 1.895652 Loss 5.200963, Accuracy 88.658%\n",
      "Epoch 21, Batch 171, LR 1.895537 Loss 5.202732, Accuracy 88.647%\n",
      "Epoch 21, Batch 172, LR 1.895423 Loss 5.201417, Accuracy 88.663%\n",
      "Epoch 21, Batch 173, LR 1.895308 Loss 5.203350, Accuracy 88.638%\n",
      "Epoch 21, Batch 174, LR 1.895193 Loss 5.206145, Accuracy 88.622%\n",
      "Epoch 21, Batch 175, LR 1.895078 Loss 5.210822, Accuracy 88.603%\n",
      "Epoch 21, Batch 176, LR 1.894964 Loss 5.212039, Accuracy 88.583%\n",
      "Epoch 21, Batch 177, LR 1.894849 Loss 5.212498, Accuracy 88.577%\n",
      "Epoch 21, Batch 178, LR 1.894734 Loss 5.211846, Accuracy 88.575%\n",
      "Epoch 21, Batch 179, LR 1.894619 Loss 5.213338, Accuracy 88.574%\n",
      "Epoch 21, Batch 180, LR 1.894505 Loss 5.210726, Accuracy 88.589%\n",
      "Epoch 21, Batch 181, LR 1.894390 Loss 5.209592, Accuracy 88.588%\n",
      "Epoch 21, Batch 182, LR 1.894275 Loss 5.210721, Accuracy 88.577%\n",
      "Epoch 21, Batch 183, LR 1.894160 Loss 5.213110, Accuracy 88.554%\n",
      "Epoch 21, Batch 184, LR 1.894046 Loss 5.215746, Accuracy 88.536%\n",
      "Epoch 21, Batch 185, LR 1.893931 Loss 5.214340, Accuracy 88.552%\n",
      "Epoch 21, Batch 186, LR 1.893816 Loss 5.218972, Accuracy 88.512%\n",
      "Epoch 21, Batch 187, LR 1.893701 Loss 5.217722, Accuracy 88.528%\n",
      "Epoch 21, Batch 188, LR 1.893586 Loss 5.217297, Accuracy 88.543%\n",
      "Epoch 21, Batch 189, LR 1.893471 Loss 5.213690, Accuracy 88.554%\n",
      "Epoch 21, Batch 190, LR 1.893357 Loss 5.208286, Accuracy 88.577%\n",
      "Epoch 21, Batch 191, LR 1.893242 Loss 5.209250, Accuracy 88.551%\n",
      "Epoch 21, Batch 192, LR 1.893127 Loss 5.206721, Accuracy 88.562%\n",
      "Epoch 21, Batch 193, LR 1.893012 Loss 5.205203, Accuracy 88.557%\n",
      "Epoch 21, Batch 194, LR 1.892897 Loss 5.204259, Accuracy 88.571%\n",
      "Epoch 21, Batch 195, LR 1.892782 Loss 5.205028, Accuracy 88.566%\n",
      "Epoch 21, Batch 196, LR 1.892667 Loss 5.204831, Accuracy 88.580%\n",
      "Epoch 21, Batch 197, LR 1.892552 Loss 5.208266, Accuracy 88.539%\n",
      "Epoch 21, Batch 198, LR 1.892438 Loss 5.208136, Accuracy 88.542%\n",
      "Epoch 21, Batch 199, LR 1.892323 Loss 5.207401, Accuracy 88.533%\n",
      "Epoch 21, Batch 200, LR 1.892208 Loss 5.204566, Accuracy 88.543%\n",
      "Epoch 21, Batch 201, LR 1.892093 Loss 5.205750, Accuracy 88.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 202, LR 1.891978 Loss 5.202662, Accuracy 88.575%\n",
      "Epoch 21, Batch 203, LR 1.891863 Loss 5.201174, Accuracy 88.578%\n",
      "Epoch 21, Batch 204, LR 1.891748 Loss 5.200696, Accuracy 88.580%\n",
      "Epoch 21, Batch 205, LR 1.891633 Loss 5.199587, Accuracy 88.582%\n",
      "Epoch 21, Batch 206, LR 1.891518 Loss 5.200952, Accuracy 88.577%\n",
      "Epoch 21, Batch 207, LR 1.891403 Loss 5.201242, Accuracy 88.579%\n",
      "Epoch 21, Batch 208, LR 1.891288 Loss 5.199982, Accuracy 88.585%\n",
      "Epoch 21, Batch 209, LR 1.891173 Loss 5.200785, Accuracy 88.595%\n",
      "Epoch 21, Batch 210, LR 1.891058 Loss 5.204795, Accuracy 88.575%\n",
      "Epoch 21, Batch 211, LR 1.890943 Loss 5.203101, Accuracy 88.585%\n",
      "Epoch 21, Batch 212, LR 1.890828 Loss 5.202064, Accuracy 88.580%\n",
      "Epoch 21, Batch 213, LR 1.890713 Loss 5.200793, Accuracy 88.582%\n",
      "Epoch 21, Batch 214, LR 1.890598 Loss 5.201413, Accuracy 88.595%\n",
      "Epoch 21, Batch 215, LR 1.890483 Loss 5.201146, Accuracy 88.597%\n",
      "Epoch 21, Batch 216, LR 1.890368 Loss 5.197304, Accuracy 88.603%\n",
      "Epoch 21, Batch 217, LR 1.890253 Loss 5.191905, Accuracy 88.630%\n",
      "Epoch 21, Batch 218, LR 1.890138 Loss 5.187902, Accuracy 88.658%\n",
      "Epoch 21, Batch 219, LR 1.890023 Loss 5.183243, Accuracy 88.684%\n",
      "Epoch 21, Batch 220, LR 1.889908 Loss 5.180990, Accuracy 88.693%\n",
      "Epoch 21, Batch 221, LR 1.889793 Loss 5.183451, Accuracy 88.677%\n",
      "Epoch 21, Batch 222, LR 1.889678 Loss 5.184595, Accuracy 88.682%\n",
      "Epoch 21, Batch 223, LR 1.889563 Loss 5.187023, Accuracy 88.667%\n",
      "Epoch 21, Batch 224, LR 1.889447 Loss 5.189889, Accuracy 88.651%\n",
      "Epoch 21, Batch 225, LR 1.889332 Loss 5.189788, Accuracy 88.653%\n",
      "Epoch 21, Batch 226, LR 1.889217 Loss 5.189756, Accuracy 88.662%\n",
      "Epoch 21, Batch 227, LR 1.889102 Loss 5.191768, Accuracy 88.646%\n",
      "Epoch 21, Batch 228, LR 1.888987 Loss 5.191730, Accuracy 88.644%\n",
      "Epoch 21, Batch 229, LR 1.888872 Loss 5.195004, Accuracy 88.636%\n",
      "Epoch 21, Batch 230, LR 1.888757 Loss 5.194267, Accuracy 88.648%\n",
      "Epoch 21, Batch 231, LR 1.888642 Loss 5.193990, Accuracy 88.663%\n",
      "Epoch 21, Batch 232, LR 1.888526 Loss 5.193415, Accuracy 88.665%\n",
      "Epoch 21, Batch 233, LR 1.888411 Loss 5.196875, Accuracy 88.637%\n",
      "Epoch 21, Batch 234, LR 1.888296 Loss 5.197697, Accuracy 88.638%\n",
      "Epoch 21, Batch 235, LR 1.888181 Loss 5.196193, Accuracy 88.647%\n",
      "Epoch 21, Batch 236, LR 1.888066 Loss 5.196881, Accuracy 88.632%\n",
      "Epoch 21, Batch 237, LR 1.887951 Loss 5.196432, Accuracy 88.637%\n",
      "Epoch 21, Batch 238, LR 1.887835 Loss 5.198208, Accuracy 88.636%\n",
      "Epoch 21, Batch 239, LR 1.887720 Loss 5.199628, Accuracy 88.628%\n",
      "Epoch 21, Batch 240, LR 1.887605 Loss 5.201290, Accuracy 88.623%\n",
      "Epoch 21, Batch 241, LR 1.887490 Loss 5.201688, Accuracy 88.625%\n",
      "Epoch 21, Batch 242, LR 1.887374 Loss 5.202908, Accuracy 88.611%\n",
      "Epoch 21, Batch 243, LR 1.887259 Loss 5.205399, Accuracy 88.593%\n",
      "Epoch 21, Batch 244, LR 1.887144 Loss 5.203225, Accuracy 88.601%\n",
      "Epoch 21, Batch 245, LR 1.887029 Loss 5.202170, Accuracy 88.616%\n",
      "Epoch 21, Batch 246, LR 1.886914 Loss 5.200343, Accuracy 88.627%\n",
      "Epoch 21, Batch 247, LR 1.886798 Loss 5.199945, Accuracy 88.626%\n",
      "Epoch 21, Batch 248, LR 1.886683 Loss 5.199168, Accuracy 88.628%\n",
      "Epoch 21, Batch 249, LR 1.886568 Loss 5.201510, Accuracy 88.611%\n",
      "Epoch 21, Batch 250, LR 1.886452 Loss 5.198656, Accuracy 88.609%\n",
      "Epoch 21, Batch 251, LR 1.886337 Loss 5.199709, Accuracy 88.617%\n",
      "Epoch 21, Batch 252, LR 1.886222 Loss 5.200889, Accuracy 88.619%\n",
      "Epoch 21, Batch 253, LR 1.886107 Loss 5.201305, Accuracy 88.618%\n",
      "Epoch 21, Batch 254, LR 1.885991 Loss 5.199877, Accuracy 88.617%\n",
      "Epoch 21, Batch 255, LR 1.885876 Loss 5.198470, Accuracy 88.618%\n",
      "Epoch 21, Batch 256, LR 1.885761 Loss 5.198291, Accuracy 88.602%\n",
      "Epoch 21, Batch 257, LR 1.885645 Loss 5.197962, Accuracy 88.594%\n",
      "Epoch 21, Batch 258, LR 1.885530 Loss 5.196037, Accuracy 88.611%\n",
      "Epoch 21, Batch 259, LR 1.885415 Loss 5.195742, Accuracy 88.613%\n",
      "Epoch 21, Batch 260, LR 1.885299 Loss 5.199329, Accuracy 88.600%\n",
      "Epoch 21, Batch 261, LR 1.885184 Loss 5.197876, Accuracy 88.611%\n",
      "Epoch 21, Batch 262, LR 1.885068 Loss 5.198216, Accuracy 88.600%\n",
      "Epoch 21, Batch 263, LR 1.884953 Loss 5.198922, Accuracy 88.596%\n",
      "Epoch 21, Batch 264, LR 1.884838 Loss 5.198424, Accuracy 88.598%\n",
      "Epoch 21, Batch 265, LR 1.884722 Loss 5.198267, Accuracy 88.597%\n",
      "Epoch 21, Batch 266, LR 1.884607 Loss 5.198598, Accuracy 88.596%\n",
      "Epoch 21, Batch 267, LR 1.884491 Loss 5.199626, Accuracy 88.571%\n",
      "Epoch 21, Batch 268, LR 1.884376 Loss 5.199479, Accuracy 88.567%\n",
      "Epoch 21, Batch 269, LR 1.884261 Loss 5.200168, Accuracy 88.551%\n",
      "Epoch 21, Batch 270, LR 1.884145 Loss 5.197158, Accuracy 88.573%\n",
      "Epoch 21, Batch 271, LR 1.884030 Loss 5.195443, Accuracy 88.587%\n",
      "Epoch 21, Batch 272, LR 1.883914 Loss 5.196063, Accuracy 88.586%\n",
      "Epoch 21, Batch 273, LR 1.883799 Loss 5.194837, Accuracy 88.585%\n",
      "Epoch 21, Batch 274, LR 1.883683 Loss 5.196669, Accuracy 88.583%\n",
      "Epoch 21, Batch 275, LR 1.883568 Loss 5.197854, Accuracy 88.588%\n",
      "Epoch 21, Batch 276, LR 1.883452 Loss 5.197440, Accuracy 88.587%\n",
      "Epoch 21, Batch 277, LR 1.883337 Loss 5.198446, Accuracy 88.583%\n",
      "Epoch 21, Batch 278, LR 1.883221 Loss 5.196000, Accuracy 88.596%\n",
      "Epoch 21, Batch 279, LR 1.883106 Loss 5.195254, Accuracy 88.592%\n",
      "Epoch 21, Batch 280, LR 1.882990 Loss 5.194970, Accuracy 88.605%\n",
      "Epoch 21, Batch 281, LR 1.882875 Loss 5.194724, Accuracy 88.607%\n",
      "Epoch 21, Batch 282, LR 1.882759 Loss 5.194459, Accuracy 88.611%\n",
      "Epoch 21, Batch 283, LR 1.882644 Loss 5.192780, Accuracy 88.629%\n",
      "Epoch 21, Batch 284, LR 1.882528 Loss 5.193166, Accuracy 88.625%\n",
      "Epoch 21, Batch 285, LR 1.882413 Loss 5.192036, Accuracy 88.629%\n",
      "Epoch 21, Batch 286, LR 1.882297 Loss 5.193173, Accuracy 88.625%\n",
      "Epoch 21, Batch 287, LR 1.882182 Loss 5.193452, Accuracy 88.616%\n",
      "Epoch 21, Batch 288, LR 1.882066 Loss 5.192194, Accuracy 88.623%\n",
      "Epoch 21, Batch 289, LR 1.881951 Loss 5.193349, Accuracy 88.616%\n",
      "Epoch 21, Batch 290, LR 1.881835 Loss 5.192056, Accuracy 88.618%\n",
      "Epoch 21, Batch 291, LR 1.881719 Loss 5.192198, Accuracy 88.611%\n",
      "Epoch 21, Batch 292, LR 1.881604 Loss 5.191206, Accuracy 88.613%\n",
      "Epoch 21, Batch 293, LR 1.881488 Loss 5.192619, Accuracy 88.612%\n",
      "Epoch 21, Batch 294, LR 1.881373 Loss 5.193425, Accuracy 88.600%\n",
      "Epoch 21, Batch 295, LR 1.881257 Loss 5.193856, Accuracy 88.599%\n",
      "Epoch 21, Batch 296, LR 1.881141 Loss 5.196250, Accuracy 88.593%\n",
      "Epoch 21, Batch 297, LR 1.881026 Loss 5.195176, Accuracy 88.610%\n",
      "Epoch 21, Batch 298, LR 1.880910 Loss 5.195803, Accuracy 88.612%\n",
      "Epoch 21, Batch 299, LR 1.880794 Loss 5.193854, Accuracy 88.616%\n",
      "Epoch 21, Batch 300, LR 1.880679 Loss 5.194760, Accuracy 88.609%\n",
      "Epoch 21, Batch 301, LR 1.880563 Loss 5.196126, Accuracy 88.606%\n",
      "Epoch 21, Batch 302, LR 1.880447 Loss 5.196743, Accuracy 88.607%\n",
      "Epoch 21, Batch 303, LR 1.880332 Loss 5.197100, Accuracy 88.604%\n",
      "Epoch 21, Batch 304, LR 1.880216 Loss 5.197560, Accuracy 88.595%\n",
      "Epoch 21, Batch 305, LR 1.880100 Loss 5.198272, Accuracy 88.586%\n",
      "Epoch 21, Batch 306, LR 1.879985 Loss 5.198195, Accuracy 88.585%\n",
      "Epoch 21, Batch 307, LR 1.879869 Loss 5.198581, Accuracy 88.574%\n",
      "Epoch 21, Batch 308, LR 1.879753 Loss 5.199829, Accuracy 88.570%\n",
      "Epoch 21, Batch 309, LR 1.879638 Loss 5.199184, Accuracy 88.575%\n",
      "Epoch 21, Batch 310, LR 1.879522 Loss 5.198603, Accuracy 88.589%\n",
      "Epoch 21, Batch 311, LR 1.879406 Loss 5.199417, Accuracy 88.573%\n",
      "Epoch 21, Batch 312, LR 1.879290 Loss 5.200228, Accuracy 88.572%\n",
      "Epoch 21, Batch 313, LR 1.879175 Loss 5.199871, Accuracy 88.578%\n",
      "Epoch 21, Batch 314, LR 1.879059 Loss 5.200341, Accuracy 88.572%\n",
      "Epoch 21, Batch 315, LR 1.878943 Loss 5.198265, Accuracy 88.586%\n",
      "Epoch 21, Batch 316, LR 1.878827 Loss 5.198005, Accuracy 88.585%\n",
      "Epoch 21, Batch 317, LR 1.878712 Loss 5.197771, Accuracy 88.589%\n",
      "Epoch 21, Batch 318, LR 1.878596 Loss 5.197001, Accuracy 88.596%\n",
      "Epoch 21, Batch 319, LR 1.878480 Loss 5.195882, Accuracy 88.600%\n",
      "Epoch 21, Batch 320, LR 1.878364 Loss 5.196534, Accuracy 88.591%\n",
      "Epoch 21, Batch 321, LR 1.878248 Loss 5.196007, Accuracy 88.590%\n",
      "Epoch 21, Batch 322, LR 1.878133 Loss 5.195800, Accuracy 88.589%\n",
      "Epoch 21, Batch 323, LR 1.878017 Loss 5.199029, Accuracy 88.581%\n",
      "Epoch 21, Batch 324, LR 1.877901 Loss 5.200998, Accuracy 88.573%\n",
      "Epoch 21, Batch 325, LR 1.877785 Loss 5.202077, Accuracy 88.572%\n",
      "Epoch 21, Batch 326, LR 1.877669 Loss 5.202516, Accuracy 88.574%\n",
      "Epoch 21, Batch 327, LR 1.877554 Loss 5.201436, Accuracy 88.578%\n",
      "Epoch 21, Batch 328, LR 1.877438 Loss 5.200295, Accuracy 88.586%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 329, LR 1.877322 Loss 5.200485, Accuracy 88.585%\n",
      "Epoch 21, Batch 330, LR 1.877206 Loss 5.200103, Accuracy 88.594%\n",
      "Epoch 21, Batch 331, LR 1.877090 Loss 5.202586, Accuracy 88.574%\n",
      "Epoch 21, Batch 332, LR 1.876974 Loss 5.202084, Accuracy 88.578%\n",
      "Epoch 21, Batch 333, LR 1.876858 Loss 5.203252, Accuracy 88.575%\n",
      "Epoch 21, Batch 334, LR 1.876742 Loss 5.204754, Accuracy 88.569%\n",
      "Epoch 21, Batch 335, LR 1.876627 Loss 5.206878, Accuracy 88.552%\n",
      "Epoch 21, Batch 336, LR 1.876511 Loss 5.208805, Accuracy 88.537%\n",
      "Epoch 21, Batch 337, LR 1.876395 Loss 5.207793, Accuracy 88.539%\n",
      "Epoch 21, Batch 338, LR 1.876279 Loss 5.208330, Accuracy 88.536%\n",
      "Epoch 21, Batch 339, LR 1.876163 Loss 5.208674, Accuracy 88.530%\n",
      "Epoch 21, Batch 340, LR 1.876047 Loss 5.211428, Accuracy 88.523%\n",
      "Epoch 21, Batch 341, LR 1.875931 Loss 5.213262, Accuracy 88.515%\n",
      "Epoch 21, Batch 342, LR 1.875815 Loss 5.214278, Accuracy 88.517%\n",
      "Epoch 21, Batch 343, LR 1.875699 Loss 5.215924, Accuracy 88.509%\n",
      "Epoch 21, Batch 344, LR 1.875583 Loss 5.214188, Accuracy 88.513%\n",
      "Epoch 21, Batch 345, LR 1.875467 Loss 5.215678, Accuracy 88.499%\n",
      "Epoch 21, Batch 346, LR 1.875351 Loss 5.213954, Accuracy 88.512%\n",
      "Epoch 21, Batch 347, LR 1.875235 Loss 5.213824, Accuracy 88.513%\n",
      "Epoch 21, Batch 348, LR 1.875119 Loss 5.214113, Accuracy 88.528%\n",
      "Epoch 21, Batch 349, LR 1.875003 Loss 5.214881, Accuracy 88.530%\n",
      "Epoch 21, Batch 350, LR 1.874887 Loss 5.215021, Accuracy 88.529%\n",
      "Epoch 21, Batch 351, LR 1.874771 Loss 5.214911, Accuracy 88.531%\n",
      "Epoch 21, Batch 352, LR 1.874655 Loss 5.212521, Accuracy 88.548%\n",
      "Epoch 21, Batch 353, LR 1.874539 Loss 5.212689, Accuracy 88.547%\n",
      "Epoch 21, Batch 354, LR 1.874423 Loss 5.213817, Accuracy 88.535%\n",
      "Epoch 21, Batch 355, LR 1.874307 Loss 5.213625, Accuracy 88.545%\n",
      "Epoch 21, Batch 356, LR 1.874191 Loss 5.214226, Accuracy 88.551%\n",
      "Epoch 21, Batch 357, LR 1.874075 Loss 5.213823, Accuracy 88.553%\n",
      "Epoch 21, Batch 358, LR 1.873959 Loss 5.214151, Accuracy 88.547%\n",
      "Epoch 21, Batch 359, LR 1.873843 Loss 5.214242, Accuracy 88.549%\n",
      "Epoch 21, Batch 360, LR 1.873727 Loss 5.215755, Accuracy 88.542%\n",
      "Epoch 21, Batch 361, LR 1.873611 Loss 5.215350, Accuracy 88.543%\n",
      "Epoch 21, Batch 362, LR 1.873494 Loss 5.216243, Accuracy 88.547%\n",
      "Epoch 21, Batch 363, LR 1.873378 Loss 5.216809, Accuracy 88.548%\n",
      "Epoch 21, Batch 364, LR 1.873262 Loss 5.216583, Accuracy 88.541%\n",
      "Epoch 21, Batch 365, LR 1.873146 Loss 5.215870, Accuracy 88.545%\n",
      "Epoch 21, Batch 366, LR 1.873030 Loss 5.216562, Accuracy 88.540%\n",
      "Epoch 21, Batch 367, LR 1.872914 Loss 5.217767, Accuracy 88.532%\n",
      "Epoch 21, Batch 368, LR 1.872798 Loss 5.217062, Accuracy 88.530%\n",
      "Epoch 21, Batch 369, LR 1.872682 Loss 5.217416, Accuracy 88.533%\n",
      "Epoch 21, Batch 370, LR 1.872565 Loss 5.220094, Accuracy 88.514%\n",
      "Epoch 21, Batch 371, LR 1.872449 Loss 5.218130, Accuracy 88.515%\n",
      "Epoch 21, Batch 372, LR 1.872333 Loss 5.218467, Accuracy 88.519%\n",
      "Epoch 21, Batch 373, LR 1.872217 Loss 5.217582, Accuracy 88.522%\n",
      "Epoch 21, Batch 374, LR 1.872101 Loss 5.217769, Accuracy 88.519%\n",
      "Epoch 21, Batch 375, LR 1.871985 Loss 5.219366, Accuracy 88.504%\n",
      "Epoch 21, Batch 376, LR 1.871868 Loss 5.219674, Accuracy 88.497%\n",
      "Epoch 21, Batch 377, LR 1.871752 Loss 5.218508, Accuracy 88.501%\n",
      "Epoch 21, Batch 378, LR 1.871636 Loss 5.220604, Accuracy 88.492%\n",
      "Epoch 21, Batch 379, LR 1.871520 Loss 5.219416, Accuracy 88.502%\n",
      "Epoch 21, Batch 380, LR 1.871403 Loss 5.218968, Accuracy 88.505%\n",
      "Epoch 21, Batch 381, LR 1.871287 Loss 5.222808, Accuracy 88.484%\n",
      "Epoch 21, Batch 382, LR 1.871171 Loss 5.222412, Accuracy 88.492%\n",
      "Epoch 21, Batch 383, LR 1.871055 Loss 5.221799, Accuracy 88.497%\n",
      "Epoch 21, Batch 384, LR 1.870938 Loss 5.222474, Accuracy 88.501%\n",
      "Epoch 21, Batch 385, LR 1.870822 Loss 5.223828, Accuracy 88.502%\n",
      "Epoch 21, Batch 386, LR 1.870706 Loss 5.225532, Accuracy 88.498%\n",
      "Epoch 21, Batch 387, LR 1.870590 Loss 5.224257, Accuracy 88.503%\n",
      "Epoch 21, Batch 388, LR 1.870473 Loss 5.224821, Accuracy 88.505%\n",
      "Epoch 21, Batch 389, LR 1.870357 Loss 5.224984, Accuracy 88.504%\n",
      "Epoch 21, Batch 390, LR 1.870241 Loss 5.224211, Accuracy 88.512%\n",
      "Epoch 21, Batch 391, LR 1.870125 Loss 5.225402, Accuracy 88.499%\n",
      "Epoch 21, Batch 392, LR 1.870008 Loss 5.225179, Accuracy 88.500%\n",
      "Epoch 21, Batch 393, LR 1.869892 Loss 5.224141, Accuracy 88.502%\n",
      "Epoch 21, Batch 394, LR 1.869776 Loss 5.223547, Accuracy 88.509%\n",
      "Epoch 21, Batch 395, LR 1.869659 Loss 5.223868, Accuracy 88.513%\n",
      "Epoch 21, Batch 396, LR 1.869543 Loss 5.224578, Accuracy 88.506%\n",
      "Epoch 21, Batch 397, LR 1.869427 Loss 5.223965, Accuracy 88.502%\n",
      "Epoch 21, Batch 398, LR 1.869310 Loss 5.222985, Accuracy 88.513%\n",
      "Epoch 21, Batch 399, LR 1.869194 Loss 5.222484, Accuracy 88.516%\n",
      "Epoch 21, Batch 400, LR 1.869077 Loss 5.223488, Accuracy 88.512%\n",
      "Epoch 21, Batch 401, LR 1.868961 Loss 5.223825, Accuracy 88.511%\n",
      "Epoch 21, Batch 402, LR 1.868845 Loss 5.223959, Accuracy 88.507%\n",
      "Epoch 21, Batch 403, LR 1.868728 Loss 5.224565, Accuracy 88.506%\n",
      "Epoch 21, Batch 404, LR 1.868612 Loss 5.223878, Accuracy 88.508%\n",
      "Epoch 21, Batch 405, LR 1.868496 Loss 5.223578, Accuracy 88.507%\n",
      "Epoch 21, Batch 406, LR 1.868379 Loss 5.222027, Accuracy 88.516%\n",
      "Epoch 21, Batch 407, LR 1.868263 Loss 5.221050, Accuracy 88.514%\n",
      "Epoch 21, Batch 408, LR 1.868146 Loss 5.221681, Accuracy 88.513%\n",
      "Epoch 21, Batch 409, LR 1.868030 Loss 5.221906, Accuracy 88.512%\n",
      "Epoch 21, Batch 410, LR 1.867913 Loss 5.222437, Accuracy 88.504%\n",
      "Epoch 21, Batch 411, LR 1.867797 Loss 5.223471, Accuracy 88.490%\n",
      "Epoch 21, Batch 412, LR 1.867681 Loss 5.224170, Accuracy 88.484%\n",
      "Epoch 21, Batch 413, LR 1.867564 Loss 5.223754, Accuracy 88.487%\n",
      "Epoch 21, Batch 414, LR 1.867448 Loss 5.225023, Accuracy 88.487%\n",
      "Epoch 21, Batch 415, LR 1.867331 Loss 5.223318, Accuracy 88.496%\n",
      "Epoch 21, Batch 416, LR 1.867215 Loss 5.223318, Accuracy 88.495%\n",
      "Epoch 21, Batch 417, LR 1.867098 Loss 5.221908, Accuracy 88.499%\n",
      "Epoch 21, Batch 418, LR 1.866982 Loss 5.221816, Accuracy 88.498%\n",
      "Epoch 21, Batch 419, LR 1.866865 Loss 5.221668, Accuracy 88.503%\n",
      "Epoch 21, Batch 420, LR 1.866749 Loss 5.221645, Accuracy 88.497%\n",
      "Epoch 21, Batch 421, LR 1.866632 Loss 5.220461, Accuracy 88.502%\n",
      "Epoch 21, Batch 422, LR 1.866516 Loss 5.222442, Accuracy 88.487%\n",
      "Epoch 21, Batch 423, LR 1.866399 Loss 5.222508, Accuracy 88.492%\n",
      "Epoch 21, Batch 424, LR 1.866283 Loss 5.223116, Accuracy 88.497%\n",
      "Epoch 21, Batch 425, LR 1.866166 Loss 5.222673, Accuracy 88.496%\n",
      "Epoch 21, Batch 426, LR 1.866049 Loss 5.223622, Accuracy 88.490%\n",
      "Epoch 21, Batch 427, LR 1.865933 Loss 5.223105, Accuracy 88.493%\n",
      "Epoch 21, Batch 428, LR 1.865816 Loss 5.222930, Accuracy 88.500%\n",
      "Epoch 21, Batch 429, LR 1.865700 Loss 5.224056, Accuracy 88.491%\n",
      "Epoch 21, Batch 430, LR 1.865583 Loss 5.223971, Accuracy 88.492%\n",
      "Epoch 21, Batch 431, LR 1.865467 Loss 5.224253, Accuracy 88.492%\n",
      "Epoch 21, Batch 432, LR 1.865350 Loss 5.225380, Accuracy 88.478%\n",
      "Epoch 21, Batch 433, LR 1.865233 Loss 5.224805, Accuracy 88.478%\n",
      "Epoch 21, Batch 434, LR 1.865117 Loss 5.224158, Accuracy 88.474%\n",
      "Epoch 21, Batch 435, LR 1.865000 Loss 5.224876, Accuracy 88.470%\n",
      "Epoch 21, Batch 436, LR 1.864884 Loss 5.225206, Accuracy 88.469%\n",
      "Epoch 21, Batch 437, LR 1.864767 Loss 5.226631, Accuracy 88.465%\n",
      "Epoch 21, Batch 438, LR 1.864650 Loss 5.227700, Accuracy 88.454%\n",
      "Epoch 21, Batch 439, LR 1.864534 Loss 5.226988, Accuracy 88.457%\n",
      "Epoch 21, Batch 440, LR 1.864417 Loss 5.226715, Accuracy 88.461%\n",
      "Epoch 21, Batch 441, LR 1.864300 Loss 5.225905, Accuracy 88.453%\n",
      "Epoch 21, Batch 442, LR 1.864184 Loss 5.226503, Accuracy 88.446%\n",
      "Epoch 21, Batch 443, LR 1.864067 Loss 5.228589, Accuracy 88.428%\n",
      "Epoch 21, Batch 444, LR 1.863950 Loss 5.228113, Accuracy 88.427%\n",
      "Epoch 21, Batch 445, LR 1.863834 Loss 5.228642, Accuracy 88.427%\n",
      "Epoch 21, Batch 446, LR 1.863717 Loss 5.227439, Accuracy 88.435%\n",
      "Epoch 21, Batch 447, LR 1.863600 Loss 5.227571, Accuracy 88.440%\n",
      "Epoch 21, Batch 448, LR 1.863483 Loss 5.226917, Accuracy 88.450%\n",
      "Epoch 21, Batch 449, LR 1.863367 Loss 5.227716, Accuracy 88.445%\n",
      "Epoch 21, Batch 450, LR 1.863250 Loss 5.226893, Accuracy 88.457%\n",
      "Epoch 21, Batch 451, LR 1.863133 Loss 5.226885, Accuracy 88.458%\n",
      "Epoch 21, Batch 452, LR 1.863017 Loss 5.226863, Accuracy 88.456%\n",
      "Epoch 21, Batch 453, LR 1.862900 Loss 5.226986, Accuracy 88.457%\n",
      "Epoch 21, Batch 454, LR 1.862783 Loss 5.228298, Accuracy 88.450%\n",
      "Epoch 21, Batch 455, LR 1.862666 Loss 5.227941, Accuracy 88.451%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 456, LR 1.862550 Loss 5.227516, Accuracy 88.456%\n",
      "Epoch 21, Batch 457, LR 1.862433 Loss 5.229745, Accuracy 88.435%\n",
      "Epoch 21, Batch 458, LR 1.862316 Loss 5.230527, Accuracy 88.442%\n",
      "Epoch 21, Batch 459, LR 1.862199 Loss 5.230270, Accuracy 88.445%\n",
      "Epoch 21, Batch 460, LR 1.862082 Loss 5.229860, Accuracy 88.448%\n",
      "Epoch 21, Batch 461, LR 1.861966 Loss 5.230003, Accuracy 88.444%\n",
      "Epoch 21, Batch 462, LR 1.861849 Loss 5.230511, Accuracy 88.442%\n",
      "Epoch 21, Batch 463, LR 1.861732 Loss 5.229177, Accuracy 88.455%\n",
      "Epoch 21, Batch 464, LR 1.861615 Loss 5.230948, Accuracy 88.450%\n",
      "Epoch 21, Batch 465, LR 1.861498 Loss 5.230280, Accuracy 88.453%\n",
      "Epoch 21, Batch 466, LR 1.861382 Loss 5.229793, Accuracy 88.459%\n",
      "Epoch 21, Batch 467, LR 1.861265 Loss 5.229642, Accuracy 88.457%\n",
      "Epoch 21, Batch 468, LR 1.861148 Loss 5.230055, Accuracy 88.452%\n",
      "Epoch 21, Batch 469, LR 1.861031 Loss 5.228674, Accuracy 88.451%\n",
      "Epoch 21, Batch 470, LR 1.860914 Loss 5.229639, Accuracy 88.442%\n",
      "Epoch 21, Batch 471, LR 1.860797 Loss 5.229580, Accuracy 88.444%\n",
      "Epoch 21, Batch 472, LR 1.860680 Loss 5.229250, Accuracy 88.438%\n",
      "Epoch 21, Batch 473, LR 1.860564 Loss 5.230852, Accuracy 88.433%\n",
      "Epoch 21, Batch 474, LR 1.860447 Loss 5.230595, Accuracy 88.438%\n",
      "Epoch 21, Batch 475, LR 1.860330 Loss 5.230457, Accuracy 88.431%\n",
      "Epoch 21, Batch 476, LR 1.860213 Loss 5.230937, Accuracy 88.426%\n",
      "Epoch 21, Batch 477, LR 1.860096 Loss 5.229612, Accuracy 88.432%\n",
      "Epoch 21, Batch 478, LR 1.859979 Loss 5.229697, Accuracy 88.430%\n",
      "Epoch 21, Batch 479, LR 1.859862 Loss 5.230288, Accuracy 88.423%\n",
      "Epoch 21, Batch 480, LR 1.859745 Loss 5.231354, Accuracy 88.420%\n",
      "Epoch 21, Batch 481, LR 1.859628 Loss 5.231154, Accuracy 88.419%\n",
      "Epoch 21, Batch 482, LR 1.859511 Loss 5.231904, Accuracy 88.409%\n",
      "Epoch 21, Batch 483, LR 1.859394 Loss 5.232856, Accuracy 88.404%\n",
      "Epoch 21, Batch 484, LR 1.859277 Loss 5.232226, Accuracy 88.414%\n",
      "Epoch 21, Batch 485, LR 1.859160 Loss 5.232427, Accuracy 88.415%\n",
      "Epoch 21, Batch 486, LR 1.859043 Loss 5.231677, Accuracy 88.415%\n",
      "Epoch 21, Batch 487, LR 1.858926 Loss 5.231727, Accuracy 88.422%\n",
      "Epoch 21, Batch 488, LR 1.858809 Loss 5.232331, Accuracy 88.413%\n",
      "Epoch 21, Batch 489, LR 1.858692 Loss 5.232253, Accuracy 88.401%\n",
      "Epoch 21, Batch 490, LR 1.858575 Loss 5.231940, Accuracy 88.409%\n",
      "Epoch 21, Batch 491, LR 1.858458 Loss 5.233701, Accuracy 88.401%\n",
      "Epoch 21, Batch 492, LR 1.858341 Loss 5.234926, Accuracy 88.394%\n",
      "Epoch 21, Batch 493, LR 1.858224 Loss 5.236518, Accuracy 88.384%\n",
      "Epoch 21, Batch 494, LR 1.858107 Loss 5.235876, Accuracy 88.387%\n",
      "Epoch 21, Batch 495, LR 1.857990 Loss 5.235026, Accuracy 88.392%\n",
      "Epoch 21, Batch 496, LR 1.857873 Loss 5.234675, Accuracy 88.395%\n",
      "Epoch 21, Batch 497, LR 1.857756 Loss 5.235367, Accuracy 88.388%\n",
      "Epoch 21, Batch 498, LR 1.857639 Loss 5.235314, Accuracy 88.386%\n",
      "Epoch 21, Batch 499, LR 1.857522 Loss 5.234654, Accuracy 88.394%\n",
      "Epoch 21, Batch 500, LR 1.857405 Loss 5.234997, Accuracy 88.391%\n",
      "Epoch 21, Batch 501, LR 1.857288 Loss 5.234559, Accuracy 88.398%\n",
      "Epoch 21, Batch 502, LR 1.857171 Loss 5.235601, Accuracy 88.393%\n",
      "Epoch 21, Batch 503, LR 1.857054 Loss 5.235119, Accuracy 88.398%\n",
      "Epoch 21, Batch 504, LR 1.856937 Loss 5.234084, Accuracy 88.402%\n",
      "Epoch 21, Batch 505, LR 1.856820 Loss 5.232976, Accuracy 88.405%\n",
      "Epoch 21, Batch 506, LR 1.856702 Loss 5.233574, Accuracy 88.397%\n",
      "Epoch 21, Batch 507, LR 1.856585 Loss 5.234554, Accuracy 88.389%\n",
      "Epoch 21, Batch 508, LR 1.856468 Loss 5.233384, Accuracy 88.392%\n",
      "Epoch 21, Batch 509, LR 1.856351 Loss 5.232363, Accuracy 88.396%\n",
      "Epoch 21, Batch 510, LR 1.856234 Loss 5.232639, Accuracy 88.398%\n",
      "Epoch 21, Batch 511, LR 1.856117 Loss 5.233390, Accuracy 88.399%\n",
      "Epoch 21, Batch 512, LR 1.856000 Loss 5.234016, Accuracy 88.397%\n",
      "Epoch 21, Batch 513, LR 1.855882 Loss 5.234046, Accuracy 88.397%\n",
      "Epoch 21, Batch 514, LR 1.855765 Loss 5.235274, Accuracy 88.389%\n",
      "Epoch 21, Batch 515, LR 1.855648 Loss 5.235086, Accuracy 88.397%\n",
      "Epoch 21, Batch 516, LR 1.855531 Loss 5.235787, Accuracy 88.393%\n",
      "Epoch 21, Batch 517, LR 1.855414 Loss 5.236762, Accuracy 88.384%\n",
      "Epoch 21, Batch 518, LR 1.855297 Loss 5.236710, Accuracy 88.379%\n",
      "Epoch 21, Batch 519, LR 1.855179 Loss 5.235602, Accuracy 88.379%\n",
      "Epoch 21, Batch 520, LR 1.855062 Loss 5.235246, Accuracy 88.376%\n",
      "Epoch 21, Batch 521, LR 1.854945 Loss 5.236125, Accuracy 88.376%\n",
      "Epoch 21, Batch 522, LR 1.854828 Loss 5.235249, Accuracy 88.373%\n",
      "Epoch 21, Batch 523, LR 1.854710 Loss 5.235130, Accuracy 88.371%\n",
      "Epoch 21, Batch 524, LR 1.854593 Loss 5.233752, Accuracy 88.375%\n",
      "Epoch 21, Batch 525, LR 1.854476 Loss 5.234413, Accuracy 88.375%\n",
      "Epoch 21, Batch 526, LR 1.854359 Loss 5.236432, Accuracy 88.366%\n",
      "Epoch 21, Batch 527, LR 1.854241 Loss 5.235480, Accuracy 88.372%\n",
      "Epoch 21, Batch 528, LR 1.854124 Loss 5.234936, Accuracy 88.372%\n",
      "Epoch 21, Batch 529, LR 1.854007 Loss 5.235256, Accuracy 88.367%\n",
      "Epoch 21, Batch 530, LR 1.853890 Loss 5.234609, Accuracy 88.367%\n",
      "Epoch 21, Batch 531, LR 1.853772 Loss 5.235183, Accuracy 88.364%\n",
      "Epoch 21, Batch 532, LR 1.853655 Loss 5.234609, Accuracy 88.365%\n",
      "Epoch 21, Batch 533, LR 1.853538 Loss 5.234299, Accuracy 88.369%\n",
      "Epoch 21, Batch 534, LR 1.853420 Loss 5.236078, Accuracy 88.357%\n",
      "Epoch 21, Batch 535, LR 1.853303 Loss 5.237245, Accuracy 88.356%\n",
      "Epoch 21, Batch 536, LR 1.853186 Loss 5.237728, Accuracy 88.353%\n",
      "Epoch 21, Batch 537, LR 1.853068 Loss 5.235983, Accuracy 88.358%\n",
      "Epoch 21, Batch 538, LR 1.852951 Loss 5.234737, Accuracy 88.363%\n",
      "Epoch 21, Batch 539, LR 1.852834 Loss 5.234341, Accuracy 88.367%\n",
      "Epoch 21, Batch 540, LR 1.852716 Loss 5.235043, Accuracy 88.368%\n",
      "Epoch 21, Batch 541, LR 1.852599 Loss 5.236088, Accuracy 88.356%\n",
      "Epoch 21, Batch 542, LR 1.852482 Loss 5.235774, Accuracy 88.355%\n",
      "Epoch 21, Batch 543, LR 1.852364 Loss 5.235076, Accuracy 88.353%\n",
      "Epoch 21, Batch 544, LR 1.852247 Loss 5.236738, Accuracy 88.349%\n",
      "Epoch 21, Batch 545, LR 1.852130 Loss 5.234496, Accuracy 88.363%\n",
      "Epoch 21, Batch 546, LR 1.852012 Loss 5.233766, Accuracy 88.367%\n",
      "Epoch 21, Batch 547, LR 1.851895 Loss 5.233661, Accuracy 88.368%\n",
      "Epoch 21, Batch 548, LR 1.851777 Loss 5.232849, Accuracy 88.371%\n",
      "Epoch 21, Batch 549, LR 1.851660 Loss 5.233661, Accuracy 88.368%\n",
      "Epoch 21, Batch 550, LR 1.851543 Loss 5.234901, Accuracy 88.361%\n",
      "Epoch 21, Batch 551, LR 1.851425 Loss 5.234002, Accuracy 88.359%\n",
      "Epoch 21, Batch 552, LR 1.851308 Loss 5.233190, Accuracy 88.361%\n",
      "Epoch 21, Batch 553, LR 1.851190 Loss 5.233081, Accuracy 88.366%\n",
      "Epoch 21, Batch 554, LR 1.851073 Loss 5.232673, Accuracy 88.372%\n",
      "Epoch 21, Batch 555, LR 1.850955 Loss 5.232929, Accuracy 88.371%\n",
      "Epoch 21, Batch 556, LR 1.850838 Loss 5.232976, Accuracy 88.378%\n",
      "Epoch 21, Batch 557, LR 1.850720 Loss 5.233000, Accuracy 88.375%\n",
      "Epoch 21, Batch 558, LR 1.850603 Loss 5.233195, Accuracy 88.371%\n",
      "Epoch 21, Batch 559, LR 1.850485 Loss 5.232105, Accuracy 88.378%\n",
      "Epoch 21, Batch 560, LR 1.850368 Loss 5.232113, Accuracy 88.382%\n",
      "Epoch 21, Batch 561, LR 1.850251 Loss 5.231955, Accuracy 88.379%\n",
      "Epoch 21, Batch 562, LR 1.850133 Loss 5.232605, Accuracy 88.372%\n",
      "Epoch 21, Batch 563, LR 1.850015 Loss 5.234009, Accuracy 88.358%\n",
      "Epoch 21, Batch 564, LR 1.849898 Loss 5.234686, Accuracy 88.357%\n",
      "Epoch 21, Batch 565, LR 1.849780 Loss 5.234937, Accuracy 88.357%\n",
      "Epoch 21, Batch 566, LR 1.849663 Loss 5.234691, Accuracy 88.363%\n",
      "Epoch 21, Batch 567, LR 1.849545 Loss 5.235572, Accuracy 88.361%\n",
      "Epoch 21, Batch 568, LR 1.849428 Loss 5.236044, Accuracy 88.353%\n",
      "Epoch 21, Batch 569, LR 1.849310 Loss 5.235167, Accuracy 88.354%\n",
      "Epoch 21, Batch 570, LR 1.849193 Loss 5.233764, Accuracy 88.361%\n",
      "Epoch 21, Batch 571, LR 1.849075 Loss 5.234123, Accuracy 88.355%\n",
      "Epoch 21, Batch 572, LR 1.848958 Loss 5.234356, Accuracy 88.348%\n",
      "Epoch 21, Batch 573, LR 1.848840 Loss 5.235908, Accuracy 88.343%\n",
      "Epoch 21, Batch 574, LR 1.848722 Loss 5.237177, Accuracy 88.336%\n",
      "Epoch 21, Batch 575, LR 1.848605 Loss 5.237742, Accuracy 88.330%\n",
      "Epoch 21, Batch 576, LR 1.848487 Loss 5.237586, Accuracy 88.330%\n",
      "Epoch 21, Batch 577, LR 1.848370 Loss 5.237401, Accuracy 88.327%\n",
      "Epoch 21, Batch 578, LR 1.848252 Loss 5.237345, Accuracy 88.331%\n",
      "Epoch 21, Batch 579, LR 1.848134 Loss 5.237972, Accuracy 88.323%\n",
      "Epoch 21, Batch 580, LR 1.848017 Loss 5.238792, Accuracy 88.322%\n",
      "Epoch 21, Batch 581, LR 1.847899 Loss 5.237552, Accuracy 88.328%\n",
      "Epoch 21, Batch 582, LR 1.847782 Loss 5.237476, Accuracy 88.331%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 583, LR 1.847664 Loss 5.237151, Accuracy 88.334%\n",
      "Epoch 21, Batch 584, LR 1.847546 Loss 5.236140, Accuracy 88.339%\n",
      "Epoch 21, Batch 585, LR 1.847429 Loss 5.236678, Accuracy 88.336%\n",
      "Epoch 21, Batch 586, LR 1.847311 Loss 5.236810, Accuracy 88.332%\n",
      "Epoch 21, Batch 587, LR 1.847193 Loss 5.237697, Accuracy 88.327%\n",
      "Epoch 21, Batch 588, LR 1.847076 Loss 5.237727, Accuracy 88.334%\n",
      "Epoch 21, Batch 589, LR 1.846958 Loss 5.238504, Accuracy 88.329%\n",
      "Epoch 21, Batch 590, LR 1.846840 Loss 5.238677, Accuracy 88.326%\n",
      "Epoch 21, Batch 591, LR 1.846722 Loss 5.239246, Accuracy 88.326%\n",
      "Epoch 21, Batch 592, LR 1.846605 Loss 5.240586, Accuracy 88.325%\n",
      "Epoch 21, Batch 593, LR 1.846487 Loss 5.240536, Accuracy 88.327%\n",
      "Epoch 21, Batch 594, LR 1.846369 Loss 5.239893, Accuracy 88.333%\n",
      "Epoch 21, Batch 595, LR 1.846252 Loss 5.239202, Accuracy 88.338%\n",
      "Epoch 21, Batch 596, LR 1.846134 Loss 5.238748, Accuracy 88.340%\n",
      "Epoch 21, Batch 597, LR 1.846016 Loss 5.239429, Accuracy 88.335%\n",
      "Epoch 21, Batch 598, LR 1.845898 Loss 5.238964, Accuracy 88.337%\n",
      "Epoch 21, Batch 599, LR 1.845781 Loss 5.239134, Accuracy 88.340%\n",
      "Epoch 21, Batch 600, LR 1.845663 Loss 5.239664, Accuracy 88.335%\n",
      "Epoch 21, Batch 601, LR 1.845545 Loss 5.239669, Accuracy 88.335%\n",
      "Epoch 21, Batch 602, LR 1.845427 Loss 5.240493, Accuracy 88.323%\n",
      "Epoch 21, Batch 603, LR 1.845310 Loss 5.240625, Accuracy 88.321%\n",
      "Epoch 21, Batch 604, LR 1.845192 Loss 5.240136, Accuracy 88.324%\n",
      "Epoch 21, Batch 605, LR 1.845074 Loss 5.240601, Accuracy 88.321%\n",
      "Epoch 21, Batch 606, LR 1.844956 Loss 5.240065, Accuracy 88.324%\n",
      "Epoch 21, Batch 607, LR 1.844838 Loss 5.240067, Accuracy 88.321%\n",
      "Epoch 21, Batch 608, LR 1.844720 Loss 5.240225, Accuracy 88.319%\n",
      "Epoch 21, Batch 609, LR 1.844603 Loss 5.240541, Accuracy 88.317%\n",
      "Epoch 21, Batch 610, LR 1.844485 Loss 5.240372, Accuracy 88.317%\n",
      "Epoch 21, Batch 611, LR 1.844367 Loss 5.241330, Accuracy 88.312%\n",
      "Epoch 21, Batch 612, LR 1.844249 Loss 5.241565, Accuracy 88.311%\n",
      "Epoch 21, Batch 613, LR 1.844131 Loss 5.241379, Accuracy 88.308%\n",
      "Epoch 21, Batch 614, LR 1.844013 Loss 5.241928, Accuracy 88.307%\n",
      "Epoch 21, Batch 615, LR 1.843896 Loss 5.242645, Accuracy 88.304%\n",
      "Epoch 21, Batch 616, LR 1.843778 Loss 5.242793, Accuracy 88.300%\n",
      "Epoch 21, Batch 617, LR 1.843660 Loss 5.243388, Accuracy 88.300%\n",
      "Epoch 21, Batch 618, LR 1.843542 Loss 5.242971, Accuracy 88.304%\n",
      "Epoch 21, Batch 619, LR 1.843424 Loss 5.242502, Accuracy 88.306%\n",
      "Epoch 21, Batch 620, LR 1.843306 Loss 5.242191, Accuracy 88.308%\n",
      "Epoch 21, Batch 621, LR 1.843188 Loss 5.241715, Accuracy 88.308%\n",
      "Epoch 21, Batch 622, LR 1.843070 Loss 5.241155, Accuracy 88.314%\n",
      "Epoch 21, Batch 623, LR 1.842952 Loss 5.240713, Accuracy 88.314%\n",
      "Epoch 21, Batch 624, LR 1.842834 Loss 5.240864, Accuracy 88.316%\n",
      "Epoch 21, Batch 625, LR 1.842717 Loss 5.240540, Accuracy 88.317%\n",
      "Epoch 21, Batch 626, LR 1.842599 Loss 5.240346, Accuracy 88.325%\n",
      "Epoch 21, Batch 627, LR 1.842481 Loss 5.240415, Accuracy 88.320%\n",
      "Epoch 21, Batch 628, LR 1.842363 Loss 5.240011, Accuracy 88.326%\n",
      "Epoch 21, Batch 629, LR 1.842245 Loss 5.240426, Accuracy 88.331%\n",
      "Epoch 21, Batch 630, LR 1.842127 Loss 5.240596, Accuracy 88.328%\n",
      "Epoch 21, Batch 631, LR 1.842009 Loss 5.240742, Accuracy 88.325%\n",
      "Epoch 21, Batch 632, LR 1.841891 Loss 5.238871, Accuracy 88.334%\n",
      "Epoch 21, Batch 633, LR 1.841773 Loss 5.237877, Accuracy 88.342%\n",
      "Epoch 21, Batch 634, LR 1.841655 Loss 5.238226, Accuracy 88.333%\n",
      "Epoch 21, Batch 635, LR 1.841537 Loss 5.238019, Accuracy 88.335%\n",
      "Epoch 21, Batch 636, LR 1.841419 Loss 5.238542, Accuracy 88.335%\n",
      "Epoch 21, Batch 637, LR 1.841301 Loss 5.237760, Accuracy 88.335%\n",
      "Epoch 21, Batch 638, LR 1.841183 Loss 5.237327, Accuracy 88.336%\n",
      "Epoch 21, Batch 639, LR 1.841065 Loss 5.237812, Accuracy 88.330%\n",
      "Epoch 21, Batch 640, LR 1.840947 Loss 5.238071, Accuracy 88.334%\n",
      "Epoch 21, Batch 641, LR 1.840829 Loss 5.237298, Accuracy 88.336%\n",
      "Epoch 21, Batch 642, LR 1.840711 Loss 5.237685, Accuracy 88.336%\n",
      "Epoch 21, Batch 643, LR 1.840593 Loss 5.237649, Accuracy 88.331%\n",
      "Epoch 21, Batch 644, LR 1.840475 Loss 5.237172, Accuracy 88.332%\n",
      "Epoch 21, Batch 645, LR 1.840356 Loss 5.237226, Accuracy 88.331%\n",
      "Epoch 21, Batch 646, LR 1.840238 Loss 5.238106, Accuracy 88.330%\n",
      "Epoch 21, Batch 647, LR 1.840120 Loss 5.237085, Accuracy 88.336%\n",
      "Epoch 21, Batch 648, LR 1.840002 Loss 5.237498, Accuracy 88.337%\n",
      "Epoch 21, Batch 649, LR 1.839884 Loss 5.238635, Accuracy 88.331%\n",
      "Epoch 21, Batch 650, LR 1.839766 Loss 5.238225, Accuracy 88.333%\n",
      "Epoch 21, Batch 651, LR 1.839648 Loss 5.237474, Accuracy 88.336%\n",
      "Epoch 21, Batch 652, LR 1.839530 Loss 5.237149, Accuracy 88.335%\n",
      "Epoch 21, Batch 653, LR 1.839412 Loss 5.238163, Accuracy 88.327%\n",
      "Epoch 21, Batch 654, LR 1.839294 Loss 5.239086, Accuracy 88.319%\n",
      "Epoch 21, Batch 655, LR 1.839175 Loss 5.238855, Accuracy 88.327%\n",
      "Epoch 21, Batch 656, LR 1.839057 Loss 5.238368, Accuracy 88.330%\n",
      "Epoch 21, Batch 657, LR 1.838939 Loss 5.238855, Accuracy 88.326%\n",
      "Epoch 21, Batch 658, LR 1.838821 Loss 5.238596, Accuracy 88.325%\n",
      "Epoch 21, Batch 659, LR 1.838703 Loss 5.238369, Accuracy 88.324%\n",
      "Epoch 21, Batch 660, LR 1.838585 Loss 5.239211, Accuracy 88.325%\n",
      "Epoch 21, Batch 661, LR 1.838466 Loss 5.238159, Accuracy 88.330%\n",
      "Epoch 21, Batch 662, LR 1.838348 Loss 5.238392, Accuracy 88.331%\n",
      "Epoch 21, Batch 663, LR 1.838230 Loss 5.238642, Accuracy 88.330%\n",
      "Epoch 21, Batch 664, LR 1.838112 Loss 5.238940, Accuracy 88.324%\n",
      "Epoch 21, Batch 665, LR 1.837994 Loss 5.238382, Accuracy 88.327%\n",
      "Epoch 21, Batch 666, LR 1.837875 Loss 5.238233, Accuracy 88.328%\n",
      "Epoch 21, Batch 667, LR 1.837757 Loss 5.238152, Accuracy 88.329%\n",
      "Epoch 21, Batch 668, LR 1.837639 Loss 5.238178, Accuracy 88.327%\n",
      "Epoch 21, Batch 669, LR 1.837521 Loss 5.238994, Accuracy 88.323%\n",
      "Epoch 21, Batch 670, LR 1.837403 Loss 5.240383, Accuracy 88.317%\n",
      "Epoch 21, Batch 671, LR 1.837284 Loss 5.241364, Accuracy 88.313%\n",
      "Epoch 21, Batch 672, LR 1.837166 Loss 5.241864, Accuracy 88.313%\n",
      "Epoch 21, Batch 673, LR 1.837048 Loss 5.241814, Accuracy 88.307%\n",
      "Epoch 21, Batch 674, LR 1.836930 Loss 5.242627, Accuracy 88.303%\n",
      "Epoch 21, Batch 675, LR 1.836811 Loss 5.242851, Accuracy 88.303%\n",
      "Epoch 21, Batch 676, LR 1.836693 Loss 5.242316, Accuracy 88.304%\n",
      "Epoch 21, Batch 677, LR 1.836575 Loss 5.242947, Accuracy 88.308%\n",
      "Epoch 21, Batch 678, LR 1.836456 Loss 5.243531, Accuracy 88.304%\n",
      "Epoch 21, Batch 679, LR 1.836338 Loss 5.244251, Accuracy 88.297%\n",
      "Epoch 21, Batch 680, LR 1.836220 Loss 5.244476, Accuracy 88.296%\n",
      "Epoch 21, Batch 681, LR 1.836101 Loss 5.243983, Accuracy 88.301%\n",
      "Epoch 21, Batch 682, LR 1.835983 Loss 5.243787, Accuracy 88.297%\n",
      "Epoch 21, Batch 683, LR 1.835865 Loss 5.243363, Accuracy 88.300%\n",
      "Epoch 21, Batch 684, LR 1.835746 Loss 5.243524, Accuracy 88.297%\n",
      "Epoch 21, Batch 685, LR 1.835628 Loss 5.242942, Accuracy 88.296%\n",
      "Epoch 21, Batch 686, LR 1.835510 Loss 5.242211, Accuracy 88.297%\n",
      "Epoch 21, Batch 687, LR 1.835391 Loss 5.243524, Accuracy 88.288%\n",
      "Epoch 21, Batch 688, LR 1.835273 Loss 5.243956, Accuracy 88.287%\n",
      "Epoch 21, Batch 689, LR 1.835155 Loss 5.244281, Accuracy 88.284%\n",
      "Epoch 21, Batch 690, LR 1.835036 Loss 5.244105, Accuracy 88.286%\n",
      "Epoch 21, Batch 691, LR 1.834918 Loss 5.245468, Accuracy 88.279%\n",
      "Epoch 21, Batch 692, LR 1.834800 Loss 5.245101, Accuracy 88.282%\n",
      "Epoch 21, Batch 693, LR 1.834681 Loss 5.245043, Accuracy 88.285%\n",
      "Epoch 21, Batch 694, LR 1.834563 Loss 5.244699, Accuracy 88.282%\n",
      "Epoch 21, Batch 695, LR 1.834444 Loss 5.244533, Accuracy 88.289%\n",
      "Epoch 21, Batch 696, LR 1.834326 Loss 5.244168, Accuracy 88.291%\n",
      "Epoch 21, Batch 697, LR 1.834208 Loss 5.244238, Accuracy 88.288%\n",
      "Epoch 21, Batch 698, LR 1.834089 Loss 5.245460, Accuracy 88.283%\n",
      "Epoch 21, Batch 699, LR 1.833971 Loss 5.245643, Accuracy 88.280%\n",
      "Epoch 21, Batch 700, LR 1.833852 Loss 5.246281, Accuracy 88.277%\n",
      "Epoch 21, Batch 701, LR 1.833734 Loss 5.244792, Accuracy 88.288%\n",
      "Epoch 21, Batch 702, LR 1.833615 Loss 5.244553, Accuracy 88.290%\n",
      "Epoch 21, Batch 703, LR 1.833497 Loss 5.244187, Accuracy 88.296%\n",
      "Epoch 21, Batch 704, LR 1.833378 Loss 5.244558, Accuracy 88.288%\n",
      "Epoch 21, Batch 705, LR 1.833260 Loss 5.244724, Accuracy 88.287%\n",
      "Epoch 21, Batch 706, LR 1.833141 Loss 5.244660, Accuracy 88.289%\n",
      "Epoch 21, Batch 707, LR 1.833023 Loss 5.244173, Accuracy 88.287%\n",
      "Epoch 21, Batch 708, LR 1.832904 Loss 5.245395, Accuracy 88.285%\n",
      "Epoch 21, Batch 709, LR 1.832786 Loss 5.245897, Accuracy 88.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 710, LR 1.832667 Loss 5.246457, Accuracy 88.271%\n",
      "Epoch 21, Batch 711, LR 1.832549 Loss 5.246457, Accuracy 88.272%\n",
      "Epoch 21, Batch 712, LR 1.832430 Loss 5.246218, Accuracy 88.271%\n",
      "Epoch 21, Batch 713, LR 1.832312 Loss 5.246455, Accuracy 88.270%\n",
      "Epoch 21, Batch 714, LR 1.832193 Loss 5.245371, Accuracy 88.275%\n",
      "Epoch 21, Batch 715, LR 1.832075 Loss 5.245138, Accuracy 88.277%\n",
      "Epoch 21, Batch 716, LR 1.831956 Loss 5.244695, Accuracy 88.280%\n",
      "Epoch 21, Batch 717, LR 1.831838 Loss 5.244451, Accuracy 88.280%\n",
      "Epoch 21, Batch 718, LR 1.831719 Loss 5.244087, Accuracy 88.285%\n",
      "Epoch 21, Batch 719, LR 1.831601 Loss 5.246154, Accuracy 88.275%\n",
      "Epoch 21, Batch 720, LR 1.831482 Loss 5.245377, Accuracy 88.280%\n",
      "Epoch 21, Batch 721, LR 1.831363 Loss 5.245412, Accuracy 88.282%\n",
      "Epoch 21, Batch 722, LR 1.831245 Loss 5.245323, Accuracy 88.283%\n",
      "Epoch 21, Batch 723, LR 1.831126 Loss 5.245440, Accuracy 88.283%\n",
      "Epoch 21, Batch 724, LR 1.831008 Loss 5.245539, Accuracy 88.282%\n",
      "Epoch 21, Batch 725, LR 1.830889 Loss 5.246577, Accuracy 88.275%\n",
      "Epoch 21, Batch 726, LR 1.830770 Loss 5.246474, Accuracy 88.274%\n",
      "Epoch 21, Batch 727, LR 1.830652 Loss 5.245530, Accuracy 88.276%\n",
      "Epoch 21, Batch 728, LR 1.830533 Loss 5.244977, Accuracy 88.280%\n",
      "Epoch 21, Batch 729, LR 1.830415 Loss 5.244613, Accuracy 88.280%\n",
      "Epoch 21, Batch 730, LR 1.830296 Loss 5.243731, Accuracy 88.282%\n",
      "Epoch 21, Batch 731, LR 1.830177 Loss 5.244290, Accuracy 88.278%\n",
      "Epoch 21, Batch 732, LR 1.830059 Loss 5.243389, Accuracy 88.281%\n",
      "Epoch 21, Batch 733, LR 1.829940 Loss 5.243872, Accuracy 88.276%\n",
      "Epoch 21, Batch 734, LR 1.829821 Loss 5.243769, Accuracy 88.281%\n",
      "Epoch 21, Batch 735, LR 1.829703 Loss 5.243304, Accuracy 88.280%\n",
      "Epoch 21, Batch 736, LR 1.829584 Loss 5.242978, Accuracy 88.281%\n",
      "Epoch 21, Batch 737, LR 1.829465 Loss 5.243298, Accuracy 88.279%\n",
      "Epoch 21, Batch 738, LR 1.829347 Loss 5.243792, Accuracy 88.281%\n",
      "Epoch 21, Batch 739, LR 1.829228 Loss 5.244349, Accuracy 88.282%\n",
      "Epoch 21, Batch 740, LR 1.829109 Loss 5.243695, Accuracy 88.282%\n",
      "Epoch 21, Batch 741, LR 1.828990 Loss 5.244048, Accuracy 88.282%\n",
      "Epoch 21, Batch 742, LR 1.828872 Loss 5.243985, Accuracy 88.284%\n",
      "Epoch 21, Batch 743, LR 1.828753 Loss 5.243974, Accuracy 88.285%\n",
      "Epoch 21, Batch 744, LR 1.828634 Loss 5.244102, Accuracy 88.285%\n",
      "Epoch 21, Batch 745, LR 1.828516 Loss 5.244298, Accuracy 88.284%\n",
      "Epoch 21, Batch 746, LR 1.828397 Loss 5.243542, Accuracy 88.284%\n",
      "Epoch 21, Batch 747, LR 1.828278 Loss 5.243202, Accuracy 88.286%\n",
      "Epoch 21, Batch 748, LR 1.828159 Loss 5.242075, Accuracy 88.292%\n",
      "Epoch 21, Batch 749, LR 1.828040 Loss 5.242800, Accuracy 88.292%\n",
      "Epoch 21, Batch 750, LR 1.827922 Loss 5.243702, Accuracy 88.282%\n",
      "Epoch 21, Batch 751, LR 1.827803 Loss 5.243359, Accuracy 88.282%\n",
      "Epoch 21, Batch 752, LR 1.827684 Loss 5.242463, Accuracy 88.290%\n",
      "Epoch 21, Batch 753, LR 1.827565 Loss 5.242013, Accuracy 88.292%\n",
      "Epoch 21, Batch 754, LR 1.827447 Loss 5.242014, Accuracy 88.292%\n",
      "Epoch 21, Batch 755, LR 1.827328 Loss 5.242318, Accuracy 88.292%\n",
      "Epoch 21, Batch 756, LR 1.827209 Loss 5.242865, Accuracy 88.296%\n",
      "Epoch 21, Batch 757, LR 1.827090 Loss 5.243120, Accuracy 88.294%\n",
      "Epoch 21, Batch 758, LR 1.826971 Loss 5.243002, Accuracy 88.299%\n",
      "Epoch 21, Batch 759, LR 1.826852 Loss 5.242765, Accuracy 88.300%\n",
      "Epoch 21, Batch 760, LR 1.826734 Loss 5.243160, Accuracy 88.299%\n",
      "Epoch 21, Batch 761, LR 1.826615 Loss 5.242940, Accuracy 88.299%\n",
      "Epoch 21, Batch 762, LR 1.826496 Loss 5.243138, Accuracy 88.297%\n",
      "Epoch 21, Batch 763, LR 1.826377 Loss 5.243457, Accuracy 88.294%\n",
      "Epoch 21, Batch 764, LR 1.826258 Loss 5.243660, Accuracy 88.297%\n",
      "Epoch 21, Batch 765, LR 1.826139 Loss 5.243457, Accuracy 88.302%\n",
      "Epoch 21, Batch 766, LR 1.826020 Loss 5.242874, Accuracy 88.300%\n",
      "Epoch 21, Batch 767, LR 1.825902 Loss 5.243020, Accuracy 88.299%\n",
      "Epoch 21, Batch 768, LR 1.825783 Loss 5.243311, Accuracy 88.303%\n",
      "Epoch 21, Batch 769, LR 1.825664 Loss 5.242540, Accuracy 88.305%\n",
      "Epoch 21, Batch 770, LR 1.825545 Loss 5.243236, Accuracy 88.300%\n",
      "Epoch 21, Batch 771, LR 1.825426 Loss 5.242436, Accuracy 88.303%\n",
      "Epoch 21, Batch 772, LR 1.825307 Loss 5.243418, Accuracy 88.301%\n",
      "Epoch 21, Batch 773, LR 1.825188 Loss 5.243193, Accuracy 88.303%\n",
      "Epoch 21, Batch 774, LR 1.825069 Loss 5.243831, Accuracy 88.302%\n",
      "Epoch 21, Batch 775, LR 1.824950 Loss 5.244299, Accuracy 88.306%\n",
      "Epoch 21, Batch 776, LR 1.824831 Loss 5.244035, Accuracy 88.308%\n",
      "Epoch 21, Batch 777, LR 1.824712 Loss 5.244724, Accuracy 88.309%\n",
      "Epoch 21, Batch 778, LR 1.824593 Loss 5.244104, Accuracy 88.314%\n",
      "Epoch 21, Batch 779, LR 1.824474 Loss 5.244436, Accuracy 88.310%\n",
      "Epoch 21, Batch 780, LR 1.824355 Loss 5.243679, Accuracy 88.316%\n",
      "Epoch 21, Batch 781, LR 1.824236 Loss 5.243838, Accuracy 88.320%\n",
      "Epoch 21, Batch 782, LR 1.824117 Loss 5.243733, Accuracy 88.321%\n",
      "Epoch 21, Batch 783, LR 1.823998 Loss 5.244099, Accuracy 88.321%\n",
      "Epoch 21, Batch 784, LR 1.823879 Loss 5.244621, Accuracy 88.320%\n",
      "Epoch 21, Batch 785, LR 1.823760 Loss 5.245055, Accuracy 88.319%\n",
      "Epoch 21, Batch 786, LR 1.823641 Loss 5.245418, Accuracy 88.317%\n",
      "Epoch 21, Batch 787, LR 1.823522 Loss 5.246024, Accuracy 88.315%\n",
      "Epoch 21, Batch 788, LR 1.823403 Loss 5.246117, Accuracy 88.313%\n",
      "Epoch 21, Batch 789, LR 1.823284 Loss 5.246809, Accuracy 88.312%\n",
      "Epoch 21, Batch 790, LR 1.823165 Loss 5.246632, Accuracy 88.313%\n",
      "Epoch 21, Batch 791, LR 1.823046 Loss 5.246053, Accuracy 88.317%\n",
      "Epoch 21, Batch 792, LR 1.822927 Loss 5.245882, Accuracy 88.315%\n",
      "Epoch 21, Batch 793, LR 1.822808 Loss 5.246135, Accuracy 88.317%\n",
      "Epoch 21, Batch 794, LR 1.822689 Loss 5.246434, Accuracy 88.315%\n",
      "Epoch 21, Batch 795, LR 1.822570 Loss 5.246845, Accuracy 88.312%\n",
      "Epoch 21, Batch 796, LR 1.822451 Loss 5.246372, Accuracy 88.318%\n",
      "Epoch 21, Batch 797, LR 1.822332 Loss 5.246328, Accuracy 88.318%\n",
      "Epoch 21, Batch 798, LR 1.822213 Loss 5.246877, Accuracy 88.317%\n",
      "Epoch 21, Batch 799, LR 1.822094 Loss 5.247535, Accuracy 88.313%\n",
      "Epoch 21, Batch 800, LR 1.821975 Loss 5.246632, Accuracy 88.315%\n",
      "Epoch 21, Batch 801, LR 1.821855 Loss 5.246235, Accuracy 88.320%\n",
      "Epoch 21, Batch 802, LR 1.821736 Loss 5.246372, Accuracy 88.312%\n",
      "Epoch 21, Batch 803, LR 1.821617 Loss 5.246282, Accuracy 88.312%\n",
      "Epoch 21, Batch 804, LR 1.821498 Loss 5.246294, Accuracy 88.308%\n",
      "Epoch 21, Batch 805, LR 1.821379 Loss 5.244799, Accuracy 88.317%\n",
      "Epoch 21, Batch 806, LR 1.821260 Loss 5.245475, Accuracy 88.311%\n",
      "Epoch 21, Batch 807, LR 1.821141 Loss 5.244943, Accuracy 88.315%\n",
      "Epoch 21, Batch 808, LR 1.821022 Loss 5.244977, Accuracy 88.319%\n",
      "Epoch 21, Batch 809, LR 1.820902 Loss 5.244584, Accuracy 88.321%\n",
      "Epoch 21, Batch 810, LR 1.820783 Loss 5.245550, Accuracy 88.314%\n",
      "Epoch 21, Batch 811, LR 1.820664 Loss 5.245754, Accuracy 88.314%\n",
      "Epoch 21, Batch 812, LR 1.820545 Loss 5.245549, Accuracy 88.318%\n",
      "Epoch 21, Batch 813, LR 1.820426 Loss 5.245051, Accuracy 88.322%\n",
      "Epoch 21, Batch 814, LR 1.820306 Loss 5.245310, Accuracy 88.321%\n",
      "Epoch 21, Batch 815, LR 1.820187 Loss 5.245037, Accuracy 88.318%\n",
      "Epoch 21, Batch 816, LR 1.820068 Loss 5.245498, Accuracy 88.320%\n",
      "Epoch 21, Batch 817, LR 1.819949 Loss 5.244505, Accuracy 88.320%\n",
      "Epoch 21, Batch 818, LR 1.819830 Loss 5.244178, Accuracy 88.319%\n",
      "Epoch 21, Batch 819, LR 1.819710 Loss 5.244142, Accuracy 88.318%\n",
      "Epoch 21, Batch 820, LR 1.819591 Loss 5.244095, Accuracy 88.315%\n",
      "Epoch 21, Batch 821, LR 1.819472 Loss 5.245077, Accuracy 88.309%\n",
      "Epoch 21, Batch 822, LR 1.819353 Loss 5.245291, Accuracy 88.305%\n",
      "Epoch 21, Batch 823, LR 1.819233 Loss 5.245546, Accuracy 88.309%\n",
      "Epoch 21, Batch 824, LR 1.819114 Loss 5.246227, Accuracy 88.300%\n",
      "Epoch 21, Batch 825, LR 1.818995 Loss 5.246857, Accuracy 88.298%\n",
      "Epoch 21, Batch 826, LR 1.818876 Loss 5.247180, Accuracy 88.295%\n",
      "Epoch 21, Batch 827, LR 1.818756 Loss 5.247475, Accuracy 88.295%\n",
      "Epoch 21, Batch 828, LR 1.818637 Loss 5.247744, Accuracy 88.292%\n",
      "Epoch 21, Batch 829, LR 1.818518 Loss 5.248995, Accuracy 88.287%\n",
      "Epoch 21, Batch 830, LR 1.818398 Loss 5.249011, Accuracy 88.284%\n",
      "Epoch 21, Batch 831, LR 1.818279 Loss 5.249880, Accuracy 88.281%\n",
      "Epoch 21, Batch 832, LR 1.818160 Loss 5.250302, Accuracy 88.286%\n",
      "Epoch 21, Batch 833, LR 1.818040 Loss 5.250617, Accuracy 88.287%\n",
      "Epoch 21, Batch 834, LR 1.817921 Loss 5.250399, Accuracy 88.284%\n",
      "Epoch 21, Batch 835, LR 1.817802 Loss 5.249823, Accuracy 88.283%\n",
      "Epoch 21, Batch 836, LR 1.817682 Loss 5.250210, Accuracy 88.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 837, LR 1.817563 Loss 5.250685, Accuracy 88.281%\n",
      "Epoch 21, Batch 838, LR 1.817444 Loss 5.251098, Accuracy 88.280%\n",
      "Epoch 21, Batch 839, LR 1.817324 Loss 5.251450, Accuracy 88.283%\n",
      "Epoch 21, Batch 840, LR 1.817205 Loss 5.251171, Accuracy 88.285%\n",
      "Epoch 21, Batch 841, LR 1.817086 Loss 5.250639, Accuracy 88.292%\n",
      "Epoch 21, Batch 842, LR 1.816966 Loss 5.250682, Accuracy 88.292%\n",
      "Epoch 21, Batch 843, LR 1.816847 Loss 5.250504, Accuracy 88.293%\n",
      "Epoch 21, Batch 844, LR 1.816728 Loss 5.250222, Accuracy 88.294%\n",
      "Epoch 21, Batch 845, LR 1.816608 Loss 5.250768, Accuracy 88.292%\n",
      "Epoch 21, Batch 846, LR 1.816489 Loss 5.251199, Accuracy 88.290%\n",
      "Epoch 21, Batch 847, LR 1.816369 Loss 5.251171, Accuracy 88.294%\n",
      "Epoch 21, Batch 848, LR 1.816250 Loss 5.250937, Accuracy 88.294%\n",
      "Epoch 21, Batch 849, LR 1.816130 Loss 5.251374, Accuracy 88.291%\n",
      "Epoch 21, Batch 850, LR 1.816011 Loss 5.251782, Accuracy 88.291%\n",
      "Epoch 21, Batch 851, LR 1.815892 Loss 5.252269, Accuracy 88.285%\n",
      "Epoch 21, Batch 852, LR 1.815772 Loss 5.252767, Accuracy 88.285%\n",
      "Epoch 21, Batch 853, LR 1.815653 Loss 5.252367, Accuracy 88.285%\n",
      "Epoch 21, Batch 854, LR 1.815533 Loss 5.252752, Accuracy 88.287%\n",
      "Epoch 21, Batch 855, LR 1.815414 Loss 5.252771, Accuracy 88.284%\n",
      "Epoch 21, Batch 856, LR 1.815294 Loss 5.253369, Accuracy 88.281%\n",
      "Epoch 21, Batch 857, LR 1.815175 Loss 5.254035, Accuracy 88.275%\n",
      "Epoch 21, Batch 858, LR 1.815055 Loss 5.253671, Accuracy 88.274%\n",
      "Epoch 21, Batch 859, LR 1.814936 Loss 5.253523, Accuracy 88.275%\n",
      "Epoch 21, Batch 860, LR 1.814816 Loss 5.253704, Accuracy 88.272%\n",
      "Epoch 21, Batch 861, LR 1.814697 Loss 5.252969, Accuracy 88.278%\n",
      "Epoch 21, Batch 862, LR 1.814577 Loss 5.252802, Accuracy 88.279%\n",
      "Epoch 21, Batch 863, LR 1.814458 Loss 5.252938, Accuracy 88.279%\n",
      "Epoch 21, Batch 864, LR 1.814338 Loss 5.253274, Accuracy 88.280%\n",
      "Epoch 21, Batch 865, LR 1.814219 Loss 5.252932, Accuracy 88.286%\n",
      "Epoch 21, Batch 866, LR 1.814099 Loss 5.252345, Accuracy 88.289%\n",
      "Epoch 21, Batch 867, LR 1.813980 Loss 5.252555, Accuracy 88.287%\n",
      "Epoch 21, Batch 868, LR 1.813860 Loss 5.252324, Accuracy 88.289%\n",
      "Epoch 21, Batch 869, LR 1.813741 Loss 5.251664, Accuracy 88.289%\n",
      "Epoch 21, Batch 870, LR 1.813621 Loss 5.251961, Accuracy 88.288%\n",
      "Epoch 21, Batch 871, LR 1.813502 Loss 5.252178, Accuracy 88.288%\n",
      "Epoch 21, Batch 872, LR 1.813382 Loss 5.251830, Accuracy 88.284%\n",
      "Epoch 21, Batch 873, LR 1.813262 Loss 5.251156, Accuracy 88.286%\n",
      "Epoch 21, Batch 874, LR 1.813143 Loss 5.250786, Accuracy 88.288%\n",
      "Epoch 21, Batch 875, LR 1.813023 Loss 5.250768, Accuracy 88.287%\n",
      "Epoch 21, Batch 876, LR 1.812904 Loss 5.249917, Accuracy 88.291%\n",
      "Epoch 21, Batch 877, LR 1.812784 Loss 5.251355, Accuracy 88.285%\n",
      "Epoch 21, Batch 878, LR 1.812664 Loss 5.251387, Accuracy 88.280%\n",
      "Epoch 21, Batch 879, LR 1.812545 Loss 5.251163, Accuracy 88.277%\n",
      "Epoch 21, Batch 880, LR 1.812425 Loss 5.251065, Accuracy 88.278%\n",
      "Epoch 21, Batch 881, LR 1.812305 Loss 5.251422, Accuracy 88.277%\n",
      "Epoch 21, Batch 882, LR 1.812186 Loss 5.250639, Accuracy 88.280%\n",
      "Epoch 21, Batch 883, LR 1.812066 Loss 5.251026, Accuracy 88.281%\n",
      "Epoch 21, Batch 884, LR 1.811947 Loss 5.251137, Accuracy 88.279%\n",
      "Epoch 21, Batch 885, LR 1.811827 Loss 5.250246, Accuracy 88.282%\n",
      "Epoch 21, Batch 886, LR 1.811707 Loss 5.250903, Accuracy 88.279%\n",
      "Epoch 21, Batch 887, LR 1.811588 Loss 5.251238, Accuracy 88.278%\n",
      "Epoch 21, Batch 888, LR 1.811468 Loss 5.250804, Accuracy 88.283%\n",
      "Epoch 21, Batch 889, LR 1.811348 Loss 5.251244, Accuracy 88.284%\n",
      "Epoch 21, Batch 890, LR 1.811229 Loss 5.251633, Accuracy 88.279%\n",
      "Epoch 21, Batch 891, LR 1.811109 Loss 5.252555, Accuracy 88.273%\n",
      "Epoch 21, Batch 892, LR 1.810989 Loss 5.252324, Accuracy 88.271%\n",
      "Epoch 21, Batch 893, LR 1.810869 Loss 5.252519, Accuracy 88.268%\n",
      "Epoch 21, Batch 894, LR 1.810750 Loss 5.253051, Accuracy 88.264%\n",
      "Epoch 21, Batch 895, LR 1.810630 Loss 5.252958, Accuracy 88.266%\n",
      "Epoch 21, Batch 896, LR 1.810510 Loss 5.252802, Accuracy 88.267%\n",
      "Epoch 21, Batch 897, LR 1.810391 Loss 5.252892, Accuracy 88.270%\n",
      "Epoch 21, Batch 898, LR 1.810271 Loss 5.252693, Accuracy 88.269%\n",
      "Epoch 21, Batch 899, LR 1.810151 Loss 5.252945, Accuracy 88.267%\n",
      "Epoch 21, Batch 900, LR 1.810031 Loss 5.253372, Accuracy 88.263%\n",
      "Epoch 21, Batch 901, LR 1.809912 Loss 5.253870, Accuracy 88.262%\n",
      "Epoch 21, Batch 902, LR 1.809792 Loss 5.254532, Accuracy 88.260%\n",
      "Epoch 21, Batch 903, LR 1.809672 Loss 5.253963, Accuracy 88.262%\n",
      "Epoch 21, Batch 904, LR 1.809552 Loss 5.253947, Accuracy 88.266%\n",
      "Epoch 21, Batch 905, LR 1.809432 Loss 5.253505, Accuracy 88.273%\n",
      "Epoch 21, Batch 906, LR 1.809313 Loss 5.253853, Accuracy 88.272%\n",
      "Epoch 21, Batch 907, LR 1.809193 Loss 5.254047, Accuracy 88.268%\n",
      "Epoch 21, Batch 908, LR 1.809073 Loss 5.254907, Accuracy 88.261%\n",
      "Epoch 21, Batch 909, LR 1.808953 Loss 5.255158, Accuracy 88.259%\n",
      "Epoch 21, Batch 910, LR 1.808833 Loss 5.255195, Accuracy 88.259%\n",
      "Epoch 21, Batch 911, LR 1.808714 Loss 5.255220, Accuracy 88.258%\n",
      "Epoch 21, Batch 912, LR 1.808594 Loss 5.255136, Accuracy 88.262%\n",
      "Epoch 21, Batch 913, LR 1.808474 Loss 5.255804, Accuracy 88.258%\n",
      "Epoch 21, Batch 914, LR 1.808354 Loss 5.255721, Accuracy 88.256%\n",
      "Epoch 21, Batch 915, LR 1.808234 Loss 5.255252, Accuracy 88.262%\n",
      "Epoch 21, Batch 916, LR 1.808114 Loss 5.255341, Accuracy 88.262%\n",
      "Epoch 21, Batch 917, LR 1.807994 Loss 5.255394, Accuracy 88.263%\n",
      "Epoch 21, Batch 918, LR 1.807875 Loss 5.254873, Accuracy 88.264%\n",
      "Epoch 21, Batch 919, LR 1.807755 Loss 5.255366, Accuracy 88.262%\n",
      "Epoch 21, Batch 920, LR 1.807635 Loss 5.255482, Accuracy 88.259%\n",
      "Epoch 21, Batch 921, LR 1.807515 Loss 5.255555, Accuracy 88.259%\n",
      "Epoch 21, Batch 922, LR 1.807395 Loss 5.255402, Accuracy 88.260%\n",
      "Epoch 21, Batch 923, LR 1.807275 Loss 5.255631, Accuracy 88.258%\n",
      "Epoch 21, Batch 924, LR 1.807155 Loss 5.255987, Accuracy 88.257%\n",
      "Epoch 21, Batch 925, LR 1.807035 Loss 5.255719, Accuracy 88.258%\n",
      "Epoch 21, Batch 926, LR 1.806915 Loss 5.255570, Accuracy 88.260%\n",
      "Epoch 21, Batch 927, LR 1.806795 Loss 5.255506, Accuracy 88.259%\n",
      "Epoch 21, Batch 928, LR 1.806676 Loss 5.255711, Accuracy 88.256%\n",
      "Epoch 21, Batch 929, LR 1.806556 Loss 5.255885, Accuracy 88.253%\n",
      "Epoch 21, Batch 930, LR 1.806436 Loss 5.256096, Accuracy 88.254%\n",
      "Epoch 21, Batch 931, LR 1.806316 Loss 5.256067, Accuracy 88.254%\n",
      "Epoch 21, Batch 932, LR 1.806196 Loss 5.256773, Accuracy 88.249%\n",
      "Epoch 21, Batch 933, LR 1.806076 Loss 5.256198, Accuracy 88.250%\n",
      "Epoch 21, Batch 934, LR 1.805956 Loss 5.256145, Accuracy 88.245%\n",
      "Epoch 21, Batch 935, LR 1.805836 Loss 5.255997, Accuracy 88.244%\n",
      "Epoch 21, Batch 936, LR 1.805716 Loss 5.256358, Accuracy 88.242%\n",
      "Epoch 21, Batch 937, LR 1.805596 Loss 5.256442, Accuracy 88.241%\n",
      "Epoch 21, Batch 938, LR 1.805476 Loss 5.255428, Accuracy 88.246%\n",
      "Epoch 21, Batch 939, LR 1.805356 Loss 5.255027, Accuracy 88.252%\n",
      "Epoch 21, Batch 940, LR 1.805236 Loss 5.254994, Accuracy 88.250%\n",
      "Epoch 21, Batch 941, LR 1.805116 Loss 5.255763, Accuracy 88.244%\n",
      "Epoch 21, Batch 942, LR 1.804996 Loss 5.255757, Accuracy 88.241%\n",
      "Epoch 21, Batch 943, LR 1.804876 Loss 5.255702, Accuracy 88.243%\n",
      "Epoch 21, Batch 944, LR 1.804756 Loss 5.256031, Accuracy 88.242%\n",
      "Epoch 21, Batch 945, LR 1.804636 Loss 5.256959, Accuracy 88.233%\n",
      "Epoch 21, Batch 946, LR 1.804516 Loss 5.256870, Accuracy 88.232%\n",
      "Epoch 21, Batch 947, LR 1.804396 Loss 5.257048, Accuracy 88.228%\n",
      "Epoch 21, Batch 948, LR 1.804276 Loss 5.257251, Accuracy 88.229%\n",
      "Epoch 21, Batch 949, LR 1.804155 Loss 5.257558, Accuracy 88.230%\n",
      "Epoch 21, Batch 950, LR 1.804035 Loss 5.257964, Accuracy 88.229%\n",
      "Epoch 21, Batch 951, LR 1.803915 Loss 5.258142, Accuracy 88.229%\n",
      "Epoch 21, Batch 952, LR 1.803795 Loss 5.258192, Accuracy 88.228%\n",
      "Epoch 21, Batch 953, LR 1.803675 Loss 5.258605, Accuracy 88.228%\n",
      "Epoch 21, Batch 954, LR 1.803555 Loss 5.258811, Accuracy 88.226%\n",
      "Epoch 21, Batch 955, LR 1.803435 Loss 5.258347, Accuracy 88.227%\n",
      "Epoch 21, Batch 956, LR 1.803315 Loss 5.257838, Accuracy 88.230%\n",
      "Epoch 21, Batch 957, LR 1.803195 Loss 5.257480, Accuracy 88.232%\n",
      "Epoch 21, Batch 958, LR 1.803075 Loss 5.256907, Accuracy 88.235%\n",
      "Epoch 21, Batch 959, LR 1.802954 Loss 5.256585, Accuracy 88.236%\n",
      "Epoch 21, Batch 960, LR 1.802834 Loss 5.257215, Accuracy 88.232%\n",
      "Epoch 21, Batch 961, LR 1.802714 Loss 5.257200, Accuracy 88.231%\n",
      "Epoch 21, Batch 962, LR 1.802594 Loss 5.257363, Accuracy 88.224%\n",
      "Epoch 21, Batch 963, LR 1.802474 Loss 5.256783, Accuracy 88.226%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 964, LR 1.802354 Loss 5.256595, Accuracy 88.227%\n",
      "Epoch 21, Batch 965, LR 1.802234 Loss 5.255957, Accuracy 88.230%\n",
      "Epoch 21, Batch 966, LR 1.802113 Loss 5.256556, Accuracy 88.229%\n",
      "Epoch 21, Batch 967, LR 1.801993 Loss 5.256471, Accuracy 88.228%\n",
      "Epoch 21, Batch 968, LR 1.801873 Loss 5.256096, Accuracy 88.228%\n",
      "Epoch 21, Batch 969, LR 1.801753 Loss 5.255645, Accuracy 88.229%\n",
      "Epoch 21, Batch 970, LR 1.801633 Loss 5.255753, Accuracy 88.226%\n",
      "Epoch 21, Batch 971, LR 1.801512 Loss 5.255410, Accuracy 88.227%\n",
      "Epoch 21, Batch 972, LR 1.801392 Loss 5.255299, Accuracy 88.227%\n",
      "Epoch 21, Batch 973, LR 1.801272 Loss 5.255554, Accuracy 88.227%\n",
      "Epoch 21, Batch 974, LR 1.801152 Loss 5.255427, Accuracy 88.229%\n",
      "Epoch 21, Batch 975, LR 1.801031 Loss 5.254528, Accuracy 88.233%\n",
      "Epoch 21, Batch 976, LR 1.800911 Loss 5.254328, Accuracy 88.234%\n",
      "Epoch 21, Batch 977, LR 1.800791 Loss 5.254247, Accuracy 88.237%\n",
      "Epoch 21, Batch 978, LR 1.800671 Loss 5.254209, Accuracy 88.240%\n",
      "Epoch 21, Batch 979, LR 1.800550 Loss 5.254140, Accuracy 88.240%\n",
      "Epoch 21, Batch 980, LR 1.800430 Loss 5.254262, Accuracy 88.240%\n",
      "Epoch 21, Batch 981, LR 1.800310 Loss 5.254099, Accuracy 88.241%\n",
      "Epoch 21, Batch 982, LR 1.800190 Loss 5.254024, Accuracy 88.238%\n",
      "Epoch 21, Batch 983, LR 1.800069 Loss 5.254904, Accuracy 88.235%\n",
      "Epoch 21, Batch 984, LR 1.799949 Loss 5.255291, Accuracy 88.235%\n",
      "Epoch 21, Batch 985, LR 1.799829 Loss 5.254995, Accuracy 88.238%\n",
      "Epoch 21, Batch 986, LR 1.799709 Loss 5.255427, Accuracy 88.234%\n",
      "Epoch 21, Batch 987, LR 1.799588 Loss 5.254922, Accuracy 88.236%\n",
      "Epoch 21, Batch 988, LR 1.799468 Loss 5.255177, Accuracy 88.236%\n",
      "Epoch 21, Batch 989, LR 1.799348 Loss 5.254748, Accuracy 88.238%\n",
      "Epoch 21, Batch 990, LR 1.799227 Loss 5.254821, Accuracy 88.235%\n",
      "Epoch 21, Batch 991, LR 1.799107 Loss 5.255269, Accuracy 88.236%\n",
      "Epoch 21, Batch 992, LR 1.798987 Loss 5.255005, Accuracy 88.237%\n",
      "Epoch 21, Batch 993, LR 1.798866 Loss 5.254826, Accuracy 88.240%\n",
      "Epoch 21, Batch 994, LR 1.798746 Loss 5.255520, Accuracy 88.236%\n",
      "Epoch 21, Batch 995, LR 1.798626 Loss 5.255848, Accuracy 88.235%\n",
      "Epoch 21, Batch 996, LR 1.798505 Loss 5.255721, Accuracy 88.233%\n",
      "Epoch 21, Batch 997, LR 1.798385 Loss 5.255916, Accuracy 88.230%\n",
      "Epoch 21, Batch 998, LR 1.798264 Loss 5.255486, Accuracy 88.235%\n",
      "Epoch 21, Batch 999, LR 1.798144 Loss 5.255389, Accuracy 88.237%\n",
      "Epoch 21, Batch 1000, LR 1.798024 Loss 5.255429, Accuracy 88.234%\n",
      "Epoch 21, Batch 1001, LR 1.797903 Loss 5.255468, Accuracy 88.235%\n",
      "Epoch 21, Batch 1002, LR 1.797783 Loss 5.254835, Accuracy 88.241%\n",
      "Epoch 21, Batch 1003, LR 1.797662 Loss 5.254571, Accuracy 88.245%\n",
      "Epoch 21, Batch 1004, LR 1.797542 Loss 5.254670, Accuracy 88.244%\n",
      "Epoch 21, Batch 1005, LR 1.797422 Loss 5.254858, Accuracy 88.245%\n",
      "Epoch 21, Batch 1006, LR 1.797301 Loss 5.254319, Accuracy 88.246%\n",
      "Epoch 21, Batch 1007, LR 1.797181 Loss 5.255217, Accuracy 88.241%\n",
      "Epoch 21, Batch 1008, LR 1.797060 Loss 5.255235, Accuracy 88.242%\n",
      "Epoch 21, Batch 1009, LR 1.796940 Loss 5.255881, Accuracy 88.243%\n",
      "Epoch 21, Batch 1010, LR 1.796819 Loss 5.256068, Accuracy 88.246%\n",
      "Epoch 21, Batch 1011, LR 1.796699 Loss 5.255774, Accuracy 88.246%\n",
      "Epoch 21, Batch 1012, LR 1.796578 Loss 5.255469, Accuracy 88.244%\n",
      "Epoch 21, Batch 1013, LR 1.796458 Loss 5.255410, Accuracy 88.242%\n",
      "Epoch 21, Batch 1014, LR 1.796338 Loss 5.255729, Accuracy 88.242%\n",
      "Epoch 21, Batch 1015, LR 1.796217 Loss 5.256034, Accuracy 88.238%\n",
      "Epoch 21, Batch 1016, LR 1.796097 Loss 5.255599, Accuracy 88.242%\n",
      "Epoch 21, Batch 1017, LR 1.795976 Loss 5.255449, Accuracy 88.241%\n",
      "Epoch 21, Batch 1018, LR 1.795856 Loss 5.254656, Accuracy 88.247%\n",
      "Epoch 21, Batch 1019, LR 1.795735 Loss 5.254590, Accuracy 88.251%\n",
      "Epoch 21, Batch 1020, LR 1.795615 Loss 5.254696, Accuracy 88.248%\n",
      "Epoch 21, Batch 1021, LR 1.795494 Loss 5.254470, Accuracy 88.249%\n",
      "Epoch 21, Batch 1022, LR 1.795373 Loss 5.253588, Accuracy 88.251%\n",
      "Epoch 21, Batch 1023, LR 1.795253 Loss 5.253514, Accuracy 88.248%\n",
      "Epoch 21, Batch 1024, LR 1.795132 Loss 5.253145, Accuracy 88.251%\n",
      "Epoch 21, Batch 1025, LR 1.795012 Loss 5.252796, Accuracy 88.255%\n",
      "Epoch 21, Batch 1026, LR 1.794891 Loss 5.252890, Accuracy 88.256%\n",
      "Epoch 21, Batch 1027, LR 1.794771 Loss 5.253040, Accuracy 88.255%\n",
      "Epoch 21, Batch 1028, LR 1.794650 Loss 5.251648, Accuracy 88.258%\n",
      "Epoch 21, Batch 1029, LR 1.794530 Loss 5.250690, Accuracy 88.261%\n",
      "Epoch 21, Batch 1030, LR 1.794409 Loss 5.250445, Accuracy 88.263%\n",
      "Epoch 21, Batch 1031, LR 1.794288 Loss 5.249754, Accuracy 88.269%\n",
      "Epoch 21, Batch 1032, LR 1.794168 Loss 5.249509, Accuracy 88.268%\n",
      "Epoch 21, Batch 1033, LR 1.794047 Loss 5.249695, Accuracy 88.264%\n",
      "Epoch 21, Batch 1034, LR 1.793927 Loss 5.250421, Accuracy 88.261%\n",
      "Epoch 21, Batch 1035, LR 1.793806 Loss 5.249945, Accuracy 88.262%\n",
      "Epoch 21, Batch 1036, LR 1.793685 Loss 5.250061, Accuracy 88.261%\n",
      "Epoch 21, Batch 1037, LR 1.793565 Loss 5.250217, Accuracy 88.262%\n",
      "Epoch 21, Batch 1038, LR 1.793444 Loss 5.250473, Accuracy 88.259%\n",
      "Epoch 21, Batch 1039, LR 1.793324 Loss 5.250595, Accuracy 88.256%\n",
      "Epoch 21, Batch 1040, LR 1.793203 Loss 5.250193, Accuracy 88.256%\n",
      "Epoch 21, Batch 1041, LR 1.793082 Loss 5.249549, Accuracy 88.259%\n",
      "Epoch 21, Batch 1042, LR 1.792962 Loss 5.248943, Accuracy 88.260%\n",
      "Epoch 21, Batch 1043, LR 1.792841 Loss 5.248960, Accuracy 88.260%\n",
      "Epoch 21, Batch 1044, LR 1.792720 Loss 5.248481, Accuracy 88.260%\n",
      "Epoch 21, Batch 1045, LR 1.792600 Loss 5.249411, Accuracy 88.257%\n",
      "Epoch 21, Batch 1046, LR 1.792479 Loss 5.250051, Accuracy 88.253%\n",
      "Epoch 21, Batch 1047, LR 1.792358 Loss 5.249929, Accuracy 88.253%\n",
      "Epoch 21, Loss (train set) 5.249929, Accuracy (train set) 88.253%\n",
      "Epoch 22, Batch 1, LR 1.792238 Loss 5.173315, Accuracy 90.625%\n",
      "Epoch 22, Batch 2, LR 1.792117 Loss 5.471241, Accuracy 89.844%\n",
      "Epoch 22, Batch 3, LR 1.791996 Loss 5.120029, Accuracy 90.365%\n",
      "Epoch 22, Batch 4, LR 1.791875 Loss 5.133471, Accuracy 90.039%\n",
      "Epoch 22, Batch 5, LR 1.791755 Loss 5.131226, Accuracy 89.844%\n",
      "Epoch 22, Batch 6, LR 1.791634 Loss 5.200152, Accuracy 88.932%\n",
      "Epoch 22, Batch 7, LR 1.791513 Loss 5.158454, Accuracy 89.174%\n",
      "Epoch 22, Batch 8, LR 1.791393 Loss 5.186201, Accuracy 89.062%\n",
      "Epoch 22, Batch 9, LR 1.791272 Loss 5.106333, Accuracy 89.583%\n",
      "Epoch 22, Batch 10, LR 1.791151 Loss 5.143697, Accuracy 89.609%\n",
      "Epoch 22, Batch 11, LR 1.791030 Loss 5.140774, Accuracy 89.489%\n",
      "Epoch 22, Batch 12, LR 1.790910 Loss 5.172765, Accuracy 88.997%\n",
      "Epoch 22, Batch 13, LR 1.790789 Loss 5.129534, Accuracy 89.423%\n",
      "Epoch 22, Batch 14, LR 1.790668 Loss 5.090938, Accuracy 89.509%\n",
      "Epoch 22, Batch 15, LR 1.790547 Loss 5.118775, Accuracy 89.479%\n",
      "Epoch 22, Batch 16, LR 1.790426 Loss 5.123680, Accuracy 89.209%\n",
      "Epoch 22, Batch 17, LR 1.790306 Loss 5.091412, Accuracy 89.338%\n",
      "Epoch 22, Batch 18, LR 1.790185 Loss 5.075341, Accuracy 89.410%\n",
      "Epoch 22, Batch 19, LR 1.790064 Loss 5.097112, Accuracy 89.227%\n",
      "Epoch 22, Batch 20, LR 1.789943 Loss 5.137323, Accuracy 89.180%\n",
      "Epoch 22, Batch 21, LR 1.789822 Loss 5.139866, Accuracy 88.988%\n",
      "Epoch 22, Batch 22, LR 1.789702 Loss 5.139812, Accuracy 88.743%\n",
      "Epoch 22, Batch 23, LR 1.789581 Loss 5.123080, Accuracy 88.757%\n",
      "Epoch 22, Batch 24, LR 1.789460 Loss 5.141191, Accuracy 88.639%\n",
      "Epoch 22, Batch 25, LR 1.789339 Loss 5.124275, Accuracy 88.781%\n",
      "Epoch 22, Batch 26, LR 1.789218 Loss 5.101342, Accuracy 89.002%\n",
      "Epoch 22, Batch 27, LR 1.789097 Loss 5.107316, Accuracy 88.947%\n",
      "Epoch 22, Batch 28, LR 1.788977 Loss 5.091131, Accuracy 89.090%\n",
      "Epoch 22, Batch 29, LR 1.788856 Loss 5.084826, Accuracy 89.116%\n",
      "Epoch 22, Batch 30, LR 1.788735 Loss 5.090840, Accuracy 89.115%\n",
      "Epoch 22, Batch 31, LR 1.788614 Loss 5.081208, Accuracy 89.113%\n",
      "Epoch 22, Batch 32, LR 1.788493 Loss 5.069453, Accuracy 89.185%\n",
      "Epoch 22, Batch 33, LR 1.788372 Loss 5.092114, Accuracy 89.015%\n",
      "Epoch 22, Batch 34, LR 1.788251 Loss 5.101440, Accuracy 88.948%\n",
      "Epoch 22, Batch 35, LR 1.788130 Loss 5.087139, Accuracy 89.018%\n",
      "Epoch 22, Batch 36, LR 1.788009 Loss 5.085913, Accuracy 88.954%\n",
      "Epoch 22, Batch 37, LR 1.787889 Loss 5.092323, Accuracy 88.957%\n",
      "Epoch 22, Batch 38, LR 1.787768 Loss 5.079973, Accuracy 89.104%\n",
      "Epoch 22, Batch 39, LR 1.787647 Loss 5.089956, Accuracy 88.982%\n",
      "Epoch 22, Batch 40, LR 1.787526 Loss 5.105998, Accuracy 88.887%\n",
      "Epoch 22, Batch 41, LR 1.787405 Loss 5.099509, Accuracy 88.929%\n",
      "Epoch 22, Batch 42, LR 1.787284 Loss 5.092915, Accuracy 88.951%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 43, LR 1.787163 Loss 5.072814, Accuracy 89.044%\n",
      "Epoch 22, Batch 44, LR 1.787042 Loss 5.066470, Accuracy 89.116%\n",
      "Epoch 22, Batch 45, LR 1.786921 Loss 5.060698, Accuracy 89.167%\n",
      "Epoch 22, Batch 46, LR 1.786800 Loss 5.059535, Accuracy 89.181%\n",
      "Epoch 22, Batch 47, LR 1.786679 Loss 5.057105, Accuracy 89.212%\n",
      "Epoch 22, Batch 48, LR 1.786558 Loss 5.058438, Accuracy 89.176%\n",
      "Epoch 22, Batch 49, LR 1.786437 Loss 5.060561, Accuracy 89.174%\n",
      "Epoch 22, Batch 50, LR 1.786316 Loss 5.058298, Accuracy 89.188%\n",
      "Epoch 22, Batch 51, LR 1.786195 Loss 5.062046, Accuracy 89.231%\n",
      "Epoch 22, Batch 52, LR 1.786074 Loss 5.060505, Accuracy 89.288%\n",
      "Epoch 22, Batch 53, LR 1.785953 Loss 5.055504, Accuracy 89.313%\n",
      "Epoch 22, Batch 54, LR 1.785832 Loss 5.064598, Accuracy 89.265%\n",
      "Epoch 22, Batch 55, LR 1.785711 Loss 5.060844, Accuracy 89.290%\n",
      "Epoch 22, Batch 56, LR 1.785590 Loss 5.061554, Accuracy 89.258%\n",
      "Epoch 22, Batch 57, LR 1.785469 Loss 5.059108, Accuracy 89.254%\n",
      "Epoch 22, Batch 58, LR 1.785348 Loss 5.045851, Accuracy 89.278%\n",
      "Epoch 22, Batch 59, LR 1.785227 Loss 5.035356, Accuracy 89.314%\n",
      "Epoch 22, Batch 60, LR 1.785106 Loss 5.036905, Accuracy 89.284%\n",
      "Epoch 22, Batch 61, LR 1.784985 Loss 5.038731, Accuracy 89.267%\n",
      "Epoch 22, Batch 62, LR 1.784864 Loss 5.035668, Accuracy 89.264%\n",
      "Epoch 22, Batch 63, LR 1.784743 Loss 5.031728, Accuracy 89.311%\n",
      "Epoch 22, Batch 64, LR 1.784622 Loss 5.035047, Accuracy 89.319%\n",
      "Epoch 22, Batch 65, LR 1.784500 Loss 5.043830, Accuracy 89.243%\n",
      "Epoch 22, Batch 66, LR 1.784379 Loss 5.051056, Accuracy 89.193%\n",
      "Epoch 22, Batch 67, LR 1.784258 Loss 5.049026, Accuracy 89.237%\n",
      "Epoch 22, Batch 68, LR 1.784137 Loss 5.048475, Accuracy 89.258%\n",
      "Epoch 22, Batch 69, LR 1.784016 Loss 5.046932, Accuracy 89.300%\n",
      "Epoch 22, Batch 70, LR 1.783895 Loss 5.041589, Accuracy 89.330%\n",
      "Epoch 22, Batch 71, LR 1.783774 Loss 5.042109, Accuracy 89.294%\n",
      "Epoch 22, Batch 72, LR 1.783653 Loss 5.040127, Accuracy 89.312%\n",
      "Epoch 22, Batch 73, LR 1.783532 Loss 5.034331, Accuracy 89.373%\n",
      "Epoch 22, Batch 74, LR 1.783410 Loss 5.044584, Accuracy 89.400%\n",
      "Epoch 22, Batch 75, LR 1.783289 Loss 5.039234, Accuracy 89.406%\n",
      "Epoch 22, Batch 76, LR 1.783168 Loss 5.037577, Accuracy 89.391%\n",
      "Epoch 22, Batch 77, LR 1.783047 Loss 5.036805, Accuracy 89.377%\n",
      "Epoch 22, Batch 78, LR 1.782926 Loss 5.043178, Accuracy 89.313%\n",
      "Epoch 22, Batch 79, LR 1.782805 Loss 5.047836, Accuracy 89.320%\n",
      "Epoch 22, Batch 80, LR 1.782683 Loss 5.042995, Accuracy 89.365%\n",
      "Epoch 22, Batch 81, LR 1.782562 Loss 5.058162, Accuracy 89.323%\n",
      "Epoch 22, Batch 82, LR 1.782441 Loss 5.053311, Accuracy 89.329%\n",
      "Epoch 22, Batch 83, LR 1.782320 Loss 5.050466, Accuracy 89.326%\n",
      "Epoch 22, Batch 84, LR 1.782199 Loss 5.054191, Accuracy 89.276%\n",
      "Epoch 22, Batch 85, LR 1.782077 Loss 5.056823, Accuracy 89.301%\n",
      "Epoch 22, Batch 86, LR 1.781956 Loss 5.058235, Accuracy 89.299%\n",
      "Epoch 22, Batch 87, LR 1.781835 Loss 5.059975, Accuracy 89.305%\n",
      "Epoch 22, Batch 88, LR 1.781714 Loss 5.066881, Accuracy 89.293%\n",
      "Epoch 22, Batch 89, LR 1.781593 Loss 5.070678, Accuracy 89.282%\n",
      "Epoch 22, Batch 90, LR 1.781471 Loss 5.075790, Accuracy 89.314%\n",
      "Epoch 22, Batch 91, LR 1.781350 Loss 5.076524, Accuracy 89.277%\n",
      "Epoch 22, Batch 92, LR 1.781229 Loss 5.069276, Accuracy 89.334%\n",
      "Epoch 22, Batch 93, LR 1.781108 Loss 5.076678, Accuracy 89.264%\n",
      "Epoch 22, Batch 94, LR 1.780986 Loss 5.072617, Accuracy 89.262%\n",
      "Epoch 22, Batch 95, LR 1.780865 Loss 5.070255, Accuracy 89.285%\n",
      "Epoch 22, Batch 96, LR 1.780744 Loss 5.071679, Accuracy 89.299%\n",
      "Epoch 22, Batch 97, LR 1.780622 Loss 5.073889, Accuracy 89.328%\n",
      "Epoch 22, Batch 98, LR 1.780501 Loss 5.076774, Accuracy 89.278%\n",
      "Epoch 22, Batch 99, LR 1.780380 Loss 5.073640, Accuracy 89.276%\n",
      "Epoch 22, Batch 100, LR 1.780259 Loss 5.079127, Accuracy 89.258%\n",
      "Epoch 22, Batch 101, LR 1.780137 Loss 5.073612, Accuracy 89.302%\n",
      "Epoch 22, Batch 102, LR 1.780016 Loss 5.078927, Accuracy 89.254%\n",
      "Epoch 22, Batch 103, LR 1.779895 Loss 5.074723, Accuracy 89.237%\n",
      "Epoch 22, Batch 104, LR 1.779773 Loss 5.073996, Accuracy 89.220%\n",
      "Epoch 22, Batch 105, LR 1.779652 Loss 5.072777, Accuracy 89.211%\n",
      "Epoch 22, Batch 106, LR 1.779531 Loss 5.077081, Accuracy 89.210%\n",
      "Epoch 22, Batch 107, LR 1.779409 Loss 5.074628, Accuracy 89.216%\n",
      "Epoch 22, Batch 108, LR 1.779288 Loss 5.071279, Accuracy 89.236%\n",
      "Epoch 22, Batch 109, LR 1.779167 Loss 5.067087, Accuracy 89.256%\n",
      "Epoch 22, Batch 110, LR 1.779045 Loss 5.063913, Accuracy 89.283%\n",
      "Epoch 22, Batch 111, LR 1.778924 Loss 5.062516, Accuracy 89.288%\n",
      "Epoch 22, Batch 112, LR 1.778802 Loss 5.058034, Accuracy 89.279%\n",
      "Epoch 22, Batch 113, LR 1.778681 Loss 5.061576, Accuracy 89.298%\n",
      "Epoch 22, Batch 114, LR 1.778560 Loss 5.064199, Accuracy 89.275%\n",
      "Epoch 22, Batch 115, LR 1.778438 Loss 5.064449, Accuracy 89.273%\n",
      "Epoch 22, Batch 116, LR 1.778317 Loss 5.065931, Accuracy 89.251%\n",
      "Epoch 22, Batch 117, LR 1.778195 Loss 5.067285, Accuracy 89.229%\n",
      "Epoch 22, Batch 118, LR 1.778074 Loss 5.074189, Accuracy 89.202%\n",
      "Epoch 22, Batch 119, LR 1.777953 Loss 5.079389, Accuracy 89.187%\n",
      "Epoch 22, Batch 120, LR 1.777831 Loss 5.078048, Accuracy 89.173%\n",
      "Epoch 22, Batch 121, LR 1.777710 Loss 5.082401, Accuracy 89.146%\n",
      "Epoch 22, Batch 122, LR 1.777588 Loss 5.082695, Accuracy 89.146%\n",
      "Epoch 22, Batch 123, LR 1.777467 Loss 5.087457, Accuracy 89.088%\n",
      "Epoch 22, Batch 124, LR 1.777346 Loss 5.094098, Accuracy 89.056%\n",
      "Epoch 22, Batch 125, LR 1.777224 Loss 5.093045, Accuracy 89.075%\n",
      "Epoch 22, Batch 126, LR 1.777103 Loss 5.085206, Accuracy 89.100%\n",
      "Epoch 22, Batch 127, LR 1.776981 Loss 5.081565, Accuracy 89.130%\n",
      "Epoch 22, Batch 128, LR 1.776860 Loss 5.076800, Accuracy 89.154%\n",
      "Epoch 22, Batch 129, LR 1.776738 Loss 5.072202, Accuracy 89.159%\n",
      "Epoch 22, Batch 130, LR 1.776617 Loss 5.071034, Accuracy 89.171%\n",
      "Epoch 22, Batch 131, LR 1.776495 Loss 5.074149, Accuracy 89.188%\n",
      "Epoch 22, Batch 132, LR 1.776374 Loss 5.074996, Accuracy 89.199%\n",
      "Epoch 22, Batch 133, LR 1.776252 Loss 5.077774, Accuracy 89.162%\n",
      "Epoch 22, Batch 134, LR 1.776131 Loss 5.074160, Accuracy 89.202%\n",
      "Epoch 22, Batch 135, LR 1.776009 Loss 5.074208, Accuracy 89.213%\n",
      "Epoch 22, Batch 136, LR 1.775888 Loss 5.081847, Accuracy 89.200%\n",
      "Epoch 22, Batch 137, LR 1.775766 Loss 5.078667, Accuracy 89.182%\n",
      "Epoch 22, Batch 138, LR 1.775645 Loss 5.075853, Accuracy 89.193%\n",
      "Epoch 22, Batch 139, LR 1.775523 Loss 5.070993, Accuracy 89.203%\n",
      "Epoch 22, Batch 140, LR 1.775402 Loss 5.071443, Accuracy 89.213%\n",
      "Epoch 22, Batch 141, LR 1.775280 Loss 5.076720, Accuracy 89.179%\n",
      "Epoch 22, Batch 142, LR 1.775158 Loss 5.077649, Accuracy 89.162%\n",
      "Epoch 22, Batch 143, LR 1.775037 Loss 5.078936, Accuracy 89.150%\n",
      "Epoch 22, Batch 144, LR 1.774915 Loss 5.077625, Accuracy 89.171%\n",
      "Epoch 22, Batch 145, LR 1.774794 Loss 5.074597, Accuracy 89.208%\n",
      "Epoch 22, Batch 146, LR 1.774672 Loss 5.077171, Accuracy 89.218%\n",
      "Epoch 22, Batch 147, LR 1.774551 Loss 5.072845, Accuracy 89.238%\n",
      "Epoch 22, Batch 148, LR 1.774429 Loss 5.075880, Accuracy 89.231%\n",
      "Epoch 22, Batch 149, LR 1.774307 Loss 5.076615, Accuracy 89.204%\n",
      "Epoch 22, Batch 150, LR 1.774186 Loss 5.077094, Accuracy 89.203%\n",
      "Epoch 22, Batch 151, LR 1.774064 Loss 5.078772, Accuracy 89.181%\n",
      "Epoch 22, Batch 152, LR 1.773943 Loss 5.079245, Accuracy 89.181%\n",
      "Epoch 22, Batch 153, LR 1.773821 Loss 5.079780, Accuracy 89.170%\n",
      "Epoch 22, Batch 154, LR 1.773699 Loss 5.078176, Accuracy 89.184%\n",
      "Epoch 22, Batch 155, LR 1.773578 Loss 5.081872, Accuracy 89.158%\n",
      "Epoch 22, Batch 156, LR 1.773456 Loss 5.083944, Accuracy 89.133%\n",
      "Epoch 22, Batch 157, LR 1.773334 Loss 5.082677, Accuracy 89.142%\n",
      "Epoch 22, Batch 158, LR 1.773213 Loss 5.077947, Accuracy 89.156%\n",
      "Epoch 22, Batch 159, LR 1.773091 Loss 5.081890, Accuracy 89.131%\n",
      "Epoch 22, Batch 160, LR 1.772969 Loss 5.083520, Accuracy 89.131%\n",
      "Epoch 22, Batch 161, LR 1.772848 Loss 5.083394, Accuracy 89.135%\n",
      "Epoch 22, Batch 162, LR 1.772726 Loss 5.087903, Accuracy 89.125%\n",
      "Epoch 22, Batch 163, LR 1.772604 Loss 5.086451, Accuracy 89.139%\n",
      "Epoch 22, Batch 164, LR 1.772483 Loss 5.091639, Accuracy 89.120%\n",
      "Epoch 22, Batch 165, LR 1.772361 Loss 5.090672, Accuracy 89.119%\n",
      "Epoch 22, Batch 166, LR 1.772239 Loss 5.090468, Accuracy 89.114%\n",
      "Epoch 22, Batch 167, LR 1.772118 Loss 5.090746, Accuracy 89.114%\n",
      "Epoch 22, Batch 168, LR 1.771996 Loss 5.089214, Accuracy 89.114%\n",
      "Epoch 22, Batch 169, LR 1.771874 Loss 5.088163, Accuracy 89.123%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 170, LR 1.771752 Loss 5.087732, Accuracy 89.131%\n",
      "Epoch 22, Batch 171, LR 1.771631 Loss 5.086351, Accuracy 89.145%\n",
      "Epoch 22, Batch 172, LR 1.771509 Loss 5.084854, Accuracy 89.131%\n",
      "Epoch 22, Batch 173, LR 1.771387 Loss 5.083342, Accuracy 89.139%\n",
      "Epoch 22, Batch 174, LR 1.771265 Loss 5.084200, Accuracy 89.130%\n",
      "Epoch 22, Batch 175, LR 1.771144 Loss 5.088284, Accuracy 89.089%\n",
      "Epoch 22, Batch 176, LR 1.771022 Loss 5.089146, Accuracy 89.089%\n",
      "Epoch 22, Batch 177, LR 1.770900 Loss 5.090031, Accuracy 89.089%\n",
      "Epoch 22, Batch 178, LR 1.770778 Loss 5.091752, Accuracy 89.098%\n",
      "Epoch 22, Batch 179, LR 1.770657 Loss 5.098591, Accuracy 89.062%\n",
      "Epoch 22, Batch 180, LR 1.770535 Loss 5.096718, Accuracy 89.080%\n",
      "Epoch 22, Batch 181, LR 1.770413 Loss 5.098746, Accuracy 89.080%\n",
      "Epoch 22, Batch 182, LR 1.770291 Loss 5.098078, Accuracy 89.101%\n",
      "Epoch 22, Batch 183, LR 1.770170 Loss 5.099417, Accuracy 89.092%\n",
      "Epoch 22, Batch 184, LR 1.770048 Loss 5.099874, Accuracy 89.096%\n",
      "Epoch 22, Batch 185, LR 1.769926 Loss 5.098842, Accuracy 89.105%\n",
      "Epoch 22, Batch 186, LR 1.769804 Loss 5.099544, Accuracy 89.088%\n",
      "Epoch 22, Batch 187, LR 1.769682 Loss 5.101196, Accuracy 89.088%\n",
      "Epoch 22, Batch 188, LR 1.769560 Loss 5.102462, Accuracy 89.083%\n",
      "Epoch 22, Batch 189, LR 1.769439 Loss 5.101095, Accuracy 89.071%\n",
      "Epoch 22, Batch 190, LR 1.769317 Loss 5.102206, Accuracy 89.050%\n",
      "Epoch 22, Batch 191, LR 1.769195 Loss 5.102600, Accuracy 89.042%\n",
      "Epoch 22, Batch 192, LR 1.769073 Loss 5.106058, Accuracy 89.022%\n",
      "Epoch 22, Batch 193, LR 1.768951 Loss 5.101387, Accuracy 89.034%\n",
      "Epoch 22, Batch 194, LR 1.768829 Loss 5.105359, Accuracy 89.006%\n",
      "Epoch 22, Batch 195, LR 1.768707 Loss 5.107039, Accuracy 89.002%\n",
      "Epoch 22, Batch 196, LR 1.768586 Loss 5.107661, Accuracy 89.007%\n",
      "Epoch 22, Batch 197, LR 1.768464 Loss 5.104981, Accuracy 89.035%\n",
      "Epoch 22, Batch 198, LR 1.768342 Loss 5.105097, Accuracy 89.035%\n",
      "Epoch 22, Batch 199, LR 1.768220 Loss 5.108709, Accuracy 89.019%\n",
      "Epoch 22, Batch 200, LR 1.768098 Loss 5.106033, Accuracy 89.031%\n",
      "Epoch 22, Batch 201, LR 1.767976 Loss 5.104034, Accuracy 89.047%\n",
      "Epoch 22, Batch 202, LR 1.767854 Loss 5.103212, Accuracy 89.051%\n",
      "Epoch 22, Batch 203, LR 1.767732 Loss 5.103948, Accuracy 89.047%\n",
      "Epoch 22, Batch 204, LR 1.767610 Loss 5.105615, Accuracy 89.036%\n",
      "Epoch 22, Batch 205, LR 1.767488 Loss 5.100218, Accuracy 89.051%\n",
      "Epoch 22, Batch 206, LR 1.767366 Loss 5.100291, Accuracy 89.059%\n",
      "Epoch 22, Batch 207, LR 1.767244 Loss 5.099965, Accuracy 89.062%\n",
      "Epoch 22, Batch 208, LR 1.767123 Loss 5.096713, Accuracy 89.070%\n",
      "Epoch 22, Batch 209, LR 1.767001 Loss 5.098419, Accuracy 89.059%\n",
      "Epoch 22, Batch 210, LR 1.766879 Loss 5.099092, Accuracy 89.062%\n",
      "Epoch 22, Batch 211, LR 1.766757 Loss 5.097236, Accuracy 89.074%\n",
      "Epoch 22, Batch 212, LR 1.766635 Loss 5.099581, Accuracy 89.055%\n",
      "Epoch 22, Batch 213, LR 1.766513 Loss 5.102741, Accuracy 89.033%\n",
      "Epoch 22, Batch 214, LR 1.766391 Loss 5.101878, Accuracy 89.026%\n",
      "Epoch 22, Batch 215, LR 1.766269 Loss 5.101807, Accuracy 89.030%\n",
      "Epoch 22, Batch 216, LR 1.766147 Loss 5.104600, Accuracy 89.005%\n",
      "Epoch 22, Batch 217, LR 1.766025 Loss 5.102375, Accuracy 89.001%\n",
      "Epoch 22, Batch 218, LR 1.765903 Loss 5.102508, Accuracy 88.998%\n",
      "Epoch 22, Batch 219, LR 1.765781 Loss 5.103675, Accuracy 88.977%\n",
      "Epoch 22, Batch 220, LR 1.765659 Loss 5.106427, Accuracy 88.952%\n",
      "Epoch 22, Batch 221, LR 1.765537 Loss 5.105680, Accuracy 88.939%\n",
      "Epoch 22, Batch 222, LR 1.765415 Loss 5.108019, Accuracy 88.918%\n",
      "Epoch 22, Batch 223, LR 1.765293 Loss 5.108203, Accuracy 88.922%\n",
      "Epoch 22, Batch 224, LR 1.765171 Loss 5.111405, Accuracy 88.909%\n",
      "Epoch 22, Batch 225, LR 1.765048 Loss 5.113095, Accuracy 88.903%\n",
      "Epoch 22, Batch 226, LR 1.764926 Loss 5.114740, Accuracy 88.890%\n",
      "Epoch 22, Batch 227, LR 1.764804 Loss 5.111837, Accuracy 88.901%\n",
      "Epoch 22, Batch 228, LR 1.764682 Loss 5.114125, Accuracy 88.888%\n",
      "Epoch 22, Batch 229, LR 1.764560 Loss 5.113693, Accuracy 88.882%\n",
      "Epoch 22, Batch 230, LR 1.764438 Loss 5.115438, Accuracy 88.869%\n",
      "Epoch 22, Batch 231, LR 1.764316 Loss 5.113200, Accuracy 88.887%\n",
      "Epoch 22, Batch 232, LR 1.764194 Loss 5.114564, Accuracy 88.867%\n",
      "Epoch 22, Batch 233, LR 1.764072 Loss 5.111850, Accuracy 88.881%\n",
      "Epoch 22, Batch 234, LR 1.763950 Loss 5.111366, Accuracy 88.879%\n",
      "Epoch 22, Batch 235, LR 1.763828 Loss 5.110523, Accuracy 88.880%\n",
      "Epoch 22, Batch 236, LR 1.763706 Loss 5.110629, Accuracy 88.877%\n",
      "Epoch 22, Batch 237, LR 1.763583 Loss 5.107408, Accuracy 88.891%\n",
      "Epoch 22, Batch 238, LR 1.763461 Loss 5.114061, Accuracy 88.836%\n",
      "Epoch 22, Batch 239, LR 1.763339 Loss 5.113754, Accuracy 88.824%\n",
      "Epoch 22, Batch 240, LR 1.763217 Loss 5.111963, Accuracy 88.838%\n",
      "Epoch 22, Batch 241, LR 1.763095 Loss 5.114367, Accuracy 88.826%\n",
      "Epoch 22, Batch 242, LR 1.762973 Loss 5.118424, Accuracy 88.811%\n",
      "Epoch 22, Batch 243, LR 1.762851 Loss 5.116268, Accuracy 88.818%\n",
      "Epoch 22, Batch 244, LR 1.762728 Loss 5.118483, Accuracy 88.810%\n",
      "Epoch 22, Batch 245, LR 1.762606 Loss 5.118680, Accuracy 88.817%\n",
      "Epoch 22, Batch 246, LR 1.762484 Loss 5.118187, Accuracy 88.818%\n",
      "Epoch 22, Batch 247, LR 1.762362 Loss 5.118255, Accuracy 88.822%\n",
      "Epoch 22, Batch 248, LR 1.762240 Loss 5.118511, Accuracy 88.823%\n",
      "Epoch 22, Batch 249, LR 1.762117 Loss 5.116958, Accuracy 88.830%\n",
      "Epoch 22, Batch 250, LR 1.761995 Loss 5.117178, Accuracy 88.831%\n",
      "Epoch 22, Batch 251, LR 1.761873 Loss 5.115675, Accuracy 88.854%\n",
      "Epoch 22, Batch 252, LR 1.761751 Loss 5.116302, Accuracy 88.852%\n",
      "Epoch 22, Batch 253, LR 1.761629 Loss 5.117463, Accuracy 88.846%\n",
      "Epoch 22, Batch 254, LR 1.761506 Loss 5.116025, Accuracy 88.841%\n",
      "Epoch 22, Batch 255, LR 1.761384 Loss 5.117253, Accuracy 88.848%\n",
      "Epoch 22, Batch 256, LR 1.761262 Loss 5.117035, Accuracy 88.843%\n",
      "Epoch 22, Batch 257, LR 1.761140 Loss 5.114252, Accuracy 88.853%\n",
      "Epoch 22, Batch 258, LR 1.761017 Loss 5.115622, Accuracy 88.844%\n",
      "Epoch 22, Batch 259, LR 1.760895 Loss 5.115032, Accuracy 88.833%\n",
      "Epoch 22, Batch 260, LR 1.760773 Loss 5.113695, Accuracy 88.828%\n",
      "Epoch 22, Batch 261, LR 1.760651 Loss 5.115521, Accuracy 88.832%\n",
      "Epoch 22, Batch 262, LR 1.760528 Loss 5.115860, Accuracy 88.839%\n",
      "Epoch 22, Batch 263, LR 1.760406 Loss 5.117172, Accuracy 88.837%\n",
      "Epoch 22, Batch 264, LR 1.760284 Loss 5.116918, Accuracy 88.838%\n",
      "Epoch 22, Batch 265, LR 1.760162 Loss 5.118732, Accuracy 88.835%\n",
      "Epoch 22, Batch 266, LR 1.760039 Loss 5.118733, Accuracy 88.828%\n",
      "Epoch 22, Batch 267, LR 1.759917 Loss 5.117376, Accuracy 88.831%\n",
      "Epoch 22, Batch 268, LR 1.759795 Loss 5.116362, Accuracy 88.832%\n",
      "Epoch 22, Batch 269, LR 1.759672 Loss 5.116271, Accuracy 88.827%\n",
      "Epoch 22, Batch 270, LR 1.759550 Loss 5.116869, Accuracy 88.822%\n",
      "Epoch 22, Batch 271, LR 1.759428 Loss 5.117651, Accuracy 88.812%\n",
      "Epoch 22, Batch 272, LR 1.759305 Loss 5.119318, Accuracy 88.813%\n",
      "Epoch 22, Batch 273, LR 1.759183 Loss 5.120688, Accuracy 88.814%\n",
      "Epoch 22, Batch 274, LR 1.759061 Loss 5.119273, Accuracy 88.829%\n",
      "Epoch 22, Batch 275, LR 1.758938 Loss 5.119583, Accuracy 88.824%\n",
      "Epoch 22, Batch 276, LR 1.758816 Loss 5.120569, Accuracy 88.813%\n",
      "Epoch 22, Batch 277, LR 1.758694 Loss 5.122643, Accuracy 88.800%\n",
      "Epoch 22, Batch 278, LR 1.758571 Loss 5.124533, Accuracy 88.801%\n",
      "Epoch 22, Batch 279, LR 1.758449 Loss 5.124949, Accuracy 88.796%\n",
      "Epoch 22, Batch 280, LR 1.758327 Loss 5.123227, Accuracy 88.806%\n",
      "Epoch 22, Batch 281, LR 1.758204 Loss 5.124211, Accuracy 88.787%\n",
      "Epoch 22, Batch 282, LR 1.758082 Loss 5.124398, Accuracy 88.783%\n",
      "Epoch 22, Batch 283, LR 1.757959 Loss 5.125005, Accuracy 88.778%\n",
      "Epoch 22, Batch 284, LR 1.757837 Loss 5.123206, Accuracy 88.774%\n",
      "Epoch 22, Batch 285, LR 1.757715 Loss 5.124346, Accuracy 88.758%\n",
      "Epoch 22, Batch 286, LR 1.757592 Loss 5.125294, Accuracy 88.751%\n",
      "Epoch 22, Batch 287, LR 1.757470 Loss 5.124466, Accuracy 88.760%\n",
      "Epoch 22, Batch 288, LR 1.757347 Loss 5.123923, Accuracy 88.759%\n",
      "Epoch 22, Batch 289, LR 1.757225 Loss 5.122612, Accuracy 88.765%\n",
      "Epoch 22, Batch 290, LR 1.757103 Loss 5.122524, Accuracy 88.774%\n",
      "Epoch 22, Batch 291, LR 1.756980 Loss 5.122060, Accuracy 88.778%\n",
      "Epoch 22, Batch 292, LR 1.756858 Loss 5.122609, Accuracy 88.771%\n",
      "Epoch 22, Batch 293, LR 1.756735 Loss 5.121836, Accuracy 88.772%\n",
      "Epoch 22, Batch 294, LR 1.756613 Loss 5.122145, Accuracy 88.754%\n",
      "Epoch 22, Batch 295, LR 1.756490 Loss 5.123209, Accuracy 88.742%\n",
      "Epoch 22, Batch 296, LR 1.756368 Loss 5.121770, Accuracy 88.743%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 297, LR 1.756245 Loss 5.120692, Accuracy 88.744%\n",
      "Epoch 22, Batch 298, LR 1.756123 Loss 5.120115, Accuracy 88.743%\n",
      "Epoch 22, Batch 299, LR 1.756000 Loss 5.120586, Accuracy 88.733%\n",
      "Epoch 22, Batch 300, LR 1.755878 Loss 5.120540, Accuracy 88.732%\n",
      "Epoch 22, Batch 301, LR 1.755755 Loss 5.119413, Accuracy 88.725%\n",
      "Epoch 22, Batch 302, LR 1.755633 Loss 5.117957, Accuracy 88.726%\n",
      "Epoch 22, Batch 303, LR 1.755510 Loss 5.117636, Accuracy 88.725%\n",
      "Epoch 22, Batch 304, LR 1.755388 Loss 5.114678, Accuracy 88.734%\n",
      "Epoch 22, Batch 305, LR 1.755265 Loss 5.113823, Accuracy 88.742%\n",
      "Epoch 22, Batch 306, LR 1.755143 Loss 5.112299, Accuracy 88.738%\n",
      "Epoch 22, Batch 307, LR 1.755020 Loss 5.110501, Accuracy 88.762%\n",
      "Epoch 22, Batch 308, LR 1.754898 Loss 5.109647, Accuracy 88.753%\n",
      "Epoch 22, Batch 309, LR 1.754775 Loss 5.108059, Accuracy 88.752%\n",
      "Epoch 22, Batch 310, LR 1.754653 Loss 5.108893, Accuracy 88.745%\n",
      "Epoch 22, Batch 311, LR 1.754530 Loss 5.108618, Accuracy 88.754%\n",
      "Epoch 22, Batch 312, LR 1.754408 Loss 5.107668, Accuracy 88.755%\n",
      "Epoch 22, Batch 313, LR 1.754285 Loss 5.106925, Accuracy 88.758%\n",
      "Epoch 22, Batch 314, LR 1.754162 Loss 5.105609, Accuracy 88.766%\n",
      "Epoch 22, Batch 315, LR 1.754040 Loss 5.105908, Accuracy 88.765%\n",
      "Epoch 22, Batch 316, LR 1.753917 Loss 5.104463, Accuracy 88.766%\n",
      "Epoch 22, Batch 317, LR 1.753795 Loss 5.105046, Accuracy 88.764%\n",
      "Epoch 22, Batch 318, LR 1.753672 Loss 5.103900, Accuracy 88.765%\n",
      "Epoch 22, Batch 319, LR 1.753549 Loss 5.101659, Accuracy 88.771%\n",
      "Epoch 22, Batch 320, LR 1.753427 Loss 5.100134, Accuracy 88.765%\n",
      "Epoch 22, Batch 321, LR 1.753304 Loss 5.099143, Accuracy 88.766%\n",
      "Epoch 22, Batch 322, LR 1.753182 Loss 5.099686, Accuracy 88.759%\n",
      "Epoch 22, Batch 323, LR 1.753059 Loss 5.101489, Accuracy 88.750%\n",
      "Epoch 22, Batch 324, LR 1.752936 Loss 5.102748, Accuracy 88.747%\n",
      "Epoch 22, Batch 325, LR 1.752814 Loss 5.103438, Accuracy 88.748%\n",
      "Epoch 22, Batch 326, LR 1.752691 Loss 5.104723, Accuracy 88.732%\n",
      "Epoch 22, Batch 327, LR 1.752568 Loss 5.106430, Accuracy 88.714%\n",
      "Epoch 22, Batch 328, LR 1.752446 Loss 5.105967, Accuracy 88.717%\n",
      "Epoch 22, Batch 329, LR 1.752323 Loss 5.108133, Accuracy 88.692%\n",
      "Epoch 22, Batch 330, LR 1.752200 Loss 5.107231, Accuracy 88.703%\n",
      "Epoch 22, Batch 331, LR 1.752078 Loss 5.104938, Accuracy 88.708%\n",
      "Epoch 22, Batch 332, LR 1.751955 Loss 5.106699, Accuracy 88.705%\n",
      "Epoch 22, Batch 333, LR 1.751832 Loss 5.106350, Accuracy 88.699%\n",
      "Epoch 22, Batch 334, LR 1.751710 Loss 5.102624, Accuracy 88.716%\n",
      "Epoch 22, Batch 335, LR 1.751587 Loss 5.104576, Accuracy 88.710%\n",
      "Epoch 22, Batch 336, LR 1.751464 Loss 5.103065, Accuracy 88.730%\n",
      "Epoch 22, Batch 337, LR 1.751342 Loss 5.103018, Accuracy 88.745%\n",
      "Epoch 22, Batch 338, LR 1.751219 Loss 5.103187, Accuracy 88.737%\n",
      "Epoch 22, Batch 339, LR 1.751096 Loss 5.105131, Accuracy 88.719%\n",
      "Epoch 22, Batch 340, LR 1.750974 Loss 5.105213, Accuracy 88.711%\n",
      "Epoch 22, Batch 341, LR 1.750851 Loss 5.104546, Accuracy 88.719%\n",
      "Epoch 22, Batch 342, LR 1.750728 Loss 5.104342, Accuracy 88.727%\n",
      "Epoch 22, Batch 343, LR 1.750605 Loss 5.104294, Accuracy 88.714%\n",
      "Epoch 22, Batch 344, LR 1.750483 Loss 5.105264, Accuracy 88.713%\n",
      "Epoch 22, Batch 345, LR 1.750360 Loss 5.104621, Accuracy 88.714%\n",
      "Epoch 22, Batch 346, LR 1.750237 Loss 5.105381, Accuracy 88.710%\n",
      "Epoch 22, Batch 347, LR 1.750114 Loss 5.104379, Accuracy 88.723%\n",
      "Epoch 22, Batch 348, LR 1.749992 Loss 5.104104, Accuracy 88.721%\n",
      "Epoch 22, Batch 349, LR 1.749869 Loss 5.103192, Accuracy 88.720%\n",
      "Epoch 22, Batch 350, LR 1.749746 Loss 5.104733, Accuracy 88.723%\n",
      "Epoch 22, Batch 351, LR 1.749623 Loss 5.104643, Accuracy 88.724%\n",
      "Epoch 22, Batch 352, LR 1.749500 Loss 5.105000, Accuracy 88.725%\n",
      "Epoch 22, Batch 353, LR 1.749378 Loss 5.106890, Accuracy 88.719%\n",
      "Epoch 22, Batch 354, LR 1.749255 Loss 5.106128, Accuracy 88.718%\n",
      "Epoch 22, Batch 355, LR 1.749132 Loss 5.106490, Accuracy 88.721%\n",
      "Epoch 22, Batch 356, LR 1.749009 Loss 5.106810, Accuracy 88.731%\n",
      "Epoch 22, Batch 357, LR 1.748886 Loss 5.106926, Accuracy 88.730%\n",
      "Epoch 22, Batch 358, LR 1.748764 Loss 5.106770, Accuracy 88.735%\n",
      "Epoch 22, Batch 359, LR 1.748641 Loss 5.105789, Accuracy 88.734%\n",
      "Epoch 22, Batch 360, LR 1.748518 Loss 5.104855, Accuracy 88.739%\n",
      "Epoch 22, Batch 361, LR 1.748395 Loss 5.105114, Accuracy 88.738%\n",
      "Epoch 22, Batch 362, LR 1.748272 Loss 5.103563, Accuracy 88.741%\n",
      "Epoch 22, Batch 363, LR 1.748149 Loss 5.106682, Accuracy 88.727%\n",
      "Epoch 22, Batch 364, LR 1.748026 Loss 5.107874, Accuracy 88.717%\n",
      "Epoch 22, Batch 365, LR 1.747904 Loss 5.107432, Accuracy 88.731%\n",
      "Epoch 22, Batch 366, LR 1.747781 Loss 5.108017, Accuracy 88.727%\n",
      "Epoch 22, Batch 367, LR 1.747658 Loss 5.107484, Accuracy 88.733%\n",
      "Epoch 22, Batch 368, LR 1.747535 Loss 5.107231, Accuracy 88.742%\n",
      "Epoch 22, Batch 369, LR 1.747412 Loss 5.109797, Accuracy 88.728%\n",
      "Epoch 22, Batch 370, LR 1.747289 Loss 5.108315, Accuracy 88.733%\n",
      "Epoch 22, Batch 371, LR 1.747166 Loss 5.107612, Accuracy 88.740%\n",
      "Epoch 22, Batch 372, LR 1.747043 Loss 5.107699, Accuracy 88.733%\n",
      "Epoch 22, Batch 373, LR 1.746920 Loss 5.107192, Accuracy 88.744%\n",
      "Epoch 22, Batch 374, LR 1.746798 Loss 5.106285, Accuracy 88.741%\n",
      "Epoch 22, Batch 375, LR 1.746675 Loss 5.106696, Accuracy 88.746%\n",
      "Epoch 22, Batch 376, LR 1.746552 Loss 5.106599, Accuracy 88.751%\n",
      "Epoch 22, Batch 377, LR 1.746429 Loss 5.105920, Accuracy 88.756%\n",
      "Epoch 22, Batch 378, LR 1.746306 Loss 5.105130, Accuracy 88.765%\n",
      "Epoch 22, Batch 379, LR 1.746183 Loss 5.105632, Accuracy 88.757%\n",
      "Epoch 22, Batch 380, LR 1.746060 Loss 5.104452, Accuracy 88.760%\n",
      "Epoch 22, Batch 381, LR 1.745937 Loss 5.101820, Accuracy 88.769%\n",
      "Epoch 22, Batch 382, LR 1.745814 Loss 5.101727, Accuracy 88.764%\n",
      "Epoch 22, Batch 383, LR 1.745691 Loss 5.102225, Accuracy 88.754%\n",
      "Epoch 22, Batch 384, LR 1.745568 Loss 5.100524, Accuracy 88.761%\n",
      "Epoch 22, Batch 385, LR 1.745445 Loss 5.101920, Accuracy 88.760%\n",
      "Epoch 22, Batch 386, LR 1.745322 Loss 5.101191, Accuracy 88.759%\n",
      "Epoch 22, Batch 387, LR 1.745199 Loss 5.102089, Accuracy 88.750%\n",
      "Epoch 22, Batch 388, LR 1.745076 Loss 5.102814, Accuracy 88.744%\n",
      "Epoch 22, Batch 389, LR 1.744953 Loss 5.104184, Accuracy 88.747%\n",
      "Epoch 22, Batch 390, LR 1.744830 Loss 5.104035, Accuracy 88.748%\n",
      "Epoch 22, Batch 391, LR 1.744707 Loss 5.103081, Accuracy 88.757%\n",
      "Epoch 22, Batch 392, LR 1.744584 Loss 5.101234, Accuracy 88.764%\n",
      "Epoch 22, Batch 393, LR 1.744461 Loss 5.101906, Accuracy 88.758%\n",
      "Epoch 22, Batch 394, LR 1.744338 Loss 5.102868, Accuracy 88.763%\n",
      "Epoch 22, Batch 395, LR 1.744215 Loss 5.101983, Accuracy 88.766%\n",
      "Epoch 22, Batch 396, LR 1.744092 Loss 5.102252, Accuracy 88.769%\n",
      "Epoch 22, Batch 397, LR 1.743969 Loss 5.102118, Accuracy 88.765%\n",
      "Epoch 22, Batch 398, LR 1.743846 Loss 5.101099, Accuracy 88.776%\n",
      "Epoch 22, Batch 399, LR 1.743723 Loss 5.103033, Accuracy 88.777%\n",
      "Epoch 22, Batch 400, LR 1.743600 Loss 5.103756, Accuracy 88.779%\n",
      "Epoch 22, Batch 401, LR 1.743477 Loss 5.103046, Accuracy 88.786%\n",
      "Epoch 22, Batch 402, LR 1.743354 Loss 5.100534, Accuracy 88.794%\n",
      "Epoch 22, Batch 403, LR 1.743231 Loss 5.101930, Accuracy 88.791%\n",
      "Epoch 22, Batch 404, LR 1.743107 Loss 5.101232, Accuracy 88.801%\n",
      "Epoch 22, Batch 405, LR 1.742984 Loss 5.101404, Accuracy 88.791%\n",
      "Epoch 22, Batch 406, LR 1.742861 Loss 5.101741, Accuracy 88.787%\n",
      "Epoch 22, Batch 407, LR 1.742738 Loss 5.103596, Accuracy 88.771%\n",
      "Epoch 22, Batch 408, LR 1.742615 Loss 5.102841, Accuracy 88.779%\n",
      "Epoch 22, Batch 409, LR 1.742492 Loss 5.102183, Accuracy 88.784%\n",
      "Epoch 22, Batch 410, LR 1.742369 Loss 5.102915, Accuracy 88.779%\n",
      "Epoch 22, Batch 411, LR 1.742246 Loss 5.104962, Accuracy 88.779%\n",
      "Epoch 22, Batch 412, LR 1.742123 Loss 5.103910, Accuracy 88.786%\n",
      "Epoch 22, Batch 413, LR 1.741999 Loss 5.103763, Accuracy 88.784%\n",
      "Epoch 22, Batch 414, LR 1.741876 Loss 5.105730, Accuracy 88.776%\n",
      "Epoch 22, Batch 415, LR 1.741753 Loss 5.105209, Accuracy 88.778%\n",
      "Epoch 22, Batch 416, LR 1.741630 Loss 5.104529, Accuracy 88.783%\n",
      "Epoch 22, Batch 417, LR 1.741507 Loss 5.103631, Accuracy 88.785%\n",
      "Epoch 22, Batch 418, LR 1.741384 Loss 5.104106, Accuracy 88.790%\n",
      "Epoch 22, Batch 419, LR 1.741260 Loss 5.106311, Accuracy 88.785%\n",
      "Epoch 22, Batch 420, LR 1.741137 Loss 5.105935, Accuracy 88.780%\n",
      "Epoch 22, Batch 421, LR 1.741014 Loss 5.106794, Accuracy 88.771%\n",
      "Epoch 22, Batch 422, LR 1.740891 Loss 5.105676, Accuracy 88.779%\n",
      "Epoch 22, Batch 423, LR 1.740768 Loss 5.104665, Accuracy 88.782%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 424, LR 1.740645 Loss 5.104007, Accuracy 88.790%\n",
      "Epoch 22, Batch 425, LR 1.740521 Loss 5.104572, Accuracy 88.778%\n",
      "Epoch 22, Batch 426, LR 1.740398 Loss 5.107085, Accuracy 88.765%\n",
      "Epoch 22, Batch 427, LR 1.740275 Loss 5.106577, Accuracy 88.772%\n",
      "Epoch 22, Batch 428, LR 1.740152 Loss 5.105614, Accuracy 88.778%\n",
      "Epoch 22, Batch 429, LR 1.740028 Loss 5.104541, Accuracy 88.786%\n",
      "Epoch 22, Batch 430, LR 1.739905 Loss 5.104999, Accuracy 88.779%\n",
      "Epoch 22, Batch 431, LR 1.739782 Loss 5.104163, Accuracy 88.782%\n",
      "Epoch 22, Batch 432, LR 1.739659 Loss 5.106700, Accuracy 88.779%\n",
      "Epoch 22, Batch 433, LR 1.739535 Loss 5.107395, Accuracy 88.779%\n",
      "Epoch 22, Batch 434, LR 1.739412 Loss 5.107813, Accuracy 88.776%\n",
      "Epoch 22, Batch 435, LR 1.739289 Loss 5.107911, Accuracy 88.772%\n",
      "Epoch 22, Batch 436, LR 1.739166 Loss 5.108937, Accuracy 88.776%\n",
      "Epoch 22, Batch 437, LR 1.739042 Loss 5.110141, Accuracy 88.769%\n",
      "Epoch 22, Batch 438, LR 1.738919 Loss 5.112106, Accuracy 88.763%\n",
      "Epoch 22, Batch 439, LR 1.738796 Loss 5.112060, Accuracy 88.765%\n",
      "Epoch 22, Batch 440, LR 1.738673 Loss 5.112857, Accuracy 88.766%\n",
      "Epoch 22, Batch 441, LR 1.738549 Loss 5.114556, Accuracy 88.756%\n",
      "Epoch 22, Batch 442, LR 1.738426 Loss 5.115617, Accuracy 88.751%\n",
      "Epoch 22, Batch 443, LR 1.738303 Loss 5.117830, Accuracy 88.743%\n",
      "Epoch 22, Batch 444, LR 1.738179 Loss 5.117251, Accuracy 88.749%\n",
      "Epoch 22, Batch 445, LR 1.738056 Loss 5.117445, Accuracy 88.754%\n",
      "Epoch 22, Batch 446, LR 1.737933 Loss 5.116474, Accuracy 88.758%\n",
      "Epoch 22, Batch 447, LR 1.737809 Loss 5.117323, Accuracy 88.755%\n",
      "Epoch 22, Batch 448, LR 1.737686 Loss 5.118035, Accuracy 88.756%\n",
      "Epoch 22, Batch 449, LR 1.737563 Loss 5.117134, Accuracy 88.760%\n",
      "Epoch 22, Batch 450, LR 1.737439 Loss 5.117164, Accuracy 88.760%\n",
      "Epoch 22, Batch 451, LR 1.737316 Loss 5.115933, Accuracy 88.770%\n",
      "Epoch 22, Batch 452, LR 1.737193 Loss 5.116732, Accuracy 88.760%\n",
      "Epoch 22, Batch 453, LR 1.737069 Loss 5.117152, Accuracy 88.759%\n",
      "Epoch 22, Batch 454, LR 1.736946 Loss 5.117226, Accuracy 88.751%\n",
      "Epoch 22, Batch 455, LR 1.736823 Loss 5.118001, Accuracy 88.747%\n",
      "Epoch 22, Batch 456, LR 1.736699 Loss 5.118218, Accuracy 88.742%\n",
      "Epoch 22, Batch 457, LR 1.736576 Loss 5.116339, Accuracy 88.748%\n",
      "Epoch 22, Batch 458, LR 1.736452 Loss 5.117104, Accuracy 88.744%\n",
      "Epoch 22, Batch 459, LR 1.736329 Loss 5.116062, Accuracy 88.751%\n",
      "Epoch 22, Batch 460, LR 1.736206 Loss 5.116574, Accuracy 88.757%\n",
      "Epoch 22, Batch 461, LR 1.736082 Loss 5.116147, Accuracy 88.761%\n",
      "Epoch 22, Batch 462, LR 1.735959 Loss 5.115228, Accuracy 88.763%\n",
      "Epoch 22, Batch 463, LR 1.735835 Loss 5.114832, Accuracy 88.759%\n",
      "Epoch 22, Batch 464, LR 1.735712 Loss 5.114944, Accuracy 88.764%\n",
      "Epoch 22, Batch 465, LR 1.735588 Loss 5.115116, Accuracy 88.765%\n",
      "Epoch 22, Batch 466, LR 1.735465 Loss 5.116007, Accuracy 88.757%\n",
      "Epoch 22, Batch 467, LR 1.735342 Loss 5.116639, Accuracy 88.758%\n",
      "Epoch 22, Batch 468, LR 1.735218 Loss 5.115848, Accuracy 88.769%\n",
      "Epoch 22, Batch 469, LR 1.735095 Loss 5.117131, Accuracy 88.759%\n",
      "Epoch 22, Batch 470, LR 1.734971 Loss 5.118332, Accuracy 88.757%\n",
      "Epoch 22, Batch 471, LR 1.734848 Loss 5.118675, Accuracy 88.756%\n",
      "Epoch 22, Batch 472, LR 1.734724 Loss 5.119168, Accuracy 88.758%\n",
      "Epoch 22, Batch 473, LR 1.734601 Loss 5.119668, Accuracy 88.755%\n",
      "Epoch 22, Batch 474, LR 1.734477 Loss 5.119571, Accuracy 88.754%\n",
      "Epoch 22, Batch 475, LR 1.734354 Loss 5.120582, Accuracy 88.747%\n",
      "Epoch 22, Batch 476, LR 1.734230 Loss 5.121593, Accuracy 88.742%\n",
      "Epoch 22, Batch 477, LR 1.734107 Loss 5.120507, Accuracy 88.750%\n",
      "Epoch 22, Batch 478, LR 1.733983 Loss 5.120389, Accuracy 88.755%\n",
      "Epoch 22, Batch 479, LR 1.733860 Loss 5.120256, Accuracy 88.756%\n",
      "Epoch 22, Batch 480, LR 1.733736 Loss 5.121086, Accuracy 88.757%\n",
      "Epoch 22, Batch 481, LR 1.733613 Loss 5.120205, Accuracy 88.762%\n",
      "Epoch 22, Batch 482, LR 1.733489 Loss 5.120859, Accuracy 88.756%\n",
      "Epoch 22, Batch 483, LR 1.733366 Loss 5.121379, Accuracy 88.747%\n",
      "Epoch 22, Batch 484, LR 1.733242 Loss 5.120793, Accuracy 88.756%\n",
      "Epoch 22, Batch 485, LR 1.733119 Loss 5.121345, Accuracy 88.747%\n",
      "Epoch 22, Batch 486, LR 1.732995 Loss 5.120571, Accuracy 88.754%\n",
      "Epoch 22, Batch 487, LR 1.732872 Loss 5.121099, Accuracy 88.748%\n",
      "Epoch 22, Batch 488, LR 1.732748 Loss 5.120293, Accuracy 88.747%\n",
      "Epoch 22, Batch 489, LR 1.732624 Loss 5.120645, Accuracy 88.741%\n",
      "Epoch 22, Batch 490, LR 1.732501 Loss 5.120364, Accuracy 88.737%\n",
      "Epoch 22, Batch 491, LR 1.732377 Loss 5.120321, Accuracy 88.736%\n",
      "Epoch 22, Batch 492, LR 1.732254 Loss 5.120006, Accuracy 88.740%\n",
      "Epoch 22, Batch 493, LR 1.732130 Loss 5.120434, Accuracy 88.741%\n",
      "Epoch 22, Batch 494, LR 1.732007 Loss 5.119587, Accuracy 88.743%\n",
      "Epoch 22, Batch 495, LR 1.731883 Loss 5.117133, Accuracy 88.756%\n",
      "Epoch 22, Batch 496, LR 1.731759 Loss 5.116537, Accuracy 88.765%\n",
      "Epoch 22, Batch 497, LR 1.731636 Loss 5.116198, Accuracy 88.767%\n",
      "Epoch 22, Batch 498, LR 1.731512 Loss 5.115988, Accuracy 88.768%\n",
      "Epoch 22, Batch 499, LR 1.731389 Loss 5.116399, Accuracy 88.771%\n",
      "Epoch 22, Batch 500, LR 1.731265 Loss 5.116832, Accuracy 88.764%\n",
      "Epoch 22, Batch 501, LR 1.731141 Loss 5.115437, Accuracy 88.777%\n",
      "Epoch 22, Batch 502, LR 1.731018 Loss 5.115185, Accuracy 88.775%\n",
      "Epoch 22, Batch 503, LR 1.730894 Loss 5.115500, Accuracy 88.772%\n",
      "Epoch 22, Batch 504, LR 1.730770 Loss 5.114712, Accuracy 88.776%\n",
      "Epoch 22, Batch 505, LR 1.730647 Loss 5.114899, Accuracy 88.776%\n",
      "Epoch 22, Batch 506, LR 1.730523 Loss 5.114640, Accuracy 88.774%\n",
      "Epoch 22, Batch 507, LR 1.730399 Loss 5.114872, Accuracy 88.767%\n",
      "Epoch 22, Batch 508, LR 1.730276 Loss 5.115566, Accuracy 88.769%\n",
      "Epoch 22, Batch 509, LR 1.730152 Loss 5.114865, Accuracy 88.771%\n",
      "Epoch 22, Batch 510, LR 1.730028 Loss 5.113716, Accuracy 88.781%\n",
      "Epoch 22, Batch 511, LR 1.729905 Loss 5.113025, Accuracy 88.784%\n",
      "Epoch 22, Batch 512, LR 1.729781 Loss 5.112001, Accuracy 88.785%\n",
      "Epoch 22, Batch 513, LR 1.729657 Loss 5.112172, Accuracy 88.790%\n",
      "Epoch 22, Batch 514, LR 1.729534 Loss 5.110979, Accuracy 88.798%\n",
      "Epoch 22, Batch 515, LR 1.729410 Loss 5.111266, Accuracy 88.796%\n",
      "Epoch 22, Batch 516, LR 1.729286 Loss 5.112613, Accuracy 88.785%\n",
      "Epoch 22, Batch 517, LR 1.729162 Loss 5.113040, Accuracy 88.781%\n",
      "Epoch 22, Batch 518, LR 1.729039 Loss 5.111556, Accuracy 88.785%\n",
      "Epoch 22, Batch 519, LR 1.728915 Loss 5.111775, Accuracy 88.789%\n",
      "Epoch 22, Batch 520, LR 1.728791 Loss 5.110762, Accuracy 88.795%\n",
      "Epoch 22, Batch 521, LR 1.728668 Loss 5.110629, Accuracy 88.790%\n",
      "Epoch 22, Batch 522, LR 1.728544 Loss 5.110859, Accuracy 88.789%\n",
      "Epoch 22, Batch 523, LR 1.728420 Loss 5.109213, Accuracy 88.792%\n",
      "Epoch 22, Batch 524, LR 1.728296 Loss 5.109052, Accuracy 88.794%\n",
      "Epoch 22, Batch 525, LR 1.728173 Loss 5.108409, Accuracy 88.793%\n",
      "Epoch 22, Batch 526, LR 1.728049 Loss 5.109649, Accuracy 88.777%\n",
      "Epoch 22, Batch 527, LR 1.727925 Loss 5.110192, Accuracy 88.776%\n",
      "Epoch 22, Batch 528, LR 1.727801 Loss 5.111978, Accuracy 88.768%\n",
      "Epoch 22, Batch 529, LR 1.727677 Loss 5.112123, Accuracy 88.761%\n",
      "Epoch 22, Batch 530, LR 1.727554 Loss 5.112362, Accuracy 88.762%\n",
      "Epoch 22, Batch 531, LR 1.727430 Loss 5.112631, Accuracy 88.761%\n",
      "Epoch 22, Batch 532, LR 1.727306 Loss 5.111423, Accuracy 88.764%\n",
      "Epoch 22, Batch 533, LR 1.727182 Loss 5.110602, Accuracy 88.766%\n",
      "Epoch 22, Batch 534, LR 1.727058 Loss 5.110025, Accuracy 88.774%\n",
      "Epoch 22, Batch 535, LR 1.726935 Loss 5.109433, Accuracy 88.773%\n",
      "Epoch 22, Batch 536, LR 1.726811 Loss 5.109838, Accuracy 88.770%\n",
      "Epoch 22, Batch 537, LR 1.726687 Loss 5.108787, Accuracy 88.776%\n",
      "Epoch 22, Batch 538, LR 1.726563 Loss 5.108094, Accuracy 88.779%\n",
      "Epoch 22, Batch 539, LR 1.726439 Loss 5.109835, Accuracy 88.776%\n",
      "Epoch 22, Batch 540, LR 1.726315 Loss 5.110899, Accuracy 88.766%\n",
      "Epoch 22, Batch 541, LR 1.726192 Loss 5.111481, Accuracy 88.761%\n",
      "Epoch 22, Batch 542, LR 1.726068 Loss 5.111238, Accuracy 88.764%\n",
      "Epoch 22, Batch 543, LR 1.725944 Loss 5.110434, Accuracy 88.775%\n",
      "Epoch 22, Batch 544, LR 1.725820 Loss 5.109405, Accuracy 88.777%\n",
      "Epoch 22, Batch 545, LR 1.725696 Loss 5.108900, Accuracy 88.779%\n",
      "Epoch 22, Batch 546, LR 1.725572 Loss 5.110130, Accuracy 88.772%\n",
      "Epoch 22, Batch 547, LR 1.725448 Loss 5.110084, Accuracy 88.773%\n",
      "Epoch 22, Batch 548, LR 1.725324 Loss 5.110587, Accuracy 88.775%\n",
      "Epoch 22, Batch 549, LR 1.725201 Loss 5.109654, Accuracy 88.782%\n",
      "Epoch 22, Batch 550, LR 1.725077 Loss 5.108666, Accuracy 88.780%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 551, LR 1.724953 Loss 5.108854, Accuracy 88.773%\n",
      "Epoch 22, Batch 552, LR 1.724829 Loss 5.107439, Accuracy 88.781%\n",
      "Epoch 22, Batch 553, LR 1.724705 Loss 5.107527, Accuracy 88.779%\n",
      "Epoch 22, Batch 554, LR 1.724581 Loss 5.108652, Accuracy 88.769%\n",
      "Epoch 22, Batch 555, LR 1.724457 Loss 5.109477, Accuracy 88.761%\n",
      "Epoch 22, Batch 556, LR 1.724333 Loss 5.109507, Accuracy 88.758%\n",
      "Epoch 22, Batch 557, LR 1.724209 Loss 5.109522, Accuracy 88.761%\n",
      "Epoch 22, Batch 558, LR 1.724085 Loss 5.107467, Accuracy 88.770%\n",
      "Epoch 22, Batch 559, LR 1.723961 Loss 5.106574, Accuracy 88.773%\n",
      "Epoch 22, Batch 560, LR 1.723837 Loss 5.106270, Accuracy 88.777%\n",
      "Epoch 22, Batch 561, LR 1.723713 Loss 5.105817, Accuracy 88.783%\n",
      "Epoch 22, Batch 562, LR 1.723589 Loss 5.106420, Accuracy 88.783%\n",
      "Epoch 22, Batch 563, LR 1.723465 Loss 5.106371, Accuracy 88.791%\n",
      "Epoch 22, Batch 564, LR 1.723342 Loss 5.107483, Accuracy 88.784%\n",
      "Epoch 22, Batch 565, LR 1.723218 Loss 5.106814, Accuracy 88.783%\n",
      "Epoch 22, Batch 566, LR 1.723094 Loss 5.106355, Accuracy 88.782%\n",
      "Epoch 22, Batch 567, LR 1.722970 Loss 5.105159, Accuracy 88.784%\n",
      "Epoch 22, Batch 568, LR 1.722846 Loss 5.106141, Accuracy 88.779%\n",
      "Epoch 22, Batch 569, LR 1.722722 Loss 5.106287, Accuracy 88.777%\n",
      "Epoch 22, Batch 570, LR 1.722598 Loss 5.106200, Accuracy 88.777%\n",
      "Epoch 22, Batch 571, LR 1.722474 Loss 5.106478, Accuracy 88.779%\n",
      "Epoch 22, Batch 572, LR 1.722350 Loss 5.105921, Accuracy 88.776%\n",
      "Epoch 22, Batch 573, LR 1.722225 Loss 5.106180, Accuracy 88.773%\n",
      "Epoch 22, Batch 574, LR 1.722101 Loss 5.105888, Accuracy 88.770%\n",
      "Epoch 22, Batch 575, LR 1.721977 Loss 5.104579, Accuracy 88.776%\n",
      "Epoch 22, Batch 576, LR 1.721853 Loss 5.105584, Accuracy 88.770%\n",
      "Epoch 22, Batch 577, LR 1.721729 Loss 5.104377, Accuracy 88.774%\n",
      "Epoch 22, Batch 578, LR 1.721605 Loss 5.104972, Accuracy 88.769%\n",
      "Epoch 22, Batch 579, LR 1.721481 Loss 5.103641, Accuracy 88.775%\n",
      "Epoch 22, Batch 580, LR 1.721357 Loss 5.102534, Accuracy 88.781%\n",
      "Epoch 22, Batch 581, LR 1.721233 Loss 5.102398, Accuracy 88.780%\n",
      "Epoch 22, Batch 582, LR 1.721109 Loss 5.102598, Accuracy 88.782%\n",
      "Epoch 22, Batch 583, LR 1.720985 Loss 5.102286, Accuracy 88.788%\n",
      "Epoch 22, Batch 584, LR 1.720861 Loss 5.102221, Accuracy 88.791%\n",
      "Epoch 22, Batch 585, LR 1.720737 Loss 5.102554, Accuracy 88.787%\n",
      "Epoch 22, Batch 586, LR 1.720613 Loss 5.101488, Accuracy 88.788%\n",
      "Epoch 22, Batch 587, LR 1.720489 Loss 5.100960, Accuracy 88.792%\n",
      "Epoch 22, Batch 588, LR 1.720364 Loss 5.101569, Accuracy 88.791%\n",
      "Epoch 22, Batch 589, LR 1.720240 Loss 5.101618, Accuracy 88.795%\n",
      "Epoch 22, Batch 590, LR 1.720116 Loss 5.102316, Accuracy 88.794%\n",
      "Epoch 22, Batch 591, LR 1.719992 Loss 5.102188, Accuracy 88.795%\n",
      "Epoch 22, Batch 592, LR 1.719868 Loss 5.101948, Accuracy 88.800%\n",
      "Epoch 22, Batch 593, LR 1.719744 Loss 5.101664, Accuracy 88.800%\n",
      "Epoch 22, Batch 594, LR 1.719620 Loss 5.100978, Accuracy 88.809%\n",
      "Epoch 22, Batch 595, LR 1.719496 Loss 5.101079, Accuracy 88.808%\n",
      "Epoch 22, Batch 596, LR 1.719371 Loss 5.101192, Accuracy 88.810%\n",
      "Epoch 22, Batch 597, LR 1.719247 Loss 5.100475, Accuracy 88.814%\n",
      "Epoch 22, Batch 598, LR 1.719123 Loss 5.100638, Accuracy 88.812%\n",
      "Epoch 22, Batch 599, LR 1.718999 Loss 5.101225, Accuracy 88.809%\n",
      "Epoch 22, Batch 600, LR 1.718875 Loss 5.100187, Accuracy 88.814%\n",
      "Epoch 22, Batch 601, LR 1.718751 Loss 5.100119, Accuracy 88.812%\n",
      "Epoch 22, Batch 602, LR 1.718626 Loss 5.100384, Accuracy 88.813%\n",
      "Epoch 22, Batch 603, LR 1.718502 Loss 5.099845, Accuracy 88.815%\n",
      "Epoch 22, Batch 604, LR 1.718378 Loss 5.100132, Accuracy 88.810%\n",
      "Epoch 22, Batch 605, LR 1.718254 Loss 5.100850, Accuracy 88.804%\n",
      "Epoch 22, Batch 606, LR 1.718130 Loss 5.100652, Accuracy 88.800%\n",
      "Epoch 22, Batch 607, LR 1.718005 Loss 5.100865, Accuracy 88.794%\n",
      "Epoch 22, Batch 608, LR 1.717881 Loss 5.101298, Accuracy 88.793%\n",
      "Epoch 22, Batch 609, LR 1.717757 Loss 5.100552, Accuracy 88.798%\n",
      "Epoch 22, Batch 610, LR 1.717633 Loss 5.101312, Accuracy 88.794%\n",
      "Epoch 22, Batch 611, LR 1.717509 Loss 5.100469, Accuracy 88.794%\n",
      "Epoch 22, Batch 612, LR 1.717384 Loss 5.100119, Accuracy 88.789%\n",
      "Epoch 22, Batch 613, LR 1.717260 Loss 5.100196, Accuracy 88.790%\n",
      "Epoch 22, Batch 614, LR 1.717136 Loss 5.100133, Accuracy 88.788%\n",
      "Epoch 22, Batch 615, LR 1.717012 Loss 5.099687, Accuracy 88.783%\n",
      "Epoch 22, Batch 616, LR 1.716887 Loss 5.099996, Accuracy 88.790%\n",
      "Epoch 22, Batch 617, LR 1.716763 Loss 5.100243, Accuracy 88.790%\n",
      "Epoch 22, Batch 618, LR 1.716639 Loss 5.100282, Accuracy 88.788%\n",
      "Epoch 22, Batch 619, LR 1.716515 Loss 5.099880, Accuracy 88.786%\n",
      "Epoch 22, Batch 620, LR 1.716390 Loss 5.100381, Accuracy 88.780%\n",
      "Epoch 22, Batch 621, LR 1.716266 Loss 5.099668, Accuracy 88.782%\n",
      "Epoch 22, Batch 622, LR 1.716142 Loss 5.099987, Accuracy 88.776%\n",
      "Epoch 22, Batch 623, LR 1.716017 Loss 5.099533, Accuracy 88.774%\n",
      "Epoch 22, Batch 624, LR 1.715893 Loss 5.098670, Accuracy 88.782%\n",
      "Epoch 22, Batch 625, LR 1.715769 Loss 5.099240, Accuracy 88.780%\n",
      "Epoch 22, Batch 626, LR 1.715644 Loss 5.098073, Accuracy 88.785%\n",
      "Epoch 22, Batch 627, LR 1.715520 Loss 5.099236, Accuracy 88.778%\n",
      "Epoch 22, Batch 628, LR 1.715396 Loss 5.099052, Accuracy 88.780%\n",
      "Epoch 22, Batch 629, LR 1.715272 Loss 5.099549, Accuracy 88.781%\n",
      "Epoch 22, Batch 630, LR 1.715147 Loss 5.099789, Accuracy 88.777%\n",
      "Epoch 22, Batch 631, LR 1.715023 Loss 5.099230, Accuracy 88.779%\n",
      "Epoch 22, Batch 632, LR 1.714899 Loss 5.098513, Accuracy 88.784%\n",
      "Epoch 22, Batch 633, LR 1.714774 Loss 5.098356, Accuracy 88.787%\n",
      "Epoch 22, Batch 634, LR 1.714650 Loss 5.096882, Accuracy 88.789%\n",
      "Epoch 22, Batch 635, LR 1.714525 Loss 5.097299, Accuracy 88.786%\n",
      "Epoch 22, Batch 636, LR 1.714401 Loss 5.096855, Accuracy 88.787%\n",
      "Epoch 22, Batch 637, LR 1.714277 Loss 5.096884, Accuracy 88.787%\n",
      "Epoch 22, Batch 638, LR 1.714152 Loss 5.096828, Accuracy 88.788%\n",
      "Epoch 22, Batch 639, LR 1.714028 Loss 5.097597, Accuracy 88.783%\n",
      "Epoch 22, Batch 640, LR 1.713904 Loss 5.097158, Accuracy 88.785%\n",
      "Epoch 22, Batch 641, LR 1.713779 Loss 5.097015, Accuracy 88.785%\n",
      "Epoch 22, Batch 642, LR 1.713655 Loss 5.097963, Accuracy 88.779%\n",
      "Epoch 22, Batch 643, LR 1.713530 Loss 5.098100, Accuracy 88.776%\n",
      "Epoch 22, Batch 644, LR 1.713406 Loss 5.097959, Accuracy 88.771%\n",
      "Epoch 22, Batch 645, LR 1.713282 Loss 5.097580, Accuracy 88.772%\n",
      "Epoch 22, Batch 646, LR 1.713157 Loss 5.097643, Accuracy 88.771%\n",
      "Epoch 22, Batch 647, LR 1.713033 Loss 5.097233, Accuracy 88.773%\n",
      "Epoch 22, Batch 648, LR 1.712908 Loss 5.097371, Accuracy 88.768%\n",
      "Epoch 22, Batch 649, LR 1.712784 Loss 5.096866, Accuracy 88.769%\n",
      "Epoch 22, Batch 650, LR 1.712659 Loss 5.096557, Accuracy 88.770%\n",
      "Epoch 22, Batch 651, LR 1.712535 Loss 5.096442, Accuracy 88.766%\n",
      "Epoch 22, Batch 652, LR 1.712411 Loss 5.097950, Accuracy 88.755%\n",
      "Epoch 22, Batch 653, LR 1.712286 Loss 5.098788, Accuracy 88.744%\n",
      "Epoch 22, Batch 654, LR 1.712162 Loss 5.099138, Accuracy 88.744%\n",
      "Epoch 22, Batch 655, LR 1.712037 Loss 5.099226, Accuracy 88.745%\n",
      "Epoch 22, Batch 656, LR 1.711913 Loss 5.099044, Accuracy 88.746%\n",
      "Epoch 22, Batch 657, LR 1.711788 Loss 5.099360, Accuracy 88.747%\n",
      "Epoch 22, Batch 658, LR 1.711664 Loss 5.100640, Accuracy 88.742%\n",
      "Epoch 22, Batch 659, LR 1.711539 Loss 5.101737, Accuracy 88.740%\n",
      "Epoch 22, Batch 660, LR 1.711415 Loss 5.102545, Accuracy 88.736%\n",
      "Epoch 22, Batch 661, LR 1.711290 Loss 5.102226, Accuracy 88.740%\n",
      "Epoch 22, Batch 662, LR 1.711166 Loss 5.102123, Accuracy 88.743%\n",
      "Epoch 22, Batch 663, LR 1.711041 Loss 5.102956, Accuracy 88.736%\n",
      "Epoch 22, Batch 664, LR 1.710917 Loss 5.102261, Accuracy 88.735%\n",
      "Epoch 22, Batch 665, LR 1.710792 Loss 5.102437, Accuracy 88.734%\n",
      "Epoch 22, Batch 666, LR 1.710668 Loss 5.102866, Accuracy 88.734%\n",
      "Epoch 22, Batch 667, LR 1.710543 Loss 5.102860, Accuracy 88.738%\n",
      "Epoch 22, Batch 668, LR 1.710419 Loss 5.103340, Accuracy 88.732%\n",
      "Epoch 22, Batch 669, LR 1.710294 Loss 5.103855, Accuracy 88.724%\n",
      "Epoch 22, Batch 670, LR 1.710170 Loss 5.105190, Accuracy 88.721%\n",
      "Epoch 22, Batch 671, LR 1.710045 Loss 5.105384, Accuracy 88.723%\n",
      "Epoch 22, Batch 672, LR 1.709921 Loss 5.106172, Accuracy 88.721%\n",
      "Epoch 22, Batch 673, LR 1.709796 Loss 5.106406, Accuracy 88.720%\n",
      "Epoch 22, Batch 674, LR 1.709671 Loss 5.106276, Accuracy 88.721%\n",
      "Epoch 22, Batch 675, LR 1.709547 Loss 5.107034, Accuracy 88.716%\n",
      "Epoch 22, Batch 676, LR 1.709422 Loss 5.107484, Accuracy 88.717%\n",
      "Epoch 22, Batch 677, LR 1.709298 Loss 5.107696, Accuracy 88.717%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 678, LR 1.709173 Loss 5.107306, Accuracy 88.716%\n",
      "Epoch 22, Batch 679, LR 1.709048 Loss 5.107307, Accuracy 88.717%\n",
      "Epoch 22, Batch 680, LR 1.708924 Loss 5.106662, Accuracy 88.720%\n",
      "Epoch 22, Batch 681, LR 1.708799 Loss 5.107760, Accuracy 88.715%\n",
      "Epoch 22, Batch 682, LR 1.708675 Loss 5.108199, Accuracy 88.711%\n",
      "Epoch 22, Batch 683, LR 1.708550 Loss 5.107834, Accuracy 88.714%\n",
      "Epoch 22, Batch 684, LR 1.708425 Loss 5.107561, Accuracy 88.715%\n",
      "Epoch 22, Batch 685, LR 1.708301 Loss 5.106628, Accuracy 88.719%\n",
      "Epoch 22, Batch 686, LR 1.708176 Loss 5.106848, Accuracy 88.721%\n",
      "Epoch 22, Batch 687, LR 1.708052 Loss 5.106646, Accuracy 88.722%\n",
      "Epoch 22, Batch 688, LR 1.707927 Loss 5.106705, Accuracy 88.723%\n",
      "Epoch 22, Batch 689, LR 1.707802 Loss 5.107198, Accuracy 88.719%\n",
      "Epoch 22, Batch 690, LR 1.707678 Loss 5.107495, Accuracy 88.715%\n",
      "Epoch 22, Batch 691, LR 1.707553 Loss 5.108124, Accuracy 88.712%\n",
      "Epoch 22, Batch 692, LR 1.707428 Loss 5.108076, Accuracy 88.710%\n",
      "Epoch 22, Batch 693, LR 1.707304 Loss 5.108471, Accuracy 88.707%\n",
      "Epoch 22, Batch 694, LR 1.707179 Loss 5.109246, Accuracy 88.698%\n",
      "Epoch 22, Batch 695, LR 1.707054 Loss 5.109501, Accuracy 88.697%\n",
      "Epoch 22, Batch 696, LR 1.706930 Loss 5.108966, Accuracy 88.702%\n",
      "Epoch 22, Batch 697, LR 1.706805 Loss 5.109083, Accuracy 88.704%\n",
      "Epoch 22, Batch 698, LR 1.706680 Loss 5.108853, Accuracy 88.700%\n",
      "Epoch 22, Batch 699, LR 1.706556 Loss 5.109019, Accuracy 88.701%\n",
      "Epoch 22, Batch 700, LR 1.706431 Loss 5.108610, Accuracy 88.705%\n",
      "Epoch 22, Batch 701, LR 1.706306 Loss 5.108845, Accuracy 88.709%\n",
      "Epoch 22, Batch 702, LR 1.706181 Loss 5.108547, Accuracy 88.711%\n",
      "Epoch 22, Batch 703, LR 1.706057 Loss 5.107836, Accuracy 88.712%\n",
      "Epoch 22, Batch 704, LR 1.705932 Loss 5.107800, Accuracy 88.712%\n",
      "Epoch 22, Batch 705, LR 1.705807 Loss 5.107562, Accuracy 88.710%\n",
      "Epoch 22, Batch 706, LR 1.705683 Loss 5.107522, Accuracy 88.711%\n",
      "Epoch 22, Batch 707, LR 1.705558 Loss 5.107010, Accuracy 88.714%\n",
      "Epoch 22, Batch 708, LR 1.705433 Loss 5.107408, Accuracy 88.713%\n",
      "Epoch 22, Batch 709, LR 1.705308 Loss 5.108516, Accuracy 88.714%\n",
      "Epoch 22, Batch 710, LR 1.705184 Loss 5.107945, Accuracy 88.713%\n",
      "Epoch 22, Batch 711, LR 1.705059 Loss 5.107525, Accuracy 88.716%\n",
      "Epoch 22, Batch 712, LR 1.704934 Loss 5.107534, Accuracy 88.716%\n",
      "Epoch 22, Batch 713, LR 1.704809 Loss 5.106706, Accuracy 88.720%\n",
      "Epoch 22, Batch 714, LR 1.704684 Loss 5.107123, Accuracy 88.717%\n",
      "Epoch 22, Batch 715, LR 1.704560 Loss 5.107741, Accuracy 88.716%\n",
      "Epoch 22, Batch 716, LR 1.704435 Loss 5.105321, Accuracy 88.728%\n",
      "Epoch 22, Batch 717, LR 1.704310 Loss 5.105568, Accuracy 88.729%\n",
      "Epoch 22, Batch 718, LR 1.704185 Loss 5.105249, Accuracy 88.727%\n",
      "Epoch 22, Batch 719, LR 1.704061 Loss 5.104529, Accuracy 88.733%\n",
      "Epoch 22, Batch 720, LR 1.703936 Loss 5.103422, Accuracy 88.737%\n",
      "Epoch 22, Batch 721, LR 1.703811 Loss 5.103565, Accuracy 88.735%\n",
      "Epoch 22, Batch 722, LR 1.703686 Loss 5.104117, Accuracy 88.738%\n",
      "Epoch 22, Batch 723, LR 1.703561 Loss 5.103939, Accuracy 88.737%\n",
      "Epoch 22, Batch 724, LR 1.703436 Loss 5.104257, Accuracy 88.736%\n",
      "Epoch 22, Batch 725, LR 1.703312 Loss 5.104377, Accuracy 88.736%\n",
      "Epoch 22, Batch 726, LR 1.703187 Loss 5.103492, Accuracy 88.736%\n",
      "Epoch 22, Batch 727, LR 1.703062 Loss 5.102998, Accuracy 88.737%\n",
      "Epoch 22, Batch 728, LR 1.702937 Loss 5.103390, Accuracy 88.737%\n",
      "Epoch 22, Batch 729, LR 1.702812 Loss 5.102822, Accuracy 88.741%\n",
      "Epoch 22, Batch 730, LR 1.702687 Loss 5.102320, Accuracy 88.743%\n",
      "Epoch 22, Batch 731, LR 1.702563 Loss 5.102353, Accuracy 88.739%\n",
      "Epoch 22, Batch 732, LR 1.702438 Loss 5.103190, Accuracy 88.731%\n",
      "Epoch 22, Batch 733, LR 1.702313 Loss 5.103800, Accuracy 88.725%\n",
      "Epoch 22, Batch 734, LR 1.702188 Loss 5.103998, Accuracy 88.723%\n",
      "Epoch 22, Batch 735, LR 1.702063 Loss 5.104277, Accuracy 88.718%\n",
      "Epoch 22, Batch 736, LR 1.701938 Loss 5.104741, Accuracy 88.712%\n",
      "Epoch 22, Batch 737, LR 1.701813 Loss 5.104858, Accuracy 88.713%\n",
      "Epoch 22, Batch 738, LR 1.701688 Loss 5.105578, Accuracy 88.708%\n",
      "Epoch 22, Batch 739, LR 1.701563 Loss 5.105371, Accuracy 88.708%\n",
      "Epoch 22, Batch 740, LR 1.701439 Loss 5.105488, Accuracy 88.709%\n",
      "Epoch 22, Batch 741, LR 1.701314 Loss 5.105251, Accuracy 88.708%\n",
      "Epoch 22, Batch 742, LR 1.701189 Loss 5.105236, Accuracy 88.710%\n",
      "Epoch 22, Batch 743, LR 1.701064 Loss 5.105064, Accuracy 88.713%\n",
      "Epoch 22, Batch 744, LR 1.700939 Loss 5.106007, Accuracy 88.709%\n",
      "Epoch 22, Batch 745, LR 1.700814 Loss 5.105822, Accuracy 88.709%\n",
      "Epoch 22, Batch 746, LR 1.700689 Loss 5.105877, Accuracy 88.711%\n",
      "Epoch 22, Batch 747, LR 1.700564 Loss 5.106201, Accuracy 88.705%\n",
      "Epoch 22, Batch 748, LR 1.700439 Loss 5.105552, Accuracy 88.704%\n",
      "Epoch 22, Batch 749, LR 1.700314 Loss 5.106436, Accuracy 88.697%\n",
      "Epoch 22, Batch 750, LR 1.700189 Loss 5.106129, Accuracy 88.697%\n",
      "Epoch 22, Batch 751, LR 1.700064 Loss 5.105863, Accuracy 88.700%\n",
      "Epoch 22, Batch 752, LR 1.699939 Loss 5.105914, Accuracy 88.700%\n",
      "Epoch 22, Batch 753, LR 1.699814 Loss 5.105239, Accuracy 88.705%\n",
      "Epoch 22, Batch 754, LR 1.699689 Loss 5.106224, Accuracy 88.697%\n",
      "Epoch 22, Batch 755, LR 1.699564 Loss 5.106447, Accuracy 88.694%\n",
      "Epoch 22, Batch 756, LR 1.699439 Loss 5.106516, Accuracy 88.695%\n",
      "Epoch 22, Batch 757, LR 1.699314 Loss 5.106114, Accuracy 88.695%\n",
      "Epoch 22, Batch 758, LR 1.699189 Loss 5.107111, Accuracy 88.687%\n",
      "Epoch 22, Batch 759, LR 1.699064 Loss 5.107343, Accuracy 88.684%\n",
      "Epoch 22, Batch 760, LR 1.698939 Loss 5.106720, Accuracy 88.686%\n",
      "Epoch 22, Batch 761, LR 1.698814 Loss 5.106504, Accuracy 88.685%\n",
      "Epoch 22, Batch 762, LR 1.698689 Loss 5.105813, Accuracy 88.688%\n",
      "Epoch 22, Batch 763, LR 1.698564 Loss 5.105430, Accuracy 88.691%\n",
      "Epoch 22, Batch 764, LR 1.698439 Loss 5.105932, Accuracy 88.690%\n",
      "Epoch 22, Batch 765, LR 1.698314 Loss 5.106226, Accuracy 88.685%\n",
      "Epoch 22, Batch 766, LR 1.698189 Loss 5.105771, Accuracy 88.688%\n",
      "Epoch 22, Batch 767, LR 1.698064 Loss 5.106813, Accuracy 88.680%\n",
      "Epoch 22, Batch 768, LR 1.697939 Loss 5.106431, Accuracy 88.681%\n",
      "Epoch 22, Batch 769, LR 1.697814 Loss 5.106565, Accuracy 88.674%\n",
      "Epoch 22, Batch 770, LR 1.697689 Loss 5.106284, Accuracy 88.676%\n",
      "Epoch 22, Batch 771, LR 1.697564 Loss 5.106257, Accuracy 88.675%\n",
      "Epoch 22, Batch 772, LR 1.697439 Loss 5.107376, Accuracy 88.669%\n",
      "Epoch 22, Batch 773, LR 1.697314 Loss 5.107080, Accuracy 88.669%\n",
      "Epoch 22, Batch 774, LR 1.697189 Loss 5.107475, Accuracy 88.669%\n",
      "Epoch 22, Batch 775, LR 1.697063 Loss 5.107831, Accuracy 88.665%\n",
      "Epoch 22, Batch 776, LR 1.696938 Loss 5.107492, Accuracy 88.663%\n",
      "Epoch 22, Batch 777, LR 1.696813 Loss 5.107317, Accuracy 88.663%\n",
      "Epoch 22, Batch 778, LR 1.696688 Loss 5.108377, Accuracy 88.656%\n",
      "Epoch 22, Batch 779, LR 1.696563 Loss 5.108440, Accuracy 88.654%\n",
      "Epoch 22, Batch 780, LR 1.696438 Loss 5.108721, Accuracy 88.658%\n",
      "Epoch 22, Batch 781, LR 1.696313 Loss 5.109077, Accuracy 88.658%\n",
      "Epoch 22, Batch 782, LR 1.696188 Loss 5.109108, Accuracy 88.655%\n",
      "Epoch 22, Batch 783, LR 1.696063 Loss 5.109467, Accuracy 88.653%\n",
      "Epoch 22, Batch 784, LR 1.695937 Loss 5.109065, Accuracy 88.652%\n",
      "Epoch 22, Batch 785, LR 1.695812 Loss 5.108636, Accuracy 88.656%\n",
      "Epoch 22, Batch 786, LR 1.695687 Loss 5.108487, Accuracy 88.660%\n",
      "Epoch 22, Batch 787, LR 1.695562 Loss 5.108240, Accuracy 88.660%\n",
      "Epoch 22, Batch 788, LR 1.695437 Loss 5.108058, Accuracy 88.658%\n",
      "Epoch 22, Batch 789, LR 1.695312 Loss 5.107803, Accuracy 88.660%\n",
      "Epoch 22, Batch 790, LR 1.695186 Loss 5.108264, Accuracy 88.661%\n",
      "Epoch 22, Batch 791, LR 1.695061 Loss 5.108561, Accuracy 88.656%\n",
      "Epoch 22, Batch 792, LR 1.694936 Loss 5.108367, Accuracy 88.659%\n",
      "Epoch 22, Batch 793, LR 1.694811 Loss 5.107621, Accuracy 88.662%\n",
      "Epoch 22, Batch 794, LR 1.694686 Loss 5.108158, Accuracy 88.659%\n",
      "Epoch 22, Batch 795, LR 1.694561 Loss 5.108356, Accuracy 88.661%\n",
      "Epoch 22, Batch 796, LR 1.694435 Loss 5.108278, Accuracy 88.662%\n",
      "Epoch 22, Batch 797, LR 1.694310 Loss 5.108466, Accuracy 88.664%\n",
      "Epoch 22, Batch 798, LR 1.694185 Loss 5.109644, Accuracy 88.659%\n",
      "Epoch 22, Batch 799, LR 1.694060 Loss 5.110075, Accuracy 88.658%\n",
      "Epoch 22, Batch 800, LR 1.693934 Loss 5.110713, Accuracy 88.653%\n",
      "Epoch 22, Batch 801, LR 1.693809 Loss 5.111039, Accuracy 88.650%\n",
      "Epoch 22, Batch 802, LR 1.693684 Loss 5.110643, Accuracy 88.653%\n",
      "Epoch 22, Batch 803, LR 1.693559 Loss 5.111067, Accuracy 88.650%\n",
      "Epoch 22, Batch 804, LR 1.693434 Loss 5.111373, Accuracy 88.647%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 805, LR 1.693308 Loss 5.111212, Accuracy 88.651%\n",
      "Epoch 22, Batch 806, LR 1.693183 Loss 5.111544, Accuracy 88.649%\n",
      "Epoch 22, Batch 807, LR 1.693058 Loss 5.111385, Accuracy 88.648%\n",
      "Epoch 22, Batch 808, LR 1.692933 Loss 5.111217, Accuracy 88.648%\n",
      "Epoch 22, Batch 809, LR 1.692807 Loss 5.110890, Accuracy 88.648%\n",
      "Epoch 22, Batch 810, LR 1.692682 Loss 5.111748, Accuracy 88.645%\n",
      "Epoch 22, Batch 811, LR 1.692557 Loss 5.111334, Accuracy 88.647%\n",
      "Epoch 22, Batch 812, LR 1.692431 Loss 5.111807, Accuracy 88.647%\n",
      "Epoch 22, Batch 813, LR 1.692306 Loss 5.111550, Accuracy 88.647%\n",
      "Epoch 22, Batch 814, LR 1.692181 Loss 5.111427, Accuracy 88.644%\n",
      "Epoch 22, Batch 815, LR 1.692056 Loss 5.111498, Accuracy 88.644%\n",
      "Epoch 22, Batch 816, LR 1.691930 Loss 5.111460, Accuracy 88.645%\n",
      "Epoch 22, Batch 817, LR 1.691805 Loss 5.111459, Accuracy 88.642%\n",
      "Epoch 22, Batch 818, LR 1.691680 Loss 5.111750, Accuracy 88.640%\n",
      "Epoch 22, Batch 819, LR 1.691554 Loss 5.112191, Accuracy 88.639%\n",
      "Epoch 22, Batch 820, LR 1.691429 Loss 5.112701, Accuracy 88.638%\n",
      "Epoch 22, Batch 821, LR 1.691304 Loss 5.112710, Accuracy 88.638%\n",
      "Epoch 22, Batch 822, LR 1.691178 Loss 5.112821, Accuracy 88.637%\n",
      "Epoch 22, Batch 823, LR 1.691053 Loss 5.112018, Accuracy 88.647%\n",
      "Epoch 22, Batch 824, LR 1.690928 Loss 5.112435, Accuracy 88.645%\n",
      "Epoch 22, Batch 825, LR 1.690802 Loss 5.112140, Accuracy 88.648%\n",
      "Epoch 22, Batch 826, LR 1.690677 Loss 5.112267, Accuracy 88.645%\n",
      "Epoch 22, Batch 827, LR 1.690552 Loss 5.112027, Accuracy 88.648%\n",
      "Epoch 22, Batch 828, LR 1.690426 Loss 5.111863, Accuracy 88.647%\n",
      "Epoch 22, Batch 829, LR 1.690301 Loss 5.110792, Accuracy 88.653%\n",
      "Epoch 22, Batch 830, LR 1.690176 Loss 5.110769, Accuracy 88.653%\n",
      "Epoch 22, Batch 831, LR 1.690050 Loss 5.109523, Accuracy 88.661%\n",
      "Epoch 22, Batch 832, LR 1.689925 Loss 5.110753, Accuracy 88.655%\n",
      "Epoch 22, Batch 833, LR 1.689799 Loss 5.110793, Accuracy 88.654%\n",
      "Epoch 22, Batch 834, LR 1.689674 Loss 5.110479, Accuracy 88.657%\n",
      "Epoch 22, Batch 835, LR 1.689549 Loss 5.110921, Accuracy 88.655%\n",
      "Epoch 22, Batch 836, LR 1.689423 Loss 5.110481, Accuracy 88.659%\n",
      "Epoch 22, Batch 837, LR 1.689298 Loss 5.110792, Accuracy 88.659%\n",
      "Epoch 22, Batch 838, LR 1.689172 Loss 5.110749, Accuracy 88.663%\n",
      "Epoch 22, Batch 839, LR 1.689047 Loss 5.110569, Accuracy 88.664%\n",
      "Epoch 22, Batch 840, LR 1.688922 Loss 5.109916, Accuracy 88.664%\n",
      "Epoch 22, Batch 841, LR 1.688796 Loss 5.109953, Accuracy 88.661%\n",
      "Epoch 22, Batch 842, LR 1.688671 Loss 5.109098, Accuracy 88.667%\n",
      "Epoch 22, Batch 843, LR 1.688545 Loss 5.108793, Accuracy 88.670%\n",
      "Epoch 22, Batch 844, LR 1.688420 Loss 5.109486, Accuracy 88.663%\n",
      "Epoch 22, Batch 845, LR 1.688294 Loss 5.109808, Accuracy 88.660%\n",
      "Epoch 22, Batch 846, LR 1.688169 Loss 5.109932, Accuracy 88.658%\n",
      "Epoch 22, Batch 847, LR 1.688043 Loss 5.108818, Accuracy 88.664%\n",
      "Epoch 22, Batch 848, LR 1.687918 Loss 5.108587, Accuracy 88.665%\n",
      "Epoch 22, Batch 849, LR 1.687793 Loss 5.108281, Accuracy 88.667%\n",
      "Epoch 22, Batch 850, LR 1.687667 Loss 5.108906, Accuracy 88.664%\n",
      "Epoch 22, Batch 851, LR 1.687542 Loss 5.108512, Accuracy 88.664%\n",
      "Epoch 22, Batch 852, LR 1.687416 Loss 5.108552, Accuracy 88.663%\n",
      "Epoch 22, Batch 853, LR 1.687291 Loss 5.108629, Accuracy 88.661%\n",
      "Epoch 22, Batch 854, LR 1.687165 Loss 5.108922, Accuracy 88.659%\n",
      "Epoch 22, Batch 855, LR 1.687040 Loss 5.109375, Accuracy 88.654%\n",
      "Epoch 22, Batch 856, LR 1.686914 Loss 5.109643, Accuracy 88.655%\n",
      "Epoch 22, Batch 857, LR 1.686789 Loss 5.109444, Accuracy 88.658%\n",
      "Epoch 22, Batch 858, LR 1.686663 Loss 5.109852, Accuracy 88.655%\n",
      "Epoch 22, Batch 859, LR 1.686538 Loss 5.110080, Accuracy 88.653%\n",
      "Epoch 22, Batch 860, LR 1.686412 Loss 5.109251, Accuracy 88.659%\n",
      "Epoch 22, Batch 861, LR 1.686287 Loss 5.109616, Accuracy 88.660%\n",
      "Epoch 22, Batch 862, LR 1.686161 Loss 5.108533, Accuracy 88.665%\n",
      "Epoch 22, Batch 863, LR 1.686036 Loss 5.108769, Accuracy 88.661%\n",
      "Epoch 22, Batch 864, LR 1.685910 Loss 5.108743, Accuracy 88.659%\n",
      "Epoch 22, Batch 865, LR 1.685784 Loss 5.109226, Accuracy 88.656%\n",
      "Epoch 22, Batch 866, LR 1.685659 Loss 5.109730, Accuracy 88.652%\n",
      "Epoch 22, Batch 867, LR 1.685533 Loss 5.110130, Accuracy 88.650%\n",
      "Epoch 22, Batch 868, LR 1.685408 Loss 5.109846, Accuracy 88.651%\n",
      "Epoch 22, Batch 869, LR 1.685282 Loss 5.110459, Accuracy 88.648%\n",
      "Epoch 22, Batch 870, LR 1.685157 Loss 5.110649, Accuracy 88.647%\n",
      "Epoch 22, Batch 871, LR 1.685031 Loss 5.110752, Accuracy 88.648%\n",
      "Epoch 22, Batch 872, LR 1.684905 Loss 5.110831, Accuracy 88.648%\n",
      "Epoch 22, Batch 873, LR 1.684780 Loss 5.110777, Accuracy 88.647%\n",
      "Epoch 22, Batch 874, LR 1.684654 Loss 5.110734, Accuracy 88.645%\n",
      "Epoch 22, Batch 875, LR 1.684529 Loss 5.110808, Accuracy 88.646%\n",
      "Epoch 22, Batch 876, LR 1.684403 Loss 5.110239, Accuracy 88.650%\n",
      "Epoch 22, Batch 877, LR 1.684277 Loss 5.110095, Accuracy 88.650%\n",
      "Epoch 22, Batch 878, LR 1.684152 Loss 5.109840, Accuracy 88.653%\n",
      "Epoch 22, Batch 879, LR 1.684026 Loss 5.109896, Accuracy 88.654%\n",
      "Epoch 22, Batch 880, LR 1.683901 Loss 5.109930, Accuracy 88.650%\n",
      "Epoch 22, Batch 881, LR 1.683775 Loss 5.109979, Accuracy 88.647%\n",
      "Epoch 22, Batch 882, LR 1.683649 Loss 5.109232, Accuracy 88.648%\n",
      "Epoch 22, Batch 883, LR 1.683524 Loss 5.109641, Accuracy 88.646%\n",
      "Epoch 22, Batch 884, LR 1.683398 Loss 5.109449, Accuracy 88.647%\n",
      "Epoch 22, Batch 885, LR 1.683272 Loss 5.109651, Accuracy 88.642%\n",
      "Epoch 22, Batch 886, LR 1.683147 Loss 5.109671, Accuracy 88.645%\n",
      "Epoch 22, Batch 887, LR 1.683021 Loss 5.109522, Accuracy 88.646%\n",
      "Epoch 22, Batch 888, LR 1.682895 Loss 5.109752, Accuracy 88.645%\n",
      "Epoch 22, Batch 889, LR 1.682770 Loss 5.109608, Accuracy 88.643%\n",
      "Epoch 22, Batch 890, LR 1.682644 Loss 5.109859, Accuracy 88.643%\n",
      "Epoch 22, Batch 891, LR 1.682518 Loss 5.109700, Accuracy 88.643%\n",
      "Epoch 22, Batch 892, LR 1.682393 Loss 5.110104, Accuracy 88.642%\n",
      "Epoch 22, Batch 893, LR 1.682267 Loss 5.110238, Accuracy 88.641%\n",
      "Epoch 22, Batch 894, LR 1.682141 Loss 5.110278, Accuracy 88.642%\n",
      "Epoch 22, Batch 895, LR 1.682016 Loss 5.111164, Accuracy 88.633%\n",
      "Epoch 22, Batch 896, LR 1.681890 Loss 5.110942, Accuracy 88.634%\n",
      "Epoch 22, Batch 897, LR 1.681764 Loss 5.110889, Accuracy 88.634%\n",
      "Epoch 22, Batch 898, LR 1.681639 Loss 5.111100, Accuracy 88.633%\n",
      "Epoch 22, Batch 899, LR 1.681513 Loss 5.111278, Accuracy 88.633%\n",
      "Epoch 22, Batch 900, LR 1.681387 Loss 5.110506, Accuracy 88.637%\n",
      "Epoch 22, Batch 901, LR 1.681261 Loss 5.111406, Accuracy 88.634%\n",
      "Epoch 22, Batch 902, LR 1.681136 Loss 5.111813, Accuracy 88.635%\n",
      "Epoch 22, Batch 903, LR 1.681010 Loss 5.111662, Accuracy 88.641%\n",
      "Epoch 22, Batch 904, LR 1.680884 Loss 5.111280, Accuracy 88.644%\n",
      "Epoch 22, Batch 905, LR 1.680758 Loss 5.111320, Accuracy 88.642%\n",
      "Epoch 22, Batch 906, LR 1.680633 Loss 5.112013, Accuracy 88.637%\n",
      "Epoch 22, Batch 907, LR 1.680507 Loss 5.112135, Accuracy 88.638%\n",
      "Epoch 22, Batch 908, LR 1.680381 Loss 5.112852, Accuracy 88.634%\n",
      "Epoch 22, Batch 909, LR 1.680255 Loss 5.112408, Accuracy 88.638%\n",
      "Epoch 22, Batch 910, LR 1.680130 Loss 5.112419, Accuracy 88.636%\n",
      "Epoch 22, Batch 911, LR 1.680004 Loss 5.112322, Accuracy 88.636%\n",
      "Epoch 22, Batch 912, LR 1.679878 Loss 5.111903, Accuracy 88.637%\n",
      "Epoch 22, Batch 913, LR 1.679752 Loss 5.111865, Accuracy 88.638%\n",
      "Epoch 22, Batch 914, LR 1.679627 Loss 5.111457, Accuracy 88.644%\n",
      "Epoch 22, Batch 915, LR 1.679501 Loss 5.111267, Accuracy 88.645%\n",
      "Epoch 22, Batch 916, LR 1.679375 Loss 5.110834, Accuracy 88.647%\n",
      "Epoch 22, Batch 917, LR 1.679249 Loss 5.110969, Accuracy 88.648%\n",
      "Epoch 22, Batch 918, LR 1.679123 Loss 5.110920, Accuracy 88.645%\n",
      "Epoch 22, Batch 919, LR 1.678997 Loss 5.111032, Accuracy 88.643%\n",
      "Epoch 22, Batch 920, LR 1.678872 Loss 5.111232, Accuracy 88.641%\n",
      "Epoch 22, Batch 921, LR 1.678746 Loss 5.111786, Accuracy 88.638%\n",
      "Epoch 22, Batch 922, LR 1.678620 Loss 5.111179, Accuracy 88.640%\n",
      "Epoch 22, Batch 923, LR 1.678494 Loss 5.111479, Accuracy 88.641%\n",
      "Epoch 22, Batch 924, LR 1.678368 Loss 5.111659, Accuracy 88.636%\n",
      "Epoch 22, Batch 925, LR 1.678243 Loss 5.111601, Accuracy 88.633%\n",
      "Epoch 22, Batch 926, LR 1.678117 Loss 5.111729, Accuracy 88.633%\n",
      "Epoch 22, Batch 927, LR 1.677991 Loss 5.110973, Accuracy 88.637%\n",
      "Epoch 22, Batch 928, LR 1.677865 Loss 5.111058, Accuracy 88.635%\n",
      "Epoch 22, Batch 929, LR 1.677739 Loss 5.110867, Accuracy 88.637%\n",
      "Epoch 22, Batch 930, LR 1.677613 Loss 5.111477, Accuracy 88.631%\n",
      "Epoch 22, Batch 931, LR 1.677487 Loss 5.111533, Accuracy 88.629%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 932, LR 1.677361 Loss 5.110844, Accuracy 88.632%\n",
      "Epoch 22, Batch 933, LR 1.677236 Loss 5.110301, Accuracy 88.638%\n",
      "Epoch 22, Batch 934, LR 1.677110 Loss 5.109882, Accuracy 88.640%\n",
      "Epoch 22, Batch 935, LR 1.676984 Loss 5.110239, Accuracy 88.639%\n",
      "Epoch 22, Batch 936, LR 1.676858 Loss 5.110095, Accuracy 88.638%\n",
      "Epoch 22, Batch 937, LR 1.676732 Loss 5.110237, Accuracy 88.634%\n",
      "Epoch 22, Batch 938, LR 1.676606 Loss 5.110182, Accuracy 88.639%\n",
      "Epoch 22, Batch 939, LR 1.676480 Loss 5.110224, Accuracy 88.642%\n",
      "Epoch 22, Batch 940, LR 1.676354 Loss 5.110354, Accuracy 88.643%\n",
      "Epoch 22, Batch 941, LR 1.676228 Loss 5.110251, Accuracy 88.642%\n",
      "Epoch 22, Batch 942, LR 1.676102 Loss 5.110590, Accuracy 88.642%\n",
      "Epoch 22, Batch 943, LR 1.675976 Loss 5.110341, Accuracy 88.643%\n",
      "Epoch 22, Batch 944, LR 1.675851 Loss 5.110134, Accuracy 88.642%\n",
      "Epoch 22, Batch 945, LR 1.675725 Loss 5.109252, Accuracy 88.644%\n",
      "Epoch 22, Batch 946, LR 1.675599 Loss 5.109082, Accuracy 88.645%\n",
      "Epoch 22, Batch 947, LR 1.675473 Loss 5.108007, Accuracy 88.649%\n",
      "Epoch 22, Batch 948, LR 1.675347 Loss 5.108221, Accuracy 88.646%\n",
      "Epoch 22, Batch 949, LR 1.675221 Loss 5.107965, Accuracy 88.645%\n",
      "Epoch 22, Batch 950, LR 1.675095 Loss 5.108761, Accuracy 88.641%\n",
      "Epoch 22, Batch 951, LR 1.674969 Loss 5.108305, Accuracy 88.645%\n",
      "Epoch 22, Batch 952, LR 1.674843 Loss 5.107635, Accuracy 88.646%\n",
      "Epoch 22, Batch 953, LR 1.674717 Loss 5.106797, Accuracy 88.652%\n",
      "Epoch 22, Batch 954, LR 1.674591 Loss 5.105990, Accuracy 88.656%\n",
      "Epoch 22, Batch 955, LR 1.674465 Loss 5.106381, Accuracy 88.656%\n",
      "Epoch 22, Batch 956, LR 1.674339 Loss 5.106718, Accuracy 88.654%\n",
      "Epoch 22, Batch 957, LR 1.674213 Loss 5.106703, Accuracy 88.651%\n",
      "Epoch 22, Batch 958, LR 1.674087 Loss 5.106799, Accuracy 88.653%\n",
      "Epoch 22, Batch 959, LR 1.673961 Loss 5.107322, Accuracy 88.652%\n",
      "Epoch 22, Batch 960, LR 1.673835 Loss 5.107638, Accuracy 88.648%\n",
      "Epoch 22, Batch 961, LR 1.673709 Loss 5.107898, Accuracy 88.648%\n",
      "Epoch 22, Batch 962, LR 1.673583 Loss 5.107160, Accuracy 88.649%\n",
      "Epoch 22, Batch 963, LR 1.673457 Loss 5.107460, Accuracy 88.649%\n",
      "Epoch 22, Batch 964, LR 1.673331 Loss 5.107779, Accuracy 88.642%\n",
      "Epoch 22, Batch 965, LR 1.673205 Loss 5.108311, Accuracy 88.639%\n",
      "Epoch 22, Batch 966, LR 1.673079 Loss 5.108506, Accuracy 88.640%\n",
      "Epoch 22, Batch 967, LR 1.672953 Loss 5.109525, Accuracy 88.634%\n",
      "Epoch 22, Batch 968, LR 1.672827 Loss 5.109848, Accuracy 88.632%\n",
      "Epoch 22, Batch 969, LR 1.672700 Loss 5.109933, Accuracy 88.634%\n",
      "Epoch 22, Batch 970, LR 1.672574 Loss 5.110620, Accuracy 88.628%\n",
      "Epoch 22, Batch 971, LR 1.672448 Loss 5.111656, Accuracy 88.622%\n",
      "Epoch 22, Batch 972, LR 1.672322 Loss 5.111672, Accuracy 88.622%\n",
      "Epoch 22, Batch 973, LR 1.672196 Loss 5.112131, Accuracy 88.620%\n",
      "Epoch 22, Batch 974, LR 1.672070 Loss 5.111878, Accuracy 88.623%\n",
      "Epoch 22, Batch 975, LR 1.671944 Loss 5.112108, Accuracy 88.622%\n",
      "Epoch 22, Batch 976, LR 1.671818 Loss 5.112590, Accuracy 88.621%\n",
      "Epoch 22, Batch 977, LR 1.671692 Loss 5.112466, Accuracy 88.623%\n",
      "Epoch 22, Batch 978, LR 1.671566 Loss 5.111908, Accuracy 88.624%\n",
      "Epoch 22, Batch 979, LR 1.671440 Loss 5.112117, Accuracy 88.624%\n",
      "Epoch 22, Batch 980, LR 1.671314 Loss 5.111950, Accuracy 88.629%\n",
      "Epoch 22, Batch 981, LR 1.671187 Loss 5.111505, Accuracy 88.631%\n",
      "Epoch 22, Batch 982, LR 1.671061 Loss 5.111938, Accuracy 88.627%\n",
      "Epoch 22, Batch 983, LR 1.670935 Loss 5.111998, Accuracy 88.625%\n",
      "Epoch 22, Batch 984, LR 1.670809 Loss 5.112282, Accuracy 88.622%\n",
      "Epoch 22, Batch 985, LR 1.670683 Loss 5.112197, Accuracy 88.623%\n",
      "Epoch 22, Batch 986, LR 1.670557 Loss 5.112465, Accuracy 88.621%\n",
      "Epoch 22, Batch 987, LR 1.670431 Loss 5.112783, Accuracy 88.618%\n",
      "Epoch 22, Batch 988, LR 1.670304 Loss 5.113190, Accuracy 88.613%\n",
      "Epoch 22, Batch 989, LR 1.670178 Loss 5.112941, Accuracy 88.611%\n",
      "Epoch 22, Batch 990, LR 1.670052 Loss 5.112954, Accuracy 88.612%\n",
      "Epoch 22, Batch 991, LR 1.669926 Loss 5.112527, Accuracy 88.614%\n",
      "Epoch 22, Batch 992, LR 1.669800 Loss 5.112837, Accuracy 88.611%\n",
      "Epoch 22, Batch 993, LR 1.669674 Loss 5.112645, Accuracy 88.613%\n",
      "Epoch 22, Batch 994, LR 1.669547 Loss 5.112555, Accuracy 88.614%\n",
      "Epoch 22, Batch 995, LR 1.669421 Loss 5.112777, Accuracy 88.610%\n",
      "Epoch 22, Batch 996, LR 1.669295 Loss 5.112763, Accuracy 88.608%\n",
      "Epoch 22, Batch 997, LR 1.669169 Loss 5.113214, Accuracy 88.606%\n",
      "Epoch 22, Batch 998, LR 1.669043 Loss 5.113795, Accuracy 88.604%\n",
      "Epoch 22, Batch 999, LR 1.668916 Loss 5.114147, Accuracy 88.600%\n",
      "Epoch 22, Batch 1000, LR 1.668790 Loss 5.114412, Accuracy 88.598%\n",
      "Epoch 22, Batch 1001, LR 1.668664 Loss 5.114746, Accuracy 88.596%\n",
      "Epoch 22, Batch 1002, LR 1.668538 Loss 5.114652, Accuracy 88.595%\n",
      "Epoch 22, Batch 1003, LR 1.668412 Loss 5.114508, Accuracy 88.595%\n",
      "Epoch 22, Batch 1004, LR 1.668285 Loss 5.114304, Accuracy 88.594%\n",
      "Epoch 22, Batch 1005, LR 1.668159 Loss 5.114199, Accuracy 88.598%\n",
      "Epoch 22, Batch 1006, LR 1.668033 Loss 5.113787, Accuracy 88.598%\n",
      "Epoch 22, Batch 1007, LR 1.667907 Loss 5.113192, Accuracy 88.602%\n",
      "Epoch 22, Batch 1008, LR 1.667780 Loss 5.113589, Accuracy 88.599%\n",
      "Epoch 22, Batch 1009, LR 1.667654 Loss 5.113736, Accuracy 88.598%\n",
      "Epoch 22, Batch 1010, LR 1.667528 Loss 5.113684, Accuracy 88.600%\n",
      "Epoch 22, Batch 1011, LR 1.667402 Loss 5.113303, Accuracy 88.600%\n",
      "Epoch 22, Batch 1012, LR 1.667275 Loss 5.113011, Accuracy 88.602%\n",
      "Epoch 22, Batch 1013, LR 1.667149 Loss 5.112924, Accuracy 88.604%\n",
      "Epoch 22, Batch 1014, LR 1.667023 Loss 5.112741, Accuracy 88.608%\n",
      "Epoch 22, Batch 1015, LR 1.666897 Loss 5.113014, Accuracy 88.607%\n",
      "Epoch 22, Batch 1016, LR 1.666770 Loss 5.112934, Accuracy 88.607%\n",
      "Epoch 22, Batch 1017, LR 1.666644 Loss 5.113266, Accuracy 88.602%\n",
      "Epoch 22, Batch 1018, LR 1.666518 Loss 5.113639, Accuracy 88.597%\n",
      "Epoch 22, Batch 1019, LR 1.666391 Loss 5.113388, Accuracy 88.598%\n",
      "Epoch 22, Batch 1020, LR 1.666265 Loss 5.113936, Accuracy 88.595%\n",
      "Epoch 22, Batch 1021, LR 1.666139 Loss 5.113852, Accuracy 88.595%\n",
      "Epoch 22, Batch 1022, LR 1.666012 Loss 5.113973, Accuracy 88.594%\n",
      "Epoch 22, Batch 1023, LR 1.665886 Loss 5.113354, Accuracy 88.595%\n",
      "Epoch 22, Batch 1024, LR 1.665760 Loss 5.113621, Accuracy 88.595%\n",
      "Epoch 22, Batch 1025, LR 1.665633 Loss 5.113976, Accuracy 88.596%\n",
      "Epoch 22, Batch 1026, LR 1.665507 Loss 5.113943, Accuracy 88.597%\n",
      "Epoch 22, Batch 1027, LR 1.665381 Loss 5.113894, Accuracy 88.595%\n",
      "Epoch 22, Batch 1028, LR 1.665254 Loss 5.113646, Accuracy 88.598%\n",
      "Epoch 22, Batch 1029, LR 1.665128 Loss 5.114147, Accuracy 88.594%\n",
      "Epoch 22, Batch 1030, LR 1.665002 Loss 5.114041, Accuracy 88.595%\n",
      "Epoch 22, Batch 1031, LR 1.664875 Loss 5.113679, Accuracy 88.595%\n",
      "Epoch 22, Batch 1032, LR 1.664749 Loss 5.114177, Accuracy 88.590%\n",
      "Epoch 22, Batch 1033, LR 1.664623 Loss 5.113955, Accuracy 88.594%\n",
      "Epoch 22, Batch 1034, LR 1.664496 Loss 5.113860, Accuracy 88.594%\n",
      "Epoch 22, Batch 1035, LR 1.664370 Loss 5.113984, Accuracy 88.595%\n",
      "Epoch 22, Batch 1036, LR 1.664244 Loss 5.113987, Accuracy 88.592%\n",
      "Epoch 22, Batch 1037, LR 1.664117 Loss 5.113735, Accuracy 88.592%\n",
      "Epoch 22, Batch 1038, LR 1.663991 Loss 5.113848, Accuracy 88.591%\n",
      "Epoch 22, Batch 1039, LR 1.663864 Loss 5.113943, Accuracy 88.592%\n",
      "Epoch 22, Batch 1040, LR 1.663738 Loss 5.114067, Accuracy 88.591%\n",
      "Epoch 22, Batch 1041, LR 1.663612 Loss 5.114632, Accuracy 88.590%\n",
      "Epoch 22, Batch 1042, LR 1.663485 Loss 5.114846, Accuracy 88.589%\n",
      "Epoch 22, Batch 1043, LR 1.663359 Loss 5.115436, Accuracy 88.589%\n",
      "Epoch 22, Batch 1044, LR 1.663232 Loss 5.115461, Accuracy 88.589%\n",
      "Epoch 22, Batch 1045, LR 1.663106 Loss 5.115417, Accuracy 88.592%\n",
      "Epoch 22, Batch 1046, LR 1.662979 Loss 5.115026, Accuracy 88.594%\n",
      "Epoch 22, Batch 1047, LR 1.662853 Loss 5.116001, Accuracy 88.587%\n",
      "Epoch 22, Loss (train set) 5.116001, Accuracy (train set) 88.587%\n",
      "Epoch 23, Batch 1, LR 1.662727 Loss 5.082441, Accuracy 90.625%\n",
      "Epoch 23, Batch 2, LR 1.662600 Loss 4.973694, Accuracy 91.016%\n",
      "Epoch 23, Batch 3, LR 1.662474 Loss 4.713828, Accuracy 91.146%\n",
      "Epoch 23, Batch 4, LR 1.662347 Loss 4.771879, Accuracy 90.820%\n",
      "Epoch 23, Batch 5, LR 1.662221 Loss 4.697766, Accuracy 90.625%\n",
      "Epoch 23, Batch 6, LR 1.662094 Loss 4.711901, Accuracy 90.365%\n",
      "Epoch 23, Batch 7, LR 1.661968 Loss 4.780314, Accuracy 89.955%\n",
      "Epoch 23, Batch 8, LR 1.661841 Loss 4.818505, Accuracy 89.648%\n",
      "Epoch 23, Batch 9, LR 1.661715 Loss 4.790785, Accuracy 90.017%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 10, LR 1.661588 Loss 4.798574, Accuracy 90.000%\n",
      "Epoch 23, Batch 11, LR 1.661462 Loss 4.872984, Accuracy 89.702%\n",
      "Epoch 23, Batch 12, LR 1.661335 Loss 4.908293, Accuracy 89.258%\n",
      "Epoch 23, Batch 13, LR 1.661209 Loss 4.904667, Accuracy 89.363%\n",
      "Epoch 23, Batch 14, LR 1.661082 Loss 4.869654, Accuracy 89.621%\n",
      "Epoch 23, Batch 15, LR 1.660956 Loss 4.911680, Accuracy 89.375%\n",
      "Epoch 23, Batch 16, LR 1.660829 Loss 4.908934, Accuracy 89.551%\n",
      "Epoch 23, Batch 17, LR 1.660703 Loss 4.927706, Accuracy 89.430%\n",
      "Epoch 23, Batch 18, LR 1.660576 Loss 4.937618, Accuracy 89.453%\n",
      "Epoch 23, Batch 19, LR 1.660450 Loss 4.925158, Accuracy 89.515%\n",
      "Epoch 23, Batch 20, LR 1.660323 Loss 4.925579, Accuracy 89.531%\n",
      "Epoch 23, Batch 21, LR 1.660197 Loss 4.916760, Accuracy 89.732%\n",
      "Epoch 23, Batch 22, LR 1.660070 Loss 4.912231, Accuracy 89.773%\n",
      "Epoch 23, Batch 23, LR 1.659944 Loss 4.948445, Accuracy 89.674%\n",
      "Epoch 23, Batch 24, LR 1.659817 Loss 4.987741, Accuracy 89.616%\n",
      "Epoch 23, Batch 25, LR 1.659691 Loss 4.974907, Accuracy 89.625%\n",
      "Epoch 23, Batch 26, LR 1.659564 Loss 4.965146, Accuracy 89.633%\n",
      "Epoch 23, Batch 27, LR 1.659438 Loss 4.942687, Accuracy 89.728%\n",
      "Epoch 23, Batch 28, LR 1.659311 Loss 4.928869, Accuracy 89.927%\n",
      "Epoch 23, Batch 29, LR 1.659184 Loss 4.928874, Accuracy 89.978%\n",
      "Epoch 23, Batch 30, LR 1.659058 Loss 4.942295, Accuracy 89.740%\n",
      "Epoch 23, Batch 31, LR 1.658931 Loss 4.918728, Accuracy 89.844%\n",
      "Epoch 23, Batch 32, LR 1.658805 Loss 4.910452, Accuracy 89.868%\n",
      "Epoch 23, Batch 33, LR 1.658678 Loss 4.900177, Accuracy 89.891%\n",
      "Epoch 23, Batch 34, LR 1.658551 Loss 4.916808, Accuracy 89.706%\n",
      "Epoch 23, Batch 35, LR 1.658425 Loss 4.894046, Accuracy 89.821%\n",
      "Epoch 23, Batch 36, LR 1.658298 Loss 4.904331, Accuracy 89.735%\n",
      "Epoch 23, Batch 37, LR 1.658172 Loss 4.912821, Accuracy 89.569%\n",
      "Epoch 23, Batch 38, LR 1.658045 Loss 4.910603, Accuracy 89.618%\n",
      "Epoch 23, Batch 39, LR 1.657918 Loss 4.918828, Accuracy 89.663%\n",
      "Epoch 23, Batch 40, LR 1.657792 Loss 4.933902, Accuracy 89.512%\n",
      "Epoch 23, Batch 41, LR 1.657665 Loss 4.934672, Accuracy 89.539%\n",
      "Epoch 23, Batch 42, LR 1.657539 Loss 4.930972, Accuracy 89.490%\n",
      "Epoch 23, Batch 43, LR 1.657412 Loss 4.937865, Accuracy 89.480%\n",
      "Epoch 23, Batch 44, LR 1.657285 Loss 4.926258, Accuracy 89.506%\n",
      "Epoch 23, Batch 45, LR 1.657159 Loss 4.929627, Accuracy 89.531%\n",
      "Epoch 23, Batch 46, LR 1.657032 Loss 4.940152, Accuracy 89.453%\n",
      "Epoch 23, Batch 47, LR 1.656905 Loss 4.935031, Accuracy 89.495%\n",
      "Epoch 23, Batch 48, LR 1.656779 Loss 4.927975, Accuracy 89.551%\n",
      "Epoch 23, Batch 49, LR 1.656652 Loss 4.908628, Accuracy 89.652%\n",
      "Epoch 23, Batch 50, LR 1.656525 Loss 4.905542, Accuracy 89.703%\n",
      "Epoch 23, Batch 51, LR 1.656399 Loss 4.912945, Accuracy 89.645%\n",
      "Epoch 23, Batch 52, LR 1.656272 Loss 4.901873, Accuracy 89.663%\n",
      "Epoch 23, Batch 53, LR 1.656145 Loss 4.918891, Accuracy 89.519%\n",
      "Epoch 23, Batch 54, LR 1.656019 Loss 4.924177, Accuracy 89.424%\n",
      "Epoch 23, Batch 55, LR 1.655892 Loss 4.933254, Accuracy 89.347%\n",
      "Epoch 23, Batch 56, LR 1.655765 Loss 4.946323, Accuracy 89.244%\n",
      "Epoch 23, Batch 57, LR 1.655638 Loss 4.939986, Accuracy 89.282%\n",
      "Epoch 23, Batch 58, LR 1.655512 Loss 4.939211, Accuracy 89.291%\n",
      "Epoch 23, Batch 59, LR 1.655385 Loss 4.933733, Accuracy 89.314%\n",
      "Epoch 23, Batch 60, LR 1.655258 Loss 4.938894, Accuracy 89.232%\n",
      "Epoch 23, Batch 61, LR 1.655132 Loss 4.931943, Accuracy 89.229%\n",
      "Epoch 23, Batch 62, LR 1.655005 Loss 4.929790, Accuracy 89.289%\n",
      "Epoch 23, Batch 63, LR 1.654878 Loss 4.935647, Accuracy 89.261%\n",
      "Epoch 23, Batch 64, LR 1.654751 Loss 4.939071, Accuracy 89.185%\n",
      "Epoch 23, Batch 65, LR 1.654625 Loss 4.947567, Accuracy 89.147%\n",
      "Epoch 23, Batch 66, LR 1.654498 Loss 4.948796, Accuracy 89.134%\n",
      "Epoch 23, Batch 67, LR 1.654371 Loss 4.947967, Accuracy 89.156%\n",
      "Epoch 23, Batch 68, LR 1.654244 Loss 4.953938, Accuracy 89.051%\n",
      "Epoch 23, Batch 69, LR 1.654118 Loss 4.952302, Accuracy 89.017%\n",
      "Epoch 23, Batch 70, LR 1.653991 Loss 4.940686, Accuracy 89.062%\n",
      "Epoch 23, Batch 71, LR 1.653864 Loss 4.942304, Accuracy 89.051%\n",
      "Epoch 23, Batch 72, LR 1.653737 Loss 4.939829, Accuracy 89.052%\n",
      "Epoch 23, Batch 73, LR 1.653611 Loss 4.942781, Accuracy 89.041%\n",
      "Epoch 23, Batch 74, LR 1.653484 Loss 4.949491, Accuracy 89.031%\n",
      "Epoch 23, Batch 75, LR 1.653357 Loss 4.955043, Accuracy 89.010%\n",
      "Epoch 23, Batch 76, LR 1.653230 Loss 4.958050, Accuracy 88.970%\n",
      "Epoch 23, Batch 77, LR 1.653103 Loss 4.953930, Accuracy 88.971%\n",
      "Epoch 23, Batch 78, LR 1.652977 Loss 4.959311, Accuracy 88.972%\n",
      "Epoch 23, Batch 79, LR 1.652850 Loss 4.959908, Accuracy 88.973%\n",
      "Epoch 23, Batch 80, LR 1.652723 Loss 4.949214, Accuracy 89.004%\n",
      "Epoch 23, Batch 81, LR 1.652596 Loss 4.948844, Accuracy 89.024%\n",
      "Epoch 23, Batch 82, LR 1.652469 Loss 4.950016, Accuracy 89.053%\n",
      "Epoch 23, Batch 83, LR 1.652343 Loss 4.953938, Accuracy 89.072%\n",
      "Epoch 23, Batch 84, LR 1.652216 Loss 4.954010, Accuracy 89.062%\n",
      "Epoch 23, Batch 85, LR 1.652089 Loss 4.954061, Accuracy 89.072%\n",
      "Epoch 23, Batch 86, LR 1.651962 Loss 4.951192, Accuracy 89.081%\n",
      "Epoch 23, Batch 87, LR 1.651835 Loss 4.951010, Accuracy 89.071%\n",
      "Epoch 23, Batch 88, LR 1.651708 Loss 4.952751, Accuracy 89.062%\n",
      "Epoch 23, Batch 89, LR 1.651582 Loss 4.958953, Accuracy 89.045%\n",
      "Epoch 23, Batch 90, LR 1.651455 Loss 4.959794, Accuracy 89.054%\n",
      "Epoch 23, Batch 91, LR 1.651328 Loss 4.959873, Accuracy 89.080%\n",
      "Epoch 23, Batch 92, LR 1.651201 Loss 4.964996, Accuracy 89.071%\n",
      "Epoch 23, Batch 93, LR 1.651074 Loss 4.957528, Accuracy 89.096%\n",
      "Epoch 23, Batch 94, LR 1.650947 Loss 4.960439, Accuracy 89.046%\n",
      "Epoch 23, Batch 95, LR 1.650820 Loss 4.962226, Accuracy 89.054%\n",
      "Epoch 23, Batch 96, LR 1.650694 Loss 4.963801, Accuracy 89.022%\n",
      "Epoch 23, Batch 97, LR 1.650567 Loss 4.963451, Accuracy 88.982%\n",
      "Epoch 23, Batch 98, LR 1.650440 Loss 4.964942, Accuracy 88.975%\n",
      "Epoch 23, Batch 99, LR 1.650313 Loss 4.964906, Accuracy 88.976%\n",
      "Epoch 23, Batch 100, LR 1.650186 Loss 4.962733, Accuracy 88.961%\n",
      "Epoch 23, Batch 101, LR 1.650059 Loss 4.956314, Accuracy 89.024%\n",
      "Epoch 23, Batch 102, LR 1.649932 Loss 4.963279, Accuracy 89.017%\n",
      "Epoch 23, Batch 103, LR 1.649805 Loss 4.960455, Accuracy 89.009%\n",
      "Epoch 23, Batch 104, LR 1.649678 Loss 4.958869, Accuracy 89.032%\n",
      "Epoch 23, Batch 105, LR 1.649551 Loss 4.953878, Accuracy 89.070%\n",
      "Epoch 23, Batch 106, LR 1.649424 Loss 4.949275, Accuracy 89.077%\n",
      "Epoch 23, Batch 107, LR 1.649297 Loss 4.954943, Accuracy 89.077%\n",
      "Epoch 23, Batch 108, LR 1.649171 Loss 4.957983, Accuracy 89.041%\n",
      "Epoch 23, Batch 109, LR 1.649044 Loss 4.964394, Accuracy 89.019%\n",
      "Epoch 23, Batch 110, LR 1.648917 Loss 4.960073, Accuracy 89.062%\n",
      "Epoch 23, Batch 111, LR 1.648790 Loss 4.953816, Accuracy 89.112%\n",
      "Epoch 23, Batch 112, LR 1.648663 Loss 4.952425, Accuracy 89.118%\n",
      "Epoch 23, Batch 113, LR 1.648536 Loss 4.953676, Accuracy 89.111%\n",
      "Epoch 23, Batch 114, LR 1.648409 Loss 4.955058, Accuracy 89.083%\n",
      "Epoch 23, Batch 115, LR 1.648282 Loss 4.957343, Accuracy 89.069%\n",
      "Epoch 23, Batch 116, LR 1.648155 Loss 4.954917, Accuracy 89.096%\n",
      "Epoch 23, Batch 117, LR 1.648028 Loss 4.951475, Accuracy 89.109%\n",
      "Epoch 23, Batch 118, LR 1.647901 Loss 4.948013, Accuracy 89.109%\n",
      "Epoch 23, Batch 119, LR 1.647774 Loss 4.951746, Accuracy 89.122%\n",
      "Epoch 23, Batch 120, LR 1.647647 Loss 4.949226, Accuracy 89.154%\n",
      "Epoch 23, Batch 121, LR 1.647520 Loss 4.945386, Accuracy 89.146%\n",
      "Epoch 23, Batch 122, LR 1.647393 Loss 4.943366, Accuracy 89.171%\n",
      "Epoch 23, Batch 123, LR 1.647266 Loss 4.935002, Accuracy 89.209%\n",
      "Epoch 23, Batch 124, LR 1.647139 Loss 4.931039, Accuracy 89.245%\n",
      "Epoch 23, Batch 125, LR 1.647012 Loss 4.928071, Accuracy 89.269%\n",
      "Epoch 23, Batch 126, LR 1.646885 Loss 4.926410, Accuracy 89.280%\n",
      "Epoch 23, Batch 127, LR 1.646758 Loss 4.927165, Accuracy 89.278%\n",
      "Epoch 23, Batch 128, LR 1.646631 Loss 4.930025, Accuracy 89.258%\n",
      "Epoch 23, Batch 129, LR 1.646504 Loss 4.928286, Accuracy 89.268%\n",
      "Epoch 23, Batch 130, LR 1.646377 Loss 4.930386, Accuracy 89.237%\n",
      "Epoch 23, Batch 131, LR 1.646250 Loss 4.934849, Accuracy 89.212%\n",
      "Epoch 23, Batch 132, LR 1.646123 Loss 4.932840, Accuracy 89.222%\n",
      "Epoch 23, Batch 133, LR 1.645996 Loss 4.936193, Accuracy 89.186%\n",
      "Epoch 23, Batch 134, LR 1.645869 Loss 4.941623, Accuracy 89.156%\n",
      "Epoch 23, Batch 135, LR 1.645742 Loss 4.945850, Accuracy 89.144%\n",
      "Epoch 23, Batch 136, LR 1.645614 Loss 4.949479, Accuracy 89.120%\n",
      "Epoch 23, Batch 137, LR 1.645487 Loss 4.952494, Accuracy 89.102%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 138, LR 1.645360 Loss 4.950419, Accuracy 89.091%\n",
      "Epoch 23, Batch 139, LR 1.645233 Loss 4.946963, Accuracy 89.107%\n",
      "Epoch 23, Batch 140, LR 1.645106 Loss 4.948635, Accuracy 89.107%\n",
      "Epoch 23, Batch 141, LR 1.644979 Loss 4.949009, Accuracy 89.107%\n",
      "Epoch 23, Batch 142, LR 1.644852 Loss 4.949658, Accuracy 89.112%\n",
      "Epoch 23, Batch 143, LR 1.644725 Loss 4.943651, Accuracy 89.144%\n",
      "Epoch 23, Batch 144, LR 1.644598 Loss 4.940719, Accuracy 89.144%\n",
      "Epoch 23, Batch 145, LR 1.644471 Loss 4.940170, Accuracy 89.149%\n",
      "Epoch 23, Batch 146, LR 1.644344 Loss 4.943223, Accuracy 89.148%\n",
      "Epoch 23, Batch 147, LR 1.644216 Loss 4.941786, Accuracy 89.148%\n",
      "Epoch 23, Batch 148, LR 1.644089 Loss 4.945478, Accuracy 89.126%\n",
      "Epoch 23, Batch 149, LR 1.643962 Loss 4.948064, Accuracy 89.094%\n",
      "Epoch 23, Batch 150, LR 1.643835 Loss 4.946779, Accuracy 89.094%\n",
      "Epoch 23, Batch 151, LR 1.643708 Loss 4.945982, Accuracy 89.099%\n",
      "Epoch 23, Batch 152, LR 1.643581 Loss 4.951429, Accuracy 89.088%\n",
      "Epoch 23, Batch 153, LR 1.643454 Loss 4.953704, Accuracy 89.047%\n",
      "Epoch 23, Batch 154, LR 1.643327 Loss 4.954218, Accuracy 89.052%\n",
      "Epoch 23, Batch 155, LR 1.643199 Loss 4.955604, Accuracy 89.062%\n",
      "Epoch 23, Batch 156, LR 1.643072 Loss 4.959029, Accuracy 89.052%\n",
      "Epoch 23, Batch 157, LR 1.642945 Loss 4.959159, Accuracy 89.048%\n",
      "Epoch 23, Batch 158, LR 1.642818 Loss 4.954043, Accuracy 89.067%\n",
      "Epoch 23, Batch 159, LR 1.642691 Loss 4.950823, Accuracy 89.092%\n",
      "Epoch 23, Batch 160, LR 1.642564 Loss 4.953177, Accuracy 89.072%\n",
      "Epoch 23, Batch 161, LR 1.642436 Loss 4.953938, Accuracy 89.062%\n",
      "Epoch 23, Batch 162, LR 1.642309 Loss 4.950334, Accuracy 89.067%\n",
      "Epoch 23, Batch 163, LR 1.642182 Loss 4.950875, Accuracy 89.072%\n",
      "Epoch 23, Batch 164, LR 1.642055 Loss 4.953510, Accuracy 89.086%\n",
      "Epoch 23, Batch 165, LR 1.641928 Loss 4.949882, Accuracy 89.119%\n",
      "Epoch 23, Batch 166, LR 1.641800 Loss 4.946196, Accuracy 89.143%\n",
      "Epoch 23, Batch 167, LR 1.641673 Loss 4.945803, Accuracy 89.123%\n",
      "Epoch 23, Batch 168, LR 1.641546 Loss 4.949017, Accuracy 89.109%\n",
      "Epoch 23, Batch 169, LR 1.641419 Loss 4.951805, Accuracy 89.118%\n",
      "Epoch 23, Batch 170, LR 1.641292 Loss 4.950443, Accuracy 89.131%\n",
      "Epoch 23, Batch 171, LR 1.641164 Loss 4.952326, Accuracy 89.113%\n",
      "Epoch 23, Batch 172, LR 1.641037 Loss 4.952932, Accuracy 89.117%\n",
      "Epoch 23, Batch 173, LR 1.640910 Loss 4.954729, Accuracy 89.121%\n",
      "Epoch 23, Batch 174, LR 1.640783 Loss 4.955994, Accuracy 89.112%\n",
      "Epoch 23, Batch 175, LR 1.640655 Loss 4.956616, Accuracy 89.125%\n",
      "Epoch 23, Batch 176, LR 1.640528 Loss 4.959101, Accuracy 89.094%\n",
      "Epoch 23, Batch 177, LR 1.640401 Loss 4.956973, Accuracy 89.089%\n",
      "Epoch 23, Batch 178, LR 1.640274 Loss 4.958294, Accuracy 89.080%\n",
      "Epoch 23, Batch 179, LR 1.640146 Loss 4.955236, Accuracy 89.102%\n",
      "Epoch 23, Batch 180, LR 1.640019 Loss 4.956402, Accuracy 89.102%\n",
      "Epoch 23, Batch 181, LR 1.639892 Loss 4.952504, Accuracy 89.127%\n",
      "Epoch 23, Batch 182, LR 1.639765 Loss 4.954224, Accuracy 89.123%\n",
      "Epoch 23, Batch 183, LR 1.639637 Loss 4.955743, Accuracy 89.131%\n",
      "Epoch 23, Batch 184, LR 1.639510 Loss 4.954779, Accuracy 89.147%\n",
      "Epoch 23, Batch 185, LR 1.639383 Loss 4.955420, Accuracy 89.177%\n",
      "Epoch 23, Batch 186, LR 1.639255 Loss 4.956697, Accuracy 89.176%\n",
      "Epoch 23, Batch 187, LR 1.639128 Loss 4.956835, Accuracy 89.171%\n",
      "Epoch 23, Batch 188, LR 1.639001 Loss 4.957848, Accuracy 89.171%\n",
      "Epoch 23, Batch 189, LR 1.638874 Loss 4.959068, Accuracy 89.174%\n",
      "Epoch 23, Batch 190, LR 1.638746 Loss 4.959568, Accuracy 89.161%\n",
      "Epoch 23, Batch 191, LR 1.638619 Loss 4.961414, Accuracy 89.132%\n",
      "Epoch 23, Batch 192, LR 1.638492 Loss 4.961921, Accuracy 89.119%\n",
      "Epoch 23, Batch 193, LR 1.638364 Loss 4.959757, Accuracy 89.139%\n",
      "Epoch 23, Batch 194, LR 1.638237 Loss 4.959711, Accuracy 89.135%\n",
      "Epoch 23, Batch 195, LR 1.638110 Loss 4.960680, Accuracy 89.119%\n",
      "Epoch 23, Batch 196, LR 1.637982 Loss 4.960021, Accuracy 89.110%\n",
      "Epoch 23, Batch 197, LR 1.637855 Loss 4.961155, Accuracy 89.090%\n",
      "Epoch 23, Batch 198, LR 1.637728 Loss 4.961195, Accuracy 89.086%\n",
      "Epoch 23, Batch 199, LR 1.637600 Loss 4.957699, Accuracy 89.094%\n",
      "Epoch 23, Batch 200, LR 1.637473 Loss 4.960475, Accuracy 89.090%\n",
      "Epoch 23, Batch 201, LR 1.637346 Loss 4.962338, Accuracy 89.094%\n",
      "Epoch 23, Batch 202, LR 1.637218 Loss 4.962967, Accuracy 89.074%\n",
      "Epoch 23, Batch 203, LR 1.637091 Loss 4.963268, Accuracy 89.062%\n",
      "Epoch 23, Batch 204, LR 1.636963 Loss 4.962053, Accuracy 89.074%\n",
      "Epoch 23, Batch 205, LR 1.636836 Loss 4.960158, Accuracy 89.089%\n",
      "Epoch 23, Batch 206, LR 1.636709 Loss 4.964168, Accuracy 89.081%\n",
      "Epoch 23, Batch 207, LR 1.636581 Loss 4.967610, Accuracy 89.066%\n",
      "Epoch 23, Batch 208, LR 1.636454 Loss 4.971910, Accuracy 89.044%\n",
      "Epoch 23, Batch 209, LR 1.636327 Loss 4.971425, Accuracy 89.040%\n",
      "Epoch 23, Batch 210, LR 1.636199 Loss 4.967982, Accuracy 89.059%\n",
      "Epoch 23, Batch 211, LR 1.636072 Loss 4.966512, Accuracy 89.048%\n",
      "Epoch 23, Batch 212, LR 1.635944 Loss 4.966141, Accuracy 89.044%\n",
      "Epoch 23, Batch 213, LR 1.635817 Loss 4.964581, Accuracy 89.048%\n",
      "Epoch 23, Batch 214, LR 1.635689 Loss 4.961603, Accuracy 89.048%\n",
      "Epoch 23, Batch 215, LR 1.635562 Loss 4.963016, Accuracy 89.033%\n",
      "Epoch 23, Batch 216, LR 1.635435 Loss 4.960204, Accuracy 89.052%\n",
      "Epoch 23, Batch 217, LR 1.635307 Loss 4.956943, Accuracy 89.073%\n",
      "Epoch 23, Batch 218, LR 1.635180 Loss 4.958404, Accuracy 89.070%\n",
      "Epoch 23, Batch 219, LR 1.635052 Loss 4.960255, Accuracy 89.070%\n",
      "Epoch 23, Batch 220, LR 1.634925 Loss 4.961184, Accuracy 89.066%\n",
      "Epoch 23, Batch 221, LR 1.634797 Loss 4.959431, Accuracy 89.066%\n",
      "Epoch 23, Batch 222, LR 1.634670 Loss 4.958880, Accuracy 89.066%\n",
      "Epoch 23, Batch 223, LR 1.634543 Loss 4.953979, Accuracy 89.091%\n",
      "Epoch 23, Batch 224, LR 1.634415 Loss 4.954230, Accuracy 89.101%\n",
      "Epoch 23, Batch 225, LR 1.634288 Loss 4.957347, Accuracy 89.076%\n",
      "Epoch 23, Batch 226, LR 1.634160 Loss 4.956815, Accuracy 89.094%\n",
      "Epoch 23, Batch 227, LR 1.634033 Loss 4.952696, Accuracy 89.111%\n",
      "Epoch 23, Batch 228, LR 1.633905 Loss 4.952164, Accuracy 89.114%\n",
      "Epoch 23, Batch 229, LR 1.633778 Loss 4.949416, Accuracy 89.127%\n",
      "Epoch 23, Batch 230, LR 1.633650 Loss 4.946846, Accuracy 89.141%\n",
      "Epoch 23, Batch 231, LR 1.633523 Loss 4.948673, Accuracy 89.123%\n",
      "Epoch 23, Batch 232, LR 1.633395 Loss 4.949911, Accuracy 89.120%\n",
      "Epoch 23, Batch 233, LR 1.633268 Loss 4.951167, Accuracy 89.126%\n",
      "Epoch 23, Batch 234, LR 1.633140 Loss 4.950580, Accuracy 89.143%\n",
      "Epoch 23, Batch 235, LR 1.633013 Loss 4.952255, Accuracy 89.146%\n",
      "Epoch 23, Batch 236, LR 1.632885 Loss 4.949998, Accuracy 89.162%\n",
      "Epoch 23, Batch 237, LR 1.632758 Loss 4.951428, Accuracy 89.142%\n",
      "Epoch 23, Batch 238, LR 1.632630 Loss 4.954062, Accuracy 89.131%\n",
      "Epoch 23, Batch 239, LR 1.632503 Loss 4.953747, Accuracy 89.134%\n",
      "Epoch 23, Batch 240, LR 1.632375 Loss 4.955126, Accuracy 89.115%\n",
      "Epoch 23, Batch 241, LR 1.632248 Loss 4.955583, Accuracy 89.127%\n",
      "Epoch 23, Batch 242, LR 1.632120 Loss 4.956357, Accuracy 89.130%\n",
      "Epoch 23, Batch 243, LR 1.631993 Loss 4.956653, Accuracy 89.130%\n",
      "Epoch 23, Batch 244, LR 1.631865 Loss 4.956338, Accuracy 89.136%\n",
      "Epoch 23, Batch 245, LR 1.631737 Loss 4.956692, Accuracy 89.133%\n",
      "Epoch 23, Batch 246, LR 1.631610 Loss 4.956116, Accuracy 89.145%\n",
      "Epoch 23, Batch 247, LR 1.631482 Loss 4.955175, Accuracy 89.161%\n",
      "Epoch 23, Batch 248, LR 1.631355 Loss 4.953887, Accuracy 89.173%\n",
      "Epoch 23, Batch 249, LR 1.631227 Loss 4.954666, Accuracy 89.163%\n",
      "Epoch 23, Batch 250, LR 1.631100 Loss 4.952393, Accuracy 89.175%\n",
      "Epoch 23, Batch 251, LR 1.630972 Loss 4.953494, Accuracy 89.168%\n",
      "Epoch 23, Batch 252, LR 1.630844 Loss 4.954629, Accuracy 89.168%\n",
      "Epoch 23, Batch 253, LR 1.630717 Loss 4.953476, Accuracy 89.167%\n",
      "Epoch 23, Batch 254, LR 1.630589 Loss 4.953418, Accuracy 89.164%\n",
      "Epoch 23, Batch 255, LR 1.630462 Loss 4.952627, Accuracy 89.176%\n",
      "Epoch 23, Batch 256, LR 1.630334 Loss 4.953051, Accuracy 89.169%\n",
      "Epoch 23, Batch 257, LR 1.630206 Loss 4.951632, Accuracy 89.184%\n",
      "Epoch 23, Batch 258, LR 1.630079 Loss 4.951057, Accuracy 89.187%\n",
      "Epoch 23, Batch 259, LR 1.629951 Loss 4.953257, Accuracy 89.183%\n",
      "Epoch 23, Batch 260, LR 1.629824 Loss 4.951776, Accuracy 89.192%\n",
      "Epoch 23, Batch 261, LR 1.629696 Loss 4.952538, Accuracy 89.194%\n",
      "Epoch 23, Batch 262, LR 1.629568 Loss 4.951945, Accuracy 89.194%\n",
      "Epoch 23, Batch 263, LR 1.629441 Loss 4.953155, Accuracy 89.193%\n",
      "Epoch 23, Batch 264, LR 1.629313 Loss 4.951344, Accuracy 89.205%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 265, LR 1.629185 Loss 4.953571, Accuracy 89.198%\n",
      "Epoch 23, Batch 266, LR 1.629058 Loss 4.953769, Accuracy 89.192%\n",
      "Epoch 23, Batch 267, LR 1.628930 Loss 4.953573, Accuracy 89.191%\n",
      "Epoch 23, Batch 268, LR 1.628803 Loss 4.951669, Accuracy 89.202%\n",
      "Epoch 23, Batch 269, LR 1.628675 Loss 4.951792, Accuracy 89.202%\n",
      "Epoch 23, Batch 270, LR 1.628547 Loss 4.954243, Accuracy 89.184%\n",
      "Epoch 23, Batch 271, LR 1.628420 Loss 4.953558, Accuracy 89.184%\n",
      "Epoch 23, Batch 272, LR 1.628292 Loss 4.954214, Accuracy 89.172%\n",
      "Epoch 23, Batch 273, LR 1.628164 Loss 4.956676, Accuracy 89.160%\n",
      "Epoch 23, Batch 274, LR 1.628037 Loss 4.956521, Accuracy 89.162%\n",
      "Epoch 23, Batch 275, LR 1.627909 Loss 4.959117, Accuracy 89.148%\n",
      "Epoch 23, Batch 276, LR 1.627781 Loss 4.958189, Accuracy 89.139%\n",
      "Epoch 23, Batch 277, LR 1.627653 Loss 4.959997, Accuracy 89.125%\n",
      "Epoch 23, Batch 278, LR 1.627526 Loss 4.961193, Accuracy 89.119%\n",
      "Epoch 23, Batch 279, LR 1.627398 Loss 4.961461, Accuracy 89.113%\n",
      "Epoch 23, Batch 280, LR 1.627270 Loss 4.959469, Accuracy 89.121%\n",
      "Epoch 23, Batch 281, LR 1.627143 Loss 4.962091, Accuracy 89.113%\n",
      "Epoch 23, Batch 282, LR 1.627015 Loss 4.962876, Accuracy 89.104%\n",
      "Epoch 23, Batch 283, LR 1.626887 Loss 4.963553, Accuracy 89.120%\n",
      "Epoch 23, Batch 284, LR 1.626760 Loss 4.964669, Accuracy 89.115%\n",
      "Epoch 23, Batch 285, LR 1.626632 Loss 4.965312, Accuracy 89.101%\n",
      "Epoch 23, Batch 286, LR 1.626504 Loss 4.966148, Accuracy 89.098%\n",
      "Epoch 23, Batch 287, LR 1.626376 Loss 4.964831, Accuracy 89.098%\n",
      "Epoch 23, Batch 288, LR 1.626249 Loss 4.963496, Accuracy 89.111%\n",
      "Epoch 23, Batch 289, LR 1.626121 Loss 4.964156, Accuracy 89.108%\n",
      "Epoch 23, Batch 290, LR 1.625993 Loss 4.962887, Accuracy 89.119%\n",
      "Epoch 23, Batch 291, LR 1.625865 Loss 4.964092, Accuracy 89.122%\n",
      "Epoch 23, Batch 292, LR 1.625738 Loss 4.963726, Accuracy 89.129%\n",
      "Epoch 23, Batch 293, LR 1.625610 Loss 4.965769, Accuracy 89.113%\n",
      "Epoch 23, Batch 294, LR 1.625482 Loss 4.962756, Accuracy 89.132%\n",
      "Epoch 23, Batch 295, LR 1.625354 Loss 4.963543, Accuracy 89.123%\n",
      "Epoch 23, Batch 296, LR 1.625227 Loss 4.963063, Accuracy 89.118%\n",
      "Epoch 23, Batch 297, LR 1.625099 Loss 4.963370, Accuracy 89.120%\n",
      "Epoch 23, Batch 298, LR 1.624971 Loss 4.962211, Accuracy 89.118%\n",
      "Epoch 23, Batch 299, LR 1.624843 Loss 4.962157, Accuracy 89.117%\n",
      "Epoch 23, Batch 300, LR 1.624715 Loss 4.961785, Accuracy 89.120%\n",
      "Epoch 23, Batch 301, LR 1.624588 Loss 4.961428, Accuracy 89.122%\n",
      "Epoch 23, Batch 302, LR 1.624460 Loss 4.961612, Accuracy 89.125%\n",
      "Epoch 23, Batch 303, LR 1.624332 Loss 4.960284, Accuracy 89.137%\n",
      "Epoch 23, Batch 304, LR 1.624204 Loss 4.958289, Accuracy 89.142%\n",
      "Epoch 23, Batch 305, LR 1.624076 Loss 4.956894, Accuracy 89.160%\n",
      "Epoch 23, Batch 306, LR 1.623949 Loss 4.955243, Accuracy 89.167%\n",
      "Epoch 23, Batch 307, LR 1.623821 Loss 4.956422, Accuracy 89.172%\n",
      "Epoch 23, Batch 308, LR 1.623693 Loss 4.956374, Accuracy 89.184%\n",
      "Epoch 23, Batch 309, LR 1.623565 Loss 4.955249, Accuracy 89.202%\n",
      "Epoch 23, Batch 310, LR 1.623437 Loss 4.958364, Accuracy 89.186%\n",
      "Epoch 23, Batch 311, LR 1.623309 Loss 4.959327, Accuracy 89.188%\n",
      "Epoch 23, Batch 312, LR 1.623182 Loss 4.958509, Accuracy 89.183%\n",
      "Epoch 23, Batch 313, LR 1.623054 Loss 4.959288, Accuracy 89.180%\n",
      "Epoch 23, Batch 314, LR 1.622926 Loss 4.959890, Accuracy 89.167%\n",
      "Epoch 23, Batch 315, LR 1.622798 Loss 4.960399, Accuracy 89.167%\n",
      "Epoch 23, Batch 316, LR 1.622670 Loss 4.960309, Accuracy 89.166%\n",
      "Epoch 23, Batch 317, LR 1.622542 Loss 4.959858, Accuracy 89.173%\n",
      "Epoch 23, Batch 318, LR 1.622414 Loss 4.959500, Accuracy 89.166%\n",
      "Epoch 23, Batch 319, LR 1.622287 Loss 4.960340, Accuracy 89.153%\n",
      "Epoch 23, Batch 320, LR 1.622159 Loss 4.960822, Accuracy 89.141%\n",
      "Epoch 23, Batch 321, LR 1.622031 Loss 4.959708, Accuracy 89.140%\n",
      "Epoch 23, Batch 322, LR 1.621903 Loss 4.960807, Accuracy 89.130%\n",
      "Epoch 23, Batch 323, LR 1.621775 Loss 4.960426, Accuracy 89.125%\n",
      "Epoch 23, Batch 324, LR 1.621647 Loss 4.961914, Accuracy 89.116%\n",
      "Epoch 23, Batch 325, LR 1.621519 Loss 4.965160, Accuracy 89.096%\n",
      "Epoch 23, Batch 326, LR 1.621391 Loss 4.963137, Accuracy 89.108%\n",
      "Epoch 23, Batch 327, LR 1.621263 Loss 4.963026, Accuracy 89.108%\n",
      "Epoch 23, Batch 328, LR 1.621135 Loss 4.963671, Accuracy 89.108%\n",
      "Epoch 23, Batch 329, LR 1.621008 Loss 4.964237, Accuracy 89.110%\n",
      "Epoch 23, Batch 330, LR 1.620880 Loss 4.965069, Accuracy 89.107%\n",
      "Epoch 23, Batch 331, LR 1.620752 Loss 4.963951, Accuracy 89.117%\n",
      "Epoch 23, Batch 332, LR 1.620624 Loss 4.963998, Accuracy 89.133%\n",
      "Epoch 23, Batch 333, LR 1.620496 Loss 4.963071, Accuracy 89.133%\n",
      "Epoch 23, Batch 334, LR 1.620368 Loss 4.961557, Accuracy 89.144%\n",
      "Epoch 23, Batch 335, LR 1.620240 Loss 4.962604, Accuracy 89.146%\n",
      "Epoch 23, Batch 336, LR 1.620112 Loss 4.962508, Accuracy 89.137%\n",
      "Epoch 23, Batch 337, LR 1.619984 Loss 4.963751, Accuracy 89.139%\n",
      "Epoch 23, Batch 338, LR 1.619856 Loss 4.963600, Accuracy 89.136%\n",
      "Epoch 23, Batch 339, LR 1.619728 Loss 4.963802, Accuracy 89.134%\n",
      "Epoch 23, Batch 340, LR 1.619600 Loss 4.960533, Accuracy 89.154%\n",
      "Epoch 23, Batch 341, LR 1.619472 Loss 4.960937, Accuracy 89.159%\n",
      "Epoch 23, Batch 342, LR 1.619344 Loss 4.962264, Accuracy 89.156%\n",
      "Epoch 23, Batch 343, LR 1.619216 Loss 4.961987, Accuracy 89.156%\n",
      "Epoch 23, Batch 344, LR 1.619088 Loss 4.961272, Accuracy 89.162%\n",
      "Epoch 23, Batch 345, LR 1.618960 Loss 4.960203, Accuracy 89.167%\n",
      "Epoch 23, Batch 346, LR 1.618832 Loss 4.959898, Accuracy 89.164%\n",
      "Epoch 23, Batch 347, LR 1.618704 Loss 4.960275, Accuracy 89.155%\n",
      "Epoch 23, Batch 348, LR 1.618576 Loss 4.957869, Accuracy 89.168%\n",
      "Epoch 23, Batch 349, LR 1.618448 Loss 4.958941, Accuracy 89.170%\n",
      "Epoch 23, Batch 350, LR 1.618320 Loss 4.960291, Accuracy 89.167%\n",
      "Epoch 23, Batch 351, LR 1.618192 Loss 4.958260, Accuracy 89.180%\n",
      "Epoch 23, Batch 352, LR 1.618064 Loss 4.957946, Accuracy 89.182%\n",
      "Epoch 23, Batch 353, LR 1.617936 Loss 4.955885, Accuracy 89.195%\n",
      "Epoch 23, Batch 354, LR 1.617808 Loss 4.956584, Accuracy 89.204%\n",
      "Epoch 23, Batch 355, LR 1.617680 Loss 4.958003, Accuracy 89.197%\n",
      "Epoch 23, Batch 356, LR 1.617552 Loss 4.958911, Accuracy 89.185%\n",
      "Epoch 23, Batch 357, LR 1.617424 Loss 4.958073, Accuracy 89.189%\n",
      "Epoch 23, Batch 358, LR 1.617296 Loss 4.959194, Accuracy 89.178%\n",
      "Epoch 23, Batch 359, LR 1.617168 Loss 4.958880, Accuracy 89.178%\n",
      "Epoch 23, Batch 360, LR 1.617040 Loss 4.960223, Accuracy 89.171%\n",
      "Epoch 23, Batch 361, LR 1.616912 Loss 4.960628, Accuracy 89.184%\n",
      "Epoch 23, Batch 362, LR 1.616784 Loss 4.960474, Accuracy 89.173%\n",
      "Epoch 23, Batch 363, LR 1.616656 Loss 4.959737, Accuracy 89.183%\n",
      "Epoch 23, Batch 364, LR 1.616528 Loss 4.960111, Accuracy 89.185%\n",
      "Epoch 23, Batch 365, LR 1.616400 Loss 4.957101, Accuracy 89.197%\n",
      "Epoch 23, Batch 366, LR 1.616272 Loss 4.957476, Accuracy 89.201%\n",
      "Epoch 23, Batch 367, LR 1.616144 Loss 4.958297, Accuracy 89.192%\n",
      "Epoch 23, Batch 368, LR 1.616016 Loss 4.956512, Accuracy 89.198%\n",
      "Epoch 23, Batch 369, LR 1.615887 Loss 4.959509, Accuracy 89.179%\n",
      "Epoch 23, Batch 370, LR 1.615759 Loss 4.959310, Accuracy 89.172%\n",
      "Epoch 23, Batch 371, LR 1.615631 Loss 4.959032, Accuracy 89.176%\n",
      "Epoch 23, Batch 372, LR 1.615503 Loss 4.957582, Accuracy 89.186%\n",
      "Epoch 23, Batch 373, LR 1.615375 Loss 4.957576, Accuracy 89.182%\n",
      "Epoch 23, Batch 374, LR 1.615247 Loss 4.958543, Accuracy 89.184%\n",
      "Epoch 23, Batch 375, LR 1.615119 Loss 4.958008, Accuracy 89.181%\n",
      "Epoch 23, Batch 376, LR 1.614991 Loss 4.957911, Accuracy 89.177%\n",
      "Epoch 23, Batch 377, LR 1.614863 Loss 4.958768, Accuracy 89.172%\n",
      "Epoch 23, Batch 378, LR 1.614735 Loss 4.958596, Accuracy 89.176%\n",
      "Epoch 23, Batch 379, LR 1.614606 Loss 4.957423, Accuracy 89.174%\n",
      "Epoch 23, Batch 380, LR 1.614478 Loss 4.957641, Accuracy 89.178%\n",
      "Epoch 23, Batch 381, LR 1.614350 Loss 4.958735, Accuracy 89.161%\n",
      "Epoch 23, Batch 382, LR 1.614222 Loss 4.958733, Accuracy 89.155%\n",
      "Epoch 23, Batch 383, LR 1.614094 Loss 4.960110, Accuracy 89.150%\n",
      "Epoch 23, Batch 384, LR 1.613966 Loss 4.960712, Accuracy 89.148%\n",
      "Epoch 23, Batch 385, LR 1.613838 Loss 4.961645, Accuracy 89.138%\n",
      "Epoch 23, Batch 386, LR 1.613709 Loss 4.962923, Accuracy 89.131%\n",
      "Epoch 23, Batch 387, LR 1.613581 Loss 4.962613, Accuracy 89.131%\n",
      "Epoch 23, Batch 388, LR 1.613453 Loss 4.963456, Accuracy 89.125%\n",
      "Epoch 23, Batch 389, LR 1.613325 Loss 4.962774, Accuracy 89.131%\n",
      "Epoch 23, Batch 390, LR 1.613197 Loss 4.961275, Accuracy 89.139%\n",
      "Epoch 23, Batch 391, LR 1.613069 Loss 4.959775, Accuracy 89.140%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 392, LR 1.612940 Loss 4.962323, Accuracy 89.126%\n",
      "Epoch 23, Batch 393, LR 1.612812 Loss 4.962811, Accuracy 89.126%\n",
      "Epoch 23, Batch 394, LR 1.612684 Loss 4.963646, Accuracy 89.120%\n",
      "Epoch 23, Batch 395, LR 1.612556 Loss 4.964594, Accuracy 89.114%\n",
      "Epoch 23, Batch 396, LR 1.612428 Loss 4.964665, Accuracy 89.112%\n",
      "Epoch 23, Batch 397, LR 1.612299 Loss 4.964683, Accuracy 89.110%\n",
      "Epoch 23, Batch 398, LR 1.612171 Loss 4.964583, Accuracy 89.110%\n",
      "Epoch 23, Batch 399, LR 1.612043 Loss 4.965391, Accuracy 89.106%\n",
      "Epoch 23, Batch 400, LR 1.611915 Loss 4.964357, Accuracy 89.111%\n",
      "Epoch 23, Batch 401, LR 1.611787 Loss 4.962525, Accuracy 89.115%\n",
      "Epoch 23, Batch 402, LR 1.611658 Loss 4.961511, Accuracy 89.123%\n",
      "Epoch 23, Batch 403, LR 1.611530 Loss 4.960320, Accuracy 89.134%\n",
      "Epoch 23, Batch 404, LR 1.611402 Loss 4.961190, Accuracy 89.122%\n",
      "Epoch 23, Batch 405, LR 1.611274 Loss 4.958996, Accuracy 89.130%\n",
      "Epoch 23, Batch 406, LR 1.611145 Loss 4.958850, Accuracy 89.130%\n",
      "Epoch 23, Batch 407, LR 1.611017 Loss 4.960284, Accuracy 89.118%\n",
      "Epoch 23, Batch 408, LR 1.610889 Loss 4.960526, Accuracy 89.118%\n",
      "Epoch 23, Batch 409, LR 1.610761 Loss 4.961399, Accuracy 89.110%\n",
      "Epoch 23, Batch 410, LR 1.610632 Loss 4.961794, Accuracy 89.110%\n",
      "Epoch 23, Batch 411, LR 1.610504 Loss 4.961867, Accuracy 89.108%\n",
      "Epoch 23, Batch 412, LR 1.610376 Loss 4.963001, Accuracy 89.100%\n",
      "Epoch 23, Batch 413, LR 1.610248 Loss 4.963911, Accuracy 89.089%\n",
      "Epoch 23, Batch 414, LR 1.610119 Loss 4.964510, Accuracy 89.085%\n",
      "Epoch 23, Batch 415, LR 1.609991 Loss 4.964607, Accuracy 89.083%\n",
      "Epoch 23, Batch 416, LR 1.609863 Loss 4.965049, Accuracy 89.078%\n",
      "Epoch 23, Batch 417, LR 1.609735 Loss 4.965375, Accuracy 89.083%\n",
      "Epoch 23, Batch 418, LR 1.609606 Loss 4.964097, Accuracy 89.092%\n",
      "Epoch 23, Batch 419, LR 1.609478 Loss 4.963242, Accuracy 89.092%\n",
      "Epoch 23, Batch 420, LR 1.609350 Loss 4.962308, Accuracy 89.098%\n",
      "Epoch 23, Batch 421, LR 1.609221 Loss 4.960784, Accuracy 89.107%\n",
      "Epoch 23, Batch 422, LR 1.609093 Loss 4.960081, Accuracy 89.107%\n",
      "Epoch 23, Batch 423, LR 1.608965 Loss 4.959579, Accuracy 89.114%\n",
      "Epoch 23, Batch 424, LR 1.608836 Loss 4.958995, Accuracy 89.127%\n",
      "Epoch 23, Batch 425, LR 1.608708 Loss 4.959118, Accuracy 89.131%\n",
      "Epoch 23, Batch 426, LR 1.608580 Loss 4.959024, Accuracy 89.130%\n",
      "Epoch 23, Batch 427, LR 1.608451 Loss 4.958281, Accuracy 89.127%\n",
      "Epoch 23, Batch 428, LR 1.608323 Loss 4.957475, Accuracy 89.130%\n",
      "Epoch 23, Batch 429, LR 1.608195 Loss 4.957287, Accuracy 89.126%\n",
      "Epoch 23, Batch 430, LR 1.608066 Loss 4.959359, Accuracy 89.122%\n",
      "Epoch 23, Batch 431, LR 1.607938 Loss 4.959595, Accuracy 89.128%\n",
      "Epoch 23, Batch 432, LR 1.607810 Loss 4.958165, Accuracy 89.138%\n",
      "Epoch 23, Batch 433, LR 1.607681 Loss 4.958014, Accuracy 89.144%\n",
      "Epoch 23, Batch 434, LR 1.607553 Loss 4.957060, Accuracy 89.145%\n",
      "Epoch 23, Batch 435, LR 1.607425 Loss 4.957164, Accuracy 89.145%\n",
      "Epoch 23, Batch 436, LR 1.607296 Loss 4.955571, Accuracy 89.157%\n",
      "Epoch 23, Batch 437, LR 1.607168 Loss 4.955144, Accuracy 89.159%\n",
      "Epoch 23, Batch 438, LR 1.607040 Loss 4.955066, Accuracy 89.159%\n",
      "Epoch 23, Batch 439, LR 1.606911 Loss 4.956107, Accuracy 89.151%\n",
      "Epoch 23, Batch 440, LR 1.606783 Loss 4.957328, Accuracy 89.146%\n",
      "Epoch 23, Batch 441, LR 1.606654 Loss 4.956118, Accuracy 89.149%\n",
      "Epoch 23, Batch 442, LR 1.606526 Loss 4.956042, Accuracy 89.160%\n",
      "Epoch 23, Batch 443, LR 1.606398 Loss 4.955503, Accuracy 89.156%\n",
      "Epoch 23, Batch 444, LR 1.606269 Loss 4.954199, Accuracy 89.161%\n",
      "Epoch 23, Batch 445, LR 1.606141 Loss 4.954862, Accuracy 89.164%\n",
      "Epoch 23, Batch 446, LR 1.606012 Loss 4.954547, Accuracy 89.173%\n",
      "Epoch 23, Batch 447, LR 1.605884 Loss 4.953465, Accuracy 89.185%\n",
      "Epoch 23, Batch 448, LR 1.605756 Loss 4.953612, Accuracy 89.179%\n",
      "Epoch 23, Batch 449, LR 1.605627 Loss 4.952033, Accuracy 89.186%\n",
      "Epoch 23, Batch 450, LR 1.605499 Loss 4.952033, Accuracy 89.196%\n",
      "Epoch 23, Batch 451, LR 1.605370 Loss 4.952553, Accuracy 89.185%\n",
      "Epoch 23, Batch 452, LR 1.605242 Loss 4.952491, Accuracy 89.183%\n",
      "Epoch 23, Batch 453, LR 1.605114 Loss 4.952735, Accuracy 89.176%\n",
      "Epoch 23, Batch 454, LR 1.604985 Loss 4.952831, Accuracy 89.176%\n",
      "Epoch 23, Batch 455, LR 1.604857 Loss 4.952748, Accuracy 89.179%\n",
      "Epoch 23, Batch 456, LR 1.604728 Loss 4.952533, Accuracy 89.184%\n",
      "Epoch 23, Batch 457, LR 1.604600 Loss 4.951934, Accuracy 89.184%\n",
      "Epoch 23, Batch 458, LR 1.604471 Loss 4.953759, Accuracy 89.178%\n",
      "Epoch 23, Batch 459, LR 1.604343 Loss 4.954187, Accuracy 89.173%\n",
      "Epoch 23, Batch 460, LR 1.604214 Loss 4.953557, Accuracy 89.181%\n",
      "Epoch 23, Batch 461, LR 1.604086 Loss 4.954152, Accuracy 89.173%\n",
      "Epoch 23, Batch 462, LR 1.603957 Loss 4.952296, Accuracy 89.179%\n",
      "Epoch 23, Batch 463, LR 1.603829 Loss 4.951702, Accuracy 89.181%\n",
      "Epoch 23, Batch 464, LR 1.603701 Loss 4.951261, Accuracy 89.185%\n",
      "Epoch 23, Batch 465, LR 1.603572 Loss 4.952476, Accuracy 89.182%\n",
      "Epoch 23, Batch 466, LR 1.603444 Loss 4.951933, Accuracy 89.190%\n",
      "Epoch 23, Batch 467, LR 1.603315 Loss 4.951156, Accuracy 89.201%\n",
      "Epoch 23, Batch 468, LR 1.603187 Loss 4.950513, Accuracy 89.199%\n",
      "Epoch 23, Batch 469, LR 1.603058 Loss 4.951000, Accuracy 89.192%\n",
      "Epoch 23, Batch 470, LR 1.602930 Loss 4.951251, Accuracy 89.192%\n",
      "Epoch 23, Batch 471, LR 1.602801 Loss 4.950659, Accuracy 89.194%\n",
      "Epoch 23, Batch 472, LR 1.602673 Loss 4.951559, Accuracy 89.185%\n",
      "Epoch 23, Batch 473, LR 1.602544 Loss 4.950218, Accuracy 89.200%\n",
      "Epoch 23, Batch 474, LR 1.602416 Loss 4.949670, Accuracy 89.196%\n",
      "Epoch 23, Batch 475, LR 1.602287 Loss 4.950505, Accuracy 89.202%\n",
      "Epoch 23, Batch 476, LR 1.602159 Loss 4.949784, Accuracy 89.210%\n",
      "Epoch 23, Batch 477, LR 1.602030 Loss 4.949171, Accuracy 89.212%\n",
      "Epoch 23, Batch 478, LR 1.601901 Loss 4.947934, Accuracy 89.218%\n",
      "Epoch 23, Batch 479, LR 1.601773 Loss 4.947968, Accuracy 89.221%\n",
      "Epoch 23, Batch 480, LR 1.601644 Loss 4.948177, Accuracy 89.215%\n",
      "Epoch 23, Batch 481, LR 1.601516 Loss 4.948240, Accuracy 89.217%\n",
      "Epoch 23, Batch 482, LR 1.601387 Loss 4.948503, Accuracy 89.215%\n",
      "Epoch 23, Batch 483, LR 1.601259 Loss 4.949118, Accuracy 89.211%\n",
      "Epoch 23, Batch 484, LR 1.601130 Loss 4.949037, Accuracy 89.216%\n",
      "Epoch 23, Batch 485, LR 1.601002 Loss 4.948056, Accuracy 89.219%\n",
      "Epoch 23, Batch 486, LR 1.600873 Loss 4.949372, Accuracy 89.217%\n",
      "Epoch 23, Batch 487, LR 1.600744 Loss 4.949674, Accuracy 89.217%\n",
      "Epoch 23, Batch 488, LR 1.600616 Loss 4.951685, Accuracy 89.207%\n",
      "Epoch 23, Batch 489, LR 1.600487 Loss 4.952338, Accuracy 89.200%\n",
      "Epoch 23, Batch 490, LR 1.600359 Loss 4.952175, Accuracy 89.203%\n",
      "Epoch 23, Batch 491, LR 1.600230 Loss 4.952007, Accuracy 89.207%\n",
      "Epoch 23, Batch 492, LR 1.600102 Loss 4.951552, Accuracy 89.209%\n",
      "Epoch 23, Batch 493, LR 1.599973 Loss 4.951784, Accuracy 89.202%\n",
      "Epoch 23, Batch 494, LR 1.599844 Loss 4.950964, Accuracy 89.202%\n",
      "Epoch 23, Batch 495, LR 1.599716 Loss 4.950992, Accuracy 89.203%\n",
      "Epoch 23, Batch 496, LR 1.599587 Loss 4.951895, Accuracy 89.195%\n",
      "Epoch 23, Batch 497, LR 1.599459 Loss 4.952175, Accuracy 89.190%\n",
      "Epoch 23, Batch 498, LR 1.599330 Loss 4.952381, Accuracy 89.194%\n",
      "Epoch 23, Batch 499, LR 1.599201 Loss 4.952792, Accuracy 89.197%\n",
      "Epoch 23, Batch 500, LR 1.599073 Loss 4.954260, Accuracy 89.186%\n",
      "Epoch 23, Batch 501, LR 1.598944 Loss 4.954058, Accuracy 89.189%\n",
      "Epoch 23, Batch 502, LR 1.598815 Loss 4.953328, Accuracy 89.198%\n",
      "Epoch 23, Batch 503, LR 1.598687 Loss 4.951973, Accuracy 89.208%\n",
      "Epoch 23, Batch 504, LR 1.598558 Loss 4.951944, Accuracy 89.208%\n",
      "Epoch 23, Batch 505, LR 1.598430 Loss 4.950527, Accuracy 89.209%\n",
      "Epoch 23, Batch 506, LR 1.598301 Loss 4.949929, Accuracy 89.214%\n",
      "Epoch 23, Batch 507, LR 1.598172 Loss 4.949550, Accuracy 89.217%\n",
      "Epoch 23, Batch 508, LR 1.598044 Loss 4.949425, Accuracy 89.216%\n",
      "Epoch 23, Batch 509, LR 1.597915 Loss 4.950294, Accuracy 89.210%\n",
      "Epoch 23, Batch 510, LR 1.597786 Loss 4.950874, Accuracy 89.206%\n",
      "Epoch 23, Batch 511, LR 1.597658 Loss 4.951307, Accuracy 89.202%\n",
      "Epoch 23, Batch 512, LR 1.597529 Loss 4.952208, Accuracy 89.198%\n",
      "Epoch 23, Batch 513, LR 1.597400 Loss 4.952397, Accuracy 89.197%\n",
      "Epoch 23, Batch 514, LR 1.597272 Loss 4.951232, Accuracy 89.207%\n",
      "Epoch 23, Batch 515, LR 1.597143 Loss 4.951730, Accuracy 89.207%\n",
      "Epoch 23, Batch 516, LR 1.597014 Loss 4.951045, Accuracy 89.205%\n",
      "Epoch 23, Batch 517, LR 1.596885 Loss 4.952176, Accuracy 89.197%\n",
      "Epoch 23, Batch 518, LR 1.596757 Loss 4.951692, Accuracy 89.204%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 519, LR 1.596628 Loss 4.951645, Accuracy 89.202%\n",
      "Epoch 23, Batch 520, LR 1.596499 Loss 4.951120, Accuracy 89.205%\n",
      "Epoch 23, Batch 521, LR 1.596371 Loss 4.951403, Accuracy 89.202%\n",
      "Epoch 23, Batch 522, LR 1.596242 Loss 4.951582, Accuracy 89.202%\n",
      "Epoch 23, Batch 523, LR 1.596113 Loss 4.952224, Accuracy 89.198%\n",
      "Epoch 23, Batch 524, LR 1.595985 Loss 4.951359, Accuracy 89.195%\n",
      "Epoch 23, Batch 525, LR 1.595856 Loss 4.950976, Accuracy 89.193%\n",
      "Epoch 23, Batch 526, LR 1.595727 Loss 4.951267, Accuracy 89.189%\n",
      "Epoch 23, Batch 527, LR 1.595598 Loss 4.949866, Accuracy 89.190%\n",
      "Epoch 23, Batch 528, LR 1.595470 Loss 4.948946, Accuracy 89.196%\n",
      "Epoch 23, Batch 529, LR 1.595341 Loss 4.947960, Accuracy 89.201%\n",
      "Epoch 23, Batch 530, LR 1.595212 Loss 4.946125, Accuracy 89.208%\n",
      "Epoch 23, Batch 531, LR 1.595083 Loss 4.946487, Accuracy 89.214%\n",
      "Epoch 23, Batch 532, LR 1.594955 Loss 4.946406, Accuracy 89.215%\n",
      "Epoch 23, Batch 533, LR 1.594826 Loss 4.946327, Accuracy 89.218%\n",
      "Epoch 23, Batch 534, LR 1.594697 Loss 4.946866, Accuracy 89.212%\n",
      "Epoch 23, Batch 535, LR 1.594568 Loss 4.946839, Accuracy 89.219%\n",
      "Epoch 23, Batch 536, LR 1.594440 Loss 4.946416, Accuracy 89.221%\n",
      "Epoch 23, Batch 537, LR 1.594311 Loss 4.946386, Accuracy 89.231%\n",
      "Epoch 23, Batch 538, LR 1.594182 Loss 4.946834, Accuracy 89.232%\n",
      "Epoch 23, Batch 539, LR 1.594053 Loss 4.945948, Accuracy 89.238%\n",
      "Epoch 23, Batch 540, LR 1.593925 Loss 4.945860, Accuracy 89.233%\n",
      "Epoch 23, Batch 541, LR 1.593796 Loss 4.945925, Accuracy 89.233%\n",
      "Epoch 23, Batch 542, LR 1.593667 Loss 4.944969, Accuracy 89.235%\n",
      "Epoch 23, Batch 543, LR 1.593538 Loss 4.944280, Accuracy 89.237%\n",
      "Epoch 23, Batch 544, LR 1.593409 Loss 4.944206, Accuracy 89.235%\n",
      "Epoch 23, Batch 545, LR 1.593281 Loss 4.945286, Accuracy 89.233%\n",
      "Epoch 23, Batch 546, LR 1.593152 Loss 4.943983, Accuracy 89.237%\n",
      "Epoch 23, Batch 547, LR 1.593023 Loss 4.944221, Accuracy 89.235%\n",
      "Epoch 23, Batch 548, LR 1.592894 Loss 4.944838, Accuracy 89.231%\n",
      "Epoch 23, Batch 549, LR 1.592765 Loss 4.945408, Accuracy 89.229%\n",
      "Epoch 23, Batch 550, LR 1.592636 Loss 4.944970, Accuracy 89.227%\n",
      "Epoch 23, Batch 551, LR 1.592508 Loss 4.944522, Accuracy 89.224%\n",
      "Epoch 23, Batch 552, LR 1.592379 Loss 4.944898, Accuracy 89.228%\n",
      "Epoch 23, Batch 553, LR 1.592250 Loss 4.945884, Accuracy 89.225%\n",
      "Epoch 23, Batch 554, LR 1.592121 Loss 4.945040, Accuracy 89.236%\n",
      "Epoch 23, Batch 555, LR 1.591992 Loss 4.943716, Accuracy 89.243%\n",
      "Epoch 23, Batch 556, LR 1.591863 Loss 4.942997, Accuracy 89.249%\n",
      "Epoch 23, Batch 557, LR 1.591735 Loss 4.944130, Accuracy 89.248%\n",
      "Epoch 23, Batch 558, LR 1.591606 Loss 4.944407, Accuracy 89.252%\n",
      "Epoch 23, Batch 559, LR 1.591477 Loss 4.944534, Accuracy 89.257%\n",
      "Epoch 23, Batch 560, LR 1.591348 Loss 4.944142, Accuracy 89.262%\n",
      "Epoch 23, Batch 561, LR 1.591219 Loss 4.944766, Accuracy 89.260%\n",
      "Epoch 23, Batch 562, LR 1.591090 Loss 4.945105, Accuracy 89.253%\n",
      "Epoch 23, Batch 563, LR 1.590961 Loss 4.944407, Accuracy 89.257%\n",
      "Epoch 23, Batch 564, LR 1.590833 Loss 4.944666, Accuracy 89.250%\n",
      "Epoch 23, Batch 565, LR 1.590704 Loss 4.944817, Accuracy 89.249%\n",
      "Epoch 23, Batch 566, LR 1.590575 Loss 4.943346, Accuracy 89.253%\n",
      "Epoch 23, Batch 567, LR 1.590446 Loss 4.943957, Accuracy 89.253%\n",
      "Epoch 23, Batch 568, LR 1.590317 Loss 4.944899, Accuracy 89.245%\n",
      "Epoch 23, Batch 569, LR 1.590188 Loss 4.944432, Accuracy 89.248%\n",
      "Epoch 23, Batch 570, LR 1.590059 Loss 4.945123, Accuracy 89.239%\n",
      "Epoch 23, Batch 571, LR 1.589930 Loss 4.944830, Accuracy 89.235%\n",
      "Epoch 23, Batch 572, LR 1.589801 Loss 4.944193, Accuracy 89.241%\n",
      "Epoch 23, Batch 573, LR 1.589673 Loss 4.943775, Accuracy 89.247%\n",
      "Epoch 23, Batch 574, LR 1.589544 Loss 4.944318, Accuracy 89.244%\n",
      "Epoch 23, Batch 575, LR 1.589415 Loss 4.944665, Accuracy 89.242%\n",
      "Epoch 23, Batch 576, LR 1.589286 Loss 4.944536, Accuracy 89.248%\n",
      "Epoch 23, Batch 577, LR 1.589157 Loss 4.944780, Accuracy 89.245%\n",
      "Epoch 23, Batch 578, LR 1.589028 Loss 4.945892, Accuracy 89.242%\n",
      "Epoch 23, Batch 579, LR 1.588899 Loss 4.946591, Accuracy 89.239%\n",
      "Epoch 23, Batch 580, LR 1.588770 Loss 4.946727, Accuracy 89.240%\n",
      "Epoch 23, Batch 581, LR 1.588641 Loss 4.947507, Accuracy 89.236%\n",
      "Epoch 23, Batch 582, LR 1.588512 Loss 4.947146, Accuracy 89.238%\n",
      "Epoch 23, Batch 583, LR 1.588383 Loss 4.947277, Accuracy 89.241%\n",
      "Epoch 23, Batch 584, LR 1.588254 Loss 4.946771, Accuracy 89.246%\n",
      "Epoch 23, Batch 585, LR 1.588125 Loss 4.946682, Accuracy 89.244%\n",
      "Epoch 23, Batch 586, LR 1.587996 Loss 4.947408, Accuracy 89.242%\n",
      "Epoch 23, Batch 587, LR 1.587867 Loss 4.947077, Accuracy 89.242%\n",
      "Epoch 23, Batch 588, LR 1.587738 Loss 4.947378, Accuracy 89.237%\n",
      "Epoch 23, Batch 589, LR 1.587609 Loss 4.947000, Accuracy 89.231%\n",
      "Epoch 23, Batch 590, LR 1.587480 Loss 4.947390, Accuracy 89.233%\n",
      "Epoch 23, Batch 591, LR 1.587351 Loss 4.946572, Accuracy 89.238%\n",
      "Epoch 23, Batch 592, LR 1.587222 Loss 4.945783, Accuracy 89.242%\n",
      "Epoch 23, Batch 593, LR 1.587093 Loss 4.945503, Accuracy 89.239%\n",
      "Epoch 23, Batch 594, LR 1.586965 Loss 4.946538, Accuracy 89.230%\n",
      "Epoch 23, Batch 595, LR 1.586836 Loss 4.946190, Accuracy 89.228%\n",
      "Epoch 23, Batch 596, LR 1.586707 Loss 4.946699, Accuracy 89.224%\n",
      "Epoch 23, Batch 597, LR 1.586578 Loss 4.947253, Accuracy 89.222%\n",
      "Epoch 23, Batch 598, LR 1.586448 Loss 4.948002, Accuracy 89.214%\n",
      "Epoch 23, Batch 599, LR 1.586319 Loss 4.947557, Accuracy 89.218%\n",
      "Epoch 23, Batch 600, LR 1.586190 Loss 4.948377, Accuracy 89.210%\n",
      "Epoch 23, Batch 601, LR 1.586061 Loss 4.947828, Accuracy 89.208%\n",
      "Epoch 23, Batch 602, LR 1.585932 Loss 4.946884, Accuracy 89.213%\n",
      "Epoch 23, Batch 603, LR 1.585803 Loss 4.947321, Accuracy 89.206%\n",
      "Epoch 23, Batch 604, LR 1.585674 Loss 4.947311, Accuracy 89.205%\n",
      "Epoch 23, Batch 605, LR 1.585545 Loss 4.947644, Accuracy 89.208%\n",
      "Epoch 23, Batch 606, LR 1.585416 Loss 4.947704, Accuracy 89.207%\n",
      "Epoch 23, Batch 607, LR 1.585287 Loss 4.947924, Accuracy 89.202%\n",
      "Epoch 23, Batch 608, LR 1.585158 Loss 4.948189, Accuracy 89.204%\n",
      "Epoch 23, Batch 609, LR 1.585029 Loss 4.948092, Accuracy 89.202%\n",
      "Epoch 23, Batch 610, LR 1.584900 Loss 4.947136, Accuracy 89.210%\n",
      "Epoch 23, Batch 611, LR 1.584771 Loss 4.947061, Accuracy 89.212%\n",
      "Epoch 23, Batch 612, LR 1.584642 Loss 4.946985, Accuracy 89.211%\n",
      "Epoch 23, Batch 613, LR 1.584513 Loss 4.947882, Accuracy 89.208%\n",
      "Epoch 23, Batch 614, LR 1.584384 Loss 4.948448, Accuracy 89.205%\n",
      "Epoch 23, Batch 615, LR 1.584255 Loss 4.948702, Accuracy 89.202%\n",
      "Epoch 23, Batch 616, LR 1.584126 Loss 4.948507, Accuracy 89.210%\n",
      "Epoch 23, Batch 617, LR 1.583997 Loss 4.949070, Accuracy 89.211%\n",
      "Epoch 23, Batch 618, LR 1.583868 Loss 4.949238, Accuracy 89.210%\n",
      "Epoch 23, Batch 619, LR 1.583738 Loss 4.949708, Accuracy 89.208%\n",
      "Epoch 23, Batch 620, LR 1.583609 Loss 4.950161, Accuracy 89.211%\n",
      "Epoch 23, Batch 621, LR 1.583480 Loss 4.950882, Accuracy 89.210%\n",
      "Epoch 23, Batch 622, LR 1.583351 Loss 4.950828, Accuracy 89.207%\n",
      "Epoch 23, Batch 623, LR 1.583222 Loss 4.951151, Accuracy 89.205%\n",
      "Epoch 23, Batch 624, LR 1.583093 Loss 4.951975, Accuracy 89.203%\n",
      "Epoch 23, Batch 625, LR 1.582964 Loss 4.951085, Accuracy 89.207%\n",
      "Epoch 23, Batch 626, LR 1.582835 Loss 4.951225, Accuracy 89.204%\n",
      "Epoch 23, Batch 627, LR 1.582706 Loss 4.952378, Accuracy 89.197%\n",
      "Epoch 23, Batch 628, LR 1.582576 Loss 4.951686, Accuracy 89.203%\n",
      "Epoch 23, Batch 629, LR 1.582447 Loss 4.950967, Accuracy 89.203%\n",
      "Epoch 23, Batch 630, LR 1.582318 Loss 4.951183, Accuracy 89.203%\n",
      "Epoch 23, Batch 631, LR 1.582189 Loss 4.951333, Accuracy 89.199%\n",
      "Epoch 23, Batch 632, LR 1.582060 Loss 4.951290, Accuracy 89.201%\n",
      "Epoch 23, Batch 633, LR 1.581931 Loss 4.950778, Accuracy 89.198%\n",
      "Epoch 23, Batch 634, LR 1.581802 Loss 4.950889, Accuracy 89.194%\n",
      "Epoch 23, Batch 635, LR 1.581673 Loss 4.951292, Accuracy 89.194%\n",
      "Epoch 23, Batch 636, LR 1.581543 Loss 4.951204, Accuracy 89.195%\n",
      "Epoch 23, Batch 637, LR 1.581414 Loss 4.951657, Accuracy 89.195%\n",
      "Epoch 23, Batch 638, LR 1.581285 Loss 4.951318, Accuracy 89.192%\n",
      "Epoch 23, Batch 639, LR 1.581156 Loss 4.951771, Accuracy 89.196%\n",
      "Epoch 23, Batch 640, LR 1.581027 Loss 4.950815, Accuracy 89.197%\n",
      "Epoch 23, Batch 641, LR 1.580898 Loss 4.951934, Accuracy 89.192%\n",
      "Epoch 23, Batch 642, LR 1.580768 Loss 4.951538, Accuracy 89.194%\n",
      "Epoch 23, Batch 643, LR 1.580639 Loss 4.950551, Accuracy 89.199%\n",
      "Epoch 23, Batch 644, LR 1.580510 Loss 4.949547, Accuracy 89.202%\n",
      "Epoch 23, Batch 645, LR 1.580381 Loss 4.948920, Accuracy 89.202%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 646, LR 1.580252 Loss 4.948158, Accuracy 89.205%\n",
      "Epoch 23, Batch 647, LR 1.580122 Loss 4.948601, Accuracy 89.199%\n",
      "Epoch 23, Batch 648, LR 1.579993 Loss 4.948909, Accuracy 89.196%\n",
      "Epoch 23, Batch 649, LR 1.579864 Loss 4.949448, Accuracy 89.193%\n",
      "Epoch 23, Batch 650, LR 1.579735 Loss 4.949818, Accuracy 89.191%\n",
      "Epoch 23, Batch 651, LR 1.579606 Loss 4.951042, Accuracy 89.178%\n",
      "Epoch 23, Batch 652, LR 1.579476 Loss 4.951788, Accuracy 89.180%\n",
      "Epoch 23, Batch 653, LR 1.579347 Loss 4.952048, Accuracy 89.180%\n",
      "Epoch 23, Batch 654, LR 1.579218 Loss 4.952843, Accuracy 89.177%\n",
      "Epoch 23, Batch 655, LR 1.579089 Loss 4.953243, Accuracy 89.176%\n",
      "Epoch 23, Batch 656, LR 1.578959 Loss 4.953293, Accuracy 89.182%\n",
      "Epoch 23, Batch 657, LR 1.578830 Loss 4.952932, Accuracy 89.180%\n",
      "Epoch 23, Batch 658, LR 1.578701 Loss 4.952803, Accuracy 89.179%\n",
      "Epoch 23, Batch 659, LR 1.578572 Loss 4.953350, Accuracy 89.174%\n",
      "Epoch 23, Batch 660, LR 1.578443 Loss 4.954049, Accuracy 89.170%\n",
      "Epoch 23, Batch 661, LR 1.578313 Loss 4.954920, Accuracy 89.172%\n",
      "Epoch 23, Batch 662, LR 1.578184 Loss 4.955244, Accuracy 89.170%\n",
      "Epoch 23, Batch 663, LR 1.578055 Loss 4.954836, Accuracy 89.167%\n",
      "Epoch 23, Batch 664, LR 1.577926 Loss 4.955667, Accuracy 89.164%\n",
      "Epoch 23, Batch 665, LR 1.577796 Loss 4.956426, Accuracy 89.159%\n",
      "Epoch 23, Batch 666, LR 1.577667 Loss 4.957211, Accuracy 89.152%\n",
      "Epoch 23, Batch 667, LR 1.577538 Loss 4.956609, Accuracy 89.155%\n",
      "Epoch 23, Batch 668, LR 1.577408 Loss 4.956585, Accuracy 89.153%\n",
      "Epoch 23, Batch 669, LR 1.577279 Loss 4.955807, Accuracy 89.154%\n",
      "Epoch 23, Batch 670, LR 1.577150 Loss 4.956103, Accuracy 89.153%\n",
      "Epoch 23, Batch 671, LR 1.577021 Loss 4.955673, Accuracy 89.154%\n",
      "Epoch 23, Batch 672, LR 1.576891 Loss 4.955759, Accuracy 89.157%\n",
      "Epoch 23, Batch 673, LR 1.576762 Loss 4.955462, Accuracy 89.153%\n",
      "Epoch 23, Batch 674, LR 1.576633 Loss 4.955411, Accuracy 89.151%\n",
      "Epoch 23, Batch 675, LR 1.576503 Loss 4.954909, Accuracy 89.153%\n",
      "Epoch 23, Batch 676, LR 1.576374 Loss 4.955049, Accuracy 89.151%\n",
      "Epoch 23, Batch 677, LR 1.576245 Loss 4.955623, Accuracy 89.153%\n",
      "Epoch 23, Batch 678, LR 1.576115 Loss 4.955289, Accuracy 89.157%\n",
      "Epoch 23, Batch 679, LR 1.575986 Loss 4.956963, Accuracy 89.153%\n",
      "Epoch 23, Batch 680, LR 1.575857 Loss 4.957267, Accuracy 89.149%\n",
      "Epoch 23, Batch 681, LR 1.575728 Loss 4.957369, Accuracy 89.150%\n",
      "Epoch 23, Batch 682, LR 1.575598 Loss 4.958257, Accuracy 89.144%\n",
      "Epoch 23, Batch 683, LR 1.575469 Loss 4.957880, Accuracy 89.143%\n",
      "Epoch 23, Batch 684, LR 1.575340 Loss 4.957895, Accuracy 89.141%\n",
      "Epoch 23, Batch 685, LR 1.575210 Loss 4.957489, Accuracy 89.146%\n",
      "Epoch 23, Batch 686, LR 1.575081 Loss 4.957909, Accuracy 89.140%\n",
      "Epoch 23, Batch 687, LR 1.574951 Loss 4.957082, Accuracy 89.151%\n",
      "Epoch 23, Batch 688, LR 1.574822 Loss 4.957843, Accuracy 89.150%\n",
      "Epoch 23, Batch 689, LR 1.574693 Loss 4.957062, Accuracy 89.154%\n",
      "Epoch 23, Batch 690, LR 1.574563 Loss 4.955682, Accuracy 89.160%\n",
      "Epoch 23, Batch 691, LR 1.574434 Loss 4.955728, Accuracy 89.159%\n",
      "Epoch 23, Batch 692, LR 1.574305 Loss 4.956480, Accuracy 89.154%\n",
      "Epoch 23, Batch 693, LR 1.574175 Loss 4.957344, Accuracy 89.145%\n",
      "Epoch 23, Batch 694, LR 1.574046 Loss 4.957750, Accuracy 89.148%\n",
      "Epoch 23, Batch 695, LR 1.573917 Loss 4.959116, Accuracy 89.141%\n",
      "Epoch 23, Batch 696, LR 1.573787 Loss 4.958643, Accuracy 89.147%\n",
      "Epoch 23, Batch 697, LR 1.573658 Loss 4.958215, Accuracy 89.148%\n",
      "Epoch 23, Batch 698, LR 1.573528 Loss 4.957638, Accuracy 89.152%\n",
      "Epoch 23, Batch 699, LR 1.573399 Loss 4.957219, Accuracy 89.154%\n",
      "Epoch 23, Batch 700, LR 1.573270 Loss 4.956462, Accuracy 89.166%\n",
      "Epoch 23, Batch 701, LR 1.573140 Loss 4.955952, Accuracy 89.168%\n",
      "Epoch 23, Batch 702, LR 1.573011 Loss 4.956866, Accuracy 89.164%\n",
      "Epoch 23, Batch 703, LR 1.572881 Loss 4.956344, Accuracy 89.170%\n",
      "Epoch 23, Batch 704, LR 1.572752 Loss 4.956409, Accuracy 89.173%\n",
      "Epoch 23, Batch 705, LR 1.572623 Loss 4.956856, Accuracy 89.174%\n",
      "Epoch 23, Batch 706, LR 1.572493 Loss 4.957451, Accuracy 89.171%\n",
      "Epoch 23, Batch 707, LR 1.572364 Loss 4.957768, Accuracy 89.170%\n",
      "Epoch 23, Batch 708, LR 1.572234 Loss 4.957994, Accuracy 89.167%\n",
      "Epoch 23, Batch 709, LR 1.572105 Loss 4.958437, Accuracy 89.163%\n",
      "Epoch 23, Batch 710, LR 1.571976 Loss 4.958008, Accuracy 89.166%\n",
      "Epoch 23, Batch 711, LR 1.571846 Loss 4.958242, Accuracy 89.166%\n",
      "Epoch 23, Batch 712, LR 1.571717 Loss 4.958496, Accuracy 89.169%\n",
      "Epoch 23, Batch 713, LR 1.571587 Loss 4.959027, Accuracy 89.165%\n",
      "Epoch 23, Batch 714, LR 1.571458 Loss 4.958802, Accuracy 89.171%\n",
      "Epoch 23, Batch 715, LR 1.571328 Loss 4.958658, Accuracy 89.167%\n",
      "Epoch 23, Batch 716, LR 1.571199 Loss 4.957125, Accuracy 89.177%\n",
      "Epoch 23, Batch 717, LR 1.571069 Loss 4.957528, Accuracy 89.173%\n",
      "Epoch 23, Batch 718, LR 1.570940 Loss 4.957108, Accuracy 89.172%\n",
      "Epoch 23, Batch 719, LR 1.570810 Loss 4.956877, Accuracy 89.174%\n",
      "Epoch 23, Batch 720, LR 1.570681 Loss 4.956762, Accuracy 89.175%\n",
      "Epoch 23, Batch 721, LR 1.570552 Loss 4.957502, Accuracy 89.173%\n",
      "Epoch 23, Batch 722, LR 1.570422 Loss 4.956936, Accuracy 89.176%\n",
      "Epoch 23, Batch 723, LR 1.570293 Loss 4.957169, Accuracy 89.177%\n",
      "Epoch 23, Batch 724, LR 1.570163 Loss 4.956323, Accuracy 89.183%\n",
      "Epoch 23, Batch 725, LR 1.570034 Loss 4.955227, Accuracy 89.186%\n",
      "Epoch 23, Batch 726, LR 1.569904 Loss 4.955912, Accuracy 89.184%\n",
      "Epoch 23, Batch 727, LR 1.569775 Loss 4.956917, Accuracy 89.182%\n",
      "Epoch 23, Batch 728, LR 1.569645 Loss 4.957048, Accuracy 89.179%\n",
      "Epoch 23, Batch 729, LR 1.569516 Loss 4.957894, Accuracy 89.181%\n",
      "Epoch 23, Batch 730, LR 1.569386 Loss 4.958919, Accuracy 89.172%\n",
      "Epoch 23, Batch 731, LR 1.569257 Loss 4.958657, Accuracy 89.172%\n",
      "Epoch 23, Batch 732, LR 1.569127 Loss 4.958495, Accuracy 89.177%\n",
      "Epoch 23, Batch 733, LR 1.568998 Loss 4.958105, Accuracy 89.174%\n",
      "Epoch 23, Batch 734, LR 1.568868 Loss 4.957675, Accuracy 89.176%\n",
      "Epoch 23, Batch 735, LR 1.568739 Loss 4.957480, Accuracy 89.178%\n",
      "Epoch 23, Batch 736, LR 1.568609 Loss 4.956338, Accuracy 89.188%\n",
      "Epoch 23, Batch 737, LR 1.568479 Loss 4.956029, Accuracy 89.193%\n",
      "Epoch 23, Batch 738, LR 1.568350 Loss 4.955639, Accuracy 89.197%\n",
      "Epoch 23, Batch 739, LR 1.568220 Loss 4.955999, Accuracy 89.193%\n",
      "Epoch 23, Batch 740, LR 1.568091 Loss 4.955834, Accuracy 89.193%\n",
      "Epoch 23, Batch 741, LR 1.567961 Loss 4.956226, Accuracy 89.193%\n",
      "Epoch 23, Batch 742, LR 1.567832 Loss 4.955385, Accuracy 89.194%\n",
      "Epoch 23, Batch 743, LR 1.567702 Loss 4.955397, Accuracy 89.190%\n",
      "Epoch 23, Batch 744, LR 1.567573 Loss 4.956484, Accuracy 89.183%\n",
      "Epoch 23, Batch 745, LR 1.567443 Loss 4.955660, Accuracy 89.189%\n",
      "Epoch 23, Batch 746, LR 1.567314 Loss 4.955406, Accuracy 89.191%\n",
      "Epoch 23, Batch 747, LR 1.567184 Loss 4.955102, Accuracy 89.194%\n",
      "Epoch 23, Batch 748, LR 1.567054 Loss 4.954155, Accuracy 89.200%\n",
      "Epoch 23, Batch 749, LR 1.566925 Loss 4.953616, Accuracy 89.206%\n",
      "Epoch 23, Batch 750, LR 1.566795 Loss 4.953909, Accuracy 89.203%\n",
      "Epoch 23, Batch 751, LR 1.566666 Loss 4.954064, Accuracy 89.200%\n",
      "Epoch 23, Batch 752, LR 1.566536 Loss 4.954659, Accuracy 89.200%\n",
      "Epoch 23, Batch 753, LR 1.566406 Loss 4.955384, Accuracy 89.196%\n",
      "Epoch 23, Batch 754, LR 1.566277 Loss 4.955419, Accuracy 89.196%\n",
      "Epoch 23, Batch 755, LR 1.566147 Loss 4.954863, Accuracy 89.198%\n",
      "Epoch 23, Batch 756, LR 1.566018 Loss 4.955377, Accuracy 89.199%\n",
      "Epoch 23, Batch 757, LR 1.565888 Loss 4.955078, Accuracy 89.200%\n",
      "Epoch 23, Batch 758, LR 1.565758 Loss 4.955060, Accuracy 89.194%\n",
      "Epoch 23, Batch 759, LR 1.565629 Loss 4.954182, Accuracy 89.197%\n",
      "Epoch 23, Batch 760, LR 1.565499 Loss 4.954987, Accuracy 89.194%\n",
      "Epoch 23, Batch 761, LR 1.565370 Loss 4.954133, Accuracy 89.198%\n",
      "Epoch 23, Batch 762, LR 1.565240 Loss 4.955258, Accuracy 89.194%\n",
      "Epoch 23, Batch 763, LR 1.565110 Loss 4.954993, Accuracy 89.195%\n",
      "Epoch 23, Batch 764, LR 1.564981 Loss 4.955472, Accuracy 89.192%\n",
      "Epoch 23, Batch 765, LR 1.564851 Loss 4.955578, Accuracy 89.193%\n",
      "Epoch 23, Batch 766, LR 1.564721 Loss 4.955636, Accuracy 89.191%\n",
      "Epoch 23, Batch 767, LR 1.564592 Loss 4.955260, Accuracy 89.190%\n",
      "Epoch 23, Batch 768, LR 1.564462 Loss 4.954239, Accuracy 89.192%\n",
      "Epoch 23, Batch 769, LR 1.564333 Loss 4.955063, Accuracy 89.187%\n",
      "Epoch 23, Batch 770, LR 1.564203 Loss 4.955365, Accuracy 89.187%\n",
      "Epoch 23, Batch 771, LR 1.564073 Loss 4.955028, Accuracy 89.192%\n",
      "Epoch 23, Batch 772, LR 1.563944 Loss 4.954133, Accuracy 89.196%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 773, LR 1.563814 Loss 4.954454, Accuracy 89.197%\n",
      "Epoch 23, Batch 774, LR 1.563684 Loss 4.954678, Accuracy 89.195%\n",
      "Epoch 23, Batch 775, LR 1.563555 Loss 4.954236, Accuracy 89.198%\n",
      "Epoch 23, Batch 776, LR 1.563425 Loss 4.953820, Accuracy 89.200%\n",
      "Epoch 23, Batch 777, LR 1.563295 Loss 4.953565, Accuracy 89.202%\n",
      "Epoch 23, Batch 778, LR 1.563166 Loss 4.953712, Accuracy 89.200%\n",
      "Epoch 23, Batch 779, LR 1.563036 Loss 4.953709, Accuracy 89.199%\n",
      "Epoch 23, Batch 780, LR 1.562906 Loss 4.954648, Accuracy 89.193%\n",
      "Epoch 23, Batch 781, LR 1.562777 Loss 4.954945, Accuracy 89.190%\n",
      "Epoch 23, Batch 782, LR 1.562647 Loss 4.954782, Accuracy 89.188%\n",
      "Epoch 23, Batch 783, LR 1.562517 Loss 4.954676, Accuracy 89.192%\n",
      "Epoch 23, Batch 784, LR 1.562387 Loss 4.954594, Accuracy 89.193%\n",
      "Epoch 23, Batch 785, LR 1.562258 Loss 4.954391, Accuracy 89.192%\n",
      "Epoch 23, Batch 786, LR 1.562128 Loss 4.953761, Accuracy 89.195%\n",
      "Epoch 23, Batch 787, LR 1.561998 Loss 4.954150, Accuracy 89.195%\n",
      "Epoch 23, Batch 788, LR 1.561869 Loss 4.953764, Accuracy 89.195%\n",
      "Epoch 23, Batch 789, LR 1.561739 Loss 4.953038, Accuracy 89.196%\n",
      "Epoch 23, Batch 790, LR 1.561609 Loss 4.952821, Accuracy 89.195%\n",
      "Epoch 23, Batch 791, LR 1.561479 Loss 4.952536, Accuracy 89.197%\n",
      "Epoch 23, Batch 792, LR 1.561350 Loss 4.952446, Accuracy 89.195%\n",
      "Epoch 23, Batch 793, LR 1.561220 Loss 4.951617, Accuracy 89.197%\n",
      "Epoch 23, Batch 794, LR 1.561090 Loss 4.951053, Accuracy 89.196%\n",
      "Epoch 23, Batch 795, LR 1.560960 Loss 4.951140, Accuracy 89.194%\n",
      "Epoch 23, Batch 796, LR 1.560831 Loss 4.951465, Accuracy 89.191%\n",
      "Epoch 23, Batch 797, LR 1.560701 Loss 4.951583, Accuracy 89.190%\n",
      "Epoch 23, Batch 798, LR 1.560571 Loss 4.951561, Accuracy 89.190%\n",
      "Epoch 23, Batch 799, LR 1.560441 Loss 4.951496, Accuracy 89.189%\n",
      "Epoch 23, Batch 800, LR 1.560312 Loss 4.951783, Accuracy 89.191%\n",
      "Epoch 23, Batch 801, LR 1.560182 Loss 4.951277, Accuracy 89.195%\n",
      "Epoch 23, Batch 802, LR 1.560052 Loss 4.950232, Accuracy 89.200%\n",
      "Epoch 23, Batch 803, LR 1.559922 Loss 4.950004, Accuracy 89.202%\n",
      "Epoch 23, Batch 804, LR 1.559793 Loss 4.949470, Accuracy 89.201%\n",
      "Epoch 23, Batch 805, LR 1.559663 Loss 4.949186, Accuracy 89.207%\n",
      "Epoch 23, Batch 806, LR 1.559533 Loss 4.948920, Accuracy 89.208%\n",
      "Epoch 23, Batch 807, LR 1.559403 Loss 4.948551, Accuracy 89.207%\n",
      "Epoch 23, Batch 808, LR 1.559274 Loss 4.949755, Accuracy 89.199%\n",
      "Epoch 23, Batch 809, LR 1.559144 Loss 4.950260, Accuracy 89.194%\n",
      "Epoch 23, Batch 810, LR 1.559014 Loss 4.950542, Accuracy 89.196%\n",
      "Epoch 23, Batch 811, LR 1.558884 Loss 4.950458, Accuracy 89.201%\n",
      "Epoch 23, Batch 812, LR 1.558754 Loss 4.950675, Accuracy 89.198%\n",
      "Epoch 23, Batch 813, LR 1.558625 Loss 4.950945, Accuracy 89.194%\n",
      "Epoch 23, Batch 814, LR 1.558495 Loss 4.950976, Accuracy 89.196%\n",
      "Epoch 23, Batch 815, LR 1.558365 Loss 4.950955, Accuracy 89.199%\n",
      "Epoch 23, Batch 816, LR 1.558235 Loss 4.950533, Accuracy 89.200%\n",
      "Epoch 23, Batch 817, LR 1.558105 Loss 4.949536, Accuracy 89.203%\n",
      "Epoch 23, Batch 818, LR 1.557975 Loss 4.949895, Accuracy 89.199%\n",
      "Epoch 23, Batch 819, LR 1.557846 Loss 4.949563, Accuracy 89.204%\n",
      "Epoch 23, Batch 820, LR 1.557716 Loss 4.949882, Accuracy 89.202%\n",
      "Epoch 23, Batch 821, LR 1.557586 Loss 4.950217, Accuracy 89.200%\n",
      "Epoch 23, Batch 822, LR 1.557456 Loss 4.950432, Accuracy 89.201%\n",
      "Epoch 23, Batch 823, LR 1.557326 Loss 4.950867, Accuracy 89.195%\n",
      "Epoch 23, Batch 824, LR 1.557196 Loss 4.950570, Accuracy 89.198%\n",
      "Epoch 23, Batch 825, LR 1.557067 Loss 4.950911, Accuracy 89.196%\n",
      "Epoch 23, Batch 826, LR 1.556937 Loss 4.950792, Accuracy 89.199%\n",
      "Epoch 23, Batch 827, LR 1.556807 Loss 4.951796, Accuracy 89.193%\n",
      "Epoch 23, Batch 828, LR 1.556677 Loss 4.951062, Accuracy 89.199%\n",
      "Epoch 23, Batch 829, LR 1.556547 Loss 4.950702, Accuracy 89.199%\n",
      "Epoch 23, Batch 830, LR 1.556417 Loss 4.950615, Accuracy 89.200%\n",
      "Epoch 23, Batch 831, LR 1.556287 Loss 4.951141, Accuracy 89.199%\n",
      "Epoch 23, Batch 832, LR 1.556158 Loss 4.951269, Accuracy 89.200%\n",
      "Epoch 23, Batch 833, LR 1.556028 Loss 4.950744, Accuracy 89.206%\n",
      "Epoch 23, Batch 834, LR 1.555898 Loss 4.950576, Accuracy 89.208%\n",
      "Epoch 23, Batch 835, LR 1.555768 Loss 4.950133, Accuracy 89.212%\n",
      "Epoch 23, Batch 836, LR 1.555638 Loss 4.949806, Accuracy 89.211%\n",
      "Epoch 23, Batch 837, LR 1.555508 Loss 4.950450, Accuracy 89.208%\n",
      "Epoch 23, Batch 838, LR 1.555378 Loss 4.949623, Accuracy 89.209%\n",
      "Epoch 23, Batch 839, LR 1.555248 Loss 4.950586, Accuracy 89.205%\n",
      "Epoch 23, Batch 840, LR 1.555118 Loss 4.950854, Accuracy 89.200%\n",
      "Epoch 23, Batch 841, LR 1.554989 Loss 4.951650, Accuracy 89.191%\n",
      "Epoch 23, Batch 842, LR 1.554859 Loss 4.951168, Accuracy 89.189%\n",
      "Epoch 23, Batch 843, LR 1.554729 Loss 4.951580, Accuracy 89.185%\n",
      "Epoch 23, Batch 844, LR 1.554599 Loss 4.951519, Accuracy 89.184%\n",
      "Epoch 23, Batch 845, LR 1.554469 Loss 4.950289, Accuracy 89.186%\n",
      "Epoch 23, Batch 846, LR 1.554339 Loss 4.950097, Accuracy 89.183%\n",
      "Epoch 23, Batch 847, LR 1.554209 Loss 4.949438, Accuracy 89.187%\n",
      "Epoch 23, Batch 848, LR 1.554079 Loss 4.949273, Accuracy 89.189%\n",
      "Epoch 23, Batch 849, LR 1.553949 Loss 4.948888, Accuracy 89.189%\n",
      "Epoch 23, Batch 850, LR 1.553819 Loss 4.948927, Accuracy 89.181%\n",
      "Epoch 23, Batch 851, LR 1.553689 Loss 4.948854, Accuracy 89.179%\n",
      "Epoch 23, Batch 852, LR 1.553559 Loss 4.948393, Accuracy 89.179%\n",
      "Epoch 23, Batch 853, LR 1.553429 Loss 4.948110, Accuracy 89.176%\n",
      "Epoch 23, Batch 854, LR 1.553300 Loss 4.948244, Accuracy 89.174%\n",
      "Epoch 23, Batch 855, LR 1.553170 Loss 4.948587, Accuracy 89.173%\n",
      "Epoch 23, Batch 856, LR 1.553040 Loss 4.948279, Accuracy 89.172%\n",
      "Epoch 23, Batch 857, LR 1.552910 Loss 4.947778, Accuracy 89.176%\n",
      "Epoch 23, Batch 858, LR 1.552780 Loss 4.947934, Accuracy 89.174%\n",
      "Epoch 23, Batch 859, LR 1.552650 Loss 4.948240, Accuracy 89.175%\n",
      "Epoch 23, Batch 860, LR 1.552520 Loss 4.948146, Accuracy 89.175%\n",
      "Epoch 23, Batch 861, LR 1.552390 Loss 4.948764, Accuracy 89.170%\n",
      "Epoch 23, Batch 862, LR 1.552260 Loss 4.949807, Accuracy 89.164%\n",
      "Epoch 23, Batch 863, LR 1.552130 Loss 4.949578, Accuracy 89.162%\n",
      "Epoch 23, Batch 864, LR 1.552000 Loss 4.950469, Accuracy 89.163%\n",
      "Epoch 23, Batch 865, LR 1.551870 Loss 4.949782, Accuracy 89.166%\n",
      "Epoch 23, Batch 866, LR 1.551740 Loss 4.950719, Accuracy 89.162%\n",
      "Epoch 23, Batch 867, LR 1.551610 Loss 4.951453, Accuracy 89.159%\n",
      "Epoch 23, Batch 868, LR 1.551480 Loss 4.951195, Accuracy 89.160%\n",
      "Epoch 23, Batch 869, LR 1.551350 Loss 4.950564, Accuracy 89.167%\n",
      "Epoch 23, Batch 870, LR 1.551220 Loss 4.950499, Accuracy 89.162%\n",
      "Epoch 23, Batch 871, LR 1.551090 Loss 4.950161, Accuracy 89.167%\n",
      "Epoch 23, Batch 872, LR 1.550960 Loss 4.950029, Accuracy 89.169%\n",
      "Epoch 23, Batch 873, LR 1.550830 Loss 4.950157, Accuracy 89.169%\n",
      "Epoch 23, Batch 874, LR 1.550700 Loss 4.950578, Accuracy 89.168%\n",
      "Epoch 23, Batch 875, LR 1.550570 Loss 4.950064, Accuracy 89.172%\n",
      "Epoch 23, Batch 876, LR 1.550440 Loss 4.949980, Accuracy 89.173%\n",
      "Epoch 23, Batch 877, LR 1.550310 Loss 4.950229, Accuracy 89.170%\n",
      "Epoch 23, Batch 878, LR 1.550180 Loss 4.950007, Accuracy 89.171%\n",
      "Epoch 23, Batch 879, LR 1.550050 Loss 4.950792, Accuracy 89.171%\n",
      "Epoch 23, Batch 880, LR 1.549920 Loss 4.950637, Accuracy 89.169%\n",
      "Epoch 23, Batch 881, LR 1.549790 Loss 4.949963, Accuracy 89.172%\n",
      "Epoch 23, Batch 882, LR 1.549660 Loss 4.949402, Accuracy 89.174%\n",
      "Epoch 23, Batch 883, LR 1.549529 Loss 4.949964, Accuracy 89.166%\n",
      "Epoch 23, Batch 884, LR 1.549399 Loss 4.950524, Accuracy 89.162%\n",
      "Epoch 23, Batch 885, LR 1.549269 Loss 4.951149, Accuracy 89.160%\n",
      "Epoch 23, Batch 886, LR 1.549139 Loss 4.951281, Accuracy 89.158%\n",
      "Epoch 23, Batch 887, LR 1.549009 Loss 4.951812, Accuracy 89.154%\n",
      "Epoch 23, Batch 888, LR 1.548879 Loss 4.952266, Accuracy 89.154%\n",
      "Epoch 23, Batch 889, LR 1.548749 Loss 4.952772, Accuracy 89.155%\n",
      "Epoch 23, Batch 890, LR 1.548619 Loss 4.953475, Accuracy 89.150%\n",
      "Epoch 23, Batch 891, LR 1.548489 Loss 4.953133, Accuracy 89.150%\n",
      "Epoch 23, Batch 892, LR 1.548359 Loss 4.952954, Accuracy 89.152%\n",
      "Epoch 23, Batch 893, LR 1.548229 Loss 4.953083, Accuracy 89.152%\n",
      "Epoch 23, Batch 894, LR 1.548099 Loss 4.952575, Accuracy 89.154%\n",
      "Epoch 23, Batch 895, LR 1.547969 Loss 4.952669, Accuracy 89.154%\n",
      "Epoch 23, Batch 896, LR 1.547839 Loss 4.953338, Accuracy 89.151%\n",
      "Epoch 23, Batch 897, LR 1.547708 Loss 4.953694, Accuracy 89.149%\n",
      "Epoch 23, Batch 898, LR 1.547578 Loss 4.954332, Accuracy 89.146%\n",
      "Epoch 23, Batch 899, LR 1.547448 Loss 4.954412, Accuracy 89.142%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 900, LR 1.547318 Loss 4.954006, Accuracy 89.141%\n",
      "Epoch 23, Batch 901, LR 1.547188 Loss 4.953851, Accuracy 89.144%\n",
      "Epoch 23, Batch 902, LR 1.547058 Loss 4.954546, Accuracy 89.140%\n",
      "Epoch 23, Batch 903, LR 1.546928 Loss 4.954613, Accuracy 89.135%\n",
      "Epoch 23, Batch 904, LR 1.546798 Loss 4.953875, Accuracy 89.137%\n",
      "Epoch 23, Batch 905, LR 1.546668 Loss 4.954166, Accuracy 89.131%\n",
      "Epoch 23, Batch 906, LR 1.546537 Loss 4.954099, Accuracy 89.134%\n",
      "Epoch 23, Batch 907, LR 1.546407 Loss 4.954880, Accuracy 89.127%\n",
      "Epoch 23, Batch 908, LR 1.546277 Loss 4.954729, Accuracy 89.128%\n",
      "Epoch 23, Batch 909, LR 1.546147 Loss 4.955195, Accuracy 89.130%\n",
      "Epoch 23, Batch 910, LR 1.546017 Loss 4.955477, Accuracy 89.127%\n",
      "Epoch 23, Batch 911, LR 1.545887 Loss 4.955495, Accuracy 89.124%\n",
      "Epoch 23, Batch 912, LR 1.545757 Loss 4.954926, Accuracy 89.128%\n",
      "Epoch 23, Batch 913, LR 1.545626 Loss 4.954515, Accuracy 89.129%\n",
      "Epoch 23, Batch 914, LR 1.545496 Loss 4.955592, Accuracy 89.121%\n",
      "Epoch 23, Batch 915, LR 1.545366 Loss 4.955412, Accuracy 89.121%\n",
      "Epoch 23, Batch 916, LR 1.545236 Loss 4.955505, Accuracy 89.120%\n",
      "Epoch 23, Batch 917, LR 1.545106 Loss 4.956181, Accuracy 89.117%\n",
      "Epoch 23, Batch 918, LR 1.544976 Loss 4.956982, Accuracy 89.118%\n",
      "Epoch 23, Batch 919, LR 1.544845 Loss 4.956894, Accuracy 89.119%\n",
      "Epoch 23, Batch 920, LR 1.544715 Loss 4.957562, Accuracy 89.120%\n",
      "Epoch 23, Batch 921, LR 1.544585 Loss 4.957237, Accuracy 89.123%\n",
      "Epoch 23, Batch 922, LR 1.544455 Loss 4.957778, Accuracy 89.121%\n",
      "Epoch 23, Batch 923, LR 1.544325 Loss 4.957905, Accuracy 89.120%\n",
      "Epoch 23, Batch 924, LR 1.544195 Loss 4.958198, Accuracy 89.117%\n",
      "Epoch 23, Batch 925, LR 1.544064 Loss 4.958126, Accuracy 89.118%\n",
      "Epoch 23, Batch 926, LR 1.543934 Loss 4.958315, Accuracy 89.121%\n",
      "Epoch 23, Batch 927, LR 1.543804 Loss 4.958106, Accuracy 89.122%\n",
      "Epoch 23, Batch 928, LR 1.543674 Loss 4.957672, Accuracy 89.126%\n",
      "Epoch 23, Batch 929, LR 1.543544 Loss 4.958386, Accuracy 89.124%\n",
      "Epoch 23, Batch 930, LR 1.543413 Loss 4.958429, Accuracy 89.126%\n",
      "Epoch 23, Batch 931, LR 1.543283 Loss 4.958352, Accuracy 89.126%\n",
      "Epoch 23, Batch 932, LR 1.543153 Loss 4.957698, Accuracy 89.130%\n",
      "Epoch 23, Batch 933, LR 1.543023 Loss 4.957239, Accuracy 89.129%\n",
      "Epoch 23, Batch 934, LR 1.542892 Loss 4.957442, Accuracy 89.129%\n",
      "Epoch 23, Batch 935, LR 1.542762 Loss 4.957151, Accuracy 89.131%\n",
      "Epoch 23, Batch 936, LR 1.542632 Loss 4.956999, Accuracy 89.131%\n",
      "Epoch 23, Batch 937, LR 1.542502 Loss 4.956740, Accuracy 89.128%\n",
      "Epoch 23, Batch 938, LR 1.542372 Loss 4.956583, Accuracy 89.130%\n",
      "Epoch 23, Batch 939, LR 1.542241 Loss 4.957170, Accuracy 89.127%\n",
      "Epoch 23, Batch 940, LR 1.542111 Loss 4.957103, Accuracy 89.129%\n",
      "Epoch 23, Batch 941, LR 1.541981 Loss 4.956987, Accuracy 89.131%\n",
      "Epoch 23, Batch 942, LR 1.541851 Loss 4.957212, Accuracy 89.129%\n",
      "Epoch 23, Batch 943, LR 1.541720 Loss 4.957518, Accuracy 89.129%\n",
      "Epoch 23, Batch 944, LR 1.541590 Loss 4.957439, Accuracy 89.131%\n",
      "Epoch 23, Batch 945, LR 1.541460 Loss 4.958018, Accuracy 89.129%\n",
      "Epoch 23, Batch 946, LR 1.541330 Loss 4.958340, Accuracy 89.128%\n",
      "Epoch 23, Batch 947, LR 1.541199 Loss 4.957938, Accuracy 89.131%\n",
      "Epoch 23, Batch 948, LR 1.541069 Loss 4.958447, Accuracy 89.130%\n",
      "Epoch 23, Batch 949, LR 1.540939 Loss 4.958288, Accuracy 89.132%\n",
      "Epoch 23, Batch 950, LR 1.540808 Loss 4.958249, Accuracy 89.136%\n",
      "Epoch 23, Batch 951, LR 1.540678 Loss 4.958029, Accuracy 89.132%\n",
      "Epoch 23, Batch 952, LR 1.540548 Loss 4.957529, Accuracy 89.132%\n",
      "Epoch 23, Batch 953, LR 1.540418 Loss 4.957748, Accuracy 89.131%\n",
      "Epoch 23, Batch 954, LR 1.540287 Loss 4.958193, Accuracy 89.128%\n",
      "Epoch 23, Batch 955, LR 1.540157 Loss 4.958070, Accuracy 89.128%\n",
      "Epoch 23, Batch 956, LR 1.540027 Loss 4.957350, Accuracy 89.133%\n",
      "Epoch 23, Batch 957, LR 1.539896 Loss 4.957872, Accuracy 89.134%\n",
      "Epoch 23, Batch 958, LR 1.539766 Loss 4.957223, Accuracy 89.141%\n",
      "Epoch 23, Batch 959, LR 1.539636 Loss 4.956797, Accuracy 89.146%\n",
      "Epoch 23, Batch 960, LR 1.539506 Loss 4.956705, Accuracy 89.146%\n",
      "Epoch 23, Batch 961, LR 1.539375 Loss 4.956927, Accuracy 89.144%\n",
      "Epoch 23, Batch 962, LR 1.539245 Loss 4.956732, Accuracy 89.145%\n",
      "Epoch 23, Batch 963, LR 1.539115 Loss 4.957378, Accuracy 89.143%\n",
      "Epoch 23, Batch 964, LR 1.538984 Loss 4.957202, Accuracy 89.140%\n",
      "Epoch 23, Batch 965, LR 1.538854 Loss 4.957279, Accuracy 89.139%\n",
      "Epoch 23, Batch 966, LR 1.538724 Loss 4.956820, Accuracy 89.143%\n",
      "Epoch 23, Batch 967, LR 1.538593 Loss 4.957136, Accuracy 89.142%\n",
      "Epoch 23, Batch 968, LR 1.538463 Loss 4.957319, Accuracy 89.138%\n",
      "Epoch 23, Batch 969, LR 1.538333 Loss 4.957373, Accuracy 89.142%\n",
      "Epoch 23, Batch 970, LR 1.538202 Loss 4.957177, Accuracy 89.145%\n",
      "Epoch 23, Batch 971, LR 1.538072 Loss 4.957190, Accuracy 89.146%\n",
      "Epoch 23, Batch 972, LR 1.537942 Loss 4.957279, Accuracy 89.144%\n",
      "Epoch 23, Batch 973, LR 1.537811 Loss 4.958113, Accuracy 89.140%\n",
      "Epoch 23, Batch 974, LR 1.537681 Loss 4.958290, Accuracy 89.141%\n",
      "Epoch 23, Batch 975, LR 1.537550 Loss 4.958534, Accuracy 89.142%\n",
      "Epoch 23, Batch 976, LR 1.537420 Loss 4.958606, Accuracy 89.145%\n",
      "Epoch 23, Batch 977, LR 1.537290 Loss 4.959340, Accuracy 89.146%\n",
      "Epoch 23, Batch 978, LR 1.537159 Loss 4.959235, Accuracy 89.146%\n",
      "Epoch 23, Batch 979, LR 1.537029 Loss 4.959634, Accuracy 89.147%\n",
      "Epoch 23, Batch 980, LR 1.536899 Loss 4.959836, Accuracy 89.146%\n",
      "Epoch 23, Batch 981, LR 1.536768 Loss 4.959254, Accuracy 89.150%\n",
      "Epoch 23, Batch 982, LR 1.536638 Loss 4.958392, Accuracy 89.152%\n",
      "Epoch 23, Batch 983, LR 1.536507 Loss 4.958452, Accuracy 89.154%\n",
      "Epoch 23, Batch 984, LR 1.536377 Loss 4.959277, Accuracy 89.150%\n",
      "Epoch 23, Batch 985, LR 1.536247 Loss 4.959288, Accuracy 89.151%\n",
      "Epoch 23, Batch 986, LR 1.536116 Loss 4.959100, Accuracy 89.152%\n",
      "Epoch 23, Batch 987, LR 1.535986 Loss 4.959065, Accuracy 89.155%\n",
      "Epoch 23, Batch 988, LR 1.535856 Loss 4.959556, Accuracy 89.151%\n",
      "Epoch 23, Batch 989, LR 1.535725 Loss 4.959622, Accuracy 89.151%\n",
      "Epoch 23, Batch 990, LR 1.535595 Loss 4.959687, Accuracy 89.148%\n",
      "Epoch 23, Batch 991, LR 1.535464 Loss 4.959873, Accuracy 89.151%\n",
      "Epoch 23, Batch 992, LR 1.535334 Loss 4.959694, Accuracy 89.151%\n",
      "Epoch 23, Batch 993, LR 1.535203 Loss 4.958928, Accuracy 89.153%\n",
      "Epoch 23, Batch 994, LR 1.535073 Loss 4.959331, Accuracy 89.150%\n",
      "Epoch 23, Batch 995, LR 1.534943 Loss 4.959613, Accuracy 89.150%\n",
      "Epoch 23, Batch 996, LR 1.534812 Loss 4.958832, Accuracy 89.152%\n",
      "Epoch 23, Batch 997, LR 1.534682 Loss 4.959383, Accuracy 89.149%\n",
      "Epoch 23, Batch 998, LR 1.534551 Loss 4.959479, Accuracy 89.147%\n",
      "Epoch 23, Batch 999, LR 1.534421 Loss 4.960167, Accuracy 89.145%\n",
      "Epoch 23, Batch 1000, LR 1.534290 Loss 4.959999, Accuracy 89.146%\n",
      "Epoch 23, Batch 1001, LR 1.534160 Loss 4.960505, Accuracy 89.143%\n",
      "Epoch 23, Batch 1002, LR 1.534030 Loss 4.961054, Accuracy 89.141%\n",
      "Epoch 23, Batch 1003, LR 1.533899 Loss 4.960673, Accuracy 89.142%\n",
      "Epoch 23, Batch 1004, LR 1.533769 Loss 4.960426, Accuracy 89.142%\n",
      "Epoch 23, Batch 1005, LR 1.533638 Loss 4.960845, Accuracy 89.142%\n",
      "Epoch 23, Batch 1006, LR 1.533508 Loss 4.960767, Accuracy 89.142%\n",
      "Epoch 23, Batch 1007, LR 1.533377 Loss 4.960877, Accuracy 89.142%\n",
      "Epoch 23, Batch 1008, LR 1.533247 Loss 4.960197, Accuracy 89.146%\n",
      "Epoch 23, Batch 1009, LR 1.533116 Loss 4.960721, Accuracy 89.145%\n",
      "Epoch 23, Batch 1010, LR 1.532986 Loss 4.960966, Accuracy 89.144%\n",
      "Epoch 23, Batch 1011, LR 1.532855 Loss 4.960199, Accuracy 89.149%\n",
      "Epoch 23, Batch 1012, LR 1.532725 Loss 4.959870, Accuracy 89.150%\n",
      "Epoch 23, Batch 1013, LR 1.532594 Loss 4.959599, Accuracy 89.153%\n",
      "Epoch 23, Batch 1014, LR 1.532464 Loss 4.959089, Accuracy 89.159%\n",
      "Epoch 23, Batch 1015, LR 1.532333 Loss 4.959637, Accuracy 89.158%\n",
      "Epoch 23, Batch 1016, LR 1.532203 Loss 4.959414, Accuracy 89.156%\n",
      "Epoch 23, Batch 1017, LR 1.532072 Loss 4.959332, Accuracy 89.156%\n",
      "Epoch 23, Batch 1018, LR 1.531942 Loss 4.959380, Accuracy 89.155%\n",
      "Epoch 23, Batch 1019, LR 1.531811 Loss 4.959541, Accuracy 89.152%\n",
      "Epoch 23, Batch 1020, LR 1.531681 Loss 4.959550, Accuracy 89.152%\n",
      "Epoch 23, Batch 1021, LR 1.531550 Loss 4.959724, Accuracy 89.149%\n",
      "Epoch 23, Batch 1022, LR 1.531420 Loss 4.959448, Accuracy 89.152%\n",
      "Epoch 23, Batch 1023, LR 1.531289 Loss 4.959729, Accuracy 89.154%\n",
      "Epoch 23, Batch 1024, LR 1.531159 Loss 4.959935, Accuracy 89.155%\n",
      "Epoch 23, Batch 1025, LR 1.531028 Loss 4.960310, Accuracy 89.152%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 1026, LR 1.530898 Loss 4.960361, Accuracy 89.151%\n",
      "Epoch 23, Batch 1027, LR 1.530767 Loss 4.961230, Accuracy 89.150%\n",
      "Epoch 23, Batch 1028, LR 1.530637 Loss 4.960949, Accuracy 89.151%\n",
      "Epoch 23, Batch 1029, LR 1.530506 Loss 4.960849, Accuracy 89.151%\n",
      "Epoch 23, Batch 1030, LR 1.530376 Loss 4.961145, Accuracy 89.147%\n",
      "Epoch 23, Batch 1031, LR 1.530245 Loss 4.960999, Accuracy 89.150%\n",
      "Epoch 23, Batch 1032, LR 1.530115 Loss 4.961315, Accuracy 89.147%\n",
      "Epoch 23, Batch 1033, LR 1.529984 Loss 4.962015, Accuracy 89.145%\n",
      "Epoch 23, Batch 1034, LR 1.529853 Loss 4.961836, Accuracy 89.146%\n",
      "Epoch 23, Batch 1035, LR 1.529723 Loss 4.962067, Accuracy 89.144%\n",
      "Epoch 23, Batch 1036, LR 1.529592 Loss 4.962750, Accuracy 89.142%\n",
      "Epoch 23, Batch 1037, LR 1.529462 Loss 4.962351, Accuracy 89.145%\n",
      "Epoch 23, Batch 1038, LR 1.529331 Loss 4.962146, Accuracy 89.145%\n",
      "Epoch 23, Batch 1039, LR 1.529201 Loss 4.962487, Accuracy 89.140%\n",
      "Epoch 23, Batch 1040, LR 1.529070 Loss 4.963144, Accuracy 89.137%\n",
      "Epoch 23, Batch 1041, LR 1.528940 Loss 4.962988, Accuracy 89.136%\n",
      "Epoch 23, Batch 1042, LR 1.528809 Loss 4.963612, Accuracy 89.133%\n",
      "Epoch 23, Batch 1043, LR 1.528678 Loss 4.964218, Accuracy 89.128%\n",
      "Epoch 23, Batch 1044, LR 1.528548 Loss 4.963398, Accuracy 89.132%\n",
      "Epoch 23, Batch 1045, LR 1.528417 Loss 4.963213, Accuracy 89.134%\n",
      "Epoch 23, Batch 1046, LR 1.528287 Loss 4.963265, Accuracy 89.136%\n",
      "Epoch 23, Batch 1047, LR 1.528156 Loss 4.964178, Accuracy 89.129%\n",
      "Epoch 23, Loss (train set) 4.964178, Accuracy (train set) 89.129%\n",
      "Epoch 24, Batch 1, LR 1.528025 Loss 4.676056, Accuracy 90.625%\n",
      "Epoch 24, Batch 2, LR 1.527895 Loss 4.559509, Accuracy 89.844%\n",
      "Epoch 24, Batch 3, LR 1.527764 Loss 4.626340, Accuracy 90.625%\n",
      "Epoch 24, Batch 4, LR 1.527634 Loss 4.842804, Accuracy 89.648%\n",
      "Epoch 24, Batch 5, LR 1.527503 Loss 4.780937, Accuracy 90.156%\n",
      "Epoch 24, Batch 6, LR 1.527372 Loss 4.812083, Accuracy 89.714%\n",
      "Epoch 24, Batch 7, LR 1.527242 Loss 4.872370, Accuracy 89.621%\n",
      "Epoch 24, Batch 8, LR 1.527111 Loss 4.852108, Accuracy 89.551%\n",
      "Epoch 24, Batch 9, LR 1.526981 Loss 4.895714, Accuracy 89.670%\n",
      "Epoch 24, Batch 10, LR 1.526850 Loss 4.880896, Accuracy 89.766%\n",
      "Epoch 24, Batch 11, LR 1.526719 Loss 4.892427, Accuracy 89.560%\n",
      "Epoch 24, Batch 12, LR 1.526589 Loss 4.926699, Accuracy 89.258%\n",
      "Epoch 24, Batch 13, LR 1.526458 Loss 4.845842, Accuracy 89.483%\n",
      "Epoch 24, Batch 14, LR 1.526327 Loss 4.828705, Accuracy 89.676%\n",
      "Epoch 24, Batch 15, LR 1.526197 Loss 4.833068, Accuracy 89.792%\n",
      "Epoch 24, Batch 16, LR 1.526066 Loss 4.798686, Accuracy 90.039%\n",
      "Epoch 24, Batch 17, LR 1.525935 Loss 4.799987, Accuracy 90.074%\n",
      "Epoch 24, Batch 18, LR 1.525805 Loss 4.817889, Accuracy 90.104%\n",
      "Epoch 24, Batch 19, LR 1.525674 Loss 4.809938, Accuracy 90.132%\n",
      "Epoch 24, Batch 20, LR 1.525543 Loss 4.815816, Accuracy 90.195%\n",
      "Epoch 24, Batch 21, LR 1.525413 Loss 4.820763, Accuracy 90.253%\n",
      "Epoch 24, Batch 22, LR 1.525282 Loss 4.785094, Accuracy 90.341%\n",
      "Epoch 24, Batch 23, LR 1.525152 Loss 4.790122, Accuracy 90.387%\n",
      "Epoch 24, Batch 24, LR 1.525021 Loss 4.762443, Accuracy 90.527%\n",
      "Epoch 24, Batch 25, LR 1.524890 Loss 4.771548, Accuracy 90.344%\n",
      "Epoch 24, Batch 26, LR 1.524759 Loss 4.793968, Accuracy 90.174%\n",
      "Epoch 24, Batch 27, LR 1.524629 Loss 4.803967, Accuracy 90.046%\n",
      "Epoch 24, Batch 28, LR 1.524498 Loss 4.814767, Accuracy 90.011%\n",
      "Epoch 24, Batch 29, LR 1.524367 Loss 4.822848, Accuracy 90.005%\n",
      "Epoch 24, Batch 30, LR 1.524237 Loss 4.835171, Accuracy 89.896%\n",
      "Epoch 24, Batch 31, LR 1.524106 Loss 4.838908, Accuracy 89.919%\n",
      "Epoch 24, Batch 32, LR 1.523975 Loss 4.839044, Accuracy 89.893%\n",
      "Epoch 24, Batch 33, LR 1.523845 Loss 4.825880, Accuracy 89.962%\n",
      "Epoch 24, Batch 34, LR 1.523714 Loss 4.814941, Accuracy 90.074%\n",
      "Epoch 24, Batch 35, LR 1.523583 Loss 4.819296, Accuracy 90.000%\n",
      "Epoch 24, Batch 36, LR 1.523453 Loss 4.823989, Accuracy 89.974%\n",
      "Epoch 24, Batch 37, LR 1.523322 Loss 4.829224, Accuracy 89.992%\n",
      "Epoch 24, Batch 38, LR 1.523191 Loss 4.819671, Accuracy 90.090%\n",
      "Epoch 24, Batch 39, LR 1.523060 Loss 4.825545, Accuracy 90.124%\n",
      "Epoch 24, Batch 40, LR 1.522930 Loss 4.791948, Accuracy 90.195%\n",
      "Epoch 24, Batch 41, LR 1.522799 Loss 4.801792, Accuracy 90.187%\n",
      "Epoch 24, Batch 42, LR 1.522668 Loss 4.803181, Accuracy 90.141%\n",
      "Epoch 24, Batch 43, LR 1.522538 Loss 4.796370, Accuracy 90.116%\n",
      "Epoch 24, Batch 44, LR 1.522407 Loss 4.812293, Accuracy 89.968%\n",
      "Epoch 24, Batch 45, LR 1.522276 Loss 4.821989, Accuracy 89.913%\n",
      "Epoch 24, Batch 46, LR 1.522145 Loss 4.819700, Accuracy 89.895%\n",
      "Epoch 24, Batch 47, LR 1.522015 Loss 4.832491, Accuracy 89.844%\n",
      "Epoch 24, Batch 48, LR 1.521884 Loss 4.819589, Accuracy 89.909%\n",
      "Epoch 24, Batch 49, LR 1.521753 Loss 4.821216, Accuracy 89.939%\n",
      "Epoch 24, Batch 50, LR 1.521622 Loss 4.817232, Accuracy 89.953%\n",
      "Epoch 24, Batch 51, LR 1.521492 Loss 4.809974, Accuracy 89.936%\n",
      "Epoch 24, Batch 52, LR 1.521361 Loss 4.826052, Accuracy 89.814%\n",
      "Epoch 24, Batch 53, LR 1.521230 Loss 4.816391, Accuracy 89.858%\n",
      "Epoch 24, Batch 54, LR 1.521099 Loss 4.820824, Accuracy 89.771%\n",
      "Epoch 24, Batch 55, LR 1.520969 Loss 4.816742, Accuracy 89.801%\n",
      "Epoch 24, Batch 56, LR 1.520838 Loss 4.823319, Accuracy 89.718%\n",
      "Epoch 24, Batch 57, LR 1.520707 Loss 4.826172, Accuracy 89.734%\n",
      "Epoch 24, Batch 58, LR 1.520576 Loss 4.831653, Accuracy 89.709%\n",
      "Epoch 24, Batch 59, LR 1.520445 Loss 4.841110, Accuracy 89.619%\n",
      "Epoch 24, Batch 60, LR 1.520315 Loss 4.849927, Accuracy 89.583%\n",
      "Epoch 24, Batch 61, LR 1.520184 Loss 4.842168, Accuracy 89.664%\n",
      "Epoch 24, Batch 62, LR 1.520053 Loss 4.837771, Accuracy 89.718%\n",
      "Epoch 24, Batch 63, LR 1.519922 Loss 4.835593, Accuracy 89.757%\n",
      "Epoch 24, Batch 64, LR 1.519791 Loss 4.843761, Accuracy 89.697%\n",
      "Epoch 24, Batch 65, LR 1.519661 Loss 4.830518, Accuracy 89.748%\n",
      "Epoch 24, Batch 66, LR 1.519530 Loss 4.835353, Accuracy 89.749%\n",
      "Epoch 24, Batch 67, LR 1.519399 Loss 4.837991, Accuracy 89.785%\n",
      "Epoch 24, Batch 68, LR 1.519268 Loss 4.826970, Accuracy 89.867%\n",
      "Epoch 24, Batch 69, LR 1.519137 Loss 4.824106, Accuracy 89.832%\n",
      "Epoch 24, Batch 70, LR 1.519007 Loss 4.822162, Accuracy 89.866%\n",
      "Epoch 24, Batch 71, LR 1.518876 Loss 4.816372, Accuracy 89.866%\n",
      "Epoch 24, Batch 72, LR 1.518745 Loss 4.825817, Accuracy 89.789%\n",
      "Epoch 24, Batch 73, LR 1.518614 Loss 4.819332, Accuracy 89.801%\n",
      "Epoch 24, Batch 74, LR 1.518483 Loss 4.825590, Accuracy 89.802%\n",
      "Epoch 24, Batch 75, LR 1.518353 Loss 4.827196, Accuracy 89.844%\n",
      "Epoch 24, Batch 76, LR 1.518222 Loss 4.827164, Accuracy 89.833%\n",
      "Epoch 24, Batch 77, LR 1.518091 Loss 4.833774, Accuracy 89.763%\n",
      "Epoch 24, Batch 78, LR 1.517960 Loss 4.837760, Accuracy 89.714%\n",
      "Epoch 24, Batch 79, LR 1.517829 Loss 4.831613, Accuracy 89.745%\n",
      "Epoch 24, Batch 80, LR 1.517698 Loss 4.829399, Accuracy 89.775%\n",
      "Epoch 24, Batch 81, LR 1.517568 Loss 4.829776, Accuracy 89.786%\n",
      "Epoch 24, Batch 82, LR 1.517437 Loss 4.823873, Accuracy 89.815%\n",
      "Epoch 24, Batch 83, LR 1.517306 Loss 4.827007, Accuracy 89.816%\n",
      "Epoch 24, Batch 84, LR 1.517175 Loss 4.833424, Accuracy 89.825%\n",
      "Epoch 24, Batch 85, LR 1.517044 Loss 4.832014, Accuracy 89.789%\n",
      "Epoch 24, Batch 86, LR 1.516913 Loss 4.836615, Accuracy 89.771%\n",
      "Epoch 24, Batch 87, LR 1.516782 Loss 4.841038, Accuracy 89.763%\n",
      "Epoch 24, Batch 88, LR 1.516651 Loss 4.838730, Accuracy 89.755%\n",
      "Epoch 24, Batch 89, LR 1.516521 Loss 4.837808, Accuracy 89.765%\n",
      "Epoch 24, Batch 90, LR 1.516390 Loss 4.836646, Accuracy 89.783%\n",
      "Epoch 24, Batch 91, LR 1.516259 Loss 4.842201, Accuracy 89.749%\n",
      "Epoch 24, Batch 92, LR 1.516128 Loss 4.839843, Accuracy 89.767%\n",
      "Epoch 24, Batch 93, LR 1.515997 Loss 4.837886, Accuracy 89.777%\n",
      "Epoch 24, Batch 94, LR 1.515866 Loss 4.843787, Accuracy 89.777%\n",
      "Epoch 24, Batch 95, LR 1.515735 Loss 4.842852, Accuracy 89.794%\n",
      "Epoch 24, Batch 96, LR 1.515604 Loss 4.842413, Accuracy 89.787%\n",
      "Epoch 24, Batch 97, LR 1.515474 Loss 4.836821, Accuracy 89.820%\n",
      "Epoch 24, Batch 98, LR 1.515343 Loss 4.829875, Accuracy 89.844%\n",
      "Epoch 24, Batch 99, LR 1.515212 Loss 4.830810, Accuracy 89.860%\n",
      "Epoch 24, Batch 100, LR 1.515081 Loss 4.837038, Accuracy 89.828%\n",
      "Epoch 24, Batch 101, LR 1.514950 Loss 4.836319, Accuracy 89.844%\n",
      "Epoch 24, Batch 102, LR 1.514819 Loss 4.842053, Accuracy 89.867%\n",
      "Epoch 24, Batch 103, LR 1.514688 Loss 4.839771, Accuracy 89.874%\n",
      "Epoch 24, Batch 104, LR 1.514557 Loss 4.831742, Accuracy 89.904%\n",
      "Epoch 24, Batch 105, LR 1.514426 Loss 4.832415, Accuracy 89.911%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 106, LR 1.514295 Loss 4.833803, Accuracy 89.903%\n",
      "Epoch 24, Batch 107, LR 1.514164 Loss 4.834812, Accuracy 89.888%\n",
      "Epoch 24, Batch 108, LR 1.514033 Loss 4.836091, Accuracy 89.902%\n",
      "Epoch 24, Batch 109, LR 1.513903 Loss 4.834846, Accuracy 89.901%\n",
      "Epoch 24, Batch 110, LR 1.513772 Loss 4.832482, Accuracy 89.879%\n",
      "Epoch 24, Batch 111, LR 1.513641 Loss 4.824937, Accuracy 89.893%\n",
      "Epoch 24, Batch 112, LR 1.513510 Loss 4.823791, Accuracy 89.893%\n",
      "Epoch 24, Batch 113, LR 1.513379 Loss 4.821526, Accuracy 89.885%\n",
      "Epoch 24, Batch 114, LR 1.513248 Loss 4.819338, Accuracy 89.878%\n",
      "Epoch 24, Batch 115, LR 1.513117 Loss 4.826522, Accuracy 89.823%\n",
      "Epoch 24, Batch 116, LR 1.512986 Loss 4.829971, Accuracy 89.790%\n",
      "Epoch 24, Batch 117, LR 1.512855 Loss 4.835889, Accuracy 89.757%\n",
      "Epoch 24, Batch 118, LR 1.512724 Loss 4.836748, Accuracy 89.751%\n",
      "Epoch 24, Batch 119, LR 1.512593 Loss 4.836319, Accuracy 89.752%\n",
      "Epoch 24, Batch 120, LR 1.512462 Loss 4.835852, Accuracy 89.753%\n",
      "Epoch 24, Batch 121, LR 1.512331 Loss 4.835845, Accuracy 89.747%\n",
      "Epoch 24, Batch 122, LR 1.512200 Loss 4.835155, Accuracy 89.773%\n",
      "Epoch 24, Batch 123, LR 1.512069 Loss 4.837620, Accuracy 89.780%\n",
      "Epoch 24, Batch 124, LR 1.511938 Loss 4.838756, Accuracy 89.774%\n",
      "Epoch 24, Batch 125, LR 1.511807 Loss 4.842043, Accuracy 89.769%\n",
      "Epoch 24, Batch 126, LR 1.511676 Loss 4.840554, Accuracy 89.782%\n",
      "Epoch 24, Batch 127, LR 1.511545 Loss 4.841944, Accuracy 89.745%\n",
      "Epoch 24, Batch 128, LR 1.511414 Loss 4.838916, Accuracy 89.758%\n",
      "Epoch 24, Batch 129, LR 1.511283 Loss 4.838769, Accuracy 89.759%\n",
      "Epoch 24, Batch 130, LR 1.511152 Loss 4.838376, Accuracy 89.766%\n",
      "Epoch 24, Batch 131, LR 1.511021 Loss 4.837427, Accuracy 89.748%\n",
      "Epoch 24, Batch 132, LR 1.510890 Loss 4.838466, Accuracy 89.749%\n",
      "Epoch 24, Batch 133, LR 1.510759 Loss 4.840415, Accuracy 89.732%\n",
      "Epoch 24, Batch 134, LR 1.510628 Loss 4.840358, Accuracy 89.727%\n",
      "Epoch 24, Batch 135, LR 1.510497 Loss 4.840742, Accuracy 89.711%\n",
      "Epoch 24, Batch 136, LR 1.510366 Loss 4.841045, Accuracy 89.735%\n",
      "Epoch 24, Batch 137, LR 1.510235 Loss 4.839114, Accuracy 89.753%\n",
      "Epoch 24, Batch 138, LR 1.510104 Loss 4.841223, Accuracy 89.748%\n",
      "Epoch 24, Batch 139, LR 1.509973 Loss 4.837427, Accuracy 89.771%\n",
      "Epoch 24, Batch 140, LR 1.509842 Loss 4.838618, Accuracy 89.749%\n",
      "Epoch 24, Batch 141, LR 1.509711 Loss 4.832216, Accuracy 89.772%\n",
      "Epoch 24, Batch 142, LR 1.509580 Loss 4.830565, Accuracy 89.767%\n",
      "Epoch 24, Batch 143, LR 1.509449 Loss 4.826047, Accuracy 89.767%\n",
      "Epoch 24, Batch 144, LR 1.509318 Loss 4.824564, Accuracy 89.762%\n",
      "Epoch 24, Batch 145, LR 1.509187 Loss 4.816239, Accuracy 89.779%\n",
      "Epoch 24, Batch 146, LR 1.509056 Loss 4.814942, Accuracy 89.796%\n",
      "Epoch 24, Batch 147, LR 1.508925 Loss 4.813453, Accuracy 89.801%\n",
      "Epoch 24, Batch 148, LR 1.508794 Loss 4.820024, Accuracy 89.770%\n",
      "Epoch 24, Batch 149, LR 1.508663 Loss 4.821254, Accuracy 89.760%\n",
      "Epoch 24, Batch 150, LR 1.508532 Loss 4.819836, Accuracy 89.771%\n",
      "Epoch 24, Batch 151, LR 1.508401 Loss 4.817551, Accuracy 89.787%\n",
      "Epoch 24, Batch 152, LR 1.508270 Loss 4.818721, Accuracy 89.767%\n",
      "Epoch 24, Batch 153, LR 1.508139 Loss 4.819612, Accuracy 89.762%\n",
      "Epoch 24, Batch 154, LR 1.508007 Loss 4.820788, Accuracy 89.742%\n",
      "Epoch 24, Batch 155, LR 1.507876 Loss 4.820865, Accuracy 89.748%\n",
      "Epoch 24, Batch 156, LR 1.507745 Loss 4.821991, Accuracy 89.724%\n",
      "Epoch 24, Batch 157, LR 1.507614 Loss 4.818889, Accuracy 89.734%\n",
      "Epoch 24, Batch 158, LR 1.507483 Loss 4.816483, Accuracy 89.740%\n",
      "Epoch 24, Batch 159, LR 1.507352 Loss 4.817147, Accuracy 89.741%\n",
      "Epoch 24, Batch 160, LR 1.507221 Loss 4.812530, Accuracy 89.756%\n",
      "Epoch 24, Batch 161, LR 1.507090 Loss 4.812720, Accuracy 89.747%\n",
      "Epoch 24, Batch 162, LR 1.506959 Loss 4.812473, Accuracy 89.733%\n",
      "Epoch 24, Batch 163, LR 1.506828 Loss 4.814885, Accuracy 89.714%\n",
      "Epoch 24, Batch 164, LR 1.506697 Loss 4.812453, Accuracy 89.720%\n",
      "Epoch 24, Batch 165, LR 1.506566 Loss 4.812440, Accuracy 89.706%\n",
      "Epoch 24, Batch 166, LR 1.506434 Loss 4.815422, Accuracy 89.688%\n",
      "Epoch 24, Batch 167, LR 1.506303 Loss 4.814763, Accuracy 89.689%\n",
      "Epoch 24, Batch 168, LR 1.506172 Loss 4.813742, Accuracy 89.690%\n",
      "Epoch 24, Batch 169, LR 1.506041 Loss 4.815433, Accuracy 89.677%\n",
      "Epoch 24, Batch 170, LR 1.505910 Loss 4.812827, Accuracy 89.701%\n",
      "Epoch 24, Batch 171, LR 1.505779 Loss 4.811247, Accuracy 89.688%\n",
      "Epoch 24, Batch 172, LR 1.505648 Loss 4.809360, Accuracy 89.694%\n",
      "Epoch 24, Batch 173, LR 1.505517 Loss 4.805134, Accuracy 89.713%\n",
      "Epoch 24, Batch 174, LR 1.505386 Loss 4.803995, Accuracy 89.718%\n",
      "Epoch 24, Batch 175, LR 1.505254 Loss 4.809151, Accuracy 89.710%\n",
      "Epoch 24, Batch 176, LR 1.505123 Loss 4.809684, Accuracy 89.693%\n",
      "Epoch 24, Batch 177, LR 1.504992 Loss 4.811136, Accuracy 89.698%\n",
      "Epoch 24, Batch 178, LR 1.504861 Loss 4.810872, Accuracy 89.708%\n",
      "Epoch 24, Batch 179, LR 1.504730 Loss 4.809489, Accuracy 89.717%\n",
      "Epoch 24, Batch 180, LR 1.504599 Loss 4.810207, Accuracy 89.705%\n",
      "Epoch 24, Batch 181, LR 1.504468 Loss 4.810330, Accuracy 89.693%\n",
      "Epoch 24, Batch 182, LR 1.504336 Loss 4.810789, Accuracy 89.672%\n",
      "Epoch 24, Batch 183, LR 1.504205 Loss 4.813102, Accuracy 89.660%\n",
      "Epoch 24, Batch 184, LR 1.504074 Loss 4.814898, Accuracy 89.631%\n",
      "Epoch 24, Batch 185, LR 1.503943 Loss 4.813165, Accuracy 89.645%\n",
      "Epoch 24, Batch 186, LR 1.503812 Loss 4.814687, Accuracy 89.646%\n",
      "Epoch 24, Batch 187, LR 1.503681 Loss 4.815055, Accuracy 89.622%\n",
      "Epoch 24, Batch 188, LR 1.503549 Loss 4.817789, Accuracy 89.603%\n",
      "Epoch 24, Batch 189, LR 1.503418 Loss 4.820776, Accuracy 89.604%\n",
      "Epoch 24, Batch 190, LR 1.503287 Loss 4.822591, Accuracy 89.593%\n",
      "Epoch 24, Batch 191, LR 1.503156 Loss 4.821616, Accuracy 89.602%\n",
      "Epoch 24, Batch 192, LR 1.503025 Loss 4.823220, Accuracy 89.587%\n",
      "Epoch 24, Batch 193, LR 1.502894 Loss 4.825076, Accuracy 89.577%\n",
      "Epoch 24, Batch 194, LR 1.502762 Loss 4.826571, Accuracy 89.570%\n",
      "Epoch 24, Batch 195, LR 1.502631 Loss 4.826401, Accuracy 89.563%\n",
      "Epoch 24, Batch 196, LR 1.502500 Loss 4.827451, Accuracy 89.569%\n",
      "Epoch 24, Batch 197, LR 1.502369 Loss 4.828420, Accuracy 89.558%\n",
      "Epoch 24, Batch 198, LR 1.502238 Loss 4.826744, Accuracy 89.568%\n",
      "Epoch 24, Batch 199, LR 1.502106 Loss 4.827742, Accuracy 89.561%\n",
      "Epoch 24, Batch 200, LR 1.501975 Loss 4.826393, Accuracy 89.566%\n",
      "Epoch 24, Batch 201, LR 1.501844 Loss 4.827902, Accuracy 89.556%\n",
      "Epoch 24, Batch 202, LR 1.501713 Loss 4.828465, Accuracy 89.550%\n",
      "Epoch 24, Batch 203, LR 1.501582 Loss 4.829202, Accuracy 89.544%\n",
      "Epoch 24, Batch 204, LR 1.501450 Loss 4.829027, Accuracy 89.553%\n",
      "Epoch 24, Batch 205, LR 1.501319 Loss 4.830732, Accuracy 89.543%\n",
      "Epoch 24, Batch 206, LR 1.501188 Loss 4.831606, Accuracy 89.548%\n",
      "Epoch 24, Batch 207, LR 1.501057 Loss 4.831043, Accuracy 89.553%\n",
      "Epoch 24, Batch 208, LR 1.500925 Loss 4.829880, Accuracy 89.581%\n",
      "Epoch 24, Batch 209, LR 1.500794 Loss 4.830066, Accuracy 89.593%\n",
      "Epoch 24, Batch 210, LR 1.500663 Loss 4.832652, Accuracy 89.561%\n",
      "Epoch 24, Batch 211, LR 1.500532 Loss 4.832367, Accuracy 89.581%\n",
      "Epoch 24, Batch 212, LR 1.500401 Loss 4.833370, Accuracy 89.560%\n",
      "Epoch 24, Batch 213, LR 1.500269 Loss 4.835724, Accuracy 89.539%\n",
      "Epoch 24, Batch 214, LR 1.500138 Loss 4.838720, Accuracy 89.515%\n",
      "Epoch 24, Batch 215, LR 1.500007 Loss 4.839661, Accuracy 89.495%\n",
      "Epoch 24, Batch 216, LR 1.499876 Loss 4.837208, Accuracy 89.507%\n",
      "Epoch 24, Batch 217, LR 1.499744 Loss 4.837272, Accuracy 89.505%\n",
      "Epoch 24, Batch 218, LR 1.499613 Loss 4.836351, Accuracy 89.507%\n",
      "Epoch 24, Batch 219, LR 1.499482 Loss 4.836244, Accuracy 89.508%\n",
      "Epoch 24, Batch 220, LR 1.499351 Loss 4.837972, Accuracy 89.499%\n",
      "Epoch 24, Batch 221, LR 1.499219 Loss 4.840862, Accuracy 89.469%\n",
      "Epoch 24, Batch 222, LR 1.499088 Loss 4.839808, Accuracy 89.481%\n",
      "Epoch 24, Batch 223, LR 1.498957 Loss 4.839887, Accuracy 89.479%\n",
      "Epoch 24, Batch 224, LR 1.498825 Loss 4.840151, Accuracy 89.485%\n",
      "Epoch 24, Batch 225, LR 1.498694 Loss 4.838706, Accuracy 89.483%\n",
      "Epoch 24, Batch 226, LR 1.498563 Loss 4.838451, Accuracy 89.495%\n",
      "Epoch 24, Batch 227, LR 1.498432 Loss 4.836988, Accuracy 89.503%\n",
      "Epoch 24, Batch 228, LR 1.498300 Loss 4.837605, Accuracy 89.505%\n",
      "Epoch 24, Batch 229, LR 1.498169 Loss 4.840030, Accuracy 89.486%\n",
      "Epoch 24, Batch 230, LR 1.498038 Loss 4.839229, Accuracy 89.480%\n",
      "Epoch 24, Batch 231, LR 1.497907 Loss 4.840129, Accuracy 89.482%\n",
      "Epoch 24, Batch 232, LR 1.497775 Loss 4.840297, Accuracy 89.490%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 233, LR 1.497644 Loss 4.841553, Accuracy 89.485%\n",
      "Epoch 24, Batch 234, LR 1.497513 Loss 4.839865, Accuracy 89.487%\n",
      "Epoch 24, Batch 235, LR 1.497381 Loss 4.839286, Accuracy 89.481%\n",
      "Epoch 24, Batch 236, LR 1.497250 Loss 4.837824, Accuracy 89.499%\n",
      "Epoch 24, Batch 237, LR 1.497119 Loss 4.841178, Accuracy 89.488%\n",
      "Epoch 24, Batch 238, LR 1.496987 Loss 4.840700, Accuracy 89.486%\n",
      "Epoch 24, Batch 239, LR 1.496856 Loss 4.842556, Accuracy 89.484%\n",
      "Epoch 24, Batch 240, LR 1.496725 Loss 4.841878, Accuracy 89.502%\n",
      "Epoch 24, Batch 241, LR 1.496593 Loss 4.840366, Accuracy 89.513%\n",
      "Epoch 24, Batch 242, LR 1.496462 Loss 4.838873, Accuracy 89.514%\n",
      "Epoch 24, Batch 243, LR 1.496331 Loss 4.839614, Accuracy 89.516%\n",
      "Epoch 24, Batch 244, LR 1.496199 Loss 4.837720, Accuracy 89.514%\n",
      "Epoch 24, Batch 245, LR 1.496068 Loss 4.833557, Accuracy 89.538%\n",
      "Epoch 24, Batch 246, LR 1.495937 Loss 4.832964, Accuracy 89.542%\n",
      "Epoch 24, Batch 247, LR 1.495805 Loss 4.831709, Accuracy 89.556%\n",
      "Epoch 24, Batch 248, LR 1.495674 Loss 4.829423, Accuracy 89.560%\n",
      "Epoch 24, Batch 249, LR 1.495543 Loss 4.829573, Accuracy 89.571%\n",
      "Epoch 24, Batch 250, LR 1.495411 Loss 4.832915, Accuracy 89.562%\n",
      "Epoch 24, Batch 251, LR 1.495280 Loss 4.834137, Accuracy 89.557%\n",
      "Epoch 24, Batch 252, LR 1.495149 Loss 4.833583, Accuracy 89.568%\n",
      "Epoch 24, Batch 253, LR 1.495017 Loss 4.831214, Accuracy 89.591%\n",
      "Epoch 24, Batch 254, LR 1.494886 Loss 4.830785, Accuracy 89.598%\n",
      "Epoch 24, Batch 255, LR 1.494755 Loss 4.832403, Accuracy 89.580%\n",
      "Epoch 24, Batch 256, LR 1.494623 Loss 4.833606, Accuracy 89.587%\n",
      "Epoch 24, Batch 257, LR 1.494492 Loss 4.832988, Accuracy 89.582%\n",
      "Epoch 24, Batch 258, LR 1.494361 Loss 4.835086, Accuracy 89.580%\n",
      "Epoch 24, Batch 259, LR 1.494229 Loss 4.835019, Accuracy 89.575%\n",
      "Epoch 24, Batch 260, LR 1.494098 Loss 4.832203, Accuracy 89.591%\n",
      "Epoch 24, Batch 261, LR 1.493966 Loss 4.833237, Accuracy 89.601%\n",
      "Epoch 24, Batch 262, LR 1.493835 Loss 4.836832, Accuracy 89.593%\n",
      "Epoch 24, Batch 263, LR 1.493704 Loss 4.835457, Accuracy 89.591%\n",
      "Epoch 24, Batch 264, LR 1.493572 Loss 4.834906, Accuracy 89.601%\n",
      "Epoch 24, Batch 265, LR 1.493441 Loss 4.836810, Accuracy 89.596%\n",
      "Epoch 24, Batch 266, LR 1.493310 Loss 4.836054, Accuracy 89.597%\n",
      "Epoch 24, Batch 267, LR 1.493178 Loss 4.835435, Accuracy 89.595%\n",
      "Epoch 24, Batch 268, LR 1.493047 Loss 4.834076, Accuracy 89.602%\n",
      "Epoch 24, Batch 269, LR 1.492915 Loss 4.832819, Accuracy 89.620%\n",
      "Epoch 24, Batch 270, LR 1.492784 Loss 4.831743, Accuracy 89.624%\n",
      "Epoch 24, Batch 271, LR 1.492653 Loss 4.832343, Accuracy 89.622%\n",
      "Epoch 24, Batch 272, LR 1.492521 Loss 4.831115, Accuracy 89.628%\n",
      "Epoch 24, Batch 273, LR 1.492390 Loss 4.832491, Accuracy 89.626%\n",
      "Epoch 24, Batch 274, LR 1.492258 Loss 4.832332, Accuracy 89.630%\n",
      "Epoch 24, Batch 275, LR 1.492127 Loss 4.831440, Accuracy 89.634%\n",
      "Epoch 24, Batch 276, LR 1.491995 Loss 4.832479, Accuracy 89.629%\n",
      "Epoch 24, Batch 277, LR 1.491864 Loss 4.832388, Accuracy 89.635%\n",
      "Epoch 24, Batch 278, LR 1.491733 Loss 4.832133, Accuracy 89.639%\n",
      "Epoch 24, Batch 279, LR 1.491601 Loss 4.831198, Accuracy 89.639%\n",
      "Epoch 24, Batch 280, LR 1.491470 Loss 4.831607, Accuracy 89.637%\n",
      "Epoch 24, Batch 281, LR 1.491338 Loss 4.831776, Accuracy 89.632%\n",
      "Epoch 24, Batch 282, LR 1.491207 Loss 4.831150, Accuracy 89.628%\n",
      "Epoch 24, Batch 283, LR 1.491075 Loss 4.830656, Accuracy 89.620%\n",
      "Epoch 24, Batch 284, LR 1.490944 Loss 4.829079, Accuracy 89.635%\n",
      "Epoch 24, Batch 285, LR 1.490813 Loss 4.827591, Accuracy 89.638%\n",
      "Epoch 24, Batch 286, LR 1.490681 Loss 4.824315, Accuracy 89.647%\n",
      "Epoch 24, Batch 287, LR 1.490550 Loss 4.824179, Accuracy 89.648%\n",
      "Epoch 24, Batch 288, LR 1.490418 Loss 4.824521, Accuracy 89.635%\n",
      "Epoch 24, Batch 289, LR 1.490287 Loss 4.825517, Accuracy 89.625%\n",
      "Epoch 24, Batch 290, LR 1.490155 Loss 4.825694, Accuracy 89.623%\n",
      "Epoch 24, Batch 291, LR 1.490024 Loss 4.824300, Accuracy 89.629%\n",
      "Epoch 24, Batch 292, LR 1.489892 Loss 4.824799, Accuracy 89.630%\n",
      "Epoch 24, Batch 293, LR 1.489761 Loss 4.826076, Accuracy 89.625%\n",
      "Epoch 24, Batch 294, LR 1.489630 Loss 4.825405, Accuracy 89.618%\n",
      "Epoch 24, Batch 295, LR 1.489498 Loss 4.826231, Accuracy 89.619%\n",
      "Epoch 24, Batch 296, LR 1.489367 Loss 4.825627, Accuracy 89.619%\n",
      "Epoch 24, Batch 297, LR 1.489235 Loss 4.827875, Accuracy 89.612%\n",
      "Epoch 24, Batch 298, LR 1.489104 Loss 4.828041, Accuracy 89.610%\n",
      "Epoch 24, Batch 299, LR 1.488972 Loss 4.829550, Accuracy 89.596%\n",
      "Epoch 24, Batch 300, LR 1.488841 Loss 4.831860, Accuracy 89.589%\n",
      "Epoch 24, Batch 301, LR 1.488709 Loss 4.832603, Accuracy 89.589%\n",
      "Epoch 24, Batch 302, LR 1.488578 Loss 4.832455, Accuracy 89.582%\n",
      "Epoch 24, Batch 303, LR 1.488446 Loss 4.830597, Accuracy 89.588%\n",
      "Epoch 24, Batch 304, LR 1.488315 Loss 4.832452, Accuracy 89.579%\n",
      "Epoch 24, Batch 305, LR 1.488183 Loss 4.833959, Accuracy 89.572%\n",
      "Epoch 24, Batch 306, LR 1.488052 Loss 4.832958, Accuracy 89.581%\n",
      "Epoch 24, Batch 307, LR 1.487920 Loss 4.831636, Accuracy 89.594%\n",
      "Epoch 24, Batch 308, LR 1.487789 Loss 4.831744, Accuracy 89.582%\n",
      "Epoch 24, Batch 309, LR 1.487657 Loss 4.831186, Accuracy 89.583%\n",
      "Epoch 24, Batch 310, LR 1.487526 Loss 4.831656, Accuracy 89.594%\n",
      "Epoch 24, Batch 311, LR 1.487394 Loss 4.832859, Accuracy 89.588%\n",
      "Epoch 24, Batch 312, LR 1.487263 Loss 4.831199, Accuracy 89.603%\n",
      "Epoch 24, Batch 313, LR 1.487131 Loss 4.830211, Accuracy 89.604%\n",
      "Epoch 24, Batch 314, LR 1.487000 Loss 4.831503, Accuracy 89.600%\n",
      "Epoch 24, Batch 315, LR 1.486868 Loss 4.832368, Accuracy 89.598%\n",
      "Epoch 24, Batch 316, LR 1.486737 Loss 4.831738, Accuracy 89.611%\n",
      "Epoch 24, Batch 317, LR 1.486605 Loss 4.831354, Accuracy 89.619%\n",
      "Epoch 24, Batch 318, LR 1.486473 Loss 4.831523, Accuracy 89.618%\n",
      "Epoch 24, Batch 319, LR 1.486342 Loss 4.831210, Accuracy 89.626%\n",
      "Epoch 24, Batch 320, LR 1.486210 Loss 4.829270, Accuracy 89.648%\n",
      "Epoch 24, Batch 321, LR 1.486079 Loss 4.831194, Accuracy 89.642%\n",
      "Epoch 24, Batch 322, LR 1.485947 Loss 4.831102, Accuracy 89.647%\n",
      "Epoch 24, Batch 323, LR 1.485816 Loss 4.831844, Accuracy 89.641%\n",
      "Epoch 24, Batch 324, LR 1.485684 Loss 4.829890, Accuracy 89.641%\n",
      "Epoch 24, Batch 325, LR 1.485553 Loss 4.831234, Accuracy 89.637%\n",
      "Epoch 24, Batch 326, LR 1.485421 Loss 4.830897, Accuracy 89.640%\n",
      "Epoch 24, Batch 327, LR 1.485290 Loss 4.832130, Accuracy 89.622%\n",
      "Epoch 24, Batch 328, LR 1.485158 Loss 4.832413, Accuracy 89.634%\n",
      "Epoch 24, Batch 329, LR 1.485026 Loss 4.830169, Accuracy 89.640%\n",
      "Epoch 24, Batch 330, LR 1.484895 Loss 4.830182, Accuracy 89.635%\n",
      "Epoch 24, Batch 331, LR 1.484763 Loss 4.829660, Accuracy 89.624%\n",
      "Epoch 24, Batch 332, LR 1.484632 Loss 4.828995, Accuracy 89.634%\n",
      "Epoch 24, Batch 333, LR 1.484500 Loss 4.827413, Accuracy 89.640%\n",
      "Epoch 24, Batch 334, LR 1.484369 Loss 4.827281, Accuracy 89.643%\n",
      "Epoch 24, Batch 335, LR 1.484237 Loss 4.826440, Accuracy 89.646%\n",
      "Epoch 24, Batch 336, LR 1.484105 Loss 4.826106, Accuracy 89.658%\n",
      "Epoch 24, Batch 337, LR 1.483974 Loss 4.825841, Accuracy 89.661%\n",
      "Epoch 24, Batch 338, LR 1.483842 Loss 4.825515, Accuracy 89.663%\n",
      "Epoch 24, Batch 339, LR 1.483711 Loss 4.825069, Accuracy 89.671%\n",
      "Epoch 24, Batch 340, LR 1.483579 Loss 4.826236, Accuracy 89.667%\n",
      "Epoch 24, Batch 341, LR 1.483447 Loss 4.825967, Accuracy 89.679%\n",
      "Epoch 24, Batch 342, LR 1.483316 Loss 4.826655, Accuracy 89.672%\n",
      "Epoch 24, Batch 343, LR 1.483184 Loss 4.826108, Accuracy 89.675%\n",
      "Epoch 24, Batch 344, LR 1.483053 Loss 4.824867, Accuracy 89.687%\n",
      "Epoch 24, Batch 345, LR 1.482921 Loss 4.825980, Accuracy 89.672%\n",
      "Epoch 24, Batch 346, LR 1.482789 Loss 4.825747, Accuracy 89.672%\n",
      "Epoch 24, Batch 347, LR 1.482658 Loss 4.825811, Accuracy 89.673%\n",
      "Epoch 24, Batch 348, LR 1.482526 Loss 4.826856, Accuracy 89.662%\n",
      "Epoch 24, Batch 349, LR 1.482395 Loss 4.826563, Accuracy 89.665%\n",
      "Epoch 24, Batch 350, LR 1.482263 Loss 4.826986, Accuracy 89.658%\n",
      "Epoch 24, Batch 351, LR 1.482131 Loss 4.826977, Accuracy 89.659%\n",
      "Epoch 24, Batch 352, LR 1.482000 Loss 4.825709, Accuracy 89.662%\n",
      "Epoch 24, Batch 353, LR 1.481868 Loss 4.826119, Accuracy 89.664%\n",
      "Epoch 24, Batch 354, LR 1.481736 Loss 4.824399, Accuracy 89.676%\n",
      "Epoch 24, Batch 355, LR 1.481605 Loss 4.823826, Accuracy 89.674%\n",
      "Epoch 24, Batch 356, LR 1.481473 Loss 4.822231, Accuracy 89.679%\n",
      "Epoch 24, Batch 357, LR 1.481342 Loss 4.821191, Accuracy 89.688%\n",
      "Epoch 24, Batch 358, LR 1.481210 Loss 4.819073, Accuracy 89.704%\n",
      "Epoch 24, Batch 359, LR 1.481078 Loss 4.819714, Accuracy 89.696%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 360, LR 1.480947 Loss 4.819807, Accuracy 89.694%\n",
      "Epoch 24, Batch 361, LR 1.480815 Loss 4.819191, Accuracy 89.697%\n",
      "Epoch 24, Batch 362, LR 1.480683 Loss 4.819414, Accuracy 89.703%\n",
      "Epoch 24, Batch 363, LR 1.480552 Loss 4.817864, Accuracy 89.710%\n",
      "Epoch 24, Batch 364, LR 1.480420 Loss 4.818025, Accuracy 89.706%\n",
      "Epoch 24, Batch 365, LR 1.480288 Loss 4.818633, Accuracy 89.707%\n",
      "Epoch 24, Batch 366, LR 1.480157 Loss 4.818476, Accuracy 89.692%\n",
      "Epoch 24, Batch 367, LR 1.480025 Loss 4.819990, Accuracy 89.684%\n",
      "Epoch 24, Batch 368, LR 1.479893 Loss 4.818899, Accuracy 89.674%\n",
      "Epoch 24, Batch 369, LR 1.479762 Loss 4.818263, Accuracy 89.681%\n",
      "Epoch 24, Batch 370, LR 1.479630 Loss 4.816383, Accuracy 89.681%\n",
      "Epoch 24, Batch 371, LR 1.479498 Loss 4.818005, Accuracy 89.671%\n",
      "Epoch 24, Batch 372, LR 1.479367 Loss 4.817018, Accuracy 89.669%\n",
      "Epoch 24, Batch 373, LR 1.479235 Loss 4.817584, Accuracy 89.659%\n",
      "Epoch 24, Batch 374, LR 1.479103 Loss 4.818688, Accuracy 89.660%\n",
      "Epoch 24, Batch 375, LR 1.478972 Loss 4.817161, Accuracy 89.658%\n",
      "Epoch 24, Batch 376, LR 1.478840 Loss 4.816026, Accuracy 89.659%\n",
      "Epoch 24, Batch 377, LR 1.478708 Loss 4.816549, Accuracy 89.653%\n",
      "Epoch 24, Batch 378, LR 1.478577 Loss 4.816051, Accuracy 89.654%\n",
      "Epoch 24, Batch 379, LR 1.478445 Loss 4.816989, Accuracy 89.652%\n",
      "Epoch 24, Batch 380, LR 1.478313 Loss 4.816912, Accuracy 89.644%\n",
      "Epoch 24, Batch 381, LR 1.478181 Loss 4.817899, Accuracy 89.645%\n",
      "Epoch 24, Batch 382, LR 1.478050 Loss 4.818040, Accuracy 89.637%\n",
      "Epoch 24, Batch 383, LR 1.477918 Loss 4.818138, Accuracy 89.642%\n",
      "Epoch 24, Batch 384, LR 1.477786 Loss 4.819751, Accuracy 89.642%\n",
      "Epoch 24, Batch 385, LR 1.477655 Loss 4.818704, Accuracy 89.649%\n",
      "Epoch 24, Batch 386, LR 1.477523 Loss 4.818508, Accuracy 89.649%\n",
      "Epoch 24, Batch 387, LR 1.477391 Loss 4.818998, Accuracy 89.650%\n",
      "Epoch 24, Batch 388, LR 1.477259 Loss 4.820601, Accuracy 89.648%\n",
      "Epoch 24, Batch 389, LR 1.477128 Loss 4.820238, Accuracy 89.653%\n",
      "Epoch 24, Batch 390, LR 1.476996 Loss 4.820706, Accuracy 89.653%\n",
      "Epoch 24, Batch 391, LR 1.476864 Loss 4.820927, Accuracy 89.660%\n",
      "Epoch 24, Batch 392, LR 1.476733 Loss 4.821867, Accuracy 89.648%\n",
      "Epoch 24, Batch 393, LR 1.476601 Loss 4.821843, Accuracy 89.655%\n",
      "Epoch 24, Batch 394, LR 1.476469 Loss 4.821086, Accuracy 89.661%\n",
      "Epoch 24, Batch 395, LR 1.476337 Loss 4.820176, Accuracy 89.668%\n",
      "Epoch 24, Batch 396, LR 1.476206 Loss 4.819676, Accuracy 89.666%\n",
      "Epoch 24, Batch 397, LR 1.476074 Loss 4.820329, Accuracy 89.657%\n",
      "Epoch 24, Batch 398, LR 1.475942 Loss 4.821241, Accuracy 89.649%\n",
      "Epoch 24, Batch 399, LR 1.475810 Loss 4.820842, Accuracy 89.652%\n",
      "Epoch 24, Batch 400, LR 1.475679 Loss 4.820273, Accuracy 89.646%\n",
      "Epoch 24, Batch 401, LR 1.475547 Loss 4.819537, Accuracy 89.653%\n",
      "Epoch 24, Batch 402, LR 1.475415 Loss 4.819232, Accuracy 89.653%\n",
      "Epoch 24, Batch 403, LR 1.475283 Loss 4.818491, Accuracy 89.656%\n",
      "Epoch 24, Batch 404, LR 1.475152 Loss 4.818249, Accuracy 89.660%\n",
      "Epoch 24, Batch 405, LR 1.475020 Loss 4.818523, Accuracy 89.653%\n",
      "Epoch 24, Batch 406, LR 1.474888 Loss 4.820183, Accuracy 89.638%\n",
      "Epoch 24, Batch 407, LR 1.474756 Loss 4.820910, Accuracy 89.635%\n",
      "Epoch 24, Batch 408, LR 1.474625 Loss 4.820829, Accuracy 89.637%\n",
      "Epoch 24, Batch 409, LR 1.474493 Loss 4.820938, Accuracy 89.639%\n",
      "Epoch 24, Batch 410, LR 1.474361 Loss 4.821689, Accuracy 89.630%\n",
      "Epoch 24, Batch 411, LR 1.474229 Loss 4.821869, Accuracy 89.631%\n",
      "Epoch 24, Batch 412, LR 1.474097 Loss 4.820226, Accuracy 89.637%\n",
      "Epoch 24, Batch 413, LR 1.473966 Loss 4.821050, Accuracy 89.636%\n",
      "Epoch 24, Batch 414, LR 1.473834 Loss 4.822228, Accuracy 89.632%\n",
      "Epoch 24, Batch 415, LR 1.473702 Loss 4.822242, Accuracy 89.633%\n",
      "Epoch 24, Batch 416, LR 1.473570 Loss 4.822725, Accuracy 89.626%\n",
      "Epoch 24, Batch 417, LR 1.473438 Loss 4.822321, Accuracy 89.628%\n",
      "Epoch 24, Batch 418, LR 1.473307 Loss 4.822044, Accuracy 89.633%\n",
      "Epoch 24, Batch 419, LR 1.473175 Loss 4.823072, Accuracy 89.641%\n",
      "Epoch 24, Batch 420, LR 1.473043 Loss 4.823828, Accuracy 89.637%\n",
      "Epoch 24, Batch 421, LR 1.472911 Loss 4.823281, Accuracy 89.638%\n",
      "Epoch 24, Batch 422, LR 1.472779 Loss 4.826003, Accuracy 89.633%\n",
      "Epoch 24, Batch 423, LR 1.472648 Loss 4.824761, Accuracy 89.642%\n",
      "Epoch 24, Batch 424, LR 1.472516 Loss 4.824593, Accuracy 89.643%\n",
      "Epoch 24, Batch 425, LR 1.472384 Loss 4.824053, Accuracy 89.645%\n",
      "Epoch 24, Batch 426, LR 1.472252 Loss 4.823341, Accuracy 89.649%\n",
      "Epoch 24, Batch 427, LR 1.472120 Loss 4.823770, Accuracy 89.650%\n",
      "Epoch 24, Batch 428, LR 1.471989 Loss 4.823700, Accuracy 89.650%\n",
      "Epoch 24, Batch 429, LR 1.471857 Loss 4.822923, Accuracy 89.654%\n",
      "Epoch 24, Batch 430, LR 1.471725 Loss 4.822685, Accuracy 89.655%\n",
      "Epoch 24, Batch 431, LR 1.471593 Loss 4.822366, Accuracy 89.662%\n",
      "Epoch 24, Batch 432, LR 1.471461 Loss 4.822711, Accuracy 89.657%\n",
      "Epoch 24, Batch 433, LR 1.471329 Loss 4.821923, Accuracy 89.660%\n",
      "Epoch 24, Batch 434, LR 1.471198 Loss 4.822857, Accuracy 89.657%\n",
      "Epoch 24, Batch 435, LR 1.471066 Loss 4.823039, Accuracy 89.657%\n",
      "Epoch 24, Batch 436, LR 1.470934 Loss 4.822717, Accuracy 89.652%\n",
      "Epoch 24, Batch 437, LR 1.470802 Loss 4.821928, Accuracy 89.660%\n",
      "Epoch 24, Batch 438, LR 1.470670 Loss 4.821905, Accuracy 89.653%\n",
      "Epoch 24, Batch 439, LR 1.470538 Loss 4.821398, Accuracy 89.657%\n",
      "Epoch 24, Batch 440, LR 1.470407 Loss 4.820845, Accuracy 89.654%\n",
      "Epoch 24, Batch 441, LR 1.470275 Loss 4.820620, Accuracy 89.656%\n",
      "Epoch 24, Batch 442, LR 1.470143 Loss 4.820451, Accuracy 89.649%\n",
      "Epoch 24, Batch 443, LR 1.470011 Loss 4.821535, Accuracy 89.653%\n",
      "Epoch 24, Batch 444, LR 1.469879 Loss 4.820206, Accuracy 89.661%\n",
      "Epoch 24, Batch 445, LR 1.469747 Loss 4.819115, Accuracy 89.668%\n",
      "Epoch 24, Batch 446, LR 1.469615 Loss 4.818409, Accuracy 89.669%\n",
      "Epoch 24, Batch 447, LR 1.469483 Loss 4.819576, Accuracy 89.664%\n",
      "Epoch 24, Batch 448, LR 1.469352 Loss 4.817986, Accuracy 89.675%\n",
      "Epoch 24, Batch 449, LR 1.469220 Loss 4.816408, Accuracy 89.682%\n",
      "Epoch 24, Batch 450, LR 1.469088 Loss 4.816684, Accuracy 89.679%\n",
      "Epoch 24, Batch 451, LR 1.468956 Loss 4.817954, Accuracy 89.671%\n",
      "Epoch 24, Batch 452, LR 1.468824 Loss 4.818100, Accuracy 89.673%\n",
      "Epoch 24, Batch 453, LR 1.468692 Loss 4.817998, Accuracy 89.680%\n",
      "Epoch 24, Batch 454, LR 1.468560 Loss 4.817232, Accuracy 89.689%\n",
      "Epoch 24, Batch 455, LR 1.468428 Loss 4.816086, Accuracy 89.694%\n",
      "Epoch 24, Batch 456, LR 1.468297 Loss 4.816704, Accuracy 89.691%\n",
      "Epoch 24, Batch 457, LR 1.468165 Loss 4.816401, Accuracy 89.685%\n",
      "Epoch 24, Batch 458, LR 1.468033 Loss 4.816325, Accuracy 89.683%\n",
      "Epoch 24, Batch 459, LR 1.467901 Loss 4.817258, Accuracy 89.675%\n",
      "Epoch 24, Batch 460, LR 1.467769 Loss 4.817020, Accuracy 89.674%\n",
      "Epoch 24, Batch 461, LR 1.467637 Loss 4.817452, Accuracy 89.664%\n",
      "Epoch 24, Batch 462, LR 1.467505 Loss 4.816548, Accuracy 89.668%\n",
      "Epoch 24, Batch 463, LR 1.467373 Loss 4.817002, Accuracy 89.673%\n",
      "Epoch 24, Batch 464, LR 1.467241 Loss 4.816150, Accuracy 89.672%\n",
      "Epoch 24, Batch 465, LR 1.467109 Loss 4.817148, Accuracy 89.666%\n",
      "Epoch 24, Batch 466, LR 1.466977 Loss 4.817465, Accuracy 89.661%\n",
      "Epoch 24, Batch 467, LR 1.466846 Loss 4.817021, Accuracy 89.666%\n",
      "Epoch 24, Batch 468, LR 1.466714 Loss 4.818166, Accuracy 89.660%\n",
      "Epoch 24, Batch 469, LR 1.466582 Loss 4.817713, Accuracy 89.664%\n",
      "Epoch 24, Batch 470, LR 1.466450 Loss 4.817484, Accuracy 89.663%\n",
      "Epoch 24, Batch 471, LR 1.466318 Loss 4.817099, Accuracy 89.668%\n",
      "Epoch 24, Batch 472, LR 1.466186 Loss 4.817363, Accuracy 89.668%\n",
      "Epoch 24, Batch 473, LR 1.466054 Loss 4.817289, Accuracy 89.672%\n",
      "Epoch 24, Batch 474, LR 1.465922 Loss 4.817520, Accuracy 89.676%\n",
      "Epoch 24, Batch 475, LR 1.465790 Loss 4.816800, Accuracy 89.683%\n",
      "Epoch 24, Batch 476, LR 1.465658 Loss 4.818229, Accuracy 89.673%\n",
      "Epoch 24, Batch 477, LR 1.465526 Loss 4.819397, Accuracy 89.657%\n",
      "Epoch 24, Batch 478, LR 1.465394 Loss 4.819406, Accuracy 89.656%\n",
      "Epoch 24, Batch 479, LR 1.465262 Loss 4.819052, Accuracy 89.658%\n",
      "Epoch 24, Batch 480, LR 1.465130 Loss 4.819220, Accuracy 89.653%\n",
      "Epoch 24, Batch 481, LR 1.464998 Loss 4.818583, Accuracy 89.663%\n",
      "Epoch 24, Batch 482, LR 1.464866 Loss 4.819238, Accuracy 89.664%\n",
      "Epoch 24, Batch 483, LR 1.464734 Loss 4.819793, Accuracy 89.664%\n",
      "Epoch 24, Batch 484, LR 1.464602 Loss 4.819231, Accuracy 89.671%\n",
      "Epoch 24, Batch 485, LR 1.464471 Loss 4.818821, Accuracy 89.671%\n",
      "Epoch 24, Batch 486, LR 1.464339 Loss 4.820252, Accuracy 89.660%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 487, LR 1.464207 Loss 4.820269, Accuracy 89.659%\n",
      "Epoch 24, Batch 488, LR 1.464075 Loss 4.819810, Accuracy 89.663%\n",
      "Epoch 24, Batch 489, LR 1.463943 Loss 4.820645, Accuracy 89.655%\n",
      "Epoch 24, Batch 490, LR 1.463811 Loss 4.821023, Accuracy 89.656%\n",
      "Epoch 24, Batch 491, LR 1.463679 Loss 4.819541, Accuracy 89.656%\n",
      "Epoch 24, Batch 492, LR 1.463547 Loss 4.820272, Accuracy 89.652%\n",
      "Epoch 24, Batch 493, LR 1.463415 Loss 4.818854, Accuracy 89.660%\n",
      "Epoch 24, Batch 494, LR 1.463283 Loss 4.818843, Accuracy 89.656%\n",
      "Epoch 24, Batch 495, LR 1.463151 Loss 4.818033, Accuracy 89.656%\n",
      "Epoch 24, Batch 496, LR 1.463019 Loss 4.818525, Accuracy 89.653%\n",
      "Epoch 24, Batch 497, LR 1.462887 Loss 4.816939, Accuracy 89.665%\n",
      "Epoch 24, Batch 498, LR 1.462755 Loss 4.816470, Accuracy 89.668%\n",
      "Epoch 24, Batch 499, LR 1.462623 Loss 4.816518, Accuracy 89.670%\n",
      "Epoch 24, Batch 500, LR 1.462491 Loss 4.815876, Accuracy 89.675%\n",
      "Epoch 24, Batch 501, LR 1.462359 Loss 4.814927, Accuracy 89.680%\n",
      "Epoch 24, Batch 502, LR 1.462227 Loss 4.813866, Accuracy 89.685%\n",
      "Epoch 24, Batch 503, LR 1.462095 Loss 4.812553, Accuracy 89.684%\n",
      "Epoch 24, Batch 504, LR 1.461963 Loss 4.812170, Accuracy 89.687%\n",
      "Epoch 24, Batch 505, LR 1.461831 Loss 4.811567, Accuracy 89.692%\n",
      "Epoch 24, Batch 506, LR 1.461699 Loss 4.812421, Accuracy 89.689%\n",
      "Epoch 24, Batch 507, LR 1.461567 Loss 4.812607, Accuracy 89.685%\n",
      "Epoch 24, Batch 508, LR 1.461435 Loss 4.813072, Accuracy 89.679%\n",
      "Epoch 24, Batch 509, LR 1.461303 Loss 4.814534, Accuracy 89.680%\n",
      "Epoch 24, Batch 510, LR 1.461171 Loss 4.814300, Accuracy 89.681%\n",
      "Epoch 24, Batch 511, LR 1.461039 Loss 4.814376, Accuracy 89.671%\n",
      "Epoch 24, Batch 512, LR 1.460907 Loss 4.814563, Accuracy 89.662%\n",
      "Epoch 24, Batch 513, LR 1.460774 Loss 4.814212, Accuracy 89.658%\n",
      "Epoch 24, Batch 514, LR 1.460642 Loss 4.816217, Accuracy 89.648%\n",
      "Epoch 24, Batch 515, LR 1.460510 Loss 4.815092, Accuracy 89.653%\n",
      "Epoch 24, Batch 516, LR 1.460378 Loss 4.814257, Accuracy 89.654%\n",
      "Epoch 24, Batch 517, LR 1.460246 Loss 4.814750, Accuracy 89.659%\n",
      "Epoch 24, Batch 518, LR 1.460114 Loss 4.815582, Accuracy 89.660%\n",
      "Epoch 24, Batch 519, LR 1.459982 Loss 4.816343, Accuracy 89.657%\n",
      "Epoch 24, Batch 520, LR 1.459850 Loss 4.815123, Accuracy 89.665%\n",
      "Epoch 24, Batch 521, LR 1.459718 Loss 4.814313, Accuracy 89.674%\n",
      "Epoch 24, Batch 522, LR 1.459586 Loss 4.814224, Accuracy 89.678%\n",
      "Epoch 24, Batch 523, LR 1.459454 Loss 4.814249, Accuracy 89.679%\n",
      "Epoch 24, Batch 524, LR 1.459322 Loss 4.813640, Accuracy 89.686%\n",
      "Epoch 24, Batch 525, LR 1.459190 Loss 4.814096, Accuracy 89.686%\n",
      "Epoch 24, Batch 526, LR 1.459058 Loss 4.814552, Accuracy 89.683%\n",
      "Epoch 24, Batch 527, LR 1.458926 Loss 4.813146, Accuracy 89.690%\n",
      "Epoch 24, Batch 528, LR 1.458794 Loss 4.813011, Accuracy 89.690%\n",
      "Epoch 24, Batch 529, LR 1.458662 Loss 4.813058, Accuracy 89.684%\n",
      "Epoch 24, Batch 530, LR 1.458530 Loss 4.814253, Accuracy 89.677%\n",
      "Epoch 24, Batch 531, LR 1.458397 Loss 4.812080, Accuracy 89.680%\n",
      "Epoch 24, Batch 532, LR 1.458265 Loss 4.812621, Accuracy 89.676%\n",
      "Epoch 24, Batch 533, LR 1.458133 Loss 4.812876, Accuracy 89.677%\n",
      "Epoch 24, Batch 534, LR 1.458001 Loss 4.813278, Accuracy 89.676%\n",
      "Epoch 24, Batch 535, LR 1.457869 Loss 4.812629, Accuracy 89.683%\n",
      "Epoch 24, Batch 536, LR 1.457737 Loss 4.811956, Accuracy 89.686%\n",
      "Epoch 24, Batch 537, LR 1.457605 Loss 4.812591, Accuracy 89.679%\n",
      "Epoch 24, Batch 538, LR 1.457473 Loss 4.812544, Accuracy 89.677%\n",
      "Epoch 24, Batch 539, LR 1.457341 Loss 4.813999, Accuracy 89.665%\n",
      "Epoch 24, Batch 540, LR 1.457209 Loss 4.814681, Accuracy 89.657%\n",
      "Epoch 24, Batch 541, LR 1.457077 Loss 4.814510, Accuracy 89.659%\n",
      "Epoch 24, Batch 542, LR 1.456944 Loss 4.814580, Accuracy 89.658%\n",
      "Epoch 24, Batch 543, LR 1.456812 Loss 4.814882, Accuracy 89.651%\n",
      "Epoch 24, Batch 544, LR 1.456680 Loss 4.813801, Accuracy 89.658%\n",
      "Epoch 24, Batch 545, LR 1.456548 Loss 4.814394, Accuracy 89.657%\n",
      "Epoch 24, Batch 546, LR 1.456416 Loss 4.813979, Accuracy 89.663%\n",
      "Epoch 24, Batch 547, LR 1.456284 Loss 4.813461, Accuracy 89.668%\n",
      "Epoch 24, Batch 548, LR 1.456152 Loss 4.811964, Accuracy 89.676%\n",
      "Epoch 24, Batch 549, LR 1.456020 Loss 4.811623, Accuracy 89.673%\n",
      "Epoch 24, Batch 550, LR 1.455888 Loss 4.811903, Accuracy 89.673%\n",
      "Epoch 24, Batch 551, LR 1.455755 Loss 4.811251, Accuracy 89.672%\n",
      "Epoch 24, Batch 552, LR 1.455623 Loss 4.810797, Accuracy 89.670%\n",
      "Epoch 24, Batch 553, LR 1.455491 Loss 4.810790, Accuracy 89.676%\n",
      "Epoch 24, Batch 554, LR 1.455359 Loss 4.810893, Accuracy 89.680%\n",
      "Epoch 24, Batch 555, LR 1.455227 Loss 4.810939, Accuracy 89.676%\n",
      "Epoch 24, Batch 556, LR 1.455095 Loss 4.813731, Accuracy 89.667%\n",
      "Epoch 24, Batch 557, LR 1.454963 Loss 4.813806, Accuracy 89.670%\n",
      "Epoch 24, Batch 558, LR 1.454830 Loss 4.812628, Accuracy 89.666%\n",
      "Epoch 24, Batch 559, LR 1.454698 Loss 4.813321, Accuracy 89.662%\n",
      "Epoch 24, Batch 560, LR 1.454566 Loss 4.811762, Accuracy 89.669%\n",
      "Epoch 24, Batch 561, LR 1.454434 Loss 4.811222, Accuracy 89.672%\n",
      "Epoch 24, Batch 562, LR 1.454302 Loss 4.811832, Accuracy 89.671%\n",
      "Epoch 24, Batch 563, LR 1.454170 Loss 4.812640, Accuracy 89.668%\n",
      "Epoch 24, Batch 564, LR 1.454038 Loss 4.811751, Accuracy 89.673%\n",
      "Epoch 24, Batch 565, LR 1.453905 Loss 4.811222, Accuracy 89.670%\n",
      "Epoch 24, Batch 566, LR 1.453773 Loss 4.810679, Accuracy 89.670%\n",
      "Epoch 24, Batch 567, LR 1.453641 Loss 4.810584, Accuracy 89.673%\n",
      "Epoch 24, Batch 568, LR 1.453509 Loss 4.811613, Accuracy 89.662%\n",
      "Epoch 24, Batch 569, LR 1.453377 Loss 4.812043, Accuracy 89.661%\n",
      "Epoch 24, Batch 570, LR 1.453245 Loss 4.811444, Accuracy 89.661%\n",
      "Epoch 24, Batch 571, LR 1.453112 Loss 4.811808, Accuracy 89.663%\n",
      "Epoch 24, Batch 572, LR 1.452980 Loss 4.813613, Accuracy 89.654%\n",
      "Epoch 24, Batch 573, LR 1.452848 Loss 4.813574, Accuracy 89.652%\n",
      "Epoch 24, Batch 574, LR 1.452716 Loss 4.813993, Accuracy 89.657%\n",
      "Epoch 24, Batch 575, LR 1.452584 Loss 4.812813, Accuracy 89.663%\n",
      "Epoch 24, Batch 576, LR 1.452452 Loss 4.812587, Accuracy 89.666%\n",
      "Epoch 24, Batch 577, LR 1.452319 Loss 4.813463, Accuracy 89.669%\n",
      "Epoch 24, Batch 578, LR 1.452187 Loss 4.812976, Accuracy 89.672%\n",
      "Epoch 24, Batch 579, LR 1.452055 Loss 4.812692, Accuracy 89.667%\n",
      "Epoch 24, Batch 580, LR 1.451923 Loss 4.813294, Accuracy 89.657%\n",
      "Epoch 24, Batch 581, LR 1.451791 Loss 4.813611, Accuracy 89.661%\n",
      "Epoch 24, Batch 582, LR 1.451658 Loss 4.812584, Accuracy 89.664%\n",
      "Epoch 24, Batch 583, LR 1.451526 Loss 4.812921, Accuracy 89.666%\n",
      "Epoch 24, Batch 584, LR 1.451394 Loss 4.812576, Accuracy 89.671%\n",
      "Epoch 24, Batch 585, LR 1.451262 Loss 4.813678, Accuracy 89.667%\n",
      "Epoch 24, Batch 586, LR 1.451130 Loss 4.813577, Accuracy 89.665%\n",
      "Epoch 24, Batch 587, LR 1.450997 Loss 4.813946, Accuracy 89.660%\n",
      "Epoch 24, Batch 588, LR 1.450865 Loss 4.813785, Accuracy 89.663%\n",
      "Epoch 24, Batch 589, LR 1.450733 Loss 4.813429, Accuracy 89.666%\n",
      "Epoch 24, Batch 590, LR 1.450601 Loss 4.813458, Accuracy 89.666%\n",
      "Epoch 24, Batch 591, LR 1.450468 Loss 4.812650, Accuracy 89.665%\n",
      "Epoch 24, Batch 592, LR 1.450336 Loss 4.812067, Accuracy 89.667%\n",
      "Epoch 24, Batch 593, LR 1.450204 Loss 4.812678, Accuracy 89.665%\n",
      "Epoch 24, Batch 594, LR 1.450072 Loss 4.813099, Accuracy 89.668%\n",
      "Epoch 24, Batch 595, LR 1.449940 Loss 4.813665, Accuracy 89.664%\n",
      "Epoch 24, Batch 596, LR 1.449807 Loss 4.813200, Accuracy 89.665%\n",
      "Epoch 24, Batch 597, LR 1.449675 Loss 4.813450, Accuracy 89.659%\n",
      "Epoch 24, Batch 598, LR 1.449543 Loss 4.813781, Accuracy 89.653%\n",
      "Epoch 24, Batch 599, LR 1.449411 Loss 4.812722, Accuracy 89.660%\n",
      "Epoch 24, Batch 600, LR 1.449278 Loss 4.813064, Accuracy 89.658%\n",
      "Epoch 24, Batch 601, LR 1.449146 Loss 4.813391, Accuracy 89.660%\n",
      "Epoch 24, Batch 602, LR 1.449014 Loss 4.811380, Accuracy 89.669%\n",
      "Epoch 24, Batch 603, LR 1.448882 Loss 4.812458, Accuracy 89.656%\n",
      "Epoch 24, Batch 604, LR 1.448749 Loss 4.812904, Accuracy 89.655%\n",
      "Epoch 24, Batch 605, LR 1.448617 Loss 4.813267, Accuracy 89.654%\n",
      "Epoch 24, Batch 606, LR 1.448485 Loss 4.813187, Accuracy 89.652%\n",
      "Epoch 24, Batch 607, LR 1.448353 Loss 4.814097, Accuracy 89.648%\n",
      "Epoch 24, Batch 608, LR 1.448220 Loss 4.815478, Accuracy 89.637%\n",
      "Epoch 24, Batch 609, LR 1.448088 Loss 4.815842, Accuracy 89.638%\n",
      "Epoch 24, Batch 610, LR 1.447956 Loss 4.814239, Accuracy 89.647%\n",
      "Epoch 24, Batch 611, LR 1.447824 Loss 4.813386, Accuracy 89.653%\n",
      "Epoch 24, Batch 612, LR 1.447691 Loss 4.813849, Accuracy 89.656%\n",
      "Epoch 24, Batch 613, LR 1.447559 Loss 4.814934, Accuracy 89.653%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 614, LR 1.447427 Loss 4.814088, Accuracy 89.654%\n",
      "Epoch 24, Batch 615, LR 1.447295 Loss 4.814341, Accuracy 89.646%\n",
      "Epoch 24, Batch 616, LR 1.447162 Loss 4.813621, Accuracy 89.652%\n",
      "Epoch 24, Batch 617, LR 1.447030 Loss 4.813788, Accuracy 89.651%\n",
      "Epoch 24, Batch 618, LR 1.446898 Loss 4.813258, Accuracy 89.653%\n",
      "Epoch 24, Batch 619, LR 1.446765 Loss 4.811921, Accuracy 89.654%\n",
      "Epoch 24, Batch 620, LR 1.446633 Loss 4.812432, Accuracy 89.652%\n",
      "Epoch 24, Batch 621, LR 1.446501 Loss 4.812655, Accuracy 89.656%\n",
      "Epoch 24, Batch 622, LR 1.446369 Loss 4.813542, Accuracy 89.657%\n",
      "Epoch 24, Batch 623, LR 1.446236 Loss 4.812522, Accuracy 89.663%\n",
      "Epoch 24, Batch 624, LR 1.446104 Loss 4.812614, Accuracy 89.666%\n",
      "Epoch 24, Batch 625, LR 1.445972 Loss 4.813235, Accuracy 89.664%\n",
      "Epoch 24, Batch 626, LR 1.445839 Loss 4.814203, Accuracy 89.659%\n",
      "Epoch 24, Batch 627, LR 1.445707 Loss 4.813948, Accuracy 89.658%\n",
      "Epoch 24, Batch 628, LR 1.445575 Loss 4.814957, Accuracy 89.657%\n",
      "Epoch 24, Batch 629, LR 1.445443 Loss 4.814692, Accuracy 89.656%\n",
      "Epoch 24, Batch 630, LR 1.445310 Loss 4.814158, Accuracy 89.654%\n",
      "Epoch 24, Batch 631, LR 1.445178 Loss 4.814277, Accuracy 89.654%\n",
      "Epoch 24, Batch 632, LR 1.445046 Loss 4.813873, Accuracy 89.656%\n",
      "Epoch 24, Batch 633, LR 1.444913 Loss 4.812549, Accuracy 89.661%\n",
      "Epoch 24, Batch 634, LR 1.444781 Loss 4.813252, Accuracy 89.654%\n",
      "Epoch 24, Batch 635, LR 1.444649 Loss 4.813965, Accuracy 89.654%\n",
      "Epoch 24, Batch 636, LR 1.444516 Loss 4.813691, Accuracy 89.651%\n",
      "Epoch 24, Batch 637, LR 1.444384 Loss 4.813046, Accuracy 89.656%\n",
      "Epoch 24, Batch 638, LR 1.444252 Loss 4.813729, Accuracy 89.655%\n",
      "Epoch 24, Batch 639, LR 1.444119 Loss 4.813702, Accuracy 89.654%\n",
      "Epoch 24, Batch 640, LR 1.443987 Loss 4.813706, Accuracy 89.659%\n",
      "Epoch 24, Batch 641, LR 1.443855 Loss 4.814621, Accuracy 89.660%\n",
      "Epoch 24, Batch 642, LR 1.443722 Loss 4.813622, Accuracy 89.664%\n",
      "Epoch 24, Batch 643, LR 1.443590 Loss 4.814289, Accuracy 89.663%\n",
      "Epoch 24, Batch 644, LR 1.443458 Loss 4.814163, Accuracy 89.657%\n",
      "Epoch 24, Batch 645, LR 1.443325 Loss 4.813384, Accuracy 89.657%\n",
      "Epoch 24, Batch 646, LR 1.443193 Loss 4.813707, Accuracy 89.658%\n",
      "Epoch 24, Batch 647, LR 1.443061 Loss 4.814109, Accuracy 89.654%\n",
      "Epoch 24, Batch 648, LR 1.442928 Loss 4.815071, Accuracy 89.645%\n",
      "Epoch 24, Batch 649, LR 1.442796 Loss 4.814651, Accuracy 89.645%\n",
      "Epoch 24, Batch 650, LR 1.442664 Loss 4.814981, Accuracy 89.645%\n",
      "Epoch 24, Batch 651, LR 1.442531 Loss 4.814200, Accuracy 89.652%\n",
      "Epoch 24, Batch 652, LR 1.442399 Loss 4.813710, Accuracy 89.653%\n",
      "Epoch 24, Batch 653, LR 1.442267 Loss 4.813470, Accuracy 89.657%\n",
      "Epoch 24, Batch 654, LR 1.442134 Loss 4.813436, Accuracy 89.656%\n",
      "Epoch 24, Batch 655, LR 1.442002 Loss 4.812617, Accuracy 89.659%\n",
      "Epoch 24, Batch 656, LR 1.441869 Loss 4.812598, Accuracy 89.654%\n",
      "Epoch 24, Batch 657, LR 1.441737 Loss 4.812789, Accuracy 89.653%\n",
      "Epoch 24, Batch 658, LR 1.441605 Loss 4.814223, Accuracy 89.643%\n",
      "Epoch 24, Batch 659, LR 1.441472 Loss 4.813592, Accuracy 89.647%\n",
      "Epoch 24, Batch 660, LR 1.441340 Loss 4.812827, Accuracy 89.651%\n",
      "Epoch 24, Batch 661, LR 1.441208 Loss 4.812697, Accuracy 89.650%\n",
      "Epoch 24, Batch 662, LR 1.441075 Loss 4.812621, Accuracy 89.649%\n",
      "Epoch 24, Batch 663, LR 1.440943 Loss 4.812775, Accuracy 89.647%\n",
      "Epoch 24, Batch 664, LR 1.440810 Loss 4.813235, Accuracy 89.639%\n",
      "Epoch 24, Batch 665, LR 1.440678 Loss 4.813425, Accuracy 89.633%\n",
      "Epoch 24, Batch 666, LR 1.440546 Loss 4.811840, Accuracy 89.642%\n",
      "Epoch 24, Batch 667, LR 1.440413 Loss 4.811681, Accuracy 89.639%\n",
      "Epoch 24, Batch 668, LR 1.440281 Loss 4.812126, Accuracy 89.640%\n",
      "Epoch 24, Batch 669, LR 1.440148 Loss 4.811030, Accuracy 89.644%\n",
      "Epoch 24, Batch 670, LR 1.440016 Loss 4.811903, Accuracy 89.643%\n",
      "Epoch 24, Batch 671, LR 1.439884 Loss 4.811782, Accuracy 89.645%\n",
      "Epoch 24, Batch 672, LR 1.439751 Loss 4.811821, Accuracy 89.646%\n",
      "Epoch 24, Batch 673, LR 1.439619 Loss 4.811948, Accuracy 89.643%\n",
      "Epoch 24, Batch 674, LR 1.439487 Loss 4.811797, Accuracy 89.644%\n",
      "Epoch 24, Batch 675, LR 1.439354 Loss 4.811715, Accuracy 89.645%\n",
      "Epoch 24, Batch 676, LR 1.439222 Loss 4.811993, Accuracy 89.642%\n",
      "Epoch 24, Batch 677, LR 1.439089 Loss 4.813145, Accuracy 89.634%\n",
      "Epoch 24, Batch 678, LR 1.438957 Loss 4.811797, Accuracy 89.640%\n",
      "Epoch 24, Batch 679, LR 1.438824 Loss 4.811746, Accuracy 89.641%\n",
      "Epoch 24, Batch 680, LR 1.438692 Loss 4.811539, Accuracy 89.645%\n",
      "Epoch 24, Batch 681, LR 1.438560 Loss 4.811073, Accuracy 89.650%\n",
      "Epoch 24, Batch 682, LR 1.438427 Loss 4.811261, Accuracy 89.649%\n",
      "Epoch 24, Batch 683, LR 1.438295 Loss 4.810060, Accuracy 89.656%\n",
      "Epoch 24, Batch 684, LR 1.438162 Loss 4.810332, Accuracy 89.653%\n",
      "Epoch 24, Batch 685, LR 1.438030 Loss 4.810438, Accuracy 89.653%\n",
      "Epoch 24, Batch 686, LR 1.437897 Loss 4.810054, Accuracy 89.657%\n",
      "Epoch 24, Batch 687, LR 1.437765 Loss 4.810460, Accuracy 89.655%\n",
      "Epoch 24, Batch 688, LR 1.437633 Loss 4.809274, Accuracy 89.661%\n",
      "Epoch 24, Batch 689, LR 1.437500 Loss 4.809723, Accuracy 89.659%\n",
      "Epoch 24, Batch 690, LR 1.437368 Loss 4.809292, Accuracy 89.666%\n",
      "Epoch 24, Batch 691, LR 1.437235 Loss 4.809525, Accuracy 89.670%\n",
      "Epoch 24, Batch 692, LR 1.437103 Loss 4.811464, Accuracy 89.663%\n",
      "Epoch 24, Batch 693, LR 1.436970 Loss 4.810389, Accuracy 89.669%\n",
      "Epoch 24, Batch 694, LR 1.436838 Loss 4.811152, Accuracy 89.665%\n",
      "Epoch 24, Batch 695, LR 1.436706 Loss 4.810818, Accuracy 89.671%\n",
      "Epoch 24, Batch 696, LR 1.436573 Loss 4.810556, Accuracy 89.672%\n",
      "Epoch 24, Batch 697, LR 1.436441 Loss 4.810773, Accuracy 89.673%\n",
      "Epoch 24, Batch 698, LR 1.436308 Loss 4.810782, Accuracy 89.674%\n",
      "Epoch 24, Batch 699, LR 1.436176 Loss 4.810980, Accuracy 89.669%\n",
      "Epoch 24, Batch 700, LR 1.436043 Loss 4.810497, Accuracy 89.674%\n",
      "Epoch 24, Batch 701, LR 1.435911 Loss 4.810805, Accuracy 89.675%\n",
      "Epoch 24, Batch 702, LR 1.435778 Loss 4.810883, Accuracy 89.676%\n",
      "Epoch 24, Batch 703, LR 1.435646 Loss 4.810685, Accuracy 89.679%\n",
      "Epoch 24, Batch 704, LR 1.435513 Loss 4.810617, Accuracy 89.678%\n",
      "Epoch 24, Batch 705, LR 1.435381 Loss 4.809912, Accuracy 89.680%\n",
      "Epoch 24, Batch 706, LR 1.435248 Loss 4.810242, Accuracy 89.676%\n",
      "Epoch 24, Batch 707, LR 1.435116 Loss 4.811319, Accuracy 89.669%\n",
      "Epoch 24, Batch 708, LR 1.434983 Loss 4.812017, Accuracy 89.665%\n",
      "Epoch 24, Batch 709, LR 1.434851 Loss 4.812954, Accuracy 89.660%\n",
      "Epoch 24, Batch 710, LR 1.434719 Loss 4.812478, Accuracy 89.665%\n",
      "Epoch 24, Batch 711, LR 1.434586 Loss 4.812324, Accuracy 89.667%\n",
      "Epoch 24, Batch 712, LR 1.434454 Loss 4.811174, Accuracy 89.675%\n",
      "Epoch 24, Batch 713, LR 1.434321 Loss 4.810949, Accuracy 89.678%\n",
      "Epoch 24, Batch 714, LR 1.434189 Loss 4.811710, Accuracy 89.671%\n",
      "Epoch 24, Batch 715, LR 1.434056 Loss 4.811981, Accuracy 89.673%\n",
      "Epoch 24, Batch 716, LR 1.433924 Loss 4.812301, Accuracy 89.676%\n",
      "Epoch 24, Batch 717, LR 1.433791 Loss 4.812676, Accuracy 89.673%\n",
      "Epoch 24, Batch 718, LR 1.433659 Loss 4.812440, Accuracy 89.676%\n",
      "Epoch 24, Batch 719, LR 1.433526 Loss 4.811309, Accuracy 89.679%\n",
      "Epoch 24, Batch 720, LR 1.433394 Loss 4.810558, Accuracy 89.685%\n",
      "Epoch 24, Batch 721, LR 1.433261 Loss 4.810777, Accuracy 89.687%\n",
      "Epoch 24, Batch 722, LR 1.433129 Loss 4.810114, Accuracy 89.694%\n",
      "Epoch 24, Batch 723, LR 1.432996 Loss 4.811144, Accuracy 89.690%\n",
      "Epoch 24, Batch 724, LR 1.432864 Loss 4.810465, Accuracy 89.693%\n",
      "Epoch 24, Batch 725, LR 1.432731 Loss 4.812081, Accuracy 89.683%\n",
      "Epoch 24, Batch 726, LR 1.432599 Loss 4.811497, Accuracy 89.686%\n",
      "Epoch 24, Batch 727, LR 1.432466 Loss 4.812190, Accuracy 89.685%\n",
      "Epoch 24, Batch 728, LR 1.432333 Loss 4.812329, Accuracy 89.680%\n",
      "Epoch 24, Batch 729, LR 1.432201 Loss 4.811709, Accuracy 89.683%\n",
      "Epoch 24, Batch 730, LR 1.432068 Loss 4.810821, Accuracy 89.689%\n",
      "Epoch 24, Batch 731, LR 1.431936 Loss 4.810218, Accuracy 89.691%\n",
      "Epoch 24, Batch 732, LR 1.431803 Loss 4.810408, Accuracy 89.689%\n",
      "Epoch 24, Batch 733, LR 1.431671 Loss 4.810152, Accuracy 89.691%\n",
      "Epoch 24, Batch 734, LR 1.431538 Loss 4.810539, Accuracy 89.690%\n",
      "Epoch 24, Batch 735, LR 1.431406 Loss 4.809783, Accuracy 89.694%\n",
      "Epoch 24, Batch 736, LR 1.431273 Loss 4.809380, Accuracy 89.694%\n",
      "Epoch 24, Batch 737, LR 1.431141 Loss 4.809480, Accuracy 89.694%\n",
      "Epoch 24, Batch 738, LR 1.431008 Loss 4.810139, Accuracy 89.687%\n",
      "Epoch 24, Batch 739, LR 1.430876 Loss 4.810101, Accuracy 89.684%\n",
      "Epoch 24, Batch 740, LR 1.430743 Loss 4.810426, Accuracy 89.679%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 741, LR 1.430611 Loss 4.810673, Accuracy 89.674%\n",
      "Epoch 24, Batch 742, LR 1.430478 Loss 4.811109, Accuracy 89.674%\n",
      "Epoch 24, Batch 743, LR 1.430345 Loss 4.810054, Accuracy 89.674%\n",
      "Epoch 24, Batch 744, LR 1.430213 Loss 4.810208, Accuracy 89.672%\n",
      "Epoch 24, Batch 745, LR 1.430080 Loss 4.810274, Accuracy 89.670%\n",
      "Epoch 24, Batch 746, LR 1.429948 Loss 4.811392, Accuracy 89.668%\n",
      "Epoch 24, Batch 747, LR 1.429815 Loss 4.811969, Accuracy 89.667%\n",
      "Epoch 24, Batch 748, LR 1.429683 Loss 4.812465, Accuracy 89.663%\n",
      "Epoch 24, Batch 749, LR 1.429550 Loss 4.812123, Accuracy 89.662%\n",
      "Epoch 24, Batch 750, LR 1.429418 Loss 4.811995, Accuracy 89.661%\n",
      "Epoch 24, Batch 751, LR 1.429285 Loss 4.812380, Accuracy 89.659%\n",
      "Epoch 24, Batch 752, LR 1.429152 Loss 4.812371, Accuracy 89.658%\n",
      "Epoch 24, Batch 753, LR 1.429020 Loss 4.812463, Accuracy 89.655%\n",
      "Epoch 24, Batch 754, LR 1.428887 Loss 4.813479, Accuracy 89.651%\n",
      "Epoch 24, Batch 755, LR 1.428755 Loss 4.813513, Accuracy 89.649%\n",
      "Epoch 24, Batch 756, LR 1.428622 Loss 4.812976, Accuracy 89.652%\n",
      "Epoch 24, Batch 757, LR 1.428490 Loss 4.812861, Accuracy 89.653%\n",
      "Epoch 24, Batch 758, LR 1.428357 Loss 4.813209, Accuracy 89.653%\n",
      "Epoch 24, Batch 759, LR 1.428224 Loss 4.812773, Accuracy 89.654%\n",
      "Epoch 24, Batch 760, LR 1.428092 Loss 4.813417, Accuracy 89.653%\n",
      "Epoch 24, Batch 761, LR 1.427959 Loss 4.814378, Accuracy 89.646%\n",
      "Epoch 24, Batch 762, LR 1.427827 Loss 4.814505, Accuracy 89.647%\n",
      "Epoch 24, Batch 763, LR 1.427694 Loss 4.814222, Accuracy 89.649%\n",
      "Epoch 24, Batch 764, LR 1.427561 Loss 4.814159, Accuracy 89.646%\n",
      "Epoch 24, Batch 765, LR 1.427429 Loss 4.813803, Accuracy 89.648%\n",
      "Epoch 24, Batch 766, LR 1.427296 Loss 4.813585, Accuracy 89.647%\n",
      "Epoch 24, Batch 767, LR 1.427164 Loss 4.813415, Accuracy 89.652%\n",
      "Epoch 24, Batch 768, LR 1.427031 Loss 4.812265, Accuracy 89.659%\n",
      "Epoch 24, Batch 769, LR 1.426898 Loss 4.812628, Accuracy 89.658%\n",
      "Epoch 24, Batch 770, LR 1.426766 Loss 4.812356, Accuracy 89.658%\n",
      "Epoch 24, Batch 771, LR 1.426633 Loss 4.813557, Accuracy 89.649%\n",
      "Epoch 24, Batch 772, LR 1.426501 Loss 4.813295, Accuracy 89.652%\n",
      "Epoch 24, Batch 773, LR 1.426368 Loss 4.812463, Accuracy 89.656%\n",
      "Epoch 24, Batch 774, LR 1.426235 Loss 4.812098, Accuracy 89.658%\n",
      "Epoch 24, Batch 775, LR 1.426103 Loss 4.812342, Accuracy 89.660%\n",
      "Epoch 24, Batch 776, LR 1.425970 Loss 4.812481, Accuracy 89.663%\n",
      "Epoch 24, Batch 777, LR 1.425837 Loss 4.812089, Accuracy 89.662%\n",
      "Epoch 24, Batch 778, LR 1.425705 Loss 4.811660, Accuracy 89.663%\n",
      "Epoch 24, Batch 779, LR 1.425572 Loss 4.812382, Accuracy 89.658%\n",
      "Epoch 24, Batch 780, LR 1.425440 Loss 4.811926, Accuracy 89.658%\n",
      "Epoch 24, Batch 781, LR 1.425307 Loss 4.811545, Accuracy 89.660%\n",
      "Epoch 24, Batch 782, LR 1.425174 Loss 4.811134, Accuracy 89.660%\n",
      "Epoch 24, Batch 783, LR 1.425042 Loss 4.812227, Accuracy 89.651%\n",
      "Epoch 24, Batch 784, LR 1.424909 Loss 4.812334, Accuracy 89.651%\n",
      "Epoch 24, Batch 785, LR 1.424776 Loss 4.812039, Accuracy 89.654%\n",
      "Epoch 24, Batch 786, LR 1.424644 Loss 4.810857, Accuracy 89.657%\n",
      "Epoch 24, Batch 787, LR 1.424511 Loss 4.810236, Accuracy 89.661%\n",
      "Epoch 24, Batch 788, LR 1.424379 Loss 4.810340, Accuracy 89.656%\n",
      "Epoch 24, Batch 789, LR 1.424246 Loss 4.810597, Accuracy 89.655%\n",
      "Epoch 24, Batch 790, LR 1.424113 Loss 4.809584, Accuracy 89.656%\n",
      "Epoch 24, Batch 791, LR 1.423981 Loss 4.810128, Accuracy 89.657%\n",
      "Epoch 24, Batch 792, LR 1.423848 Loss 4.809374, Accuracy 89.659%\n",
      "Epoch 24, Batch 793, LR 1.423715 Loss 4.809509, Accuracy 89.663%\n",
      "Epoch 24, Batch 794, LR 1.423583 Loss 4.810327, Accuracy 89.661%\n",
      "Epoch 24, Batch 795, LR 1.423450 Loss 4.810154, Accuracy 89.664%\n",
      "Epoch 24, Batch 796, LR 1.423317 Loss 4.810477, Accuracy 89.661%\n",
      "Epoch 24, Batch 797, LR 1.423185 Loss 4.811088, Accuracy 89.659%\n",
      "Epoch 24, Batch 798, LR 1.423052 Loss 4.811255, Accuracy 89.656%\n",
      "Epoch 24, Batch 799, LR 1.422919 Loss 4.811543, Accuracy 89.657%\n",
      "Epoch 24, Batch 800, LR 1.422787 Loss 4.811520, Accuracy 89.658%\n",
      "Epoch 24, Batch 801, LR 1.422654 Loss 4.812516, Accuracy 89.657%\n",
      "Epoch 24, Batch 802, LR 1.422521 Loss 4.812752, Accuracy 89.658%\n",
      "Epoch 24, Batch 803, LR 1.422389 Loss 4.812044, Accuracy 89.661%\n",
      "Epoch 24, Batch 804, LR 1.422256 Loss 4.811702, Accuracy 89.663%\n",
      "Epoch 24, Batch 805, LR 1.422123 Loss 4.811717, Accuracy 89.663%\n",
      "Epoch 24, Batch 806, LR 1.421991 Loss 4.811791, Accuracy 89.663%\n",
      "Epoch 24, Batch 807, LR 1.421858 Loss 4.812181, Accuracy 89.660%\n",
      "Epoch 24, Batch 808, LR 1.421725 Loss 4.813046, Accuracy 89.655%\n",
      "Epoch 24, Batch 809, LR 1.421593 Loss 4.813000, Accuracy 89.657%\n",
      "Epoch 24, Batch 810, LR 1.421460 Loss 4.812842, Accuracy 89.663%\n",
      "Epoch 24, Batch 811, LR 1.421327 Loss 4.813382, Accuracy 89.660%\n",
      "Epoch 24, Batch 812, LR 1.421195 Loss 4.814305, Accuracy 89.655%\n",
      "Epoch 24, Batch 813, LR 1.421062 Loss 4.814561, Accuracy 89.657%\n",
      "Epoch 24, Batch 814, LR 1.420929 Loss 4.814429, Accuracy 89.660%\n",
      "Epoch 24, Batch 815, LR 1.420796 Loss 4.814224, Accuracy 89.660%\n",
      "Epoch 24, Batch 816, LR 1.420664 Loss 4.814793, Accuracy 89.656%\n",
      "Epoch 24, Batch 817, LR 1.420531 Loss 4.815421, Accuracy 89.656%\n",
      "Epoch 24, Batch 818, LR 1.420398 Loss 4.814954, Accuracy 89.661%\n",
      "Epoch 24, Batch 819, LR 1.420266 Loss 4.815354, Accuracy 89.660%\n",
      "Epoch 24, Batch 820, LR 1.420133 Loss 4.815957, Accuracy 89.660%\n",
      "Epoch 24, Batch 821, LR 1.420000 Loss 4.815645, Accuracy 89.662%\n",
      "Epoch 24, Batch 822, LR 1.419868 Loss 4.816167, Accuracy 89.660%\n",
      "Epoch 24, Batch 823, LR 1.419735 Loss 4.817252, Accuracy 89.662%\n",
      "Epoch 24, Batch 824, LR 1.419602 Loss 4.817648, Accuracy 89.656%\n",
      "Epoch 24, Batch 825, LR 1.419469 Loss 4.816954, Accuracy 89.660%\n",
      "Epoch 24, Batch 826, LR 1.419337 Loss 4.817646, Accuracy 89.657%\n",
      "Epoch 24, Batch 827, LR 1.419204 Loss 4.817616, Accuracy 89.660%\n",
      "Epoch 24, Batch 828, LR 1.419071 Loss 4.817733, Accuracy 89.658%\n",
      "Epoch 24, Batch 829, LR 1.418938 Loss 4.817609, Accuracy 89.655%\n",
      "Epoch 24, Batch 830, LR 1.418806 Loss 4.817464, Accuracy 89.657%\n",
      "Epoch 24, Batch 831, LR 1.418673 Loss 4.817148, Accuracy 89.656%\n",
      "Epoch 24, Batch 832, LR 1.418540 Loss 4.817132, Accuracy 89.656%\n",
      "Epoch 24, Batch 833, LR 1.418408 Loss 4.817377, Accuracy 89.655%\n",
      "Epoch 24, Batch 834, LR 1.418275 Loss 4.817908, Accuracy 89.650%\n",
      "Epoch 24, Batch 835, LR 1.418142 Loss 4.818225, Accuracy 89.650%\n",
      "Epoch 24, Batch 836, LR 1.418009 Loss 4.817991, Accuracy 89.652%\n",
      "Epoch 24, Batch 837, LR 1.417877 Loss 4.817181, Accuracy 89.657%\n",
      "Epoch 24, Batch 838, LR 1.417744 Loss 4.817355, Accuracy 89.658%\n",
      "Epoch 24, Batch 839, LR 1.417611 Loss 4.818462, Accuracy 89.650%\n",
      "Epoch 24, Batch 840, LR 1.417478 Loss 4.818393, Accuracy 89.652%\n",
      "Epoch 24, Batch 841, LR 1.417346 Loss 4.818104, Accuracy 89.653%\n",
      "Epoch 24, Batch 842, LR 1.417213 Loss 4.817872, Accuracy 89.654%\n",
      "Epoch 24, Batch 843, LR 1.417080 Loss 4.817945, Accuracy 89.654%\n",
      "Epoch 24, Batch 844, LR 1.416947 Loss 4.817254, Accuracy 89.657%\n",
      "Epoch 24, Batch 845, LR 1.416815 Loss 4.817410, Accuracy 89.654%\n",
      "Epoch 24, Batch 846, LR 1.416682 Loss 4.817848, Accuracy 89.648%\n",
      "Epoch 24, Batch 847, LR 1.416549 Loss 4.817692, Accuracy 89.647%\n",
      "Epoch 24, Batch 848, LR 1.416416 Loss 4.817638, Accuracy 89.646%\n",
      "Epoch 24, Batch 849, LR 1.416284 Loss 4.817881, Accuracy 89.646%\n",
      "Epoch 24, Batch 850, LR 1.416151 Loss 4.817716, Accuracy 89.645%\n",
      "Epoch 24, Batch 851, LR 1.416018 Loss 4.817728, Accuracy 89.644%\n",
      "Epoch 24, Batch 852, LR 1.415885 Loss 4.818525, Accuracy 89.641%\n",
      "Epoch 24, Batch 853, LR 1.415753 Loss 4.818652, Accuracy 89.640%\n",
      "Epoch 24, Batch 854, LR 1.415620 Loss 4.819853, Accuracy 89.634%\n",
      "Epoch 24, Batch 855, LR 1.415487 Loss 4.820450, Accuracy 89.630%\n",
      "Epoch 24, Batch 856, LR 1.415354 Loss 4.821119, Accuracy 89.620%\n",
      "Epoch 24, Batch 857, LR 1.415221 Loss 4.820439, Accuracy 89.624%\n",
      "Epoch 24, Batch 858, LR 1.415089 Loss 4.821234, Accuracy 89.616%\n",
      "Epoch 24, Batch 859, LR 1.414956 Loss 4.820907, Accuracy 89.615%\n",
      "Epoch 24, Batch 860, LR 1.414823 Loss 4.821828, Accuracy 89.609%\n",
      "Epoch 24, Batch 861, LR 1.414690 Loss 4.822057, Accuracy 89.608%\n",
      "Epoch 24, Batch 862, LR 1.414558 Loss 4.821841, Accuracy 89.606%\n",
      "Epoch 24, Batch 863, LR 1.414425 Loss 4.821636, Accuracy 89.607%\n",
      "Epoch 24, Batch 864, LR 1.414292 Loss 4.822295, Accuracy 89.601%\n",
      "Epoch 24, Batch 865, LR 1.414159 Loss 4.821938, Accuracy 89.604%\n",
      "Epoch 24, Batch 866, LR 1.414026 Loss 4.821627, Accuracy 89.606%\n",
      "Epoch 24, Batch 867, LR 1.413894 Loss 4.821864, Accuracy 89.608%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 868, LR 1.413761 Loss 4.822322, Accuracy 89.607%\n",
      "Epoch 24, Batch 869, LR 1.413628 Loss 4.821668, Accuracy 89.609%\n",
      "Epoch 24, Batch 870, LR 1.413495 Loss 4.822677, Accuracy 89.604%\n",
      "Epoch 24, Batch 871, LR 1.413362 Loss 4.823160, Accuracy 89.599%\n",
      "Epoch 24, Batch 872, LR 1.413230 Loss 4.822766, Accuracy 89.601%\n",
      "Epoch 24, Batch 873, LR 1.413097 Loss 4.822541, Accuracy 89.603%\n",
      "Epoch 24, Batch 874, LR 1.412964 Loss 4.822310, Accuracy 89.605%\n",
      "Epoch 24, Batch 875, LR 1.412831 Loss 4.822726, Accuracy 89.602%\n",
      "Epoch 24, Batch 876, LR 1.412698 Loss 4.822618, Accuracy 89.602%\n",
      "Epoch 24, Batch 877, LR 1.412566 Loss 4.822274, Accuracy 89.602%\n",
      "Epoch 24, Batch 878, LR 1.412433 Loss 4.821864, Accuracy 89.604%\n",
      "Epoch 24, Batch 879, LR 1.412300 Loss 4.822001, Accuracy 89.602%\n",
      "Epoch 24, Batch 880, LR 1.412167 Loss 4.822072, Accuracy 89.600%\n",
      "Epoch 24, Batch 881, LR 1.412034 Loss 4.821912, Accuracy 89.605%\n",
      "Epoch 24, Batch 882, LR 1.411901 Loss 4.821629, Accuracy 89.605%\n",
      "Epoch 24, Batch 883, LR 1.411769 Loss 4.821525, Accuracy 89.606%\n",
      "Epoch 24, Batch 884, LR 1.411636 Loss 4.821292, Accuracy 89.606%\n",
      "Epoch 24, Batch 885, LR 1.411503 Loss 4.821113, Accuracy 89.605%\n",
      "Epoch 24, Batch 886, LR 1.411370 Loss 4.821374, Accuracy 89.601%\n",
      "Epoch 24, Batch 887, LR 1.411237 Loss 4.821203, Accuracy 89.600%\n",
      "Epoch 24, Batch 888, LR 1.411104 Loss 4.821794, Accuracy 89.599%\n",
      "Epoch 24, Batch 889, LR 1.410972 Loss 4.821364, Accuracy 89.605%\n",
      "Epoch 24, Batch 890, LR 1.410839 Loss 4.822025, Accuracy 89.597%\n",
      "Epoch 24, Batch 891, LR 1.410706 Loss 4.822152, Accuracy 89.595%\n",
      "Epoch 24, Batch 892, LR 1.410573 Loss 4.822635, Accuracy 89.592%\n",
      "Epoch 24, Batch 893, LR 1.410440 Loss 4.822801, Accuracy 89.594%\n",
      "Epoch 24, Batch 894, LR 1.410307 Loss 4.822476, Accuracy 89.593%\n",
      "Epoch 24, Batch 895, LR 1.410175 Loss 4.822607, Accuracy 89.591%\n",
      "Epoch 24, Batch 896, LR 1.410042 Loss 4.823509, Accuracy 89.586%\n",
      "Epoch 24, Batch 897, LR 1.409909 Loss 4.823904, Accuracy 89.581%\n",
      "Epoch 24, Batch 898, LR 1.409776 Loss 4.823458, Accuracy 89.582%\n",
      "Epoch 24, Batch 899, LR 1.409643 Loss 4.823809, Accuracy 89.583%\n",
      "Epoch 24, Batch 900, LR 1.409510 Loss 4.823378, Accuracy 89.583%\n",
      "Epoch 24, Batch 901, LR 1.409377 Loss 4.823780, Accuracy 89.581%\n",
      "Epoch 24, Batch 902, LR 1.409245 Loss 4.824378, Accuracy 89.582%\n",
      "Epoch 24, Batch 903, LR 1.409112 Loss 4.825191, Accuracy 89.577%\n",
      "Epoch 24, Batch 904, LR 1.408979 Loss 4.824921, Accuracy 89.580%\n",
      "Epoch 24, Batch 905, LR 1.408846 Loss 4.825550, Accuracy 89.577%\n",
      "Epoch 24, Batch 906, LR 1.408713 Loss 4.825151, Accuracy 89.581%\n",
      "Epoch 24, Batch 907, LR 1.408580 Loss 4.824843, Accuracy 89.583%\n",
      "Epoch 24, Batch 908, LR 1.408447 Loss 4.825254, Accuracy 89.580%\n",
      "Epoch 24, Batch 909, LR 1.408314 Loss 4.824654, Accuracy 89.586%\n",
      "Epoch 24, Batch 910, LR 1.408182 Loss 4.824297, Accuracy 89.590%\n",
      "Epoch 24, Batch 911, LR 1.408049 Loss 4.824961, Accuracy 89.586%\n",
      "Epoch 24, Batch 912, LR 1.407916 Loss 4.824648, Accuracy 89.588%\n",
      "Epoch 24, Batch 913, LR 1.407783 Loss 4.824287, Accuracy 89.590%\n",
      "Epoch 24, Batch 914, LR 1.407650 Loss 4.825052, Accuracy 89.586%\n",
      "Epoch 24, Batch 915, LR 1.407517 Loss 4.825821, Accuracy 89.585%\n",
      "Epoch 24, Batch 916, LR 1.407384 Loss 4.825378, Accuracy 89.580%\n",
      "Epoch 24, Batch 917, LR 1.407251 Loss 4.826729, Accuracy 89.573%\n",
      "Epoch 24, Batch 918, LR 1.407119 Loss 4.826585, Accuracy 89.576%\n",
      "Epoch 24, Batch 919, LR 1.406986 Loss 4.826705, Accuracy 89.574%\n",
      "Epoch 24, Batch 920, LR 1.406853 Loss 4.827003, Accuracy 89.568%\n",
      "Epoch 24, Batch 921, LR 1.406720 Loss 4.827521, Accuracy 89.566%\n",
      "Epoch 24, Batch 922, LR 1.406587 Loss 4.826727, Accuracy 89.571%\n",
      "Epoch 24, Batch 923, LR 1.406454 Loss 4.827089, Accuracy 89.568%\n",
      "Epoch 24, Batch 924, LR 1.406321 Loss 4.826905, Accuracy 89.570%\n",
      "Epoch 24, Batch 925, LR 1.406188 Loss 4.826931, Accuracy 89.570%\n",
      "Epoch 24, Batch 926, LR 1.406055 Loss 4.827192, Accuracy 89.567%\n",
      "Epoch 24, Batch 927, LR 1.405922 Loss 4.827599, Accuracy 89.567%\n",
      "Epoch 24, Batch 928, LR 1.405790 Loss 4.827528, Accuracy 89.570%\n",
      "Epoch 24, Batch 929, LR 1.405657 Loss 4.826997, Accuracy 89.573%\n",
      "Epoch 24, Batch 930, LR 1.405524 Loss 4.827019, Accuracy 89.571%\n",
      "Epoch 24, Batch 931, LR 1.405391 Loss 4.826900, Accuracy 89.573%\n",
      "Epoch 24, Batch 932, LR 1.405258 Loss 4.826674, Accuracy 89.572%\n",
      "Epoch 24, Batch 933, LR 1.405125 Loss 4.826105, Accuracy 89.575%\n",
      "Epoch 24, Batch 934, LR 1.404992 Loss 4.825559, Accuracy 89.578%\n",
      "Epoch 24, Batch 935, LR 1.404859 Loss 4.824924, Accuracy 89.580%\n",
      "Epoch 24, Batch 936, LR 1.404726 Loss 4.824742, Accuracy 89.580%\n",
      "Epoch 24, Batch 937, LR 1.404593 Loss 4.825651, Accuracy 89.579%\n",
      "Epoch 24, Batch 938, LR 1.404460 Loss 4.825030, Accuracy 89.580%\n",
      "Epoch 24, Batch 939, LR 1.404327 Loss 4.824897, Accuracy 89.580%\n",
      "Epoch 24, Batch 940, LR 1.404194 Loss 4.824582, Accuracy 89.579%\n",
      "Epoch 24, Batch 941, LR 1.404062 Loss 4.823811, Accuracy 89.583%\n",
      "Epoch 24, Batch 942, LR 1.403929 Loss 4.822911, Accuracy 89.587%\n",
      "Epoch 24, Batch 943, LR 1.403796 Loss 4.822939, Accuracy 89.588%\n",
      "Epoch 24, Batch 944, LR 1.403663 Loss 4.822732, Accuracy 89.589%\n",
      "Epoch 24, Batch 945, LR 1.403530 Loss 4.822693, Accuracy 89.587%\n",
      "Epoch 24, Batch 946, LR 1.403397 Loss 4.823166, Accuracy 89.584%\n",
      "Epoch 24, Batch 947, LR 1.403264 Loss 4.823690, Accuracy 89.581%\n",
      "Epoch 24, Batch 948, LR 1.403131 Loss 4.823993, Accuracy 89.578%\n",
      "Epoch 24, Batch 949, LR 1.402998 Loss 4.824316, Accuracy 89.575%\n",
      "Epoch 24, Batch 950, LR 1.402865 Loss 4.824064, Accuracy 89.572%\n",
      "Epoch 24, Batch 951, LR 1.402732 Loss 4.824006, Accuracy 89.574%\n",
      "Epoch 24, Batch 952, LR 1.402599 Loss 4.823512, Accuracy 89.576%\n",
      "Epoch 24, Batch 953, LR 1.402466 Loss 4.823681, Accuracy 89.577%\n",
      "Epoch 24, Batch 954, LR 1.402333 Loss 4.823811, Accuracy 89.578%\n",
      "Epoch 24, Batch 955, LR 1.402200 Loss 4.823909, Accuracy 89.575%\n",
      "Epoch 24, Batch 956, LR 1.402067 Loss 4.823327, Accuracy 89.579%\n",
      "Epoch 24, Batch 957, LR 1.401934 Loss 4.823077, Accuracy 89.578%\n",
      "Epoch 24, Batch 958, LR 1.401801 Loss 4.824112, Accuracy 89.575%\n",
      "Epoch 24, Batch 959, LR 1.401669 Loss 4.823629, Accuracy 89.581%\n",
      "Epoch 24, Batch 960, LR 1.401536 Loss 4.823466, Accuracy 89.578%\n",
      "Epoch 24, Batch 961, LR 1.401403 Loss 4.823837, Accuracy 89.575%\n",
      "Epoch 24, Batch 962, LR 1.401270 Loss 4.824223, Accuracy 89.573%\n",
      "Epoch 24, Batch 963, LR 1.401137 Loss 4.824579, Accuracy 89.570%\n",
      "Epoch 24, Batch 964, LR 1.401004 Loss 4.823766, Accuracy 89.572%\n",
      "Epoch 24, Batch 965, LR 1.400871 Loss 4.823964, Accuracy 89.573%\n",
      "Epoch 24, Batch 966, LR 1.400738 Loss 4.824042, Accuracy 89.571%\n",
      "Epoch 24, Batch 967, LR 1.400605 Loss 4.824262, Accuracy 89.567%\n",
      "Epoch 24, Batch 968, LR 1.400472 Loss 4.824773, Accuracy 89.567%\n",
      "Epoch 24, Batch 969, LR 1.400339 Loss 4.824725, Accuracy 89.568%\n",
      "Epoch 24, Batch 970, LR 1.400206 Loss 4.825089, Accuracy 89.567%\n",
      "Epoch 24, Batch 971, LR 1.400073 Loss 4.824871, Accuracy 89.568%\n",
      "Epoch 24, Batch 972, LR 1.399940 Loss 4.825001, Accuracy 89.566%\n",
      "Epoch 24, Batch 973, LR 1.399807 Loss 4.824446, Accuracy 89.567%\n",
      "Epoch 24, Batch 974, LR 1.399674 Loss 4.824020, Accuracy 89.568%\n",
      "Epoch 24, Batch 975, LR 1.399541 Loss 4.823481, Accuracy 89.568%\n",
      "Epoch 24, Batch 976, LR 1.399408 Loss 4.823859, Accuracy 89.564%\n",
      "Epoch 24, Batch 977, LR 1.399275 Loss 4.823944, Accuracy 89.564%\n",
      "Epoch 24, Batch 978, LR 1.399142 Loss 4.824095, Accuracy 89.567%\n",
      "Epoch 24, Batch 979, LR 1.399009 Loss 4.824587, Accuracy 89.565%\n",
      "Epoch 24, Batch 980, LR 1.398876 Loss 4.824633, Accuracy 89.565%\n",
      "Epoch 24, Batch 981, LR 1.398743 Loss 4.824786, Accuracy 89.565%\n",
      "Epoch 24, Batch 982, LR 1.398610 Loss 4.824859, Accuracy 89.564%\n",
      "Epoch 24, Batch 983, LR 1.398477 Loss 4.825457, Accuracy 89.562%\n",
      "Epoch 24, Batch 984, LR 1.398344 Loss 4.825478, Accuracy 89.562%\n",
      "Epoch 24, Batch 985, LR 1.398211 Loss 4.825250, Accuracy 89.564%\n",
      "Epoch 24, Batch 986, LR 1.398078 Loss 4.825460, Accuracy 89.562%\n",
      "Epoch 24, Batch 987, LR 1.397945 Loss 4.826077, Accuracy 89.560%\n",
      "Epoch 24, Batch 988, LR 1.397812 Loss 4.826062, Accuracy 89.557%\n",
      "Epoch 24, Batch 989, LR 1.397679 Loss 4.825606, Accuracy 89.563%\n",
      "Epoch 24, Batch 990, LR 1.397546 Loss 4.825406, Accuracy 89.564%\n",
      "Epoch 24, Batch 991, LR 1.397413 Loss 4.824834, Accuracy 89.565%\n",
      "Epoch 24, Batch 992, LR 1.397280 Loss 4.825238, Accuracy 89.560%\n",
      "Epoch 24, Batch 993, LR 1.397147 Loss 4.824949, Accuracy 89.564%\n",
      "Epoch 24, Batch 994, LR 1.397014 Loss 4.825015, Accuracy 89.567%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 995, LR 1.396881 Loss 4.825056, Accuracy 89.567%\n",
      "Epoch 24, Batch 996, LR 1.396748 Loss 4.824829, Accuracy 89.565%\n",
      "Epoch 24, Batch 997, LR 1.396615 Loss 4.824125, Accuracy 89.566%\n",
      "Epoch 24, Batch 998, LR 1.396482 Loss 4.823956, Accuracy 89.569%\n",
      "Epoch 24, Batch 999, LR 1.396349 Loss 4.824131, Accuracy 89.567%\n",
      "Epoch 24, Batch 1000, LR 1.396216 Loss 4.824062, Accuracy 89.565%\n",
      "Epoch 24, Batch 1001, LR 1.396083 Loss 4.823963, Accuracy 89.564%\n",
      "Epoch 24, Batch 1002, LR 1.395949 Loss 4.824434, Accuracy 89.562%\n",
      "Epoch 24, Batch 1003, LR 1.395816 Loss 4.824200, Accuracy 89.560%\n",
      "Epoch 24, Batch 1004, LR 1.395683 Loss 4.824673, Accuracy 89.560%\n",
      "Epoch 24, Batch 1005, LR 1.395550 Loss 4.824944, Accuracy 89.556%\n",
      "Epoch 24, Batch 1006, LR 1.395417 Loss 4.824580, Accuracy 89.556%\n",
      "Epoch 24, Batch 1007, LR 1.395284 Loss 4.824733, Accuracy 89.551%\n",
      "Epoch 24, Batch 1008, LR 1.395151 Loss 4.824771, Accuracy 89.555%\n",
      "Epoch 24, Batch 1009, LR 1.395018 Loss 4.824793, Accuracy 89.553%\n",
      "Epoch 24, Batch 1010, LR 1.394885 Loss 4.825545, Accuracy 89.551%\n",
      "Epoch 24, Batch 1011, LR 1.394752 Loss 4.824932, Accuracy 89.555%\n",
      "Epoch 24, Batch 1012, LR 1.394619 Loss 4.824237, Accuracy 89.560%\n",
      "Epoch 24, Batch 1013, LR 1.394486 Loss 4.823575, Accuracy 89.563%\n",
      "Epoch 24, Batch 1014, LR 1.394353 Loss 4.823429, Accuracy 89.566%\n",
      "Epoch 24, Batch 1015, LR 1.394220 Loss 4.823564, Accuracy 89.565%\n",
      "Epoch 24, Batch 1016, LR 1.394087 Loss 4.823597, Accuracy 89.565%\n",
      "Epoch 24, Batch 1017, LR 1.393954 Loss 4.823168, Accuracy 89.565%\n",
      "Epoch 24, Batch 1018, LR 1.393821 Loss 4.823556, Accuracy 89.564%\n",
      "Epoch 24, Batch 1019, LR 1.393688 Loss 4.823893, Accuracy 89.564%\n",
      "Epoch 24, Batch 1020, LR 1.393555 Loss 4.824068, Accuracy 89.568%\n",
      "Epoch 24, Batch 1021, LR 1.393421 Loss 4.823714, Accuracy 89.571%\n",
      "Epoch 24, Batch 1022, LR 1.393288 Loss 4.823325, Accuracy 89.575%\n",
      "Epoch 24, Batch 1023, LR 1.393155 Loss 4.823036, Accuracy 89.579%\n",
      "Epoch 24, Batch 1024, LR 1.393022 Loss 4.821967, Accuracy 89.583%\n",
      "Epoch 24, Batch 1025, LR 1.392889 Loss 4.821827, Accuracy 89.581%\n",
      "Epoch 24, Batch 1026, LR 1.392756 Loss 4.821942, Accuracy 89.584%\n",
      "Epoch 24, Batch 1027, LR 1.392623 Loss 4.821810, Accuracy 89.586%\n",
      "Epoch 24, Batch 1028, LR 1.392490 Loss 4.822494, Accuracy 89.582%\n",
      "Epoch 24, Batch 1029, LR 1.392357 Loss 4.822738, Accuracy 89.582%\n",
      "Epoch 24, Batch 1030, LR 1.392224 Loss 4.823003, Accuracy 89.581%\n",
      "Epoch 24, Batch 1031, LR 1.392091 Loss 4.823046, Accuracy 89.577%\n",
      "Epoch 24, Batch 1032, LR 1.391958 Loss 4.822790, Accuracy 89.577%\n",
      "Epoch 24, Batch 1033, LR 1.391825 Loss 4.822611, Accuracy 89.578%\n",
      "Epoch 24, Batch 1034, LR 1.391691 Loss 4.822140, Accuracy 89.579%\n",
      "Epoch 24, Batch 1035, LR 1.391558 Loss 4.822101, Accuracy 89.580%\n",
      "Epoch 24, Batch 1036, LR 1.391425 Loss 4.822880, Accuracy 89.575%\n",
      "Epoch 24, Batch 1037, LR 1.391292 Loss 4.823058, Accuracy 89.573%\n",
      "Epoch 24, Batch 1038, LR 1.391159 Loss 4.823259, Accuracy 89.571%\n",
      "Epoch 24, Batch 1039, LR 1.391026 Loss 4.822197, Accuracy 89.572%\n",
      "Epoch 24, Batch 1040, LR 1.390893 Loss 4.823050, Accuracy 89.569%\n",
      "Epoch 24, Batch 1041, LR 1.390760 Loss 4.823079, Accuracy 89.569%\n",
      "Epoch 24, Batch 1042, LR 1.390627 Loss 4.823631, Accuracy 89.566%\n",
      "Epoch 24, Batch 1043, LR 1.390494 Loss 4.823833, Accuracy 89.563%\n",
      "Epoch 24, Batch 1044, LR 1.390360 Loss 4.823102, Accuracy 89.565%\n",
      "Epoch 24, Batch 1045, LR 1.390227 Loss 4.823104, Accuracy 89.568%\n",
      "Epoch 24, Batch 1046, LR 1.390094 Loss 4.823699, Accuracy 89.565%\n",
      "Epoch 24, Batch 1047, LR 1.389961 Loss 4.823478, Accuracy 89.566%\n",
      "Epoch 24, Loss (train set) 4.823478, Accuracy (train set) 89.566%\n",
      "Epoch 24, Accuracy (validation set) 82.663%\n",
      "Epoch 25, Batch 1, LR 1.389828 Loss 4.269878, Accuracy 92.188%\n",
      "Epoch 25, Batch 2, LR 1.389695 Loss 4.471952, Accuracy 90.625%\n",
      "Epoch 25, Batch 3, LR 1.389562 Loss 4.566374, Accuracy 89.844%\n",
      "Epoch 25, Batch 4, LR 1.389429 Loss 4.460767, Accuracy 89.648%\n",
      "Epoch 25, Batch 5, LR 1.389296 Loss 4.529283, Accuracy 89.844%\n",
      "Epoch 25, Batch 6, LR 1.389162 Loss 4.487109, Accuracy 90.625%\n",
      "Epoch 25, Batch 7, LR 1.389029 Loss 4.407074, Accuracy 90.960%\n",
      "Epoch 25, Batch 8, LR 1.388896 Loss 4.452475, Accuracy 91.113%\n",
      "Epoch 25, Batch 9, LR 1.388763 Loss 4.464355, Accuracy 90.712%\n",
      "Epoch 25, Batch 10, LR 1.388630 Loss 4.500654, Accuracy 90.859%\n",
      "Epoch 25, Batch 11, LR 1.388497 Loss 4.495480, Accuracy 90.696%\n",
      "Epoch 25, Batch 12, LR 1.388364 Loss 4.445558, Accuracy 91.016%\n",
      "Epoch 25, Batch 13, LR 1.388231 Loss 4.404805, Accuracy 91.346%\n",
      "Epoch 25, Batch 14, LR 1.388097 Loss 4.431399, Accuracy 91.406%\n",
      "Epoch 25, Batch 15, LR 1.387964 Loss 4.440622, Accuracy 91.198%\n",
      "Epoch 25, Batch 16, LR 1.387831 Loss 4.455324, Accuracy 91.016%\n",
      "Epoch 25, Batch 17, LR 1.387698 Loss 4.467560, Accuracy 91.039%\n",
      "Epoch 25, Batch 18, LR 1.387565 Loss 4.458564, Accuracy 91.102%\n",
      "Epoch 25, Batch 19, LR 1.387432 Loss 4.455386, Accuracy 91.118%\n",
      "Epoch 25, Batch 20, LR 1.387299 Loss 4.478227, Accuracy 90.898%\n",
      "Epoch 25, Batch 21, LR 1.387165 Loss 4.500255, Accuracy 90.960%\n",
      "Epoch 25, Batch 22, LR 1.387032 Loss 4.480006, Accuracy 91.016%\n",
      "Epoch 25, Batch 23, LR 1.386899 Loss 4.515609, Accuracy 90.897%\n",
      "Epoch 25, Batch 24, LR 1.386766 Loss 4.507948, Accuracy 90.918%\n",
      "Epoch 25, Batch 25, LR 1.386633 Loss 4.516048, Accuracy 90.875%\n",
      "Epoch 25, Batch 26, LR 1.386500 Loss 4.517764, Accuracy 90.745%\n",
      "Epoch 25, Batch 27, LR 1.386367 Loss 4.505512, Accuracy 90.856%\n",
      "Epoch 25, Batch 28, LR 1.386233 Loss 4.517337, Accuracy 90.681%\n",
      "Epoch 25, Batch 29, LR 1.386100 Loss 4.517895, Accuracy 90.679%\n",
      "Epoch 25, Batch 30, LR 1.385967 Loss 4.515230, Accuracy 90.651%\n",
      "Epoch 25, Batch 31, LR 1.385834 Loss 4.489469, Accuracy 90.776%\n",
      "Epoch 25, Batch 32, LR 1.385701 Loss 4.502989, Accuracy 90.625%\n",
      "Epoch 25, Batch 33, LR 1.385568 Loss 4.485839, Accuracy 90.649%\n",
      "Epoch 25, Batch 34, LR 1.385434 Loss 4.497336, Accuracy 90.648%\n",
      "Epoch 25, Batch 35, LR 1.385301 Loss 4.506774, Accuracy 90.603%\n",
      "Epoch 25, Batch 36, LR 1.385168 Loss 4.504831, Accuracy 90.603%\n",
      "Epoch 25, Batch 37, LR 1.385035 Loss 4.506080, Accuracy 90.604%\n",
      "Epoch 25, Batch 38, LR 1.384902 Loss 4.519692, Accuracy 90.481%\n",
      "Epoch 25, Batch 39, LR 1.384769 Loss 4.522734, Accuracy 90.545%\n",
      "Epoch 25, Batch 40, LR 1.384635 Loss 4.537666, Accuracy 90.488%\n",
      "Epoch 25, Batch 41, LR 1.384502 Loss 4.539198, Accuracy 90.492%\n",
      "Epoch 25, Batch 42, LR 1.384369 Loss 4.560684, Accuracy 90.346%\n",
      "Epoch 25, Batch 43, LR 1.384236 Loss 4.556287, Accuracy 90.425%\n",
      "Epoch 25, Batch 44, LR 1.384103 Loss 4.562973, Accuracy 90.376%\n",
      "Epoch 25, Batch 45, LR 1.383970 Loss 4.571051, Accuracy 90.278%\n",
      "Epoch 25, Batch 46, LR 1.383836 Loss 4.572172, Accuracy 90.217%\n",
      "Epoch 25, Batch 47, LR 1.383703 Loss 4.565205, Accuracy 90.259%\n",
      "Epoch 25, Batch 48, LR 1.383570 Loss 4.560416, Accuracy 90.267%\n",
      "Epoch 25, Batch 49, LR 1.383437 Loss 4.570754, Accuracy 90.226%\n",
      "Epoch 25, Batch 50, LR 1.383304 Loss 4.557968, Accuracy 90.328%\n",
      "Epoch 25, Batch 51, LR 1.383170 Loss 4.558536, Accuracy 90.380%\n",
      "Epoch 25, Batch 52, LR 1.383037 Loss 4.575713, Accuracy 90.294%\n",
      "Epoch 25, Batch 53, LR 1.382904 Loss 4.578158, Accuracy 90.212%\n",
      "Epoch 25, Batch 54, LR 1.382771 Loss 4.566453, Accuracy 90.220%\n",
      "Epoch 25, Batch 55, LR 1.382638 Loss 4.566917, Accuracy 90.213%\n",
      "Epoch 25, Batch 56, LR 1.382504 Loss 4.574980, Accuracy 90.206%\n",
      "Epoch 25, Batch 57, LR 1.382371 Loss 4.574680, Accuracy 90.200%\n",
      "Epoch 25, Batch 58, LR 1.382238 Loss 4.573279, Accuracy 90.207%\n",
      "Epoch 25, Batch 59, LR 1.382105 Loss 4.569216, Accuracy 90.215%\n",
      "Epoch 25, Batch 60, LR 1.381972 Loss 4.554378, Accuracy 90.312%\n",
      "Epoch 25, Batch 61, LR 1.381838 Loss 4.548792, Accuracy 90.369%\n",
      "Epoch 25, Batch 62, LR 1.381705 Loss 4.566979, Accuracy 90.310%\n",
      "Epoch 25, Batch 63, LR 1.381572 Loss 4.566434, Accuracy 90.290%\n",
      "Epoch 25, Batch 64, LR 1.381439 Loss 4.577583, Accuracy 90.222%\n",
      "Epoch 25, Batch 65, LR 1.381306 Loss 4.578165, Accuracy 90.192%\n",
      "Epoch 25, Batch 66, LR 1.381172 Loss 4.566631, Accuracy 90.258%\n",
      "Epoch 25, Batch 67, LR 1.381039 Loss 4.566960, Accuracy 90.264%\n",
      "Epoch 25, Batch 68, LR 1.380906 Loss 4.552786, Accuracy 90.349%\n",
      "Epoch 25, Batch 69, LR 1.380773 Loss 4.552044, Accuracy 90.399%\n",
      "Epoch 25, Batch 70, LR 1.380640 Loss 4.549399, Accuracy 90.413%\n",
      "Epoch 25, Batch 71, LR 1.380506 Loss 4.552663, Accuracy 90.427%\n",
      "Epoch 25, Batch 72, LR 1.380373 Loss 4.556168, Accuracy 90.397%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 73, LR 1.380240 Loss 4.560924, Accuracy 90.390%\n",
      "Epoch 25, Batch 74, LR 1.380107 Loss 4.564324, Accuracy 90.382%\n",
      "Epoch 25, Batch 75, LR 1.379973 Loss 4.571068, Accuracy 90.323%\n",
      "Epoch 25, Batch 76, LR 1.379840 Loss 4.565805, Accuracy 90.368%\n",
      "Epoch 25, Batch 77, LR 1.379707 Loss 4.567943, Accuracy 90.392%\n",
      "Epoch 25, Batch 78, LR 1.379574 Loss 4.572130, Accuracy 90.355%\n",
      "Epoch 25, Batch 79, LR 1.379440 Loss 4.578599, Accuracy 90.328%\n",
      "Epoch 25, Batch 80, LR 1.379307 Loss 4.581832, Accuracy 90.303%\n",
      "Epoch 25, Batch 81, LR 1.379174 Loss 4.589474, Accuracy 90.316%\n",
      "Epoch 25, Batch 82, LR 1.379041 Loss 4.596345, Accuracy 90.253%\n",
      "Epoch 25, Batch 83, LR 1.378908 Loss 4.593444, Accuracy 90.267%\n",
      "Epoch 25, Batch 84, LR 1.378774 Loss 4.588397, Accuracy 90.290%\n",
      "Epoch 25, Batch 85, LR 1.378641 Loss 4.594521, Accuracy 90.267%\n",
      "Epoch 25, Batch 86, LR 1.378508 Loss 4.594838, Accuracy 90.271%\n",
      "Epoch 25, Batch 87, LR 1.378375 Loss 4.598907, Accuracy 90.275%\n",
      "Epoch 25, Batch 88, LR 1.378241 Loss 4.596036, Accuracy 90.297%\n",
      "Epoch 25, Batch 89, LR 1.378108 Loss 4.599977, Accuracy 90.248%\n",
      "Epoch 25, Batch 90, LR 1.377975 Loss 4.595982, Accuracy 90.269%\n",
      "Epoch 25, Batch 91, LR 1.377842 Loss 4.603537, Accuracy 90.230%\n",
      "Epoch 25, Batch 92, LR 1.377708 Loss 4.608560, Accuracy 90.217%\n",
      "Epoch 25, Batch 93, LR 1.377575 Loss 4.607802, Accuracy 90.255%\n",
      "Epoch 25, Batch 94, LR 1.377442 Loss 4.608007, Accuracy 90.251%\n",
      "Epoch 25, Batch 95, LR 1.377309 Loss 4.616326, Accuracy 90.238%\n",
      "Epoch 25, Batch 96, LR 1.377175 Loss 4.621311, Accuracy 90.234%\n",
      "Epoch 25, Batch 97, LR 1.377042 Loss 4.622917, Accuracy 90.246%\n",
      "Epoch 25, Batch 98, LR 1.376909 Loss 4.625695, Accuracy 90.242%\n",
      "Epoch 25, Batch 99, LR 1.376776 Loss 4.627805, Accuracy 90.199%\n",
      "Epoch 25, Batch 100, LR 1.376642 Loss 4.624918, Accuracy 90.203%\n",
      "Epoch 25, Batch 101, LR 1.376509 Loss 4.631478, Accuracy 90.153%\n",
      "Epoch 25, Batch 102, LR 1.376376 Loss 4.633839, Accuracy 90.158%\n",
      "Epoch 25, Batch 103, LR 1.376242 Loss 4.630530, Accuracy 90.170%\n",
      "Epoch 25, Batch 104, LR 1.376109 Loss 4.640229, Accuracy 90.144%\n",
      "Epoch 25, Batch 105, LR 1.375976 Loss 4.644420, Accuracy 90.126%\n",
      "Epoch 25, Batch 106, LR 1.375843 Loss 4.647820, Accuracy 90.080%\n",
      "Epoch 25, Batch 107, LR 1.375709 Loss 4.645265, Accuracy 90.099%\n",
      "Epoch 25, Batch 108, LR 1.375576 Loss 4.644001, Accuracy 90.119%\n",
      "Epoch 25, Batch 109, LR 1.375443 Loss 4.638220, Accuracy 90.159%\n",
      "Epoch 25, Batch 110, LR 1.375310 Loss 4.638579, Accuracy 90.163%\n",
      "Epoch 25, Batch 111, LR 1.375176 Loss 4.632863, Accuracy 90.210%\n",
      "Epoch 25, Batch 112, LR 1.375043 Loss 4.636936, Accuracy 90.206%\n",
      "Epoch 25, Batch 113, LR 1.374910 Loss 4.639044, Accuracy 90.231%\n",
      "Epoch 25, Batch 114, LR 1.374776 Loss 4.636432, Accuracy 90.262%\n",
      "Epoch 25, Batch 115, LR 1.374643 Loss 4.634069, Accuracy 90.299%\n",
      "Epoch 25, Batch 116, LR 1.374510 Loss 4.634469, Accuracy 90.295%\n",
      "Epoch 25, Batch 117, LR 1.374377 Loss 4.637254, Accuracy 90.264%\n",
      "Epoch 25, Batch 118, LR 1.374243 Loss 4.638569, Accuracy 90.274%\n",
      "Epoch 25, Batch 119, LR 1.374110 Loss 4.638058, Accuracy 90.277%\n",
      "Epoch 25, Batch 120, LR 1.373977 Loss 4.635537, Accuracy 90.299%\n",
      "Epoch 25, Batch 121, LR 1.373843 Loss 4.636340, Accuracy 90.296%\n",
      "Epoch 25, Batch 122, LR 1.373710 Loss 4.639200, Accuracy 90.292%\n",
      "Epoch 25, Batch 123, LR 1.373577 Loss 4.636383, Accuracy 90.314%\n",
      "Epoch 25, Batch 124, LR 1.373444 Loss 4.633573, Accuracy 90.316%\n",
      "Epoch 25, Batch 125, LR 1.373310 Loss 4.634431, Accuracy 90.319%\n",
      "Epoch 25, Batch 126, LR 1.373177 Loss 4.631261, Accuracy 90.340%\n",
      "Epoch 25, Batch 127, LR 1.373044 Loss 4.627711, Accuracy 90.373%\n",
      "Epoch 25, Batch 128, LR 1.372910 Loss 4.628244, Accuracy 90.393%\n",
      "Epoch 25, Batch 129, LR 1.372777 Loss 4.627486, Accuracy 90.383%\n",
      "Epoch 25, Batch 130, LR 1.372644 Loss 4.626428, Accuracy 90.397%\n",
      "Epoch 25, Batch 131, LR 1.372510 Loss 4.631740, Accuracy 90.386%\n",
      "Epoch 25, Batch 132, LR 1.372377 Loss 4.628867, Accuracy 90.382%\n",
      "Epoch 25, Batch 133, LR 1.372244 Loss 4.629739, Accuracy 90.378%\n",
      "Epoch 25, Batch 134, LR 1.372110 Loss 4.631181, Accuracy 90.380%\n",
      "Epoch 25, Batch 135, LR 1.371977 Loss 4.630404, Accuracy 90.359%\n",
      "Epoch 25, Batch 136, LR 1.371844 Loss 4.624672, Accuracy 90.401%\n",
      "Epoch 25, Batch 137, LR 1.371711 Loss 4.622130, Accuracy 90.420%\n",
      "Epoch 25, Batch 138, LR 1.371577 Loss 4.620327, Accuracy 90.410%\n",
      "Epoch 25, Batch 139, LR 1.371444 Loss 4.619387, Accuracy 90.417%\n",
      "Epoch 25, Batch 140, LR 1.371311 Loss 4.617535, Accuracy 90.424%\n",
      "Epoch 25, Batch 141, LR 1.371177 Loss 4.617125, Accuracy 90.420%\n",
      "Epoch 25, Batch 142, LR 1.371044 Loss 4.619412, Accuracy 90.405%\n",
      "Epoch 25, Batch 143, LR 1.370911 Loss 4.620509, Accuracy 90.396%\n",
      "Epoch 25, Batch 144, LR 1.370777 Loss 4.618402, Accuracy 90.365%\n",
      "Epoch 25, Batch 145, LR 1.370644 Loss 4.614862, Accuracy 90.377%\n",
      "Epoch 25, Batch 146, LR 1.370511 Loss 4.612144, Accuracy 90.368%\n",
      "Epoch 25, Batch 147, LR 1.370377 Loss 4.612737, Accuracy 90.349%\n",
      "Epoch 25, Batch 148, LR 1.370244 Loss 4.612758, Accuracy 90.329%\n",
      "Epoch 25, Batch 149, LR 1.370111 Loss 4.607917, Accuracy 90.363%\n",
      "Epoch 25, Batch 150, LR 1.369977 Loss 4.607658, Accuracy 90.391%\n",
      "Epoch 25, Batch 151, LR 1.369844 Loss 4.605282, Accuracy 90.392%\n",
      "Epoch 25, Batch 152, LR 1.369711 Loss 4.607181, Accuracy 90.383%\n",
      "Epoch 25, Batch 153, LR 1.369577 Loss 4.609299, Accuracy 90.354%\n",
      "Epoch 25, Batch 154, LR 1.369444 Loss 4.609973, Accuracy 90.346%\n",
      "Epoch 25, Batch 155, LR 1.369311 Loss 4.615306, Accuracy 90.323%\n",
      "Epoch 25, Batch 156, LR 1.369177 Loss 4.622106, Accuracy 90.274%\n",
      "Epoch 25, Batch 157, LR 1.369044 Loss 4.624611, Accuracy 90.257%\n",
      "Epoch 25, Batch 158, LR 1.368911 Loss 4.620489, Accuracy 90.264%\n",
      "Epoch 25, Batch 159, LR 1.368777 Loss 4.621713, Accuracy 90.266%\n",
      "Epoch 25, Batch 160, LR 1.368644 Loss 4.618822, Accuracy 90.259%\n",
      "Epoch 25, Batch 161, LR 1.368511 Loss 4.619300, Accuracy 90.285%\n",
      "Epoch 25, Batch 162, LR 1.368377 Loss 4.620091, Accuracy 90.283%\n",
      "Epoch 25, Batch 163, LR 1.368244 Loss 4.622601, Accuracy 90.266%\n",
      "Epoch 25, Batch 164, LR 1.368110 Loss 4.623271, Accuracy 90.239%\n",
      "Epoch 25, Batch 165, LR 1.367977 Loss 4.625846, Accuracy 90.208%\n",
      "Epoch 25, Batch 166, LR 1.367844 Loss 4.626493, Accuracy 90.197%\n",
      "Epoch 25, Batch 167, LR 1.367710 Loss 4.624925, Accuracy 90.199%\n",
      "Epoch 25, Batch 168, LR 1.367577 Loss 4.627557, Accuracy 90.179%\n",
      "Epoch 25, Batch 169, LR 1.367444 Loss 4.628944, Accuracy 90.158%\n",
      "Epoch 25, Batch 170, LR 1.367310 Loss 4.626997, Accuracy 90.170%\n",
      "Epoch 25, Batch 171, LR 1.367177 Loss 4.627301, Accuracy 90.159%\n",
      "Epoch 25, Batch 172, LR 1.367044 Loss 4.630353, Accuracy 90.130%\n",
      "Epoch 25, Batch 173, LR 1.366910 Loss 4.631932, Accuracy 90.124%\n",
      "Epoch 25, Batch 174, LR 1.366777 Loss 4.631224, Accuracy 90.122%\n",
      "Epoch 25, Batch 175, LR 1.366643 Loss 4.627779, Accuracy 90.134%\n",
      "Epoch 25, Batch 176, LR 1.366510 Loss 4.628217, Accuracy 90.150%\n",
      "Epoch 25, Batch 177, LR 1.366377 Loss 4.631414, Accuracy 90.144%\n",
      "Epoch 25, Batch 178, LR 1.366243 Loss 4.631085, Accuracy 90.129%\n",
      "Epoch 25, Batch 179, LR 1.366110 Loss 4.634448, Accuracy 90.114%\n",
      "Epoch 25, Batch 180, LR 1.365977 Loss 4.630212, Accuracy 90.139%\n",
      "Epoch 25, Batch 181, LR 1.365843 Loss 4.629344, Accuracy 90.155%\n",
      "Epoch 25, Batch 182, LR 1.365710 Loss 4.626520, Accuracy 90.157%\n",
      "Epoch 25, Batch 183, LR 1.365577 Loss 4.625889, Accuracy 90.164%\n",
      "Epoch 25, Batch 184, LR 1.365443 Loss 4.625959, Accuracy 90.149%\n",
      "Epoch 25, Batch 185, LR 1.365310 Loss 4.630206, Accuracy 90.139%\n",
      "Epoch 25, Batch 186, LR 1.365176 Loss 4.627773, Accuracy 90.129%\n",
      "Epoch 25, Batch 187, LR 1.365043 Loss 4.624197, Accuracy 90.161%\n",
      "Epoch 25, Batch 188, LR 1.364910 Loss 4.626567, Accuracy 90.143%\n",
      "Epoch 25, Batch 189, LR 1.364776 Loss 4.624899, Accuracy 90.166%\n",
      "Epoch 25, Batch 190, LR 1.364643 Loss 4.627403, Accuracy 90.156%\n",
      "Epoch 25, Batch 191, LR 1.364509 Loss 4.627290, Accuracy 90.142%\n",
      "Epoch 25, Batch 192, LR 1.364376 Loss 4.627478, Accuracy 90.137%\n",
      "Epoch 25, Batch 193, LR 1.364243 Loss 4.628266, Accuracy 90.127%\n",
      "Epoch 25, Batch 194, LR 1.364109 Loss 4.627048, Accuracy 90.142%\n",
      "Epoch 25, Batch 195, LR 1.363976 Loss 4.627861, Accuracy 90.116%\n",
      "Epoch 25, Batch 196, LR 1.363842 Loss 4.624999, Accuracy 90.135%\n",
      "Epoch 25, Batch 197, LR 1.363709 Loss 4.625009, Accuracy 90.145%\n",
      "Epoch 25, Batch 198, LR 1.363576 Loss 4.626147, Accuracy 90.136%\n",
      "Epoch 25, Batch 199, LR 1.363442 Loss 4.625186, Accuracy 90.142%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 200, LR 1.363309 Loss 4.628899, Accuracy 90.121%\n",
      "Epoch 25, Batch 201, LR 1.363175 Loss 4.627956, Accuracy 90.131%\n",
      "Epoch 25, Batch 202, LR 1.363042 Loss 4.628405, Accuracy 90.118%\n",
      "Epoch 25, Batch 203, LR 1.362909 Loss 4.629331, Accuracy 90.109%\n",
      "Epoch 25, Batch 204, LR 1.362775 Loss 4.629498, Accuracy 90.108%\n",
      "Epoch 25, Batch 205, LR 1.362642 Loss 4.630197, Accuracy 90.095%\n",
      "Epoch 25, Batch 206, LR 1.362508 Loss 4.633381, Accuracy 90.075%\n",
      "Epoch 25, Batch 207, LR 1.362375 Loss 4.628385, Accuracy 90.100%\n",
      "Epoch 25, Batch 208, LR 1.362242 Loss 4.631361, Accuracy 90.092%\n",
      "Epoch 25, Batch 209, LR 1.362108 Loss 4.630601, Accuracy 90.102%\n",
      "Epoch 25, Batch 210, LR 1.361975 Loss 4.632564, Accuracy 90.097%\n",
      "Epoch 25, Batch 211, LR 1.361841 Loss 4.634116, Accuracy 90.107%\n",
      "Epoch 25, Batch 212, LR 1.361708 Loss 4.632543, Accuracy 90.109%\n",
      "Epoch 25, Batch 213, LR 1.361575 Loss 4.633936, Accuracy 90.100%\n",
      "Epoch 25, Batch 214, LR 1.361441 Loss 4.634902, Accuracy 90.088%\n",
      "Epoch 25, Batch 215, LR 1.361308 Loss 4.636812, Accuracy 90.084%\n",
      "Epoch 25, Batch 216, LR 1.361174 Loss 4.642099, Accuracy 90.046%\n",
      "Epoch 25, Batch 217, LR 1.361041 Loss 4.639116, Accuracy 90.067%\n",
      "Epoch 25, Batch 218, LR 1.360907 Loss 4.638788, Accuracy 90.066%\n",
      "Epoch 25, Batch 219, LR 1.360774 Loss 4.639007, Accuracy 90.061%\n",
      "Epoch 25, Batch 220, LR 1.360641 Loss 4.637144, Accuracy 90.053%\n",
      "Epoch 25, Batch 221, LR 1.360507 Loss 4.637568, Accuracy 90.070%\n",
      "Epoch 25, Batch 222, LR 1.360374 Loss 4.636766, Accuracy 90.072%\n",
      "Epoch 25, Batch 223, LR 1.360240 Loss 4.632649, Accuracy 90.078%\n",
      "Epoch 25, Batch 224, LR 1.360107 Loss 4.632736, Accuracy 90.067%\n",
      "Epoch 25, Batch 225, LR 1.359973 Loss 4.631610, Accuracy 90.076%\n",
      "Epoch 25, Batch 226, LR 1.359840 Loss 4.630745, Accuracy 90.089%\n",
      "Epoch 25, Batch 227, LR 1.359707 Loss 4.629540, Accuracy 90.095%\n",
      "Epoch 25, Batch 228, LR 1.359573 Loss 4.630259, Accuracy 90.084%\n",
      "Epoch 25, Batch 229, LR 1.359440 Loss 4.631450, Accuracy 90.083%\n",
      "Epoch 25, Batch 230, LR 1.359306 Loss 4.632622, Accuracy 90.078%\n",
      "Epoch 25, Batch 231, LR 1.359173 Loss 4.630868, Accuracy 90.094%\n",
      "Epoch 25, Batch 232, LR 1.359039 Loss 4.628988, Accuracy 90.110%\n",
      "Epoch 25, Batch 233, LR 1.358906 Loss 4.628768, Accuracy 90.119%\n",
      "Epoch 25, Batch 234, LR 1.358772 Loss 4.627729, Accuracy 90.111%\n",
      "Epoch 25, Batch 235, LR 1.358639 Loss 4.627198, Accuracy 90.116%\n",
      "Epoch 25, Batch 236, LR 1.358506 Loss 4.628962, Accuracy 90.112%\n",
      "Epoch 25, Batch 237, LR 1.358372 Loss 4.629041, Accuracy 90.127%\n",
      "Epoch 25, Batch 238, LR 1.358239 Loss 4.630528, Accuracy 90.116%\n",
      "Epoch 25, Batch 239, LR 1.358105 Loss 4.632609, Accuracy 90.125%\n",
      "Epoch 25, Batch 240, LR 1.357972 Loss 4.632843, Accuracy 90.127%\n",
      "Epoch 25, Batch 241, LR 1.357838 Loss 4.631287, Accuracy 90.132%\n",
      "Epoch 25, Batch 242, LR 1.357705 Loss 4.629518, Accuracy 90.150%\n",
      "Epoch 25, Batch 243, LR 1.357571 Loss 4.631620, Accuracy 90.146%\n",
      "Epoch 25, Batch 244, LR 1.357438 Loss 4.631907, Accuracy 90.154%\n",
      "Epoch 25, Batch 245, LR 1.357304 Loss 4.631616, Accuracy 90.153%\n",
      "Epoch 25, Batch 246, LR 1.357171 Loss 4.634020, Accuracy 90.152%\n",
      "Epoch 25, Batch 247, LR 1.357038 Loss 4.635606, Accuracy 90.144%\n",
      "Epoch 25, Batch 248, LR 1.356904 Loss 4.634747, Accuracy 90.140%\n",
      "Epoch 25, Batch 249, LR 1.356771 Loss 4.636811, Accuracy 90.139%\n",
      "Epoch 25, Batch 250, LR 1.356637 Loss 4.639528, Accuracy 90.128%\n",
      "Epoch 25, Batch 251, LR 1.356504 Loss 4.641332, Accuracy 90.115%\n",
      "Epoch 25, Batch 252, LR 1.356370 Loss 4.642343, Accuracy 90.117%\n",
      "Epoch 25, Batch 253, LR 1.356237 Loss 4.645720, Accuracy 90.100%\n",
      "Epoch 25, Batch 254, LR 1.356103 Loss 4.646418, Accuracy 90.093%\n",
      "Epoch 25, Batch 255, LR 1.355970 Loss 4.649408, Accuracy 90.077%\n",
      "Epoch 25, Batch 256, LR 1.355836 Loss 4.648602, Accuracy 90.082%\n",
      "Epoch 25, Batch 257, LR 1.355703 Loss 4.648490, Accuracy 90.093%\n",
      "Epoch 25, Batch 258, LR 1.355569 Loss 4.649879, Accuracy 90.089%\n",
      "Epoch 25, Batch 259, LR 1.355436 Loss 4.650030, Accuracy 90.094%\n",
      "Epoch 25, Batch 260, LR 1.355302 Loss 4.649292, Accuracy 90.105%\n",
      "Epoch 25, Batch 261, LR 1.355169 Loss 4.650523, Accuracy 90.092%\n",
      "Epoch 25, Batch 262, LR 1.355036 Loss 4.652584, Accuracy 90.088%\n",
      "Epoch 25, Batch 263, LR 1.354902 Loss 4.649605, Accuracy 90.102%\n",
      "Epoch 25, Batch 264, LR 1.354769 Loss 4.652104, Accuracy 90.092%\n",
      "Epoch 25, Batch 265, LR 1.354635 Loss 4.649570, Accuracy 90.097%\n",
      "Epoch 25, Batch 266, LR 1.354502 Loss 4.650434, Accuracy 90.105%\n",
      "Epoch 25, Batch 267, LR 1.354368 Loss 4.650274, Accuracy 90.104%\n",
      "Epoch 25, Batch 268, LR 1.354235 Loss 4.651414, Accuracy 90.089%\n",
      "Epoch 25, Batch 269, LR 1.354101 Loss 4.653888, Accuracy 90.067%\n",
      "Epoch 25, Batch 270, LR 1.353968 Loss 4.655314, Accuracy 90.061%\n",
      "Epoch 25, Batch 271, LR 1.353834 Loss 4.657017, Accuracy 90.046%\n",
      "Epoch 25, Batch 272, LR 1.353701 Loss 4.655496, Accuracy 90.059%\n",
      "Epoch 25, Batch 273, LR 1.353567 Loss 4.654218, Accuracy 90.064%\n",
      "Epoch 25, Batch 274, LR 1.353434 Loss 4.652923, Accuracy 90.069%\n",
      "Epoch 25, Batch 275, LR 1.353300 Loss 4.653886, Accuracy 90.060%\n",
      "Epoch 25, Batch 276, LR 1.353167 Loss 4.652296, Accuracy 90.065%\n",
      "Epoch 25, Batch 277, LR 1.353033 Loss 4.652587, Accuracy 90.064%\n",
      "Epoch 25, Batch 278, LR 1.352900 Loss 4.654956, Accuracy 90.063%\n",
      "Epoch 25, Batch 279, LR 1.352766 Loss 4.654917, Accuracy 90.059%\n",
      "Epoch 25, Batch 280, LR 1.352633 Loss 4.654430, Accuracy 90.053%\n",
      "Epoch 25, Batch 281, LR 1.352499 Loss 4.656375, Accuracy 90.041%\n",
      "Epoch 25, Batch 282, LR 1.352366 Loss 4.657016, Accuracy 90.024%\n",
      "Epoch 25, Batch 283, LR 1.352232 Loss 4.655998, Accuracy 90.031%\n",
      "Epoch 25, Batch 284, LR 1.352099 Loss 4.657814, Accuracy 90.025%\n",
      "Epoch 25, Batch 285, LR 1.351965 Loss 4.655462, Accuracy 90.027%\n",
      "Epoch 25, Batch 286, LR 1.351832 Loss 4.654270, Accuracy 90.040%\n",
      "Epoch 25, Batch 287, LR 1.351698 Loss 4.655105, Accuracy 90.040%\n",
      "Epoch 25, Batch 288, LR 1.351565 Loss 4.655017, Accuracy 90.042%\n",
      "Epoch 25, Batch 289, LR 1.351431 Loss 4.654663, Accuracy 90.036%\n",
      "Epoch 25, Batch 290, LR 1.351298 Loss 4.653430, Accuracy 90.046%\n",
      "Epoch 25, Batch 291, LR 1.351164 Loss 4.653624, Accuracy 90.056%\n",
      "Epoch 25, Batch 292, LR 1.351031 Loss 4.652091, Accuracy 90.060%\n",
      "Epoch 25, Batch 293, LR 1.350897 Loss 4.652784, Accuracy 90.068%\n",
      "Epoch 25, Batch 294, LR 1.350764 Loss 4.652422, Accuracy 90.070%\n",
      "Epoch 25, Batch 295, LR 1.350630 Loss 4.651549, Accuracy 90.072%\n",
      "Epoch 25, Batch 296, LR 1.350497 Loss 4.651260, Accuracy 90.071%\n",
      "Epoch 25, Batch 297, LR 1.350363 Loss 4.652098, Accuracy 90.070%\n",
      "Epoch 25, Batch 298, LR 1.350229 Loss 4.651636, Accuracy 90.072%\n",
      "Epoch 25, Batch 299, LR 1.350096 Loss 4.652558, Accuracy 90.058%\n",
      "Epoch 25, Batch 300, LR 1.349962 Loss 4.651267, Accuracy 90.062%\n",
      "Epoch 25, Batch 301, LR 1.349829 Loss 4.652579, Accuracy 90.059%\n",
      "Epoch 25, Batch 302, LR 1.349695 Loss 4.651978, Accuracy 90.066%\n",
      "Epoch 25, Batch 303, LR 1.349562 Loss 4.651769, Accuracy 90.073%\n",
      "Epoch 25, Batch 304, LR 1.349428 Loss 4.652791, Accuracy 90.072%\n",
      "Epoch 25, Batch 305, LR 1.349295 Loss 4.654729, Accuracy 90.077%\n",
      "Epoch 25, Batch 306, LR 1.349161 Loss 4.654614, Accuracy 90.076%\n",
      "Epoch 25, Batch 307, LR 1.349028 Loss 4.655910, Accuracy 90.080%\n",
      "Epoch 25, Batch 308, LR 1.348894 Loss 4.655247, Accuracy 90.082%\n",
      "Epoch 25, Batch 309, LR 1.348761 Loss 4.655402, Accuracy 90.081%\n",
      "Epoch 25, Batch 310, LR 1.348627 Loss 4.656940, Accuracy 90.076%\n",
      "Epoch 25, Batch 311, LR 1.348494 Loss 4.656472, Accuracy 90.082%\n",
      "Epoch 25, Batch 312, LR 1.348360 Loss 4.655396, Accuracy 90.089%\n",
      "Epoch 25, Batch 313, LR 1.348227 Loss 4.655745, Accuracy 90.083%\n",
      "Epoch 25, Batch 314, LR 1.348093 Loss 4.655413, Accuracy 90.083%\n",
      "Epoch 25, Batch 315, LR 1.347959 Loss 4.656267, Accuracy 90.089%\n",
      "Epoch 25, Batch 316, LR 1.347826 Loss 4.656710, Accuracy 90.091%\n",
      "Epoch 25, Batch 317, LR 1.347692 Loss 4.659561, Accuracy 90.075%\n",
      "Epoch 25, Batch 318, LR 1.347559 Loss 4.659547, Accuracy 90.075%\n",
      "Epoch 25, Batch 319, LR 1.347425 Loss 4.659723, Accuracy 90.084%\n",
      "Epoch 25, Batch 320, LR 1.347292 Loss 4.659587, Accuracy 90.081%\n",
      "Epoch 25, Batch 321, LR 1.347158 Loss 4.660178, Accuracy 90.090%\n",
      "Epoch 25, Batch 322, LR 1.347025 Loss 4.660398, Accuracy 90.096%\n",
      "Epoch 25, Batch 323, LR 1.346891 Loss 4.659744, Accuracy 90.098%\n",
      "Epoch 25, Batch 324, LR 1.346758 Loss 4.661345, Accuracy 90.099%\n",
      "Epoch 25, Batch 325, LR 1.346624 Loss 4.661481, Accuracy 90.101%\n",
      "Epoch 25, Batch 326, LR 1.346490 Loss 4.662662, Accuracy 90.107%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 327, LR 1.346357 Loss 4.662124, Accuracy 90.116%\n",
      "Epoch 25, Batch 328, LR 1.346223 Loss 4.664083, Accuracy 90.101%\n",
      "Epoch 25, Batch 329, LR 1.346090 Loss 4.662301, Accuracy 90.112%\n",
      "Epoch 25, Batch 330, LR 1.345956 Loss 4.662339, Accuracy 90.118%\n",
      "Epoch 25, Batch 331, LR 1.345823 Loss 4.665245, Accuracy 90.108%\n",
      "Epoch 25, Batch 332, LR 1.345689 Loss 4.665560, Accuracy 90.114%\n",
      "Epoch 25, Batch 333, LR 1.345556 Loss 4.665949, Accuracy 90.116%\n",
      "Epoch 25, Batch 334, LR 1.345422 Loss 4.664799, Accuracy 90.124%\n",
      "Epoch 25, Batch 335, LR 1.345288 Loss 4.665565, Accuracy 90.121%\n",
      "Epoch 25, Batch 336, LR 1.345155 Loss 4.667503, Accuracy 90.116%\n",
      "Epoch 25, Batch 337, LR 1.345021 Loss 4.665865, Accuracy 90.124%\n",
      "Epoch 25, Batch 338, LR 1.344888 Loss 4.664489, Accuracy 90.133%\n",
      "Epoch 25, Batch 339, LR 1.344754 Loss 4.663223, Accuracy 90.130%\n",
      "Epoch 25, Batch 340, LR 1.344621 Loss 4.664656, Accuracy 90.119%\n",
      "Epoch 25, Batch 341, LR 1.344487 Loss 4.662435, Accuracy 90.126%\n",
      "Epoch 25, Batch 342, LR 1.344353 Loss 4.661679, Accuracy 90.125%\n",
      "Epoch 25, Batch 343, LR 1.344220 Loss 4.662167, Accuracy 90.124%\n",
      "Epoch 25, Batch 344, LR 1.344086 Loss 4.663684, Accuracy 90.116%\n",
      "Epoch 25, Batch 345, LR 1.343953 Loss 4.664762, Accuracy 90.111%\n",
      "Epoch 25, Batch 346, LR 1.343819 Loss 4.665493, Accuracy 90.106%\n",
      "Epoch 25, Batch 347, LR 1.343686 Loss 4.666157, Accuracy 90.103%\n",
      "Epoch 25, Batch 348, LR 1.343552 Loss 4.667431, Accuracy 90.106%\n",
      "Epoch 25, Batch 349, LR 1.343418 Loss 4.670176, Accuracy 90.088%\n",
      "Epoch 25, Batch 350, LR 1.343285 Loss 4.670463, Accuracy 90.085%\n",
      "Epoch 25, Batch 351, LR 1.343151 Loss 4.670101, Accuracy 90.084%\n",
      "Epoch 25, Batch 352, LR 1.343018 Loss 4.670251, Accuracy 90.068%\n",
      "Epoch 25, Batch 353, LR 1.342884 Loss 4.669687, Accuracy 90.061%\n",
      "Epoch 25, Batch 354, LR 1.342750 Loss 4.668752, Accuracy 90.060%\n",
      "Epoch 25, Batch 355, LR 1.342617 Loss 4.670002, Accuracy 90.051%\n",
      "Epoch 25, Batch 356, LR 1.342483 Loss 4.670823, Accuracy 90.039%\n",
      "Epoch 25, Batch 357, LR 1.342350 Loss 4.668533, Accuracy 90.047%\n",
      "Epoch 25, Batch 358, LR 1.342216 Loss 4.670886, Accuracy 90.034%\n",
      "Epoch 25, Batch 359, LR 1.342083 Loss 4.671315, Accuracy 90.042%\n",
      "Epoch 25, Batch 360, LR 1.341949 Loss 4.671098, Accuracy 90.043%\n",
      "Epoch 25, Batch 361, LR 1.341815 Loss 4.669964, Accuracy 90.041%\n",
      "Epoch 25, Batch 362, LR 1.341682 Loss 4.670737, Accuracy 90.044%\n",
      "Epoch 25, Batch 363, LR 1.341548 Loss 4.670420, Accuracy 90.042%\n",
      "Epoch 25, Batch 364, LR 1.341415 Loss 4.669829, Accuracy 90.050%\n",
      "Epoch 25, Batch 365, LR 1.341281 Loss 4.670392, Accuracy 90.041%\n",
      "Epoch 25, Batch 366, LR 1.341147 Loss 4.669061, Accuracy 90.051%\n",
      "Epoch 25, Batch 367, LR 1.341014 Loss 4.670848, Accuracy 90.027%\n",
      "Epoch 25, Batch 368, LR 1.340880 Loss 4.669970, Accuracy 90.035%\n",
      "Epoch 25, Batch 369, LR 1.340747 Loss 4.669485, Accuracy 90.047%\n",
      "Epoch 25, Batch 370, LR 1.340613 Loss 4.670352, Accuracy 90.044%\n",
      "Epoch 25, Batch 371, LR 1.340479 Loss 4.669995, Accuracy 90.048%\n",
      "Epoch 25, Batch 372, LR 1.340346 Loss 4.670358, Accuracy 90.047%\n",
      "Epoch 25, Batch 373, LR 1.340212 Loss 4.670256, Accuracy 90.041%\n",
      "Epoch 25, Batch 374, LR 1.340079 Loss 4.670765, Accuracy 90.042%\n",
      "Epoch 25, Batch 375, LR 1.339945 Loss 4.669920, Accuracy 90.050%\n",
      "Epoch 25, Batch 376, LR 1.339811 Loss 4.670568, Accuracy 90.037%\n",
      "Epoch 25, Batch 377, LR 1.339678 Loss 4.670504, Accuracy 90.036%\n",
      "Epoch 25, Batch 378, LR 1.339544 Loss 4.670342, Accuracy 90.034%\n",
      "Epoch 25, Batch 379, LR 1.339411 Loss 4.668335, Accuracy 90.048%\n",
      "Epoch 25, Batch 380, LR 1.339277 Loss 4.668381, Accuracy 90.045%\n",
      "Epoch 25, Batch 381, LR 1.339143 Loss 4.669022, Accuracy 90.047%\n",
      "Epoch 25, Batch 382, LR 1.339010 Loss 4.667747, Accuracy 90.050%\n",
      "Epoch 25, Batch 383, LR 1.338876 Loss 4.668839, Accuracy 90.044%\n",
      "Epoch 25, Batch 384, LR 1.338743 Loss 4.668971, Accuracy 90.047%\n",
      "Epoch 25, Batch 385, LR 1.338609 Loss 4.668275, Accuracy 90.049%\n",
      "Epoch 25, Batch 386, LR 1.338475 Loss 4.667066, Accuracy 90.054%\n",
      "Epoch 25, Batch 387, LR 1.338342 Loss 4.666034, Accuracy 90.060%\n",
      "Epoch 25, Batch 388, LR 1.338208 Loss 4.665746, Accuracy 90.057%\n",
      "Epoch 25, Batch 389, LR 1.338074 Loss 4.666269, Accuracy 90.049%\n",
      "Epoch 25, Batch 390, LR 1.337941 Loss 4.667286, Accuracy 90.040%\n",
      "Epoch 25, Batch 391, LR 1.337807 Loss 4.666154, Accuracy 90.040%\n",
      "Epoch 25, Batch 392, LR 1.337674 Loss 4.666205, Accuracy 90.035%\n",
      "Epoch 25, Batch 393, LR 1.337540 Loss 4.665424, Accuracy 90.039%\n",
      "Epoch 25, Batch 394, LR 1.337406 Loss 4.666480, Accuracy 90.038%\n",
      "Epoch 25, Batch 395, LR 1.337273 Loss 4.667479, Accuracy 90.038%\n",
      "Epoch 25, Batch 396, LR 1.337139 Loss 4.666014, Accuracy 90.049%\n",
      "Epoch 25, Batch 397, LR 1.337005 Loss 4.666093, Accuracy 90.044%\n",
      "Epoch 25, Batch 398, LR 1.336872 Loss 4.665039, Accuracy 90.046%\n",
      "Epoch 25, Batch 399, LR 1.336738 Loss 4.667796, Accuracy 90.028%\n",
      "Epoch 25, Batch 400, LR 1.336605 Loss 4.667220, Accuracy 90.029%\n",
      "Epoch 25, Batch 401, LR 1.336471 Loss 4.667818, Accuracy 90.025%\n",
      "Epoch 25, Batch 402, LR 1.336337 Loss 4.667966, Accuracy 90.021%\n",
      "Epoch 25, Batch 403, LR 1.336204 Loss 4.669976, Accuracy 90.009%\n",
      "Epoch 25, Batch 404, LR 1.336070 Loss 4.668891, Accuracy 90.012%\n",
      "Epoch 25, Batch 405, LR 1.335936 Loss 4.668782, Accuracy 90.015%\n",
      "Epoch 25, Batch 406, LR 1.335803 Loss 4.669407, Accuracy 90.011%\n",
      "Epoch 25, Batch 407, LR 1.335669 Loss 4.669930, Accuracy 90.015%\n",
      "Epoch 25, Batch 408, LR 1.335535 Loss 4.671128, Accuracy 90.003%\n",
      "Epoch 25, Batch 409, LR 1.335402 Loss 4.670127, Accuracy 90.010%\n",
      "Epoch 25, Batch 410, LR 1.335268 Loss 4.670711, Accuracy 90.011%\n",
      "Epoch 25, Batch 411, LR 1.335135 Loss 4.670225, Accuracy 90.015%\n",
      "Epoch 25, Batch 412, LR 1.335001 Loss 4.669652, Accuracy 90.014%\n",
      "Epoch 25, Batch 413, LR 1.334867 Loss 4.669684, Accuracy 90.016%\n",
      "Epoch 25, Batch 414, LR 1.334734 Loss 4.669300, Accuracy 90.015%\n",
      "Epoch 25, Batch 415, LR 1.334600 Loss 4.668586, Accuracy 90.011%\n",
      "Epoch 25, Batch 416, LR 1.334466 Loss 4.668059, Accuracy 90.017%\n",
      "Epoch 25, Batch 417, LR 1.334333 Loss 4.667885, Accuracy 90.010%\n",
      "Epoch 25, Batch 418, LR 1.334199 Loss 4.669081, Accuracy 89.995%\n",
      "Epoch 25, Batch 419, LR 1.334065 Loss 4.668978, Accuracy 89.995%\n",
      "Epoch 25, Batch 420, LR 1.333932 Loss 4.669492, Accuracy 89.994%\n",
      "Epoch 25, Batch 421, LR 1.333798 Loss 4.670703, Accuracy 89.988%\n",
      "Epoch 25, Batch 422, LR 1.333664 Loss 4.669140, Accuracy 89.999%\n",
      "Epoch 25, Batch 423, LR 1.333531 Loss 4.668377, Accuracy 90.004%\n",
      "Epoch 25, Batch 424, LR 1.333397 Loss 4.667184, Accuracy 90.004%\n",
      "Epoch 25, Batch 425, LR 1.333263 Loss 4.666902, Accuracy 90.011%\n",
      "Epoch 25, Batch 426, LR 1.333130 Loss 4.667126, Accuracy 90.009%\n",
      "Epoch 25, Batch 427, LR 1.332996 Loss 4.667655, Accuracy 90.001%\n",
      "Epoch 25, Batch 428, LR 1.332862 Loss 4.668848, Accuracy 89.992%\n",
      "Epoch 25, Batch 429, LR 1.332729 Loss 4.669324, Accuracy 89.988%\n",
      "Epoch 25, Batch 430, LR 1.332595 Loss 4.669146, Accuracy 89.995%\n",
      "Epoch 25, Batch 431, LR 1.332461 Loss 4.668282, Accuracy 90.003%\n",
      "Epoch 25, Batch 432, LR 1.332328 Loss 4.667701, Accuracy 90.008%\n",
      "Epoch 25, Batch 433, LR 1.332194 Loss 4.666905, Accuracy 90.015%\n",
      "Epoch 25, Batch 434, LR 1.332060 Loss 4.668073, Accuracy 90.008%\n",
      "Epoch 25, Batch 435, LR 1.331927 Loss 4.668206, Accuracy 90.009%\n",
      "Epoch 25, Batch 436, LR 1.331793 Loss 4.668584, Accuracy 90.009%\n",
      "Epoch 25, Batch 437, LR 1.331660 Loss 4.667641, Accuracy 90.014%\n",
      "Epoch 25, Batch 438, LR 1.331526 Loss 4.666830, Accuracy 90.019%\n",
      "Epoch 25, Batch 439, LR 1.331392 Loss 4.667527, Accuracy 90.013%\n",
      "Epoch 25, Batch 440, LR 1.331258 Loss 4.667858, Accuracy 90.012%\n",
      "Epoch 25, Batch 441, LR 1.331125 Loss 4.666938, Accuracy 90.019%\n",
      "Epoch 25, Batch 442, LR 1.330991 Loss 4.666479, Accuracy 90.022%\n",
      "Epoch 25, Batch 443, LR 1.330857 Loss 4.665985, Accuracy 90.025%\n",
      "Epoch 25, Batch 444, LR 1.330724 Loss 4.666730, Accuracy 90.029%\n",
      "Epoch 25, Batch 445, LR 1.330590 Loss 4.666468, Accuracy 90.021%\n",
      "Epoch 25, Batch 446, LR 1.330456 Loss 4.666562, Accuracy 90.024%\n",
      "Epoch 25, Batch 447, LR 1.330323 Loss 4.665918, Accuracy 90.026%\n",
      "Epoch 25, Batch 448, LR 1.330189 Loss 4.665434, Accuracy 90.032%\n",
      "Epoch 25, Batch 449, LR 1.330055 Loss 4.664816, Accuracy 90.040%\n",
      "Epoch 25, Batch 450, LR 1.329922 Loss 4.665621, Accuracy 90.035%\n",
      "Epoch 25, Batch 451, LR 1.329788 Loss 4.666724, Accuracy 90.031%\n",
      "Epoch 25, Batch 452, LR 1.329654 Loss 4.667995, Accuracy 90.018%\n",
      "Epoch 25, Batch 453, LR 1.329521 Loss 4.667220, Accuracy 90.018%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 454, LR 1.329387 Loss 4.667592, Accuracy 90.009%\n",
      "Epoch 25, Batch 455, LR 1.329253 Loss 4.666370, Accuracy 90.009%\n",
      "Epoch 25, Batch 456, LR 1.329120 Loss 4.667275, Accuracy 90.010%\n",
      "Epoch 25, Batch 457, LR 1.328986 Loss 4.665240, Accuracy 90.018%\n",
      "Epoch 25, Batch 458, LR 1.328852 Loss 4.665043, Accuracy 90.025%\n",
      "Epoch 25, Batch 459, LR 1.328719 Loss 4.664311, Accuracy 90.033%\n",
      "Epoch 25, Batch 460, LR 1.328585 Loss 4.663612, Accuracy 90.039%\n",
      "Epoch 25, Batch 461, LR 1.328451 Loss 4.663293, Accuracy 90.034%\n",
      "Epoch 25, Batch 462, LR 1.328318 Loss 4.663320, Accuracy 90.030%\n",
      "Epoch 25, Batch 463, LR 1.328184 Loss 4.664521, Accuracy 90.033%\n",
      "Epoch 25, Batch 464, LR 1.328050 Loss 4.665017, Accuracy 90.034%\n",
      "Epoch 25, Batch 465, LR 1.327916 Loss 4.665315, Accuracy 90.030%\n",
      "Epoch 25, Batch 466, LR 1.327783 Loss 4.665838, Accuracy 90.032%\n",
      "Epoch 25, Batch 467, LR 1.327649 Loss 4.665144, Accuracy 90.031%\n",
      "Epoch 25, Batch 468, LR 1.327515 Loss 4.664798, Accuracy 90.026%\n",
      "Epoch 25, Batch 469, LR 1.327382 Loss 4.664517, Accuracy 90.022%\n",
      "Epoch 25, Batch 470, LR 1.327248 Loss 4.663670, Accuracy 90.028%\n",
      "Epoch 25, Batch 471, LR 1.327114 Loss 4.663929, Accuracy 90.023%\n",
      "Epoch 25, Batch 472, LR 1.326981 Loss 4.662377, Accuracy 90.031%\n",
      "Epoch 25, Batch 473, LR 1.326847 Loss 4.662944, Accuracy 90.037%\n",
      "Epoch 25, Batch 474, LR 1.326713 Loss 4.663499, Accuracy 90.038%\n",
      "Epoch 25, Batch 475, LR 1.326579 Loss 4.662395, Accuracy 90.041%\n",
      "Epoch 25, Batch 476, LR 1.326446 Loss 4.662835, Accuracy 90.042%\n",
      "Epoch 25, Batch 477, LR 1.326312 Loss 4.662430, Accuracy 90.048%\n",
      "Epoch 25, Batch 478, LR 1.326178 Loss 4.663754, Accuracy 90.043%\n",
      "Epoch 25, Batch 479, LR 1.326045 Loss 4.664416, Accuracy 90.044%\n",
      "Epoch 25, Batch 480, LR 1.325911 Loss 4.664283, Accuracy 90.039%\n",
      "Epoch 25, Batch 481, LR 1.325777 Loss 4.663976, Accuracy 90.044%\n",
      "Epoch 25, Batch 482, LR 1.325644 Loss 4.663737, Accuracy 90.045%\n",
      "Epoch 25, Batch 483, LR 1.325510 Loss 4.663746, Accuracy 90.046%\n",
      "Epoch 25, Batch 484, LR 1.325376 Loss 4.662019, Accuracy 90.058%\n",
      "Epoch 25, Batch 485, LR 1.325242 Loss 4.662391, Accuracy 90.055%\n",
      "Epoch 25, Batch 486, LR 1.325109 Loss 4.661452, Accuracy 90.058%\n",
      "Epoch 25, Batch 487, LR 1.324975 Loss 4.661151, Accuracy 90.060%\n",
      "Epoch 25, Batch 488, LR 1.324841 Loss 4.660855, Accuracy 90.058%\n",
      "Epoch 25, Batch 489, LR 1.324708 Loss 4.660583, Accuracy 90.066%\n",
      "Epoch 25, Batch 490, LR 1.324574 Loss 4.661124, Accuracy 90.065%\n",
      "Epoch 25, Batch 491, LR 1.324440 Loss 4.661738, Accuracy 90.063%\n",
      "Epoch 25, Batch 492, LR 1.324306 Loss 4.660709, Accuracy 90.063%\n",
      "Epoch 25, Batch 493, LR 1.324173 Loss 4.661358, Accuracy 90.055%\n",
      "Epoch 25, Batch 494, LR 1.324039 Loss 4.660295, Accuracy 90.059%\n",
      "Epoch 25, Batch 495, LR 1.323905 Loss 4.661426, Accuracy 90.049%\n",
      "Epoch 25, Batch 496, LR 1.323772 Loss 4.661404, Accuracy 90.052%\n",
      "Epoch 25, Batch 497, LR 1.323638 Loss 4.661660, Accuracy 90.054%\n",
      "Epoch 25, Batch 498, LR 1.323504 Loss 4.660359, Accuracy 90.052%\n",
      "Epoch 25, Batch 499, LR 1.323370 Loss 4.661234, Accuracy 90.047%\n",
      "Epoch 25, Batch 500, LR 1.323237 Loss 4.660574, Accuracy 90.050%\n",
      "Epoch 25, Batch 501, LR 1.323103 Loss 4.660153, Accuracy 90.053%\n",
      "Epoch 25, Batch 502, LR 1.322969 Loss 4.659382, Accuracy 90.052%\n",
      "Epoch 25, Batch 503, LR 1.322836 Loss 4.658966, Accuracy 90.053%\n",
      "Epoch 25, Batch 504, LR 1.322702 Loss 4.659525, Accuracy 90.050%\n",
      "Epoch 25, Batch 505, LR 1.322568 Loss 4.660077, Accuracy 90.046%\n",
      "Epoch 25, Batch 506, LR 1.322434 Loss 4.659701, Accuracy 90.051%\n",
      "Epoch 25, Batch 507, LR 1.322301 Loss 4.660161, Accuracy 90.043%\n",
      "Epoch 25, Batch 508, LR 1.322167 Loss 4.660817, Accuracy 90.039%\n",
      "Epoch 25, Batch 509, LR 1.322033 Loss 4.661045, Accuracy 90.036%\n",
      "Epoch 25, Batch 510, LR 1.321899 Loss 4.661069, Accuracy 90.035%\n",
      "Epoch 25, Batch 511, LR 1.321766 Loss 4.662154, Accuracy 90.029%\n",
      "Epoch 25, Batch 512, LR 1.321632 Loss 4.662570, Accuracy 90.024%\n",
      "Epoch 25, Batch 513, LR 1.321498 Loss 4.661324, Accuracy 90.036%\n",
      "Epoch 25, Batch 514, LR 1.321364 Loss 4.662143, Accuracy 90.032%\n",
      "Epoch 25, Batch 515, LR 1.321231 Loss 4.661078, Accuracy 90.036%\n",
      "Epoch 25, Batch 516, LR 1.321097 Loss 4.662160, Accuracy 90.041%\n",
      "Epoch 25, Batch 517, LR 1.320963 Loss 4.663058, Accuracy 90.034%\n",
      "Epoch 25, Batch 518, LR 1.320830 Loss 4.663563, Accuracy 90.031%\n",
      "Epoch 25, Batch 519, LR 1.320696 Loss 4.663074, Accuracy 90.033%\n",
      "Epoch 25, Batch 520, LR 1.320562 Loss 4.663061, Accuracy 90.036%\n",
      "Epoch 25, Batch 521, LR 1.320428 Loss 4.662989, Accuracy 90.034%\n",
      "Epoch 25, Batch 522, LR 1.320295 Loss 4.664222, Accuracy 90.035%\n",
      "Epoch 25, Batch 523, LR 1.320161 Loss 4.664378, Accuracy 90.033%\n",
      "Epoch 25, Batch 524, LR 1.320027 Loss 4.665187, Accuracy 90.033%\n",
      "Epoch 25, Batch 525, LR 1.319893 Loss 4.665039, Accuracy 90.028%\n",
      "Epoch 25, Batch 526, LR 1.319760 Loss 4.665113, Accuracy 90.028%\n",
      "Epoch 25, Batch 527, LR 1.319626 Loss 4.664476, Accuracy 90.032%\n",
      "Epoch 25, Batch 528, LR 1.319492 Loss 4.663731, Accuracy 90.033%\n",
      "Epoch 25, Batch 529, LR 1.319358 Loss 4.664371, Accuracy 90.027%\n",
      "Epoch 25, Batch 530, LR 1.319225 Loss 4.664804, Accuracy 90.027%\n",
      "Epoch 25, Batch 531, LR 1.319091 Loss 4.665356, Accuracy 90.023%\n",
      "Epoch 25, Batch 532, LR 1.318957 Loss 4.665616, Accuracy 90.023%\n",
      "Epoch 25, Batch 533, LR 1.318823 Loss 4.667105, Accuracy 90.017%\n",
      "Epoch 25, Batch 534, LR 1.318690 Loss 4.667178, Accuracy 90.015%\n",
      "Epoch 25, Batch 535, LR 1.318556 Loss 4.666993, Accuracy 90.016%\n",
      "Epoch 25, Batch 536, LR 1.318422 Loss 4.668656, Accuracy 90.013%\n",
      "Epoch 25, Batch 537, LR 1.318288 Loss 4.668359, Accuracy 90.017%\n",
      "Epoch 25, Batch 538, LR 1.318155 Loss 4.668223, Accuracy 90.019%\n",
      "Epoch 25, Batch 539, LR 1.318021 Loss 4.668654, Accuracy 90.016%\n",
      "Epoch 25, Batch 540, LR 1.317887 Loss 4.668299, Accuracy 90.016%\n",
      "Epoch 25, Batch 541, LR 1.317753 Loss 4.668196, Accuracy 90.014%\n",
      "Epoch 25, Batch 542, LR 1.317620 Loss 4.668377, Accuracy 90.017%\n",
      "Epoch 25, Batch 543, LR 1.317486 Loss 4.668893, Accuracy 90.012%\n",
      "Epoch 25, Batch 544, LR 1.317352 Loss 4.668747, Accuracy 90.009%\n",
      "Epoch 25, Batch 545, LR 1.317218 Loss 4.669010, Accuracy 90.007%\n",
      "Epoch 25, Batch 546, LR 1.317085 Loss 4.668845, Accuracy 90.001%\n",
      "Epoch 25, Batch 547, LR 1.316951 Loss 4.668750, Accuracy 89.995%\n",
      "Epoch 25, Batch 548, LR 1.316817 Loss 4.668067, Accuracy 89.998%\n",
      "Epoch 25, Batch 549, LR 1.316683 Loss 4.668139, Accuracy 90.002%\n",
      "Epoch 25, Batch 550, LR 1.316550 Loss 4.668051, Accuracy 90.003%\n",
      "Epoch 25, Batch 551, LR 1.316416 Loss 4.668286, Accuracy 89.998%\n",
      "Epoch 25, Batch 552, LR 1.316282 Loss 4.668154, Accuracy 89.994%\n",
      "Epoch 25, Batch 553, LR 1.316148 Loss 4.669749, Accuracy 89.982%\n",
      "Epoch 25, Batch 554, LR 1.316014 Loss 4.670802, Accuracy 89.976%\n",
      "Epoch 25, Batch 555, LR 1.315881 Loss 4.671105, Accuracy 89.980%\n",
      "Epoch 25, Batch 556, LR 1.315747 Loss 4.671041, Accuracy 89.981%\n",
      "Epoch 25, Batch 557, LR 1.315613 Loss 4.671525, Accuracy 89.977%\n",
      "Epoch 25, Batch 558, LR 1.315479 Loss 4.670979, Accuracy 89.985%\n",
      "Epoch 25, Batch 559, LR 1.315346 Loss 4.671844, Accuracy 89.984%\n",
      "Epoch 25, Batch 560, LR 1.315212 Loss 4.670883, Accuracy 89.987%\n",
      "Epoch 25, Batch 561, LR 1.315078 Loss 4.671114, Accuracy 89.989%\n",
      "Epoch 25, Batch 562, LR 1.314944 Loss 4.670721, Accuracy 89.988%\n",
      "Epoch 25, Batch 563, LR 1.314811 Loss 4.669236, Accuracy 89.995%\n",
      "Epoch 25, Batch 564, LR 1.314677 Loss 4.668820, Accuracy 90.000%\n",
      "Epoch 25, Batch 565, LR 1.314543 Loss 4.669616, Accuracy 89.996%\n",
      "Epoch 25, Batch 566, LR 1.314409 Loss 4.671147, Accuracy 89.985%\n",
      "Epoch 25, Batch 567, LR 1.314275 Loss 4.671556, Accuracy 89.983%\n",
      "Epoch 25, Batch 568, LR 1.314142 Loss 4.671600, Accuracy 89.987%\n",
      "Epoch 25, Batch 569, LR 1.314008 Loss 4.672582, Accuracy 89.978%\n",
      "Epoch 25, Batch 570, LR 1.313874 Loss 4.671207, Accuracy 89.984%\n",
      "Epoch 25, Batch 571, LR 1.313740 Loss 4.670488, Accuracy 89.983%\n",
      "Epoch 25, Batch 572, LR 1.313607 Loss 4.670085, Accuracy 89.989%\n",
      "Epoch 25, Batch 573, LR 1.313473 Loss 4.669941, Accuracy 89.991%\n",
      "Epoch 25, Batch 574, LR 1.313339 Loss 4.669734, Accuracy 89.989%\n",
      "Epoch 25, Batch 575, LR 1.313205 Loss 4.670078, Accuracy 89.990%\n",
      "Epoch 25, Batch 576, LR 1.313071 Loss 4.670223, Accuracy 89.990%\n",
      "Epoch 25, Batch 577, LR 1.312938 Loss 4.669839, Accuracy 89.994%\n",
      "Epoch 25, Batch 578, LR 1.312804 Loss 4.669991, Accuracy 89.991%\n",
      "Epoch 25, Batch 579, LR 1.312670 Loss 4.669813, Accuracy 89.995%\n",
      "Epoch 25, Batch 580, LR 1.312536 Loss 4.670274, Accuracy 89.992%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 581, LR 1.312402 Loss 4.669401, Accuracy 89.992%\n",
      "Epoch 25, Batch 582, LR 1.312269 Loss 4.669019, Accuracy 89.991%\n",
      "Epoch 25, Batch 583, LR 1.312135 Loss 4.670158, Accuracy 89.983%\n",
      "Epoch 25, Batch 584, LR 1.312001 Loss 4.670124, Accuracy 89.979%\n",
      "Epoch 25, Batch 585, LR 1.311867 Loss 4.668962, Accuracy 89.983%\n",
      "Epoch 25, Batch 586, LR 1.311734 Loss 4.668706, Accuracy 89.986%\n",
      "Epoch 25, Batch 587, LR 1.311600 Loss 4.668922, Accuracy 89.985%\n",
      "Epoch 25, Batch 588, LR 1.311466 Loss 4.668093, Accuracy 89.990%\n",
      "Epoch 25, Batch 589, LR 1.311332 Loss 4.668044, Accuracy 89.992%\n",
      "Epoch 25, Batch 590, LR 1.311198 Loss 4.669351, Accuracy 89.985%\n",
      "Epoch 25, Batch 591, LR 1.311065 Loss 4.669072, Accuracy 89.988%\n",
      "Epoch 25, Batch 592, LR 1.310931 Loss 4.669275, Accuracy 89.986%\n",
      "Epoch 25, Batch 593, LR 1.310797 Loss 4.669893, Accuracy 89.982%\n",
      "Epoch 25, Batch 594, LR 1.310663 Loss 4.669201, Accuracy 89.986%\n",
      "Epoch 25, Batch 595, LR 1.310529 Loss 4.669405, Accuracy 89.986%\n",
      "Epoch 25, Batch 596, LR 1.310396 Loss 4.669462, Accuracy 89.988%\n",
      "Epoch 25, Batch 597, LR 1.310262 Loss 4.670278, Accuracy 89.981%\n",
      "Epoch 25, Batch 598, LR 1.310128 Loss 4.669616, Accuracy 89.985%\n",
      "Epoch 25, Batch 599, LR 1.309994 Loss 4.670238, Accuracy 89.979%\n",
      "Epoch 25, Batch 600, LR 1.309860 Loss 4.669734, Accuracy 89.982%\n",
      "Epoch 25, Batch 601, LR 1.309727 Loss 4.669652, Accuracy 89.984%\n",
      "Epoch 25, Batch 602, LR 1.309593 Loss 4.669067, Accuracy 89.985%\n",
      "Epoch 25, Batch 603, LR 1.309459 Loss 4.668963, Accuracy 89.993%\n",
      "Epoch 25, Batch 604, LR 1.309325 Loss 4.668557, Accuracy 89.996%\n",
      "Epoch 25, Batch 605, LR 1.309191 Loss 4.668888, Accuracy 89.991%\n",
      "Epoch 25, Batch 606, LR 1.309058 Loss 4.669004, Accuracy 89.996%\n",
      "Epoch 25, Batch 607, LR 1.308924 Loss 4.669242, Accuracy 89.994%\n",
      "Epoch 25, Batch 608, LR 1.308790 Loss 4.668475, Accuracy 90.002%\n",
      "Epoch 25, Batch 609, LR 1.308656 Loss 4.667369, Accuracy 90.003%\n",
      "Epoch 25, Batch 610, LR 1.308522 Loss 4.668110, Accuracy 90.001%\n",
      "Epoch 25, Batch 611, LR 1.308389 Loss 4.667254, Accuracy 90.004%\n",
      "Epoch 25, Batch 612, LR 1.308255 Loss 4.666885, Accuracy 90.003%\n",
      "Epoch 25, Batch 613, LR 1.308121 Loss 4.667190, Accuracy 90.007%\n",
      "Epoch 25, Batch 614, LR 1.307987 Loss 4.666830, Accuracy 90.003%\n",
      "Epoch 25, Batch 615, LR 1.307853 Loss 4.665884, Accuracy 90.009%\n",
      "Epoch 25, Batch 616, LR 1.307720 Loss 4.665445, Accuracy 90.007%\n",
      "Epoch 25, Batch 617, LR 1.307586 Loss 4.665255, Accuracy 89.999%\n",
      "Epoch 25, Batch 618, LR 1.307452 Loss 4.665564, Accuracy 89.993%\n",
      "Epoch 25, Batch 619, LR 1.307318 Loss 4.664188, Accuracy 90.003%\n",
      "Epoch 25, Batch 620, LR 1.307184 Loss 4.663681, Accuracy 90.008%\n",
      "Epoch 25, Batch 621, LR 1.307050 Loss 4.665275, Accuracy 89.998%\n",
      "Epoch 25, Batch 622, LR 1.306917 Loss 4.665988, Accuracy 89.997%\n",
      "Epoch 25, Batch 623, LR 1.306783 Loss 4.666826, Accuracy 89.993%\n",
      "Epoch 25, Batch 624, LR 1.306649 Loss 4.666260, Accuracy 89.996%\n",
      "Epoch 25, Batch 625, LR 1.306515 Loss 4.665153, Accuracy 90.002%\n",
      "Epoch 25, Batch 626, LR 1.306381 Loss 4.665310, Accuracy 90.001%\n",
      "Epoch 25, Batch 627, LR 1.306248 Loss 4.663969, Accuracy 90.009%\n",
      "Epoch 25, Batch 628, LR 1.306114 Loss 4.663707, Accuracy 90.014%\n",
      "Epoch 25, Batch 629, LR 1.305980 Loss 4.665093, Accuracy 90.006%\n",
      "Epoch 25, Batch 630, LR 1.305846 Loss 4.664734, Accuracy 90.010%\n",
      "Epoch 25, Batch 631, LR 1.305712 Loss 4.664455, Accuracy 90.011%\n",
      "Epoch 25, Batch 632, LR 1.305578 Loss 4.664979, Accuracy 90.013%\n",
      "Epoch 25, Batch 633, LR 1.305445 Loss 4.664961, Accuracy 90.015%\n",
      "Epoch 25, Batch 634, LR 1.305311 Loss 4.665643, Accuracy 90.013%\n",
      "Epoch 25, Batch 635, LR 1.305177 Loss 4.665953, Accuracy 90.004%\n",
      "Epoch 25, Batch 636, LR 1.305043 Loss 4.666611, Accuracy 90.001%\n",
      "Epoch 25, Batch 637, LR 1.304909 Loss 4.667800, Accuracy 89.998%\n",
      "Epoch 25, Batch 638, LR 1.304776 Loss 4.667683, Accuracy 89.998%\n",
      "Epoch 25, Batch 639, LR 1.304642 Loss 4.667635, Accuracy 90.000%\n",
      "Epoch 25, Batch 640, LR 1.304508 Loss 4.667420, Accuracy 90.004%\n",
      "Epoch 25, Batch 641, LR 1.304374 Loss 4.667376, Accuracy 90.005%\n",
      "Epoch 25, Batch 642, LR 1.304240 Loss 4.668571, Accuracy 89.998%\n",
      "Epoch 25, Batch 643, LR 1.304106 Loss 4.668379, Accuracy 90.002%\n",
      "Epoch 25, Batch 644, LR 1.303973 Loss 4.668664, Accuracy 90.001%\n",
      "Epoch 25, Batch 645, LR 1.303839 Loss 4.669388, Accuracy 89.998%\n",
      "Epoch 25, Batch 646, LR 1.303705 Loss 4.667740, Accuracy 90.006%\n",
      "Epoch 25, Batch 647, LR 1.303571 Loss 4.667698, Accuracy 90.008%\n",
      "Epoch 25, Batch 648, LR 1.303437 Loss 4.667632, Accuracy 90.011%\n",
      "Epoch 25, Batch 649, LR 1.303303 Loss 4.667585, Accuracy 90.017%\n",
      "Epoch 25, Batch 650, LR 1.303170 Loss 4.667155, Accuracy 90.022%\n",
      "Epoch 25, Batch 651, LR 1.303036 Loss 4.666814, Accuracy 90.024%\n",
      "Epoch 25, Batch 652, LR 1.302902 Loss 4.666348, Accuracy 90.029%\n",
      "Epoch 25, Batch 653, LR 1.302768 Loss 4.666772, Accuracy 90.029%\n",
      "Epoch 25, Batch 654, LR 1.302634 Loss 4.666859, Accuracy 90.027%\n",
      "Epoch 25, Batch 655, LR 1.302500 Loss 4.667125, Accuracy 90.021%\n",
      "Epoch 25, Batch 656, LR 1.302367 Loss 4.666918, Accuracy 90.018%\n",
      "Epoch 25, Batch 657, LR 1.302233 Loss 4.667049, Accuracy 90.015%\n",
      "Epoch 25, Batch 658, LR 1.302099 Loss 4.667113, Accuracy 90.018%\n",
      "Epoch 25, Batch 659, LR 1.301965 Loss 4.666593, Accuracy 90.022%\n",
      "Epoch 25, Batch 660, LR 1.301831 Loss 4.666696, Accuracy 90.014%\n",
      "Epoch 25, Batch 661, LR 1.301697 Loss 4.666834, Accuracy 90.012%\n",
      "Epoch 25, Batch 662, LR 1.301564 Loss 4.666828, Accuracy 90.008%\n",
      "Epoch 25, Batch 663, LR 1.301430 Loss 4.666865, Accuracy 90.009%\n",
      "Epoch 25, Batch 664, LR 1.301296 Loss 4.668008, Accuracy 90.006%\n",
      "Epoch 25, Batch 665, LR 1.301162 Loss 4.666646, Accuracy 90.009%\n",
      "Epoch 25, Batch 666, LR 1.301028 Loss 4.666678, Accuracy 90.003%\n",
      "Epoch 25, Batch 667, LR 1.300894 Loss 4.666422, Accuracy 90.002%\n",
      "Epoch 25, Batch 668, LR 1.300761 Loss 4.665964, Accuracy 90.005%\n",
      "Epoch 25, Batch 669, LR 1.300627 Loss 4.665876, Accuracy 90.008%\n",
      "Epoch 25, Batch 670, LR 1.300493 Loss 4.665970, Accuracy 90.012%\n",
      "Epoch 25, Batch 671, LR 1.300359 Loss 4.666195, Accuracy 90.013%\n",
      "Epoch 25, Batch 672, LR 1.300225 Loss 4.665689, Accuracy 90.017%\n",
      "Epoch 25, Batch 673, LR 1.300091 Loss 4.666545, Accuracy 90.011%\n",
      "Epoch 25, Batch 674, LR 1.299957 Loss 4.665782, Accuracy 90.016%\n",
      "Epoch 25, Batch 675, LR 1.299824 Loss 4.665808, Accuracy 90.013%\n",
      "Epoch 25, Batch 676, LR 1.299690 Loss 4.666193, Accuracy 90.008%\n",
      "Epoch 25, Batch 677, LR 1.299556 Loss 4.666668, Accuracy 90.004%\n",
      "Epoch 25, Batch 678, LR 1.299422 Loss 4.665290, Accuracy 90.010%\n",
      "Epoch 25, Batch 679, LR 1.299288 Loss 4.665133, Accuracy 90.009%\n",
      "Epoch 25, Batch 680, LR 1.299154 Loss 4.665253, Accuracy 90.008%\n",
      "Epoch 25, Batch 681, LR 1.299021 Loss 4.664653, Accuracy 90.011%\n",
      "Epoch 25, Batch 682, LR 1.298887 Loss 4.664363, Accuracy 90.014%\n",
      "Epoch 25, Batch 683, LR 1.298753 Loss 4.663869, Accuracy 90.022%\n",
      "Epoch 25, Batch 684, LR 1.298619 Loss 4.663608, Accuracy 90.022%\n",
      "Epoch 25, Batch 685, LR 1.298485 Loss 4.664187, Accuracy 90.018%\n",
      "Epoch 25, Batch 686, LR 1.298351 Loss 4.664546, Accuracy 90.013%\n",
      "Epoch 25, Batch 687, LR 1.298217 Loss 4.665630, Accuracy 90.005%\n",
      "Epoch 25, Batch 688, LR 1.298084 Loss 4.666312, Accuracy 89.999%\n",
      "Epoch 25, Batch 689, LR 1.297950 Loss 4.667015, Accuracy 89.995%\n",
      "Epoch 25, Batch 690, LR 1.297816 Loss 4.667670, Accuracy 89.991%\n",
      "Epoch 25, Batch 691, LR 1.297682 Loss 4.667425, Accuracy 89.994%\n",
      "Epoch 25, Batch 692, LR 1.297548 Loss 4.667253, Accuracy 90.000%\n",
      "Epoch 25, Batch 693, LR 1.297414 Loss 4.667082, Accuracy 90.002%\n",
      "Epoch 25, Batch 694, LR 1.297280 Loss 4.666963, Accuracy 90.005%\n",
      "Epoch 25, Batch 695, LR 1.297147 Loss 4.667628, Accuracy 90.004%\n",
      "Epoch 25, Batch 696, LR 1.297013 Loss 4.667313, Accuracy 90.005%\n",
      "Epoch 25, Batch 697, LR 1.296879 Loss 4.667257, Accuracy 90.004%\n",
      "Epoch 25, Batch 698, LR 1.296745 Loss 4.668160, Accuracy 89.998%\n",
      "Epoch 25, Batch 699, LR 1.296611 Loss 4.667953, Accuracy 90.000%\n",
      "Epoch 25, Batch 700, LR 1.296477 Loss 4.667577, Accuracy 90.004%\n",
      "Epoch 25, Batch 701, LR 1.296343 Loss 4.666799, Accuracy 90.008%\n",
      "Epoch 25, Batch 702, LR 1.296210 Loss 4.666961, Accuracy 90.010%\n",
      "Epoch 25, Batch 703, LR 1.296076 Loss 4.665995, Accuracy 90.010%\n",
      "Epoch 25, Batch 704, LR 1.295942 Loss 4.666339, Accuracy 90.009%\n",
      "Epoch 25, Batch 705, LR 1.295808 Loss 4.665967, Accuracy 90.013%\n",
      "Epoch 25, Batch 706, LR 1.295674 Loss 4.666150, Accuracy 90.009%\n",
      "Epoch 25, Batch 707, LR 1.295540 Loss 4.665203, Accuracy 90.013%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 708, LR 1.295406 Loss 4.664316, Accuracy 90.015%\n",
      "Epoch 25, Batch 709, LR 1.295273 Loss 4.663917, Accuracy 90.018%\n",
      "Epoch 25, Batch 710, LR 1.295139 Loss 4.664064, Accuracy 90.015%\n",
      "Epoch 25, Batch 711, LR 1.295005 Loss 4.662945, Accuracy 90.022%\n",
      "Epoch 25, Batch 712, LR 1.294871 Loss 4.661758, Accuracy 90.023%\n",
      "Epoch 25, Batch 713, LR 1.294737 Loss 4.661922, Accuracy 90.021%\n",
      "Epoch 25, Batch 714, LR 1.294603 Loss 4.661862, Accuracy 90.021%\n",
      "Epoch 25, Batch 715, LR 1.294469 Loss 4.661003, Accuracy 90.022%\n",
      "Epoch 25, Batch 716, LR 1.294335 Loss 4.661051, Accuracy 90.022%\n",
      "Epoch 25, Batch 717, LR 1.294202 Loss 4.661074, Accuracy 90.024%\n",
      "Epoch 25, Batch 718, LR 1.294068 Loss 4.662438, Accuracy 90.021%\n",
      "Epoch 25, Batch 719, LR 1.293934 Loss 4.661617, Accuracy 90.025%\n",
      "Epoch 25, Batch 720, LR 1.293800 Loss 4.661383, Accuracy 90.026%\n",
      "Epoch 25, Batch 721, LR 1.293666 Loss 4.662282, Accuracy 90.023%\n",
      "Epoch 25, Batch 722, LR 1.293532 Loss 4.662014, Accuracy 90.020%\n",
      "Epoch 25, Batch 723, LR 1.293398 Loss 4.661638, Accuracy 90.022%\n",
      "Epoch 25, Batch 724, LR 1.293264 Loss 4.661605, Accuracy 90.021%\n",
      "Epoch 25, Batch 725, LR 1.293131 Loss 4.661089, Accuracy 90.026%\n",
      "Epoch 25, Batch 726, LR 1.292997 Loss 4.661220, Accuracy 90.027%\n",
      "Epoch 25, Batch 727, LR 1.292863 Loss 4.660894, Accuracy 90.030%\n",
      "Epoch 25, Batch 728, LR 1.292729 Loss 4.660206, Accuracy 90.033%\n",
      "Epoch 25, Batch 729, LR 1.292595 Loss 4.660538, Accuracy 90.029%\n",
      "Epoch 25, Batch 730, LR 1.292461 Loss 4.660032, Accuracy 90.034%\n",
      "Epoch 25, Batch 731, LR 1.292327 Loss 4.660071, Accuracy 90.035%\n",
      "Epoch 25, Batch 732, LR 1.292193 Loss 4.660624, Accuracy 90.033%\n",
      "Epoch 25, Batch 733, LR 1.292060 Loss 4.661444, Accuracy 90.029%\n",
      "Epoch 25, Batch 734, LR 1.291926 Loss 4.662118, Accuracy 90.026%\n",
      "Epoch 25, Batch 735, LR 1.291792 Loss 4.662194, Accuracy 90.024%\n",
      "Epoch 25, Batch 736, LR 1.291658 Loss 4.661836, Accuracy 90.023%\n",
      "Epoch 25, Batch 737, LR 1.291524 Loss 4.661150, Accuracy 90.025%\n",
      "Epoch 25, Batch 738, LR 1.291390 Loss 4.660657, Accuracy 90.029%\n",
      "Epoch 25, Batch 739, LR 1.291256 Loss 4.659187, Accuracy 90.034%\n",
      "Epoch 25, Batch 740, LR 1.291122 Loss 4.659442, Accuracy 90.036%\n",
      "Epoch 25, Batch 741, LR 1.290989 Loss 4.658045, Accuracy 90.042%\n",
      "Epoch 25, Batch 742, LR 1.290855 Loss 4.657664, Accuracy 90.046%\n",
      "Epoch 25, Batch 743, LR 1.290721 Loss 4.657395, Accuracy 90.047%\n",
      "Epoch 25, Batch 744, LR 1.290587 Loss 4.657526, Accuracy 90.052%\n",
      "Epoch 25, Batch 745, LR 1.290453 Loss 4.658305, Accuracy 90.050%\n",
      "Epoch 25, Batch 746, LR 1.290319 Loss 4.657743, Accuracy 90.053%\n",
      "Epoch 25, Batch 747, LR 1.290185 Loss 4.657826, Accuracy 90.051%\n",
      "Epoch 25, Batch 748, LR 1.290051 Loss 4.658587, Accuracy 90.054%\n",
      "Epoch 25, Batch 749, LR 1.289918 Loss 4.658699, Accuracy 90.050%\n",
      "Epoch 25, Batch 750, LR 1.289784 Loss 4.660120, Accuracy 90.041%\n",
      "Epoch 25, Batch 751, LR 1.289650 Loss 4.660054, Accuracy 90.042%\n",
      "Epoch 25, Batch 752, LR 1.289516 Loss 4.661261, Accuracy 90.039%\n",
      "Epoch 25, Batch 753, LR 1.289382 Loss 4.661239, Accuracy 90.039%\n",
      "Epoch 25, Batch 754, LR 1.289248 Loss 4.660918, Accuracy 90.044%\n",
      "Epoch 25, Batch 755, LR 1.289114 Loss 4.661719, Accuracy 90.042%\n",
      "Epoch 25, Batch 756, LR 1.288980 Loss 4.662279, Accuracy 90.042%\n",
      "Epoch 25, Batch 757, LR 1.288846 Loss 4.662454, Accuracy 90.042%\n",
      "Epoch 25, Batch 758, LR 1.288713 Loss 4.662343, Accuracy 90.043%\n",
      "Epoch 25, Batch 759, LR 1.288579 Loss 4.662758, Accuracy 90.044%\n",
      "Epoch 25, Batch 760, LR 1.288445 Loss 4.662473, Accuracy 90.044%\n",
      "Epoch 25, Batch 761, LR 1.288311 Loss 4.661176, Accuracy 90.050%\n",
      "Epoch 25, Batch 762, LR 1.288177 Loss 4.660923, Accuracy 90.051%\n",
      "Epoch 25, Batch 763, LR 1.288043 Loss 4.661719, Accuracy 90.048%\n",
      "Epoch 25, Batch 764, LR 1.287909 Loss 4.662096, Accuracy 90.043%\n",
      "Epoch 25, Batch 765, LR 1.287775 Loss 4.662291, Accuracy 90.043%\n",
      "Epoch 25, Batch 766, LR 1.287641 Loss 4.662835, Accuracy 90.041%\n",
      "Epoch 25, Batch 767, LR 1.287508 Loss 4.662939, Accuracy 90.044%\n",
      "Epoch 25, Batch 768, LR 1.287374 Loss 4.662962, Accuracy 90.041%\n",
      "Epoch 25, Batch 769, LR 1.287240 Loss 4.662192, Accuracy 90.043%\n",
      "Epoch 25, Batch 770, LR 1.287106 Loss 4.662544, Accuracy 90.040%\n",
      "Epoch 25, Batch 771, LR 1.286972 Loss 4.662254, Accuracy 90.036%\n",
      "Epoch 25, Batch 772, LR 1.286838 Loss 4.661560, Accuracy 90.038%\n",
      "Epoch 25, Batch 773, LR 1.286704 Loss 4.661636, Accuracy 90.037%\n",
      "Epoch 25, Batch 774, LR 1.286570 Loss 4.662264, Accuracy 90.039%\n",
      "Epoch 25, Batch 775, LR 1.286436 Loss 4.661760, Accuracy 90.039%\n",
      "Epoch 25, Batch 776, LR 1.286302 Loss 4.661735, Accuracy 90.040%\n",
      "Epoch 25, Batch 777, LR 1.286169 Loss 4.662309, Accuracy 90.040%\n",
      "Epoch 25, Batch 778, LR 1.286035 Loss 4.662368, Accuracy 90.041%\n",
      "Epoch 25, Batch 779, LR 1.285901 Loss 4.662485, Accuracy 90.040%\n",
      "Epoch 25, Batch 780, LR 1.285767 Loss 4.663013, Accuracy 90.038%\n",
      "Epoch 25, Batch 781, LR 1.285633 Loss 4.661881, Accuracy 90.042%\n",
      "Epoch 25, Batch 782, LR 1.285499 Loss 4.661984, Accuracy 90.037%\n",
      "Epoch 25, Batch 783, LR 1.285365 Loss 4.660969, Accuracy 90.041%\n",
      "Epoch 25, Batch 784, LR 1.285231 Loss 4.660702, Accuracy 90.043%\n",
      "Epoch 25, Batch 785, LR 1.285097 Loss 4.661537, Accuracy 90.038%\n",
      "Epoch 25, Batch 786, LR 1.284963 Loss 4.661123, Accuracy 90.042%\n",
      "Epoch 25, Batch 787, LR 1.284830 Loss 4.661090, Accuracy 90.040%\n",
      "Epoch 25, Batch 788, LR 1.284696 Loss 4.661697, Accuracy 90.036%\n",
      "Epoch 25, Batch 789, LR 1.284562 Loss 4.660881, Accuracy 90.043%\n",
      "Epoch 25, Batch 790, LR 1.284428 Loss 4.660829, Accuracy 90.045%\n",
      "Epoch 25, Batch 791, LR 1.284294 Loss 4.661138, Accuracy 90.039%\n",
      "Epoch 25, Batch 792, LR 1.284160 Loss 4.661461, Accuracy 90.034%\n",
      "Epoch 25, Batch 793, LR 1.284026 Loss 4.661287, Accuracy 90.036%\n",
      "Epoch 25, Batch 794, LR 1.283892 Loss 4.661333, Accuracy 90.037%\n",
      "Epoch 25, Batch 795, LR 1.283758 Loss 4.662134, Accuracy 90.029%\n",
      "Epoch 25, Batch 796, LR 1.283624 Loss 4.662055, Accuracy 90.030%\n",
      "Epoch 25, Batch 797, LR 1.283491 Loss 4.662600, Accuracy 90.029%\n",
      "Epoch 25, Batch 798, LR 1.283357 Loss 4.662227, Accuracy 90.031%\n",
      "Epoch 25, Batch 799, LR 1.283223 Loss 4.662777, Accuracy 90.025%\n",
      "Epoch 25, Batch 800, LR 1.283089 Loss 4.662654, Accuracy 90.027%\n",
      "Epoch 25, Batch 801, LR 1.282955 Loss 4.662227, Accuracy 90.026%\n",
      "Epoch 25, Batch 802, LR 1.282821 Loss 4.661874, Accuracy 90.026%\n",
      "Epoch 25, Batch 803, LR 1.282687 Loss 4.661486, Accuracy 90.026%\n",
      "Epoch 25, Batch 804, LR 1.282553 Loss 4.661242, Accuracy 90.025%\n",
      "Epoch 25, Batch 805, LR 1.282419 Loss 4.661237, Accuracy 90.025%\n",
      "Epoch 25, Batch 806, LR 1.282285 Loss 4.660739, Accuracy 90.027%\n",
      "Epoch 25, Batch 807, LR 1.282151 Loss 4.661882, Accuracy 90.022%\n",
      "Epoch 25, Batch 808, LR 1.282018 Loss 4.661862, Accuracy 90.024%\n",
      "Epoch 25, Batch 809, LR 1.281884 Loss 4.661631, Accuracy 90.024%\n",
      "Epoch 25, Batch 810, LR 1.281750 Loss 4.661582, Accuracy 90.024%\n",
      "Epoch 25, Batch 811, LR 1.281616 Loss 4.660802, Accuracy 90.030%\n",
      "Epoch 25, Batch 812, LR 1.281482 Loss 4.660841, Accuracy 90.026%\n",
      "Epoch 25, Batch 813, LR 1.281348 Loss 4.660721, Accuracy 90.025%\n",
      "Epoch 25, Batch 814, LR 1.281214 Loss 4.661124, Accuracy 90.024%\n",
      "Epoch 25, Batch 815, LR 1.281080 Loss 4.661586, Accuracy 90.024%\n",
      "Epoch 25, Batch 816, LR 1.280946 Loss 4.660779, Accuracy 90.027%\n",
      "Epoch 25, Batch 817, LR 1.280812 Loss 4.659799, Accuracy 90.032%\n",
      "Epoch 25, Batch 818, LR 1.280678 Loss 4.659923, Accuracy 90.028%\n",
      "Epoch 25, Batch 819, LR 1.280545 Loss 4.659712, Accuracy 90.029%\n",
      "Epoch 25, Batch 820, LR 1.280411 Loss 4.659848, Accuracy 90.028%\n",
      "Epoch 25, Batch 821, LR 1.280277 Loss 4.659982, Accuracy 90.027%\n",
      "Epoch 25, Batch 822, LR 1.280143 Loss 4.660291, Accuracy 90.024%\n",
      "Epoch 25, Batch 823, LR 1.280009 Loss 4.660697, Accuracy 90.017%\n",
      "Epoch 25, Batch 824, LR 1.279875 Loss 4.660820, Accuracy 90.018%\n",
      "Epoch 25, Batch 825, LR 1.279741 Loss 4.660472, Accuracy 90.016%\n",
      "Epoch 25, Batch 826, LR 1.279607 Loss 4.660300, Accuracy 90.019%\n",
      "Epoch 25, Batch 827, LR 1.279473 Loss 4.661154, Accuracy 90.012%\n",
      "Epoch 25, Batch 828, LR 1.279339 Loss 4.660764, Accuracy 90.013%\n",
      "Epoch 25, Batch 829, LR 1.279205 Loss 4.661193, Accuracy 90.013%\n",
      "Epoch 25, Batch 830, LR 1.279071 Loss 4.661719, Accuracy 90.010%\n",
      "Epoch 25, Batch 831, LR 1.278938 Loss 4.661988, Accuracy 90.011%\n",
      "Epoch 25, Batch 832, LR 1.278804 Loss 4.661964, Accuracy 90.008%\n",
      "Epoch 25, Batch 833, LR 1.278670 Loss 4.662309, Accuracy 90.006%\n",
      "Epoch 25, Batch 834, LR 1.278536 Loss 4.662159, Accuracy 90.009%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 835, LR 1.278402 Loss 4.662255, Accuracy 90.007%\n",
      "Epoch 25, Batch 836, LR 1.278268 Loss 4.662141, Accuracy 90.005%\n",
      "Epoch 25, Batch 837, LR 1.278134 Loss 4.662345, Accuracy 90.002%\n",
      "Epoch 25, Batch 838, LR 1.278000 Loss 4.662440, Accuracy 89.999%\n",
      "Epoch 25, Batch 839, LR 1.277866 Loss 4.662602, Accuracy 89.996%\n",
      "Epoch 25, Batch 840, LR 1.277732 Loss 4.662211, Accuracy 89.997%\n",
      "Epoch 25, Batch 841, LR 1.277598 Loss 4.662170, Accuracy 89.998%\n",
      "Epoch 25, Batch 842, LR 1.277464 Loss 4.661841, Accuracy 89.999%\n",
      "Epoch 25, Batch 843, LR 1.277331 Loss 4.662364, Accuracy 89.996%\n",
      "Epoch 25, Batch 844, LR 1.277197 Loss 4.662418, Accuracy 89.990%\n",
      "Epoch 25, Batch 845, LR 1.277063 Loss 4.662651, Accuracy 89.988%\n",
      "Epoch 25, Batch 846, LR 1.276929 Loss 4.662901, Accuracy 89.985%\n",
      "Epoch 25, Batch 847, LR 1.276795 Loss 4.662846, Accuracy 89.984%\n",
      "Epoch 25, Batch 848, LR 1.276661 Loss 4.663662, Accuracy 89.985%\n",
      "Epoch 25, Batch 849, LR 1.276527 Loss 4.663879, Accuracy 89.983%\n",
      "Epoch 25, Batch 850, LR 1.276393 Loss 4.663421, Accuracy 89.981%\n",
      "Epoch 25, Batch 851, LR 1.276259 Loss 4.663005, Accuracy 89.985%\n",
      "Epoch 25, Batch 852, LR 1.276125 Loss 4.663176, Accuracy 89.981%\n",
      "Epoch 25, Batch 853, LR 1.275991 Loss 4.663105, Accuracy 89.982%\n",
      "Epoch 25, Batch 854, LR 1.275857 Loss 4.663207, Accuracy 89.983%\n",
      "Epoch 25, Batch 855, LR 1.275723 Loss 4.664055, Accuracy 89.981%\n",
      "Epoch 25, Batch 856, LR 1.275590 Loss 4.664754, Accuracy 89.978%\n",
      "Epoch 25, Batch 857, LR 1.275456 Loss 4.664466, Accuracy 89.975%\n",
      "Epoch 25, Batch 858, LR 1.275322 Loss 4.664714, Accuracy 89.972%\n",
      "Epoch 25, Batch 859, LR 1.275188 Loss 4.664845, Accuracy 89.971%\n",
      "Epoch 25, Batch 860, LR 1.275054 Loss 4.664697, Accuracy 89.972%\n",
      "Epoch 25, Batch 861, LR 1.274920 Loss 4.665119, Accuracy 89.971%\n",
      "Epoch 25, Batch 862, LR 1.274786 Loss 4.664764, Accuracy 89.972%\n",
      "Epoch 25, Batch 863, LR 1.274652 Loss 4.665044, Accuracy 89.973%\n",
      "Epoch 25, Batch 864, LR 1.274518 Loss 4.665550, Accuracy 89.971%\n",
      "Epoch 25, Batch 865, LR 1.274384 Loss 4.665580, Accuracy 89.969%\n",
      "Epoch 25, Batch 866, LR 1.274250 Loss 4.666136, Accuracy 89.965%\n",
      "Epoch 25, Batch 867, LR 1.274116 Loss 4.665809, Accuracy 89.968%\n",
      "Epoch 25, Batch 868, LR 1.273982 Loss 4.666155, Accuracy 89.967%\n",
      "Epoch 25, Batch 869, LR 1.273848 Loss 4.666242, Accuracy 89.964%\n",
      "Epoch 25, Batch 870, LR 1.273715 Loss 4.665842, Accuracy 89.964%\n",
      "Epoch 25, Batch 871, LR 1.273581 Loss 4.665246, Accuracy 89.965%\n",
      "Epoch 25, Batch 872, LR 1.273447 Loss 4.664938, Accuracy 89.965%\n",
      "Epoch 25, Batch 873, LR 1.273313 Loss 4.664851, Accuracy 89.965%\n",
      "Epoch 25, Batch 874, LR 1.273179 Loss 4.665121, Accuracy 89.962%\n",
      "Epoch 25, Batch 875, LR 1.273045 Loss 4.664962, Accuracy 89.958%\n",
      "Epoch 25, Batch 876, LR 1.272911 Loss 4.665336, Accuracy 89.956%\n",
      "Epoch 25, Batch 877, LR 1.272777 Loss 4.664849, Accuracy 89.959%\n",
      "Epoch 25, Batch 878, LR 1.272643 Loss 4.664809, Accuracy 89.959%\n",
      "Epoch 25, Batch 879, LR 1.272509 Loss 4.665295, Accuracy 89.956%\n",
      "Epoch 25, Batch 880, LR 1.272375 Loss 4.666076, Accuracy 89.954%\n",
      "Epoch 25, Batch 881, LR 1.272241 Loss 4.666061, Accuracy 89.954%\n",
      "Epoch 25, Batch 882, LR 1.272107 Loss 4.666153, Accuracy 89.953%\n",
      "Epoch 25, Batch 883, LR 1.271973 Loss 4.666933, Accuracy 89.946%\n",
      "Epoch 25, Batch 884, LR 1.271840 Loss 4.666384, Accuracy 89.949%\n",
      "Epoch 25, Batch 885, LR 1.271706 Loss 4.665907, Accuracy 89.947%\n",
      "Epoch 25, Batch 886, LR 1.271572 Loss 4.666361, Accuracy 89.943%\n",
      "Epoch 25, Batch 887, LR 1.271438 Loss 4.666245, Accuracy 89.940%\n",
      "Epoch 25, Batch 888, LR 1.271304 Loss 4.667443, Accuracy 89.933%\n",
      "Epoch 25, Batch 889, LR 1.271170 Loss 4.666936, Accuracy 89.936%\n",
      "Epoch 25, Batch 890, LR 1.271036 Loss 4.667098, Accuracy 89.936%\n",
      "Epoch 25, Batch 891, LR 1.270902 Loss 4.666882, Accuracy 89.940%\n",
      "Epoch 25, Batch 892, LR 1.270768 Loss 4.667204, Accuracy 89.940%\n",
      "Epoch 25, Batch 893, LR 1.270634 Loss 4.666928, Accuracy 89.941%\n",
      "Epoch 25, Batch 894, LR 1.270500 Loss 4.666332, Accuracy 89.944%\n",
      "Epoch 25, Batch 895, LR 1.270366 Loss 4.667708, Accuracy 89.935%\n",
      "Epoch 25, Batch 896, LR 1.270232 Loss 4.667792, Accuracy 89.934%\n",
      "Epoch 25, Batch 897, LR 1.270098 Loss 4.668293, Accuracy 89.933%\n",
      "Epoch 25, Batch 898, LR 1.269964 Loss 4.668422, Accuracy 89.936%\n",
      "Epoch 25, Batch 899, LR 1.269830 Loss 4.668546, Accuracy 89.937%\n",
      "Epoch 25, Batch 900, LR 1.269697 Loss 4.668378, Accuracy 89.939%\n",
      "Epoch 25, Batch 901, LR 1.269563 Loss 4.668131, Accuracy 89.942%\n",
      "Epoch 25, Batch 902, LR 1.269429 Loss 4.667751, Accuracy 89.945%\n",
      "Epoch 25, Batch 903, LR 1.269295 Loss 4.668397, Accuracy 89.944%\n",
      "Epoch 25, Batch 904, LR 1.269161 Loss 4.668479, Accuracy 89.945%\n",
      "Epoch 25, Batch 905, LR 1.269027 Loss 4.668763, Accuracy 89.943%\n",
      "Epoch 25, Batch 906, LR 1.268893 Loss 4.668580, Accuracy 89.946%\n",
      "Epoch 25, Batch 907, LR 1.268759 Loss 4.668737, Accuracy 89.947%\n",
      "Epoch 25, Batch 908, LR 1.268625 Loss 4.668468, Accuracy 89.947%\n",
      "Epoch 25, Batch 909, LR 1.268491 Loss 4.668366, Accuracy 89.948%\n",
      "Epoch 25, Batch 910, LR 1.268357 Loss 4.668581, Accuracy 89.948%\n",
      "Epoch 25, Batch 911, LR 1.268223 Loss 4.668946, Accuracy 89.945%\n",
      "Epoch 25, Batch 912, LR 1.268089 Loss 4.669090, Accuracy 89.944%\n",
      "Epoch 25, Batch 913, LR 1.267955 Loss 4.668575, Accuracy 89.949%\n",
      "Epoch 25, Batch 914, LR 1.267821 Loss 4.668139, Accuracy 89.951%\n",
      "Epoch 25, Batch 915, LR 1.267687 Loss 4.668213, Accuracy 89.948%\n",
      "Epoch 25, Batch 916, LR 1.267554 Loss 4.668273, Accuracy 89.945%\n",
      "Epoch 25, Batch 917, LR 1.267420 Loss 4.668355, Accuracy 89.949%\n",
      "Epoch 25, Batch 918, LR 1.267286 Loss 4.668809, Accuracy 89.944%\n",
      "Epoch 25, Batch 919, LR 1.267152 Loss 4.668583, Accuracy 89.944%\n",
      "Epoch 25, Batch 920, LR 1.267018 Loss 4.668373, Accuracy 89.943%\n",
      "Epoch 25, Batch 921, LR 1.266884 Loss 4.667472, Accuracy 89.945%\n",
      "Epoch 25, Batch 922, LR 1.266750 Loss 4.668898, Accuracy 89.936%\n",
      "Epoch 25, Batch 923, LR 1.266616 Loss 4.668447, Accuracy 89.937%\n",
      "Epoch 25, Batch 924, LR 1.266482 Loss 4.667831, Accuracy 89.942%\n",
      "Epoch 25, Batch 925, LR 1.266348 Loss 4.667277, Accuracy 89.944%\n",
      "Epoch 25, Batch 926, LR 1.266214 Loss 4.666985, Accuracy 89.945%\n",
      "Epoch 25, Batch 927, LR 1.266080 Loss 4.666846, Accuracy 89.945%\n",
      "Epoch 25, Batch 928, LR 1.265946 Loss 4.666862, Accuracy 89.944%\n",
      "Epoch 25, Batch 929, LR 1.265812 Loss 4.666995, Accuracy 89.940%\n",
      "Epoch 25, Batch 930, LR 1.265678 Loss 4.666382, Accuracy 89.942%\n",
      "Epoch 25, Batch 931, LR 1.265544 Loss 4.666183, Accuracy 89.944%\n",
      "Epoch 25, Batch 932, LR 1.265410 Loss 4.667318, Accuracy 89.942%\n",
      "Epoch 25, Batch 933, LR 1.265277 Loss 4.667881, Accuracy 89.940%\n",
      "Epoch 25, Batch 934, LR 1.265143 Loss 4.667472, Accuracy 89.944%\n",
      "Epoch 25, Batch 935, LR 1.265009 Loss 4.667295, Accuracy 89.942%\n",
      "Epoch 25, Batch 936, LR 1.264875 Loss 4.666561, Accuracy 89.944%\n",
      "Epoch 25, Batch 937, LR 1.264741 Loss 4.666551, Accuracy 89.945%\n",
      "Epoch 25, Batch 938, LR 1.264607 Loss 4.666126, Accuracy 89.948%\n",
      "Epoch 25, Batch 939, LR 1.264473 Loss 4.666412, Accuracy 89.947%\n",
      "Epoch 25, Batch 940, LR 1.264339 Loss 4.666011, Accuracy 89.950%\n",
      "Epoch 25, Batch 941, LR 1.264205 Loss 4.665925, Accuracy 89.957%\n",
      "Epoch 25, Batch 942, LR 1.264071 Loss 4.665846, Accuracy 89.962%\n",
      "Epoch 25, Batch 943, LR 1.263937 Loss 4.665836, Accuracy 89.961%\n",
      "Epoch 25, Batch 944, LR 1.263803 Loss 4.665039, Accuracy 89.960%\n",
      "Epoch 25, Batch 945, LR 1.263669 Loss 4.664976, Accuracy 89.962%\n",
      "Epoch 25, Batch 946, LR 1.263535 Loss 4.665053, Accuracy 89.961%\n",
      "Epoch 25, Batch 947, LR 1.263401 Loss 4.664601, Accuracy 89.965%\n",
      "Epoch 25, Batch 948, LR 1.263267 Loss 4.664461, Accuracy 89.966%\n",
      "Epoch 25, Batch 949, LR 1.263133 Loss 4.664619, Accuracy 89.966%\n",
      "Epoch 25, Batch 950, LR 1.262999 Loss 4.664143, Accuracy 89.970%\n",
      "Epoch 25, Batch 951, LR 1.262866 Loss 4.664702, Accuracy 89.966%\n",
      "Epoch 25, Batch 952, LR 1.262732 Loss 4.665034, Accuracy 89.968%\n",
      "Epoch 25, Batch 953, LR 1.262598 Loss 4.664916, Accuracy 89.968%\n",
      "Epoch 25, Batch 954, LR 1.262464 Loss 4.664411, Accuracy 89.969%\n",
      "Epoch 25, Batch 955, LR 1.262330 Loss 4.664712, Accuracy 89.968%\n",
      "Epoch 25, Batch 956, LR 1.262196 Loss 4.664305, Accuracy 89.965%\n",
      "Epoch 25, Batch 957, LR 1.262062 Loss 4.664396, Accuracy 89.966%\n",
      "Epoch 25, Batch 958, LR 1.261928 Loss 4.663674, Accuracy 89.971%\n",
      "Epoch 25, Batch 959, LR 1.261794 Loss 4.664018, Accuracy 89.971%\n",
      "Epoch 25, Batch 960, LR 1.261660 Loss 4.664228, Accuracy 89.970%\n",
      "Epoch 25, Batch 961, LR 1.261526 Loss 4.663542, Accuracy 89.970%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 962, LR 1.261392 Loss 4.662902, Accuracy 89.971%\n",
      "Epoch 25, Batch 963, LR 1.261258 Loss 4.662628, Accuracy 89.974%\n",
      "Epoch 25, Batch 964, LR 1.261124 Loss 4.662431, Accuracy 89.976%\n",
      "Epoch 25, Batch 965, LR 1.260990 Loss 4.662309, Accuracy 89.980%\n",
      "Epoch 25, Batch 966, LR 1.260856 Loss 4.661631, Accuracy 89.984%\n",
      "Epoch 25, Batch 967, LR 1.260722 Loss 4.661398, Accuracy 89.985%\n",
      "Epoch 25, Batch 968, LR 1.260588 Loss 4.660921, Accuracy 89.988%\n",
      "Epoch 25, Batch 969, LR 1.260454 Loss 4.660676, Accuracy 89.989%\n",
      "Epoch 25, Batch 970, LR 1.260321 Loss 4.660365, Accuracy 89.987%\n",
      "Epoch 25, Batch 971, LR 1.260187 Loss 4.659175, Accuracy 89.991%\n",
      "Epoch 25, Batch 972, LR 1.260053 Loss 4.658569, Accuracy 89.993%\n",
      "Epoch 25, Batch 973, LR 1.259919 Loss 4.658674, Accuracy 89.990%\n",
      "Epoch 25, Batch 974, LR 1.259785 Loss 4.658772, Accuracy 89.992%\n",
      "Epoch 25, Batch 975, LR 1.259651 Loss 4.658131, Accuracy 89.992%\n",
      "Epoch 25, Batch 976, LR 1.259517 Loss 4.658010, Accuracy 89.992%\n",
      "Epoch 25, Batch 977, LR 1.259383 Loss 4.657847, Accuracy 89.992%\n",
      "Epoch 25, Batch 978, LR 1.259249 Loss 4.657436, Accuracy 89.990%\n",
      "Epoch 25, Batch 979, LR 1.259115 Loss 4.657099, Accuracy 89.994%\n",
      "Epoch 25, Batch 980, LR 1.258981 Loss 4.657196, Accuracy 89.995%\n",
      "Epoch 25, Batch 981, LR 1.258847 Loss 4.657195, Accuracy 89.995%\n",
      "Epoch 25, Batch 982, LR 1.258713 Loss 4.657719, Accuracy 89.992%\n",
      "Epoch 25, Batch 983, LR 1.258579 Loss 4.657634, Accuracy 89.992%\n",
      "Epoch 25, Batch 984, LR 1.258445 Loss 4.657773, Accuracy 89.991%\n",
      "Epoch 25, Batch 985, LR 1.258311 Loss 4.658102, Accuracy 89.988%\n",
      "Epoch 25, Batch 986, LR 1.258177 Loss 4.657897, Accuracy 89.989%\n",
      "Epoch 25, Batch 987, LR 1.258043 Loss 4.658028, Accuracy 89.985%\n",
      "Epoch 25, Batch 988, LR 1.257909 Loss 4.658070, Accuracy 89.985%\n",
      "Epoch 25, Batch 989, LR 1.257775 Loss 4.657886, Accuracy 89.988%\n",
      "Epoch 25, Batch 990, LR 1.257642 Loss 4.656987, Accuracy 89.993%\n",
      "Epoch 25, Batch 991, LR 1.257508 Loss 4.656465, Accuracy 89.994%\n",
      "Epoch 25, Batch 992, LR 1.257374 Loss 4.655544, Accuracy 89.997%\n",
      "Epoch 25, Batch 993, LR 1.257240 Loss 4.655859, Accuracy 89.994%\n",
      "Epoch 25, Batch 994, LR 1.257106 Loss 4.655581, Accuracy 89.995%\n",
      "Epoch 25, Batch 995, LR 1.256972 Loss 4.656122, Accuracy 89.992%\n",
      "Epoch 25, Batch 996, LR 1.256838 Loss 4.655971, Accuracy 89.994%\n",
      "Epoch 25, Batch 997, LR 1.256704 Loss 4.655683, Accuracy 89.996%\n",
      "Epoch 25, Batch 998, LR 1.256570 Loss 4.656007, Accuracy 89.993%\n",
      "Epoch 25, Batch 999, LR 1.256436 Loss 4.655290, Accuracy 89.995%\n",
      "Epoch 25, Batch 1000, LR 1.256302 Loss 4.654682, Accuracy 89.997%\n",
      "Epoch 25, Batch 1001, LR 1.256168 Loss 4.654685, Accuracy 89.999%\n",
      "Epoch 25, Batch 1002, LR 1.256034 Loss 4.654473, Accuracy 89.996%\n",
      "Epoch 25, Batch 1003, LR 1.255900 Loss 4.653987, Accuracy 90.000%\n",
      "Epoch 25, Batch 1004, LR 1.255766 Loss 4.653447, Accuracy 90.002%\n",
      "Epoch 25, Batch 1005, LR 1.255632 Loss 4.653281, Accuracy 90.003%\n",
      "Epoch 25, Batch 1006, LR 1.255498 Loss 4.653225, Accuracy 90.006%\n",
      "Epoch 25, Batch 1007, LR 1.255364 Loss 4.653141, Accuracy 90.007%\n",
      "Epoch 25, Batch 1008, LR 1.255230 Loss 4.652586, Accuracy 90.011%\n",
      "Epoch 25, Batch 1009, LR 1.255096 Loss 4.653149, Accuracy 90.008%\n",
      "Epoch 25, Batch 1010, LR 1.254963 Loss 4.652571, Accuracy 90.009%\n",
      "Epoch 25, Batch 1011, LR 1.254829 Loss 4.652141, Accuracy 90.011%\n",
      "Epoch 25, Batch 1012, LR 1.254695 Loss 4.651374, Accuracy 90.013%\n",
      "Epoch 25, Batch 1013, LR 1.254561 Loss 4.651221, Accuracy 90.012%\n",
      "Epoch 25, Batch 1014, LR 1.254427 Loss 4.651362, Accuracy 90.011%\n",
      "Epoch 25, Batch 1015, LR 1.254293 Loss 4.651549, Accuracy 90.009%\n",
      "Epoch 25, Batch 1016, LR 1.254159 Loss 4.651547, Accuracy 90.011%\n",
      "Epoch 25, Batch 1017, LR 1.254025 Loss 4.651439, Accuracy 90.010%\n",
      "Epoch 25, Batch 1018, LR 1.253891 Loss 4.651374, Accuracy 90.011%\n",
      "Epoch 25, Batch 1019, LR 1.253757 Loss 4.651271, Accuracy 90.012%\n",
      "Epoch 25, Batch 1020, LR 1.253623 Loss 4.650954, Accuracy 90.011%\n",
      "Epoch 25, Batch 1021, LR 1.253489 Loss 4.651412, Accuracy 90.007%\n",
      "Epoch 25, Batch 1022, LR 1.253355 Loss 4.650949, Accuracy 90.009%\n",
      "Epoch 25, Batch 1023, LR 1.253221 Loss 4.651213, Accuracy 90.005%\n",
      "Epoch 25, Batch 1024, LR 1.253087 Loss 4.651455, Accuracy 90.005%\n",
      "Epoch 25, Batch 1025, LR 1.252953 Loss 4.651610, Accuracy 90.008%\n",
      "Epoch 25, Batch 1026, LR 1.252819 Loss 4.651867, Accuracy 90.004%\n",
      "Epoch 25, Batch 1027, LR 1.252685 Loss 4.651733, Accuracy 90.002%\n",
      "Epoch 25, Batch 1028, LR 1.252551 Loss 4.651869, Accuracy 89.997%\n",
      "Epoch 25, Batch 1029, LR 1.252417 Loss 4.651531, Accuracy 89.998%\n",
      "Epoch 25, Batch 1030, LR 1.252283 Loss 4.651200, Accuracy 89.998%\n",
      "Epoch 25, Batch 1031, LR 1.252149 Loss 4.650969, Accuracy 89.998%\n",
      "Epoch 25, Batch 1032, LR 1.252016 Loss 4.651081, Accuracy 89.996%\n",
      "Epoch 25, Batch 1033, LR 1.251882 Loss 4.651437, Accuracy 89.996%\n",
      "Epoch 25, Batch 1034, LR 1.251748 Loss 4.651404, Accuracy 89.996%\n",
      "Epoch 25, Batch 1035, LR 1.251614 Loss 4.650688, Accuracy 89.995%\n",
      "Epoch 25, Batch 1036, LR 1.251480 Loss 4.650874, Accuracy 89.995%\n",
      "Epoch 25, Batch 1037, LR 1.251346 Loss 4.650746, Accuracy 89.992%\n",
      "Epoch 25, Batch 1038, LR 1.251212 Loss 4.650145, Accuracy 89.997%\n",
      "Epoch 25, Batch 1039, LR 1.251078 Loss 4.650632, Accuracy 89.996%\n",
      "Epoch 25, Batch 1040, LR 1.250944 Loss 4.650741, Accuracy 89.995%\n",
      "Epoch 25, Batch 1041, LR 1.250810 Loss 4.650502, Accuracy 89.995%\n",
      "Epoch 25, Batch 1042, LR 1.250676 Loss 4.650794, Accuracy 89.993%\n",
      "Epoch 25, Batch 1043, LR 1.250542 Loss 4.650434, Accuracy 89.994%\n",
      "Epoch 25, Batch 1044, LR 1.250408 Loss 4.649894, Accuracy 89.994%\n",
      "Epoch 25, Batch 1045, LR 1.250274 Loss 4.649686, Accuracy 89.998%\n",
      "Epoch 25, Batch 1046, LR 1.250140 Loss 4.649842, Accuracy 89.995%\n",
      "Epoch 25, Batch 1047, LR 1.250006 Loss 4.651147, Accuracy 89.986%\n",
      "Epoch 25, Loss (train set) 4.651147, Accuracy (train set) 89.986%\n",
      "Epoch 26, Batch 1, LR 1.249872 Loss 4.848066, Accuracy 89.844%\n",
      "Epoch 26, Batch 2, LR 1.249738 Loss 4.660905, Accuracy 89.062%\n",
      "Epoch 26, Batch 3, LR 1.249604 Loss 4.716932, Accuracy 89.062%\n",
      "Epoch 26, Batch 4, LR 1.249470 Loss 4.595963, Accuracy 88.867%\n",
      "Epoch 26, Batch 5, LR 1.249336 Loss 4.470620, Accuracy 90.469%\n",
      "Epoch 26, Batch 6, LR 1.249203 Loss 4.491001, Accuracy 89.583%\n",
      "Epoch 26, Batch 7, LR 1.249069 Loss 4.457728, Accuracy 90.179%\n",
      "Epoch 26, Batch 8, LR 1.248935 Loss 4.460532, Accuracy 90.234%\n",
      "Epoch 26, Batch 9, LR 1.248801 Loss 4.452895, Accuracy 90.451%\n",
      "Epoch 26, Batch 10, LR 1.248667 Loss 4.444610, Accuracy 90.547%\n",
      "Epoch 26, Batch 11, LR 1.248533 Loss 4.440169, Accuracy 90.767%\n",
      "Epoch 26, Batch 12, LR 1.248399 Loss 4.433204, Accuracy 90.951%\n",
      "Epoch 26, Batch 13, LR 1.248265 Loss 4.463689, Accuracy 90.565%\n",
      "Epoch 26, Batch 14, LR 1.248131 Loss 4.444873, Accuracy 90.569%\n",
      "Epoch 26, Batch 15, LR 1.247997 Loss 4.446595, Accuracy 90.625%\n",
      "Epoch 26, Batch 16, LR 1.247863 Loss 4.423214, Accuracy 90.771%\n",
      "Epoch 26, Batch 17, LR 1.247729 Loss 4.462514, Accuracy 90.763%\n",
      "Epoch 26, Batch 18, LR 1.247595 Loss 4.445031, Accuracy 90.929%\n",
      "Epoch 26, Batch 19, LR 1.247461 Loss 4.449675, Accuracy 90.789%\n",
      "Epoch 26, Batch 20, LR 1.247327 Loss 4.429260, Accuracy 90.898%\n",
      "Epoch 26, Batch 21, LR 1.247193 Loss 4.423646, Accuracy 90.960%\n",
      "Epoch 26, Batch 22, LR 1.247059 Loss 4.439644, Accuracy 90.909%\n",
      "Epoch 26, Batch 23, LR 1.246925 Loss 4.416704, Accuracy 91.033%\n",
      "Epoch 26, Batch 24, LR 1.246791 Loss 4.413643, Accuracy 91.146%\n",
      "Epoch 26, Batch 25, LR 1.246657 Loss 4.419298, Accuracy 91.188%\n",
      "Epoch 26, Batch 26, LR 1.246523 Loss 4.427305, Accuracy 91.136%\n",
      "Epoch 26, Batch 27, LR 1.246390 Loss 4.456991, Accuracy 91.001%\n",
      "Epoch 26, Batch 28, LR 1.246256 Loss 4.445152, Accuracy 91.044%\n",
      "Epoch 26, Batch 29, LR 1.246122 Loss 4.459707, Accuracy 90.975%\n",
      "Epoch 26, Batch 30, LR 1.245988 Loss 4.476530, Accuracy 90.859%\n",
      "Epoch 26, Batch 31, LR 1.245854 Loss 4.463707, Accuracy 90.953%\n",
      "Epoch 26, Batch 32, LR 1.245720 Loss 4.465682, Accuracy 90.942%\n",
      "Epoch 26, Batch 33, LR 1.245586 Loss 4.484028, Accuracy 90.838%\n",
      "Epoch 26, Batch 34, LR 1.245452 Loss 4.471946, Accuracy 90.878%\n",
      "Epoch 26, Batch 35, LR 1.245318 Loss 4.473416, Accuracy 90.871%\n",
      "Epoch 26, Batch 36, LR 1.245184 Loss 4.485821, Accuracy 90.799%\n",
      "Epoch 26, Batch 37, LR 1.245050 Loss 4.489555, Accuracy 90.773%\n",
      "Epoch 26, Batch 38, LR 1.244916 Loss 4.485951, Accuracy 90.810%\n",
      "Epoch 26, Batch 39, LR 1.244782 Loss 4.488959, Accuracy 90.825%\n",
      "Epoch 26, Batch 40, LR 1.244648 Loss 4.471564, Accuracy 90.918%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 41, LR 1.244514 Loss 4.466941, Accuracy 90.987%\n",
      "Epoch 26, Batch 42, LR 1.244380 Loss 4.462359, Accuracy 90.997%\n",
      "Epoch 26, Batch 43, LR 1.244246 Loss 4.449940, Accuracy 91.043%\n",
      "Epoch 26, Batch 44, LR 1.244112 Loss 4.445572, Accuracy 91.033%\n",
      "Epoch 26, Batch 45, LR 1.243978 Loss 4.465189, Accuracy 90.972%\n",
      "Epoch 26, Batch 46, LR 1.243844 Loss 4.464819, Accuracy 90.931%\n",
      "Epoch 26, Batch 47, LR 1.243710 Loss 4.474665, Accuracy 90.908%\n",
      "Epoch 26, Batch 48, LR 1.243577 Loss 4.482203, Accuracy 90.951%\n",
      "Epoch 26, Batch 49, LR 1.243443 Loss 4.486896, Accuracy 90.880%\n",
      "Epoch 26, Batch 50, LR 1.243309 Loss 4.484198, Accuracy 90.906%\n",
      "Epoch 26, Batch 51, LR 1.243175 Loss 4.481721, Accuracy 90.962%\n",
      "Epoch 26, Batch 52, LR 1.243041 Loss 4.485983, Accuracy 90.956%\n",
      "Epoch 26, Batch 53, LR 1.242907 Loss 4.503344, Accuracy 90.890%\n",
      "Epoch 26, Batch 54, LR 1.242773 Loss 4.511731, Accuracy 90.914%\n",
      "Epoch 26, Batch 55, LR 1.242639 Loss 4.511830, Accuracy 90.895%\n",
      "Epoch 26, Batch 56, LR 1.242505 Loss 4.508684, Accuracy 90.932%\n",
      "Epoch 26, Batch 57, LR 1.242371 Loss 4.508810, Accuracy 90.913%\n",
      "Epoch 26, Batch 58, LR 1.242237 Loss 4.511567, Accuracy 90.881%\n",
      "Epoch 26, Batch 59, LR 1.242103 Loss 4.511039, Accuracy 90.797%\n",
      "Epoch 26, Batch 60, LR 1.241969 Loss 4.510294, Accuracy 90.768%\n",
      "Epoch 26, Batch 61, LR 1.241835 Loss 4.513887, Accuracy 90.779%\n",
      "Epoch 26, Batch 62, LR 1.241701 Loss 4.509038, Accuracy 90.789%\n",
      "Epoch 26, Batch 63, LR 1.241567 Loss 4.508842, Accuracy 90.786%\n",
      "Epoch 26, Batch 64, LR 1.241433 Loss 4.508229, Accuracy 90.808%\n",
      "Epoch 26, Batch 65, LR 1.241299 Loss 4.513059, Accuracy 90.769%\n",
      "Epoch 26, Batch 66, LR 1.241165 Loss 4.509461, Accuracy 90.767%\n",
      "Epoch 26, Batch 67, LR 1.241031 Loss 4.518038, Accuracy 90.730%\n",
      "Epoch 26, Batch 68, LR 1.240898 Loss 4.516800, Accuracy 90.751%\n",
      "Epoch 26, Batch 69, LR 1.240764 Loss 4.523200, Accuracy 90.727%\n",
      "Epoch 26, Batch 70, LR 1.240630 Loss 4.530381, Accuracy 90.714%\n",
      "Epoch 26, Batch 71, LR 1.240496 Loss 4.525137, Accuracy 90.757%\n",
      "Epoch 26, Batch 72, LR 1.240362 Loss 4.525146, Accuracy 90.777%\n",
      "Epoch 26, Batch 73, LR 1.240228 Loss 4.535969, Accuracy 90.721%\n",
      "Epoch 26, Batch 74, LR 1.240094 Loss 4.541578, Accuracy 90.731%\n",
      "Epoch 26, Batch 75, LR 1.239960 Loss 4.549411, Accuracy 90.667%\n",
      "Epoch 26, Batch 76, LR 1.239826 Loss 4.551181, Accuracy 90.635%\n",
      "Epoch 26, Batch 77, LR 1.239692 Loss 4.550348, Accuracy 90.605%\n",
      "Epoch 26, Batch 78, LR 1.239558 Loss 4.539702, Accuracy 90.655%\n",
      "Epoch 26, Batch 79, LR 1.239424 Loss 4.534433, Accuracy 90.635%\n",
      "Epoch 26, Batch 80, LR 1.239290 Loss 4.532718, Accuracy 90.674%\n",
      "Epoch 26, Batch 81, LR 1.239156 Loss 4.524249, Accuracy 90.702%\n",
      "Epoch 26, Batch 82, LR 1.239022 Loss 4.519005, Accuracy 90.739%\n",
      "Epoch 26, Batch 83, LR 1.238888 Loss 4.511779, Accuracy 90.785%\n",
      "Epoch 26, Batch 84, LR 1.238754 Loss 4.506953, Accuracy 90.802%\n",
      "Epoch 26, Batch 85, LR 1.238620 Loss 4.507375, Accuracy 90.800%\n",
      "Epoch 26, Batch 86, LR 1.238486 Loss 4.512966, Accuracy 90.752%\n",
      "Epoch 26, Batch 87, LR 1.238352 Loss 4.510899, Accuracy 90.733%\n",
      "Epoch 26, Batch 88, LR 1.238219 Loss 4.509209, Accuracy 90.749%\n",
      "Epoch 26, Batch 89, LR 1.238085 Loss 4.511841, Accuracy 90.730%\n",
      "Epoch 26, Batch 90, LR 1.237951 Loss 4.509761, Accuracy 90.738%\n",
      "Epoch 26, Batch 91, LR 1.237817 Loss 4.509822, Accuracy 90.719%\n",
      "Epoch 26, Batch 92, LR 1.237683 Loss 4.505046, Accuracy 90.769%\n",
      "Epoch 26, Batch 93, LR 1.237549 Loss 4.504087, Accuracy 90.785%\n",
      "Epoch 26, Batch 94, LR 1.237415 Loss 4.508934, Accuracy 90.816%\n",
      "Epoch 26, Batch 95, LR 1.237281 Loss 4.504378, Accuracy 90.872%\n",
      "Epoch 26, Batch 96, LR 1.237147 Loss 4.496288, Accuracy 90.861%\n",
      "Epoch 26, Batch 97, LR 1.237013 Loss 4.498348, Accuracy 90.810%\n",
      "Epoch 26, Batch 98, LR 1.236879 Loss 4.497612, Accuracy 90.816%\n",
      "Epoch 26, Batch 99, LR 1.236745 Loss 4.493397, Accuracy 90.838%\n",
      "Epoch 26, Batch 100, LR 1.236611 Loss 4.495992, Accuracy 90.844%\n",
      "Epoch 26, Batch 101, LR 1.236477 Loss 4.494790, Accuracy 90.857%\n",
      "Epoch 26, Batch 102, LR 1.236343 Loss 4.501048, Accuracy 90.832%\n",
      "Epoch 26, Batch 103, LR 1.236209 Loss 4.494569, Accuracy 90.875%\n",
      "Epoch 26, Batch 104, LR 1.236075 Loss 4.498552, Accuracy 90.850%\n",
      "Epoch 26, Batch 105, LR 1.235941 Loss 4.494613, Accuracy 90.878%\n",
      "Epoch 26, Batch 106, LR 1.235808 Loss 4.498236, Accuracy 90.809%\n",
      "Epoch 26, Batch 107, LR 1.235674 Loss 4.502559, Accuracy 90.786%\n",
      "Epoch 26, Batch 108, LR 1.235540 Loss 4.504136, Accuracy 90.748%\n",
      "Epoch 26, Batch 109, LR 1.235406 Loss 4.504847, Accuracy 90.747%\n",
      "Epoch 26, Batch 110, LR 1.235272 Loss 4.503046, Accuracy 90.767%\n",
      "Epoch 26, Batch 111, LR 1.235138 Loss 4.508552, Accuracy 90.752%\n",
      "Epoch 26, Batch 112, LR 1.235004 Loss 4.510458, Accuracy 90.737%\n",
      "Epoch 26, Batch 113, LR 1.234870 Loss 4.509301, Accuracy 90.763%\n",
      "Epoch 26, Batch 114, LR 1.234736 Loss 4.513617, Accuracy 90.755%\n",
      "Epoch 26, Batch 115, LR 1.234602 Loss 4.524341, Accuracy 90.754%\n",
      "Epoch 26, Batch 116, LR 1.234468 Loss 4.525620, Accuracy 90.739%\n",
      "Epoch 26, Batch 117, LR 1.234334 Loss 4.525366, Accuracy 90.759%\n",
      "Epoch 26, Batch 118, LR 1.234200 Loss 4.527431, Accuracy 90.738%\n",
      "Epoch 26, Batch 119, LR 1.234066 Loss 4.523419, Accuracy 90.783%\n",
      "Epoch 26, Batch 120, LR 1.233932 Loss 4.529372, Accuracy 90.742%\n",
      "Epoch 26, Batch 121, LR 1.233798 Loss 4.531736, Accuracy 90.690%\n",
      "Epoch 26, Batch 122, LR 1.233664 Loss 4.534909, Accuracy 90.670%\n",
      "Epoch 26, Batch 123, LR 1.233530 Loss 4.534307, Accuracy 90.669%\n",
      "Epoch 26, Batch 124, LR 1.233397 Loss 4.534069, Accuracy 90.669%\n",
      "Epoch 26, Batch 125, LR 1.233263 Loss 4.529875, Accuracy 90.681%\n",
      "Epoch 26, Batch 126, LR 1.233129 Loss 4.530046, Accuracy 90.675%\n",
      "Epoch 26, Batch 127, LR 1.232995 Loss 4.530934, Accuracy 90.668%\n",
      "Epoch 26, Batch 128, LR 1.232861 Loss 4.531563, Accuracy 90.668%\n",
      "Epoch 26, Batch 129, LR 1.232727 Loss 4.534268, Accuracy 90.667%\n",
      "Epoch 26, Batch 130, LR 1.232593 Loss 4.535798, Accuracy 90.649%\n",
      "Epoch 26, Batch 131, LR 1.232459 Loss 4.541469, Accuracy 90.625%\n",
      "Epoch 26, Batch 132, LR 1.232325 Loss 4.538686, Accuracy 90.625%\n",
      "Epoch 26, Batch 133, LR 1.232191 Loss 4.531201, Accuracy 90.654%\n",
      "Epoch 26, Batch 134, LR 1.232057 Loss 4.531313, Accuracy 90.654%\n",
      "Epoch 26, Batch 135, LR 1.231923 Loss 4.530352, Accuracy 90.642%\n",
      "Epoch 26, Batch 136, LR 1.231789 Loss 4.527285, Accuracy 90.642%\n",
      "Epoch 26, Batch 137, LR 1.231655 Loss 4.526446, Accuracy 90.642%\n",
      "Epoch 26, Batch 138, LR 1.231521 Loss 4.524898, Accuracy 90.625%\n",
      "Epoch 26, Batch 139, LR 1.231387 Loss 4.524509, Accuracy 90.631%\n",
      "Epoch 26, Batch 140, LR 1.231254 Loss 4.522478, Accuracy 90.642%\n",
      "Epoch 26, Batch 141, LR 1.231120 Loss 4.523993, Accuracy 90.625%\n",
      "Epoch 26, Batch 142, LR 1.230986 Loss 4.521962, Accuracy 90.636%\n",
      "Epoch 26, Batch 143, LR 1.230852 Loss 4.521444, Accuracy 90.636%\n",
      "Epoch 26, Batch 144, LR 1.230718 Loss 4.524660, Accuracy 90.641%\n",
      "Epoch 26, Batch 145, LR 1.230584 Loss 4.521304, Accuracy 90.657%\n",
      "Epoch 26, Batch 146, LR 1.230450 Loss 4.523830, Accuracy 90.652%\n",
      "Epoch 26, Batch 147, LR 1.230316 Loss 4.522486, Accuracy 90.668%\n",
      "Epoch 26, Batch 148, LR 1.230182 Loss 4.520726, Accuracy 90.673%\n",
      "Epoch 26, Batch 149, LR 1.230048 Loss 4.520570, Accuracy 90.667%\n",
      "Epoch 26, Batch 150, LR 1.229914 Loss 4.518187, Accuracy 90.672%\n",
      "Epoch 26, Batch 151, LR 1.229780 Loss 4.521785, Accuracy 90.635%\n",
      "Epoch 26, Batch 152, LR 1.229646 Loss 4.517969, Accuracy 90.635%\n",
      "Epoch 26, Batch 153, LR 1.229512 Loss 4.514874, Accuracy 90.640%\n",
      "Epoch 26, Batch 154, LR 1.229378 Loss 4.512947, Accuracy 90.640%\n",
      "Epoch 26, Batch 155, LR 1.229244 Loss 4.513461, Accuracy 90.630%\n",
      "Epoch 26, Batch 156, LR 1.229111 Loss 4.510661, Accuracy 90.665%\n",
      "Epoch 26, Batch 157, LR 1.228977 Loss 4.512715, Accuracy 90.665%\n",
      "Epoch 26, Batch 158, LR 1.228843 Loss 4.510949, Accuracy 90.674%\n",
      "Epoch 26, Batch 159, LR 1.228709 Loss 4.510812, Accuracy 90.679%\n",
      "Epoch 26, Batch 160, LR 1.228575 Loss 4.515127, Accuracy 90.649%\n",
      "Epoch 26, Batch 161, LR 1.228441 Loss 4.514238, Accuracy 90.654%\n",
      "Epoch 26, Batch 162, LR 1.228307 Loss 4.515958, Accuracy 90.630%\n",
      "Epoch 26, Batch 163, LR 1.228173 Loss 4.512585, Accuracy 90.663%\n",
      "Epoch 26, Batch 164, LR 1.228039 Loss 4.512406, Accuracy 90.663%\n",
      "Epoch 26, Batch 165, LR 1.227905 Loss 4.510951, Accuracy 90.663%\n",
      "Epoch 26, Batch 166, LR 1.227771 Loss 4.512515, Accuracy 90.653%\n",
      "Epoch 26, Batch 167, LR 1.227637 Loss 4.512020, Accuracy 90.634%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 168, LR 1.227503 Loss 4.512277, Accuracy 90.630%\n",
      "Epoch 26, Batch 169, LR 1.227369 Loss 4.514882, Accuracy 90.607%\n",
      "Epoch 26, Batch 170, LR 1.227235 Loss 4.517392, Accuracy 90.611%\n",
      "Epoch 26, Batch 171, LR 1.227102 Loss 4.514802, Accuracy 90.634%\n",
      "Epoch 26, Batch 172, LR 1.226968 Loss 4.518952, Accuracy 90.639%\n",
      "Epoch 26, Batch 173, LR 1.226834 Loss 4.519145, Accuracy 90.634%\n",
      "Epoch 26, Batch 174, LR 1.226700 Loss 4.519666, Accuracy 90.638%\n",
      "Epoch 26, Batch 175, LR 1.226566 Loss 4.520427, Accuracy 90.652%\n",
      "Epoch 26, Batch 176, LR 1.226432 Loss 4.517502, Accuracy 90.647%\n",
      "Epoch 26, Batch 177, LR 1.226298 Loss 4.520266, Accuracy 90.621%\n",
      "Epoch 26, Batch 178, LR 1.226164 Loss 4.519567, Accuracy 90.607%\n",
      "Epoch 26, Batch 179, LR 1.226030 Loss 4.522762, Accuracy 90.577%\n",
      "Epoch 26, Batch 180, LR 1.225896 Loss 4.522155, Accuracy 90.569%\n",
      "Epoch 26, Batch 181, LR 1.225762 Loss 4.525001, Accuracy 90.552%\n",
      "Epoch 26, Batch 182, LR 1.225628 Loss 4.524110, Accuracy 90.561%\n",
      "Epoch 26, Batch 183, LR 1.225494 Loss 4.522432, Accuracy 90.591%\n",
      "Epoch 26, Batch 184, LR 1.225360 Loss 4.523853, Accuracy 90.574%\n",
      "Epoch 26, Batch 185, LR 1.225227 Loss 4.524406, Accuracy 90.562%\n",
      "Epoch 26, Batch 186, LR 1.225093 Loss 4.521457, Accuracy 90.566%\n",
      "Epoch 26, Batch 187, LR 1.224959 Loss 4.523297, Accuracy 90.562%\n",
      "Epoch 26, Batch 188, LR 1.224825 Loss 4.522413, Accuracy 90.575%\n",
      "Epoch 26, Batch 189, LR 1.224691 Loss 4.519772, Accuracy 90.604%\n",
      "Epoch 26, Batch 190, LR 1.224557 Loss 4.517466, Accuracy 90.609%\n",
      "Epoch 26, Batch 191, LR 1.224423 Loss 4.521239, Accuracy 90.576%\n",
      "Epoch 26, Batch 192, LR 1.224289 Loss 4.523112, Accuracy 90.588%\n",
      "Epoch 26, Batch 193, LR 1.224155 Loss 4.525545, Accuracy 90.568%\n",
      "Epoch 26, Batch 194, LR 1.224021 Loss 4.528761, Accuracy 90.569%\n",
      "Epoch 26, Batch 195, LR 1.223887 Loss 4.529454, Accuracy 90.561%\n",
      "Epoch 26, Batch 196, LR 1.223753 Loss 4.527632, Accuracy 90.573%\n",
      "Epoch 26, Batch 197, LR 1.223619 Loss 4.526433, Accuracy 90.581%\n",
      "Epoch 26, Batch 198, LR 1.223486 Loss 4.524819, Accuracy 90.586%\n",
      "Epoch 26, Batch 199, LR 1.223352 Loss 4.526816, Accuracy 90.582%\n",
      "Epoch 26, Batch 200, LR 1.223218 Loss 4.524260, Accuracy 90.586%\n",
      "Epoch 26, Batch 201, LR 1.223084 Loss 4.526427, Accuracy 90.567%\n",
      "Epoch 26, Batch 202, LR 1.222950 Loss 4.524975, Accuracy 90.575%\n",
      "Epoch 26, Batch 203, LR 1.222816 Loss 4.523060, Accuracy 90.579%\n",
      "Epoch 26, Batch 204, LR 1.222682 Loss 4.521243, Accuracy 90.587%\n",
      "Epoch 26, Batch 205, LR 1.222548 Loss 4.521179, Accuracy 90.575%\n",
      "Epoch 26, Batch 206, LR 1.222414 Loss 4.524991, Accuracy 90.561%\n",
      "Epoch 26, Batch 207, LR 1.222280 Loss 4.523380, Accuracy 90.583%\n",
      "Epoch 26, Batch 208, LR 1.222146 Loss 4.519022, Accuracy 90.599%\n",
      "Epoch 26, Batch 209, LR 1.222012 Loss 4.518401, Accuracy 90.606%\n",
      "Epoch 26, Batch 210, LR 1.221878 Loss 4.519638, Accuracy 90.599%\n",
      "Epoch 26, Batch 211, LR 1.221745 Loss 4.522453, Accuracy 90.573%\n",
      "Epoch 26, Batch 212, LR 1.221611 Loss 4.524326, Accuracy 90.562%\n",
      "Epoch 26, Batch 213, LR 1.221477 Loss 4.525216, Accuracy 90.574%\n",
      "Epoch 26, Batch 214, LR 1.221343 Loss 4.524309, Accuracy 90.585%\n",
      "Epoch 26, Batch 215, LR 1.221209 Loss 4.525339, Accuracy 90.567%\n",
      "Epoch 26, Batch 216, LR 1.221075 Loss 4.528146, Accuracy 90.574%\n",
      "Epoch 26, Batch 217, LR 1.220941 Loss 4.524522, Accuracy 90.600%\n",
      "Epoch 26, Batch 218, LR 1.220807 Loss 4.525304, Accuracy 90.589%\n",
      "Epoch 26, Batch 219, LR 1.220673 Loss 4.524584, Accuracy 90.593%\n",
      "Epoch 26, Batch 220, LR 1.220539 Loss 4.523728, Accuracy 90.600%\n",
      "Epoch 26, Batch 221, LR 1.220405 Loss 4.522538, Accuracy 90.600%\n",
      "Epoch 26, Batch 222, LR 1.220271 Loss 4.523248, Accuracy 90.590%\n",
      "Epoch 26, Batch 223, LR 1.220138 Loss 4.523126, Accuracy 90.586%\n",
      "Epoch 26, Batch 224, LR 1.220004 Loss 4.522189, Accuracy 90.597%\n",
      "Epoch 26, Batch 225, LR 1.219870 Loss 4.522323, Accuracy 90.594%\n",
      "Epoch 26, Batch 226, LR 1.219736 Loss 4.522767, Accuracy 90.584%\n",
      "Epoch 26, Batch 227, LR 1.219602 Loss 4.522003, Accuracy 90.584%\n",
      "Epoch 26, Batch 228, LR 1.219468 Loss 4.521844, Accuracy 90.584%\n",
      "Epoch 26, Batch 229, LR 1.219334 Loss 4.522261, Accuracy 90.584%\n",
      "Epoch 26, Batch 230, LR 1.219200 Loss 4.525322, Accuracy 90.574%\n",
      "Epoch 26, Batch 231, LR 1.219066 Loss 4.525169, Accuracy 90.571%\n",
      "Epoch 26, Batch 232, LR 1.218932 Loss 4.523212, Accuracy 90.591%\n",
      "Epoch 26, Batch 233, LR 1.218798 Loss 4.520486, Accuracy 90.598%\n",
      "Epoch 26, Batch 234, LR 1.218664 Loss 4.519835, Accuracy 90.602%\n",
      "Epoch 26, Batch 235, LR 1.218531 Loss 4.518262, Accuracy 90.612%\n",
      "Epoch 26, Batch 236, LR 1.218397 Loss 4.516523, Accuracy 90.615%\n",
      "Epoch 26, Batch 237, LR 1.218263 Loss 4.520168, Accuracy 90.612%\n",
      "Epoch 26, Batch 238, LR 1.218129 Loss 4.521729, Accuracy 90.592%\n",
      "Epoch 26, Batch 239, LR 1.217995 Loss 4.520356, Accuracy 90.592%\n",
      "Epoch 26, Batch 240, LR 1.217861 Loss 4.522852, Accuracy 90.566%\n",
      "Epoch 26, Batch 241, LR 1.217727 Loss 4.524742, Accuracy 90.547%\n",
      "Epoch 26, Batch 242, LR 1.217593 Loss 4.523923, Accuracy 90.554%\n",
      "Epoch 26, Batch 243, LR 1.217459 Loss 4.525974, Accuracy 90.554%\n",
      "Epoch 26, Batch 244, LR 1.217325 Loss 4.525539, Accuracy 90.555%\n",
      "Epoch 26, Batch 245, LR 1.217191 Loss 4.523279, Accuracy 90.568%\n",
      "Epoch 26, Batch 246, LR 1.217058 Loss 4.525937, Accuracy 90.539%\n",
      "Epoch 26, Batch 247, LR 1.216924 Loss 4.527510, Accuracy 90.533%\n",
      "Epoch 26, Batch 248, LR 1.216790 Loss 4.528604, Accuracy 90.521%\n",
      "Epoch 26, Batch 249, LR 1.216656 Loss 4.526106, Accuracy 90.534%\n",
      "Epoch 26, Batch 250, LR 1.216522 Loss 4.526054, Accuracy 90.531%\n",
      "Epoch 26, Batch 251, LR 1.216388 Loss 4.529658, Accuracy 90.510%\n",
      "Epoch 26, Batch 252, LR 1.216254 Loss 4.530064, Accuracy 90.520%\n",
      "Epoch 26, Batch 253, LR 1.216120 Loss 4.528161, Accuracy 90.529%\n",
      "Epoch 26, Batch 254, LR 1.215986 Loss 4.525381, Accuracy 90.539%\n",
      "Epoch 26, Batch 255, LR 1.215852 Loss 4.525419, Accuracy 90.527%\n",
      "Epoch 26, Batch 256, LR 1.215719 Loss 4.523997, Accuracy 90.536%\n",
      "Epoch 26, Batch 257, LR 1.215585 Loss 4.524811, Accuracy 90.540%\n",
      "Epoch 26, Batch 258, LR 1.215451 Loss 4.526078, Accuracy 90.540%\n",
      "Epoch 26, Batch 259, LR 1.215317 Loss 4.524491, Accuracy 90.547%\n",
      "Epoch 26, Batch 260, LR 1.215183 Loss 4.527737, Accuracy 90.523%\n",
      "Epoch 26, Batch 261, LR 1.215049 Loss 4.526577, Accuracy 90.535%\n",
      "Epoch 26, Batch 262, LR 1.214915 Loss 4.525981, Accuracy 90.553%\n",
      "Epoch 26, Batch 263, LR 1.214781 Loss 4.525868, Accuracy 90.554%\n",
      "Epoch 26, Batch 264, LR 1.214647 Loss 4.525096, Accuracy 90.560%\n",
      "Epoch 26, Batch 265, LR 1.214513 Loss 4.524449, Accuracy 90.563%\n",
      "Epoch 26, Batch 266, LR 1.214380 Loss 4.524895, Accuracy 90.557%\n",
      "Epoch 26, Batch 267, LR 1.214246 Loss 4.527996, Accuracy 90.531%\n",
      "Epoch 26, Batch 268, LR 1.214112 Loss 4.530346, Accuracy 90.520%\n",
      "Epoch 26, Batch 269, LR 1.213978 Loss 4.533060, Accuracy 90.512%\n",
      "Epoch 26, Batch 270, LR 1.213844 Loss 4.535609, Accuracy 90.486%\n",
      "Epoch 26, Batch 271, LR 1.213710 Loss 4.535619, Accuracy 90.484%\n",
      "Epoch 26, Batch 272, LR 1.213576 Loss 4.536164, Accuracy 90.479%\n",
      "Epoch 26, Batch 273, LR 1.213442 Loss 4.539389, Accuracy 90.473%\n",
      "Epoch 26, Batch 274, LR 1.213308 Loss 4.538999, Accuracy 90.474%\n",
      "Epoch 26, Batch 275, LR 1.213174 Loss 4.536935, Accuracy 90.483%\n",
      "Epoch 26, Batch 276, LR 1.213041 Loss 4.538367, Accuracy 90.469%\n",
      "Epoch 26, Batch 277, LR 1.212907 Loss 4.538299, Accuracy 90.478%\n",
      "Epoch 26, Batch 278, LR 1.212773 Loss 4.539509, Accuracy 90.473%\n",
      "Epoch 26, Batch 279, LR 1.212639 Loss 4.539140, Accuracy 90.479%\n",
      "Epoch 26, Batch 280, LR 1.212505 Loss 4.537809, Accuracy 90.491%\n",
      "Epoch 26, Batch 281, LR 1.212371 Loss 4.538153, Accuracy 90.478%\n",
      "Epoch 26, Batch 282, LR 1.212237 Loss 4.536375, Accuracy 90.489%\n",
      "Epoch 26, Batch 283, LR 1.212103 Loss 4.535978, Accuracy 90.498%\n",
      "Epoch 26, Batch 284, LR 1.211969 Loss 4.534079, Accuracy 90.507%\n",
      "Epoch 26, Batch 285, LR 1.211836 Loss 4.534813, Accuracy 90.493%\n",
      "Epoch 26, Batch 286, LR 1.211702 Loss 4.533988, Accuracy 90.502%\n",
      "Epoch 26, Batch 287, LR 1.211568 Loss 4.535626, Accuracy 90.505%\n",
      "Epoch 26, Batch 288, LR 1.211434 Loss 4.535244, Accuracy 90.506%\n",
      "Epoch 26, Batch 289, LR 1.211300 Loss 4.534055, Accuracy 90.511%\n",
      "Epoch 26, Batch 290, LR 1.211166 Loss 4.535727, Accuracy 90.504%\n",
      "Epoch 26, Batch 291, LR 1.211032 Loss 4.533059, Accuracy 90.507%\n",
      "Epoch 26, Batch 292, LR 1.210898 Loss 4.535471, Accuracy 90.505%\n",
      "Epoch 26, Batch 293, LR 1.210764 Loss 4.533236, Accuracy 90.505%\n",
      "Epoch 26, Batch 294, LR 1.210631 Loss 4.533746, Accuracy 90.513%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 295, LR 1.210497 Loss 4.535891, Accuracy 90.495%\n",
      "Epoch 26, Batch 296, LR 1.210363 Loss 4.536816, Accuracy 90.485%\n",
      "Epoch 26, Batch 297, LR 1.210229 Loss 4.539919, Accuracy 90.472%\n",
      "Epoch 26, Batch 298, LR 1.210095 Loss 4.541547, Accuracy 90.462%\n",
      "Epoch 26, Batch 299, LR 1.209961 Loss 4.539418, Accuracy 90.468%\n",
      "Epoch 26, Batch 300, LR 1.209827 Loss 4.538975, Accuracy 90.474%\n",
      "Epoch 26, Batch 301, LR 1.209693 Loss 4.538409, Accuracy 90.480%\n",
      "Epoch 26, Batch 302, LR 1.209559 Loss 4.540175, Accuracy 90.470%\n",
      "Epoch 26, Batch 303, LR 1.209426 Loss 4.540093, Accuracy 90.473%\n",
      "Epoch 26, Batch 304, LR 1.209292 Loss 4.538836, Accuracy 90.479%\n",
      "Epoch 26, Batch 305, LR 1.209158 Loss 4.538363, Accuracy 90.482%\n",
      "Epoch 26, Batch 306, LR 1.209024 Loss 4.540315, Accuracy 90.479%\n",
      "Epoch 26, Batch 307, LR 1.208890 Loss 4.540516, Accuracy 90.482%\n",
      "Epoch 26, Batch 308, LR 1.208756 Loss 4.539307, Accuracy 90.491%\n",
      "Epoch 26, Batch 309, LR 1.208622 Loss 4.538696, Accuracy 90.481%\n",
      "Epoch 26, Batch 310, LR 1.208488 Loss 4.537951, Accuracy 90.481%\n",
      "Epoch 26, Batch 311, LR 1.208355 Loss 4.537219, Accuracy 90.479%\n",
      "Epoch 26, Batch 312, LR 1.208221 Loss 4.535458, Accuracy 90.487%\n",
      "Epoch 26, Batch 313, LR 1.208087 Loss 4.536850, Accuracy 90.475%\n",
      "Epoch 26, Batch 314, LR 1.207953 Loss 4.537577, Accuracy 90.466%\n",
      "Epoch 26, Batch 315, LR 1.207819 Loss 4.536624, Accuracy 90.471%\n",
      "Epoch 26, Batch 316, LR 1.207685 Loss 4.536375, Accuracy 90.477%\n",
      "Epoch 26, Batch 317, LR 1.207551 Loss 4.536553, Accuracy 90.467%\n",
      "Epoch 26, Batch 318, LR 1.207417 Loss 4.537605, Accuracy 90.463%\n",
      "Epoch 26, Batch 319, LR 1.207284 Loss 4.539549, Accuracy 90.461%\n",
      "Epoch 26, Batch 320, LR 1.207150 Loss 4.539152, Accuracy 90.464%\n",
      "Epoch 26, Batch 321, LR 1.207016 Loss 4.537226, Accuracy 90.467%\n",
      "Epoch 26, Batch 322, LR 1.206882 Loss 4.535502, Accuracy 90.472%\n",
      "Epoch 26, Batch 323, LR 1.206748 Loss 4.533066, Accuracy 90.485%\n",
      "Epoch 26, Batch 324, LR 1.206614 Loss 4.531476, Accuracy 90.495%\n",
      "Epoch 26, Batch 325, LR 1.206480 Loss 4.534294, Accuracy 90.481%\n",
      "Epoch 26, Batch 326, LR 1.206346 Loss 4.533194, Accuracy 90.486%\n",
      "Epoch 26, Batch 327, LR 1.206213 Loss 4.534595, Accuracy 90.472%\n",
      "Epoch 26, Batch 328, LR 1.206079 Loss 4.535457, Accuracy 90.477%\n",
      "Epoch 26, Batch 329, LR 1.205945 Loss 4.534273, Accuracy 90.483%\n",
      "Epoch 26, Batch 330, LR 1.205811 Loss 4.532922, Accuracy 90.495%\n",
      "Epoch 26, Batch 331, LR 1.205677 Loss 4.534852, Accuracy 90.488%\n",
      "Epoch 26, Batch 332, LR 1.205543 Loss 4.533434, Accuracy 90.500%\n",
      "Epoch 26, Batch 333, LR 1.205409 Loss 4.534770, Accuracy 90.498%\n",
      "Epoch 26, Batch 334, LR 1.205275 Loss 4.533392, Accuracy 90.501%\n",
      "Epoch 26, Batch 335, LR 1.205142 Loss 4.532709, Accuracy 90.511%\n",
      "Epoch 26, Batch 336, LR 1.205008 Loss 4.533021, Accuracy 90.506%\n",
      "Epoch 26, Batch 337, LR 1.204874 Loss 4.531453, Accuracy 90.516%\n",
      "Epoch 26, Batch 338, LR 1.204740 Loss 4.531500, Accuracy 90.519%\n",
      "Epoch 26, Batch 339, LR 1.204606 Loss 4.530361, Accuracy 90.526%\n",
      "Epoch 26, Batch 340, LR 1.204472 Loss 4.530291, Accuracy 90.528%\n",
      "Epoch 26, Batch 341, LR 1.204338 Loss 4.529324, Accuracy 90.543%\n",
      "Epoch 26, Batch 342, LR 1.204205 Loss 4.531524, Accuracy 90.536%\n",
      "Epoch 26, Batch 343, LR 1.204071 Loss 4.533411, Accuracy 90.527%\n",
      "Epoch 26, Batch 344, LR 1.203937 Loss 4.532841, Accuracy 90.530%\n",
      "Epoch 26, Batch 345, LR 1.203803 Loss 4.533331, Accuracy 90.530%\n",
      "Epoch 26, Batch 346, LR 1.203669 Loss 4.532070, Accuracy 90.530%\n",
      "Epoch 26, Batch 347, LR 1.203535 Loss 4.531482, Accuracy 90.528%\n",
      "Epoch 26, Batch 348, LR 1.203401 Loss 4.530987, Accuracy 90.533%\n",
      "Epoch 26, Batch 349, LR 1.203267 Loss 4.530834, Accuracy 90.533%\n",
      "Epoch 26, Batch 350, LR 1.203134 Loss 4.531091, Accuracy 90.531%\n",
      "Epoch 26, Batch 351, LR 1.203000 Loss 4.529521, Accuracy 90.547%\n",
      "Epoch 26, Batch 352, LR 1.202866 Loss 4.527792, Accuracy 90.554%\n",
      "Epoch 26, Batch 353, LR 1.202732 Loss 4.525968, Accuracy 90.561%\n",
      "Epoch 26, Batch 354, LR 1.202598 Loss 4.526356, Accuracy 90.563%\n",
      "Epoch 26, Batch 355, LR 1.202464 Loss 4.526531, Accuracy 90.561%\n",
      "Epoch 26, Batch 356, LR 1.202330 Loss 4.528389, Accuracy 90.544%\n",
      "Epoch 26, Batch 357, LR 1.202197 Loss 4.531215, Accuracy 90.520%\n",
      "Epoch 26, Batch 358, LR 1.202063 Loss 4.531040, Accuracy 90.522%\n",
      "Epoch 26, Batch 359, LR 1.201929 Loss 4.530719, Accuracy 90.529%\n",
      "Epoch 26, Batch 360, LR 1.201795 Loss 4.531061, Accuracy 90.519%\n",
      "Epoch 26, Batch 361, LR 1.201661 Loss 4.531231, Accuracy 90.525%\n",
      "Epoch 26, Batch 362, LR 1.201527 Loss 4.531302, Accuracy 90.530%\n",
      "Epoch 26, Batch 363, LR 1.201394 Loss 4.530336, Accuracy 90.532%\n",
      "Epoch 26, Batch 364, LR 1.201260 Loss 4.529467, Accuracy 90.535%\n",
      "Epoch 26, Batch 365, LR 1.201126 Loss 4.528064, Accuracy 90.542%\n",
      "Epoch 26, Batch 366, LR 1.200992 Loss 4.528847, Accuracy 90.542%\n",
      "Epoch 26, Batch 367, LR 1.200858 Loss 4.530187, Accuracy 90.540%\n",
      "Epoch 26, Batch 368, LR 1.200724 Loss 4.529481, Accuracy 90.542%\n",
      "Epoch 26, Batch 369, LR 1.200590 Loss 4.530920, Accuracy 90.532%\n",
      "Epoch 26, Batch 370, LR 1.200457 Loss 4.529193, Accuracy 90.538%\n",
      "Epoch 26, Batch 371, LR 1.200323 Loss 4.527329, Accuracy 90.545%\n",
      "Epoch 26, Batch 372, LR 1.200189 Loss 4.527512, Accuracy 90.543%\n",
      "Epoch 26, Batch 373, LR 1.200055 Loss 4.529176, Accuracy 90.535%\n",
      "Epoch 26, Batch 374, LR 1.199921 Loss 4.528452, Accuracy 90.537%\n",
      "Epoch 26, Batch 375, LR 1.199787 Loss 4.530547, Accuracy 90.519%\n",
      "Epoch 26, Batch 376, LR 1.199653 Loss 4.530260, Accuracy 90.519%\n",
      "Epoch 26, Batch 377, LR 1.199520 Loss 4.529344, Accuracy 90.521%\n",
      "Epoch 26, Batch 378, LR 1.199386 Loss 4.531623, Accuracy 90.505%\n",
      "Epoch 26, Batch 379, LR 1.199252 Loss 4.533233, Accuracy 90.497%\n",
      "Epoch 26, Batch 380, LR 1.199118 Loss 4.532810, Accuracy 90.498%\n",
      "Epoch 26, Batch 381, LR 1.198984 Loss 4.534402, Accuracy 90.488%\n",
      "Epoch 26, Batch 382, LR 1.198850 Loss 4.533406, Accuracy 90.498%\n",
      "Epoch 26, Batch 383, LR 1.198717 Loss 4.532135, Accuracy 90.499%\n",
      "Epoch 26, Batch 384, LR 1.198583 Loss 4.533397, Accuracy 90.493%\n",
      "Epoch 26, Batch 385, LR 1.198449 Loss 4.534694, Accuracy 90.487%\n",
      "Epoch 26, Batch 386, LR 1.198315 Loss 4.533703, Accuracy 90.497%\n",
      "Epoch 26, Batch 387, LR 1.198181 Loss 4.533197, Accuracy 90.502%\n",
      "Epoch 26, Batch 388, LR 1.198047 Loss 4.535106, Accuracy 90.498%\n",
      "Epoch 26, Batch 389, LR 1.197914 Loss 4.535365, Accuracy 90.496%\n",
      "Epoch 26, Batch 390, LR 1.197780 Loss 4.537722, Accuracy 90.493%\n",
      "Epoch 26, Batch 391, LR 1.197646 Loss 4.537095, Accuracy 90.485%\n",
      "Epoch 26, Batch 392, LR 1.197512 Loss 4.537421, Accuracy 90.476%\n",
      "Epoch 26, Batch 393, LR 1.197378 Loss 4.536970, Accuracy 90.480%\n",
      "Epoch 26, Batch 394, LR 1.197244 Loss 4.536348, Accuracy 90.482%\n",
      "Epoch 26, Batch 395, LR 1.197111 Loss 4.537711, Accuracy 90.479%\n",
      "Epoch 26, Batch 396, LR 1.196977 Loss 4.538639, Accuracy 90.469%\n",
      "Epoch 26, Batch 397, LR 1.196843 Loss 4.539188, Accuracy 90.473%\n",
      "Epoch 26, Batch 398, LR 1.196709 Loss 4.539175, Accuracy 90.476%\n",
      "Epoch 26, Batch 399, LR 1.196575 Loss 4.540196, Accuracy 90.476%\n",
      "Epoch 26, Batch 400, LR 1.196441 Loss 4.542171, Accuracy 90.471%\n",
      "Epoch 26, Batch 401, LR 1.196308 Loss 4.540932, Accuracy 90.485%\n",
      "Epoch 26, Batch 402, LR 1.196174 Loss 4.542797, Accuracy 90.473%\n",
      "Epoch 26, Batch 403, LR 1.196040 Loss 4.542017, Accuracy 90.480%\n",
      "Epoch 26, Batch 404, LR 1.195906 Loss 4.543511, Accuracy 90.472%\n",
      "Epoch 26, Batch 405, LR 1.195772 Loss 4.542638, Accuracy 90.484%\n",
      "Epoch 26, Batch 406, LR 1.195638 Loss 4.542261, Accuracy 90.479%\n",
      "Epoch 26, Batch 407, LR 1.195505 Loss 4.542823, Accuracy 90.479%\n",
      "Epoch 26, Batch 408, LR 1.195371 Loss 4.544117, Accuracy 90.478%\n",
      "Epoch 26, Batch 409, LR 1.195237 Loss 4.543218, Accuracy 90.474%\n",
      "Epoch 26, Batch 410, LR 1.195103 Loss 4.543254, Accuracy 90.482%\n",
      "Epoch 26, Batch 411, LR 1.194969 Loss 4.543217, Accuracy 90.486%\n",
      "Epoch 26, Batch 412, LR 1.194835 Loss 4.542881, Accuracy 90.488%\n",
      "Epoch 26, Batch 413, LR 1.194702 Loss 4.543110, Accuracy 90.481%\n",
      "Epoch 26, Batch 414, LR 1.194568 Loss 4.544248, Accuracy 90.472%\n",
      "Epoch 26, Batch 415, LR 1.194434 Loss 4.543454, Accuracy 90.469%\n",
      "Epoch 26, Batch 416, LR 1.194300 Loss 4.545001, Accuracy 90.471%\n",
      "Epoch 26, Batch 417, LR 1.194166 Loss 4.544466, Accuracy 90.477%\n",
      "Epoch 26, Batch 418, LR 1.194033 Loss 4.544718, Accuracy 90.466%\n",
      "Epoch 26, Batch 419, LR 1.193899 Loss 4.543990, Accuracy 90.467%\n",
      "Epoch 26, Batch 420, LR 1.193765 Loss 4.543314, Accuracy 90.469%\n",
      "Epoch 26, Batch 421, LR 1.193631 Loss 4.542860, Accuracy 90.473%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 422, LR 1.193497 Loss 4.543468, Accuracy 90.471%\n",
      "Epoch 26, Batch 423, LR 1.193363 Loss 4.543242, Accuracy 90.477%\n",
      "Epoch 26, Batch 424, LR 1.193230 Loss 4.542367, Accuracy 90.478%\n",
      "Epoch 26, Batch 425, LR 1.193096 Loss 4.541469, Accuracy 90.478%\n",
      "Epoch 26, Batch 426, LR 1.192962 Loss 4.540164, Accuracy 90.475%\n",
      "Epoch 26, Batch 427, LR 1.192828 Loss 4.540089, Accuracy 90.469%\n",
      "Epoch 26, Batch 428, LR 1.192694 Loss 4.540391, Accuracy 90.466%\n",
      "Epoch 26, Batch 429, LR 1.192561 Loss 4.539224, Accuracy 90.472%\n",
      "Epoch 26, Batch 430, LR 1.192427 Loss 4.538278, Accuracy 90.472%\n",
      "Epoch 26, Batch 431, LR 1.192293 Loss 4.537245, Accuracy 90.482%\n",
      "Epoch 26, Batch 432, LR 1.192159 Loss 4.538211, Accuracy 90.477%\n",
      "Epoch 26, Batch 433, LR 1.192025 Loss 4.537493, Accuracy 90.484%\n",
      "Epoch 26, Batch 434, LR 1.191892 Loss 4.536955, Accuracy 90.483%\n",
      "Epoch 26, Batch 435, LR 1.191758 Loss 4.536441, Accuracy 90.485%\n",
      "Epoch 26, Batch 436, LR 1.191624 Loss 4.536834, Accuracy 90.485%\n",
      "Epoch 26, Batch 437, LR 1.191490 Loss 4.536551, Accuracy 90.484%\n",
      "Epoch 26, Batch 438, LR 1.191356 Loss 4.536169, Accuracy 90.491%\n",
      "Epoch 26, Batch 439, LR 1.191222 Loss 4.535850, Accuracy 90.500%\n",
      "Epoch 26, Batch 440, LR 1.191089 Loss 4.537085, Accuracy 90.501%\n",
      "Epoch 26, Batch 441, LR 1.190955 Loss 4.538083, Accuracy 90.489%\n",
      "Epoch 26, Batch 442, LR 1.190821 Loss 4.537983, Accuracy 90.484%\n",
      "Epoch 26, Batch 443, LR 1.190687 Loss 4.538534, Accuracy 90.479%\n",
      "Epoch 26, Batch 444, LR 1.190553 Loss 4.539308, Accuracy 90.481%\n",
      "Epoch 26, Batch 445, LR 1.190420 Loss 4.539806, Accuracy 90.483%\n",
      "Epoch 26, Batch 446, LR 1.190286 Loss 4.538952, Accuracy 90.485%\n",
      "Epoch 26, Batch 447, LR 1.190152 Loss 4.539965, Accuracy 90.480%\n",
      "Epoch 26, Batch 448, LR 1.190018 Loss 4.539066, Accuracy 90.489%\n",
      "Epoch 26, Batch 449, LR 1.189884 Loss 4.539309, Accuracy 90.489%\n",
      "Epoch 26, Batch 450, LR 1.189751 Loss 4.540166, Accuracy 90.486%\n",
      "Epoch 26, Batch 451, LR 1.189617 Loss 4.541832, Accuracy 90.483%\n",
      "Epoch 26, Batch 452, LR 1.189483 Loss 4.542471, Accuracy 90.483%\n",
      "Epoch 26, Batch 453, LR 1.189349 Loss 4.541453, Accuracy 90.492%\n",
      "Epoch 26, Batch 454, LR 1.189215 Loss 4.542765, Accuracy 90.487%\n",
      "Epoch 26, Batch 455, LR 1.189082 Loss 4.543025, Accuracy 90.482%\n",
      "Epoch 26, Batch 456, LR 1.188948 Loss 4.543665, Accuracy 90.481%\n",
      "Epoch 26, Batch 457, LR 1.188814 Loss 4.543456, Accuracy 90.483%\n",
      "Epoch 26, Batch 458, LR 1.188680 Loss 4.543015, Accuracy 90.483%\n",
      "Epoch 26, Batch 459, LR 1.188547 Loss 4.543526, Accuracy 90.480%\n",
      "Epoch 26, Batch 460, LR 1.188413 Loss 4.543377, Accuracy 90.482%\n",
      "Epoch 26, Batch 461, LR 1.188279 Loss 4.542979, Accuracy 90.484%\n",
      "Epoch 26, Batch 462, LR 1.188145 Loss 4.542318, Accuracy 90.491%\n",
      "Epoch 26, Batch 463, LR 1.188011 Loss 4.543397, Accuracy 90.478%\n",
      "Epoch 26, Batch 464, LR 1.187878 Loss 4.543589, Accuracy 90.475%\n",
      "Epoch 26, Batch 465, LR 1.187744 Loss 4.544170, Accuracy 90.469%\n",
      "Epoch 26, Batch 466, LR 1.187610 Loss 4.544374, Accuracy 90.469%\n",
      "Epoch 26, Batch 467, LR 1.187476 Loss 4.545224, Accuracy 90.464%\n",
      "Epoch 26, Batch 468, LR 1.187342 Loss 4.543744, Accuracy 90.463%\n",
      "Epoch 26, Batch 469, LR 1.187209 Loss 4.543500, Accuracy 90.467%\n",
      "Epoch 26, Batch 470, LR 1.187075 Loss 4.542891, Accuracy 90.470%\n",
      "Epoch 26, Batch 471, LR 1.186941 Loss 4.542746, Accuracy 90.467%\n",
      "Epoch 26, Batch 472, LR 1.186807 Loss 4.541296, Accuracy 90.476%\n",
      "Epoch 26, Batch 473, LR 1.186674 Loss 4.542512, Accuracy 90.478%\n",
      "Epoch 26, Batch 474, LR 1.186540 Loss 4.541965, Accuracy 90.478%\n",
      "Epoch 26, Batch 475, LR 1.186406 Loss 4.541650, Accuracy 90.485%\n",
      "Epoch 26, Batch 476, LR 1.186272 Loss 4.541664, Accuracy 90.479%\n",
      "Epoch 26, Batch 477, LR 1.186138 Loss 4.540884, Accuracy 90.486%\n",
      "Epoch 26, Batch 478, LR 1.186005 Loss 4.539565, Accuracy 90.496%\n",
      "Epoch 26, Batch 479, LR 1.185871 Loss 4.540747, Accuracy 90.488%\n",
      "Epoch 26, Batch 480, LR 1.185737 Loss 4.541675, Accuracy 90.482%\n",
      "Epoch 26, Batch 481, LR 1.185603 Loss 4.542289, Accuracy 90.474%\n",
      "Epoch 26, Batch 482, LR 1.185470 Loss 4.543092, Accuracy 90.468%\n",
      "Epoch 26, Batch 483, LR 1.185336 Loss 4.543436, Accuracy 90.466%\n",
      "Epoch 26, Batch 484, LR 1.185202 Loss 4.543668, Accuracy 90.457%\n",
      "Epoch 26, Batch 485, LR 1.185068 Loss 4.543503, Accuracy 90.453%\n",
      "Epoch 26, Batch 486, LR 1.184934 Loss 4.544043, Accuracy 90.455%\n",
      "Epoch 26, Batch 487, LR 1.184801 Loss 4.543171, Accuracy 90.457%\n",
      "Epoch 26, Batch 488, LR 1.184667 Loss 4.543172, Accuracy 90.455%\n",
      "Epoch 26, Batch 489, LR 1.184533 Loss 4.542266, Accuracy 90.459%\n",
      "Epoch 26, Batch 490, LR 1.184399 Loss 4.542011, Accuracy 90.459%\n",
      "Epoch 26, Batch 491, LR 1.184266 Loss 4.541389, Accuracy 90.469%\n",
      "Epoch 26, Batch 492, LR 1.184132 Loss 4.540579, Accuracy 90.471%\n",
      "Epoch 26, Batch 493, LR 1.183998 Loss 4.540104, Accuracy 90.481%\n",
      "Epoch 26, Batch 494, LR 1.183864 Loss 4.539832, Accuracy 90.476%\n",
      "Epoch 26, Batch 495, LR 1.183731 Loss 4.539832, Accuracy 90.478%\n",
      "Epoch 26, Batch 496, LR 1.183597 Loss 4.539770, Accuracy 90.482%\n",
      "Epoch 26, Batch 497, LR 1.183463 Loss 4.539920, Accuracy 90.480%\n",
      "Epoch 26, Batch 498, LR 1.183329 Loss 4.540582, Accuracy 90.485%\n",
      "Epoch 26, Batch 499, LR 1.183195 Loss 4.541877, Accuracy 90.475%\n",
      "Epoch 26, Batch 500, LR 1.183062 Loss 4.540974, Accuracy 90.473%\n",
      "Epoch 26, Batch 501, LR 1.182928 Loss 4.541730, Accuracy 90.474%\n",
      "Epoch 26, Batch 502, LR 1.182794 Loss 4.541783, Accuracy 90.476%\n",
      "Epoch 26, Batch 503, LR 1.182660 Loss 4.541058, Accuracy 90.481%\n",
      "Epoch 26, Batch 504, LR 1.182527 Loss 4.542637, Accuracy 90.472%\n",
      "Epoch 26, Batch 505, LR 1.182393 Loss 4.542057, Accuracy 90.464%\n",
      "Epoch 26, Batch 506, LR 1.182259 Loss 4.542289, Accuracy 90.464%\n",
      "Epoch 26, Batch 507, LR 1.182125 Loss 4.541918, Accuracy 90.465%\n",
      "Epoch 26, Batch 508, LR 1.181992 Loss 4.541693, Accuracy 90.467%\n",
      "Epoch 26, Batch 509, LR 1.181858 Loss 4.541357, Accuracy 90.467%\n",
      "Epoch 26, Batch 510, LR 1.181724 Loss 4.541644, Accuracy 90.472%\n",
      "Epoch 26, Batch 511, LR 1.181590 Loss 4.540986, Accuracy 90.472%\n",
      "Epoch 26, Batch 512, LR 1.181457 Loss 4.542389, Accuracy 90.462%\n",
      "Epoch 26, Batch 513, LR 1.181323 Loss 4.542465, Accuracy 90.461%\n",
      "Epoch 26, Batch 514, LR 1.181189 Loss 4.542754, Accuracy 90.459%\n",
      "Epoch 26, Batch 515, LR 1.181055 Loss 4.542175, Accuracy 90.463%\n",
      "Epoch 26, Batch 516, LR 1.180922 Loss 4.541421, Accuracy 90.461%\n",
      "Epoch 26, Batch 517, LR 1.180788 Loss 4.542032, Accuracy 90.448%\n",
      "Epoch 26, Batch 518, LR 1.180654 Loss 4.540779, Accuracy 90.452%\n",
      "Epoch 26, Batch 519, LR 1.180520 Loss 4.541228, Accuracy 90.446%\n",
      "Epoch 26, Batch 520, LR 1.180387 Loss 4.542313, Accuracy 90.443%\n",
      "Epoch 26, Batch 521, LR 1.180253 Loss 4.543563, Accuracy 90.429%\n",
      "Epoch 26, Batch 522, LR 1.180119 Loss 4.543314, Accuracy 90.429%\n",
      "Epoch 26, Batch 523, LR 1.179985 Loss 4.543899, Accuracy 90.429%\n",
      "Epoch 26, Batch 524, LR 1.179852 Loss 4.543414, Accuracy 90.433%\n",
      "Epoch 26, Batch 525, LR 1.179718 Loss 4.543457, Accuracy 90.433%\n",
      "Epoch 26, Batch 526, LR 1.179584 Loss 4.542596, Accuracy 90.429%\n",
      "Epoch 26, Batch 527, LR 1.179450 Loss 4.542153, Accuracy 90.432%\n",
      "Epoch 26, Batch 528, LR 1.179317 Loss 4.541810, Accuracy 90.431%\n",
      "Epoch 26, Batch 529, LR 1.179183 Loss 4.541707, Accuracy 90.437%\n",
      "Epoch 26, Batch 530, LR 1.179049 Loss 4.541384, Accuracy 90.436%\n",
      "Epoch 26, Batch 531, LR 1.178915 Loss 4.540187, Accuracy 90.440%\n",
      "Epoch 26, Batch 532, LR 1.178782 Loss 4.541328, Accuracy 90.434%\n",
      "Epoch 26, Batch 533, LR 1.178648 Loss 4.540646, Accuracy 90.432%\n",
      "Epoch 26, Batch 534, LR 1.178514 Loss 4.540472, Accuracy 90.430%\n",
      "Epoch 26, Batch 535, LR 1.178381 Loss 4.541301, Accuracy 90.425%\n",
      "Epoch 26, Batch 536, LR 1.178247 Loss 4.541673, Accuracy 90.422%\n",
      "Epoch 26, Batch 537, LR 1.178113 Loss 4.542201, Accuracy 90.417%\n",
      "Epoch 26, Batch 538, LR 1.177979 Loss 4.542411, Accuracy 90.413%\n",
      "Epoch 26, Batch 539, LR 1.177846 Loss 4.542726, Accuracy 90.410%\n",
      "Epoch 26, Batch 540, LR 1.177712 Loss 4.542511, Accuracy 90.408%\n",
      "Epoch 26, Batch 541, LR 1.177578 Loss 4.543237, Accuracy 90.394%\n",
      "Epoch 26, Batch 542, LR 1.177444 Loss 4.543406, Accuracy 90.389%\n",
      "Epoch 26, Batch 543, LR 1.177311 Loss 4.543997, Accuracy 90.389%\n",
      "Epoch 26, Batch 544, LR 1.177177 Loss 4.543775, Accuracy 90.389%\n",
      "Epoch 26, Batch 545, LR 1.177043 Loss 4.543561, Accuracy 90.394%\n",
      "Epoch 26, Batch 546, LR 1.176910 Loss 4.544846, Accuracy 90.393%\n",
      "Epoch 26, Batch 547, LR 1.176776 Loss 4.545528, Accuracy 90.395%\n",
      "Epoch 26, Batch 548, LR 1.176642 Loss 4.545540, Accuracy 90.397%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 549, LR 1.176508 Loss 4.546477, Accuracy 90.392%\n",
      "Epoch 26, Batch 550, LR 1.176375 Loss 4.546126, Accuracy 90.396%\n",
      "Epoch 26, Batch 551, LR 1.176241 Loss 4.546312, Accuracy 90.392%\n",
      "Epoch 26, Batch 552, LR 1.176107 Loss 4.546163, Accuracy 90.394%\n",
      "Epoch 26, Batch 553, LR 1.175973 Loss 4.545199, Accuracy 90.402%\n",
      "Epoch 26, Batch 554, LR 1.175840 Loss 4.543649, Accuracy 90.409%\n",
      "Epoch 26, Batch 555, LR 1.175706 Loss 4.543465, Accuracy 90.407%\n",
      "Epoch 26, Batch 556, LR 1.175572 Loss 4.543003, Accuracy 90.407%\n",
      "Epoch 26, Batch 557, LR 1.175439 Loss 4.542890, Accuracy 90.405%\n",
      "Epoch 26, Batch 558, LR 1.175305 Loss 4.542115, Accuracy 90.407%\n",
      "Epoch 26, Batch 559, LR 1.175171 Loss 4.542768, Accuracy 90.406%\n",
      "Epoch 26, Batch 560, LR 1.175037 Loss 4.541673, Accuracy 90.407%\n",
      "Epoch 26, Batch 561, LR 1.174904 Loss 4.541596, Accuracy 90.406%\n",
      "Epoch 26, Batch 562, LR 1.174770 Loss 4.540531, Accuracy 90.415%\n",
      "Epoch 26, Batch 563, LR 1.174636 Loss 4.541401, Accuracy 90.415%\n",
      "Epoch 26, Batch 564, LR 1.174503 Loss 4.540795, Accuracy 90.424%\n",
      "Epoch 26, Batch 565, LR 1.174369 Loss 4.541490, Accuracy 90.419%\n",
      "Epoch 26, Batch 566, LR 1.174235 Loss 4.542452, Accuracy 90.419%\n",
      "Epoch 26, Batch 567, LR 1.174102 Loss 4.541674, Accuracy 90.421%\n",
      "Epoch 26, Batch 568, LR 1.173968 Loss 4.541512, Accuracy 90.421%\n",
      "Epoch 26, Batch 569, LR 1.173834 Loss 4.541474, Accuracy 90.425%\n",
      "Epoch 26, Batch 570, LR 1.173700 Loss 4.540451, Accuracy 90.432%\n",
      "Epoch 26, Batch 571, LR 1.173567 Loss 4.541253, Accuracy 90.423%\n",
      "Epoch 26, Batch 572, LR 1.173433 Loss 4.540910, Accuracy 90.426%\n",
      "Epoch 26, Batch 573, LR 1.173299 Loss 4.539901, Accuracy 90.431%\n",
      "Epoch 26, Batch 574, LR 1.173166 Loss 4.540538, Accuracy 90.429%\n",
      "Epoch 26, Batch 575, LR 1.173032 Loss 4.539877, Accuracy 90.433%\n",
      "Epoch 26, Batch 576, LR 1.172898 Loss 4.540254, Accuracy 90.436%\n",
      "Epoch 26, Batch 577, LR 1.172765 Loss 4.540137, Accuracy 90.437%\n",
      "Epoch 26, Batch 578, LR 1.172631 Loss 4.539931, Accuracy 90.436%\n",
      "Epoch 26, Batch 579, LR 1.172497 Loss 4.541354, Accuracy 90.425%\n",
      "Epoch 26, Batch 580, LR 1.172363 Loss 4.541479, Accuracy 90.424%\n",
      "Epoch 26, Batch 581, LR 1.172230 Loss 4.540875, Accuracy 90.433%\n",
      "Epoch 26, Batch 582, LR 1.172096 Loss 4.540506, Accuracy 90.429%\n",
      "Epoch 26, Batch 583, LR 1.171962 Loss 4.539580, Accuracy 90.429%\n",
      "Epoch 26, Batch 584, LR 1.171829 Loss 4.540980, Accuracy 90.423%\n",
      "Epoch 26, Batch 585, LR 1.171695 Loss 4.541506, Accuracy 90.418%\n",
      "Epoch 26, Batch 586, LR 1.171561 Loss 4.541053, Accuracy 90.422%\n",
      "Epoch 26, Batch 587, LR 1.171428 Loss 4.540275, Accuracy 90.424%\n",
      "Epoch 26, Batch 588, LR 1.171294 Loss 4.541340, Accuracy 90.416%\n",
      "Epoch 26, Batch 589, LR 1.171160 Loss 4.540617, Accuracy 90.421%\n",
      "Epoch 26, Batch 590, LR 1.171027 Loss 4.541387, Accuracy 90.421%\n",
      "Epoch 26, Batch 591, LR 1.170893 Loss 4.541706, Accuracy 90.420%\n",
      "Epoch 26, Batch 592, LR 1.170759 Loss 4.540081, Accuracy 90.424%\n",
      "Epoch 26, Batch 593, LR 1.170625 Loss 4.540537, Accuracy 90.425%\n",
      "Epoch 26, Batch 594, LR 1.170492 Loss 4.540882, Accuracy 90.420%\n",
      "Epoch 26, Batch 595, LR 1.170358 Loss 4.540404, Accuracy 90.423%\n",
      "Epoch 26, Batch 596, LR 1.170224 Loss 4.539646, Accuracy 90.427%\n",
      "Epoch 26, Batch 597, LR 1.170091 Loss 4.539707, Accuracy 90.427%\n",
      "Epoch 26, Batch 598, LR 1.169957 Loss 4.540342, Accuracy 90.423%\n",
      "Epoch 26, Batch 599, LR 1.169823 Loss 4.541370, Accuracy 90.420%\n",
      "Epoch 26, Batch 600, LR 1.169690 Loss 4.542214, Accuracy 90.421%\n",
      "Epoch 26, Batch 601, LR 1.169556 Loss 4.542186, Accuracy 90.421%\n",
      "Epoch 26, Batch 602, LR 1.169422 Loss 4.541054, Accuracy 90.423%\n",
      "Epoch 26, Batch 603, LR 1.169289 Loss 4.540859, Accuracy 90.424%\n",
      "Epoch 26, Batch 604, LR 1.169155 Loss 4.541753, Accuracy 90.418%\n",
      "Epoch 26, Batch 605, LR 1.169021 Loss 4.541668, Accuracy 90.417%\n",
      "Epoch 26, Batch 606, LR 1.168888 Loss 4.541402, Accuracy 90.415%\n",
      "Epoch 26, Batch 607, LR 1.168754 Loss 4.541700, Accuracy 90.411%\n",
      "Epoch 26, Batch 608, LR 1.168620 Loss 4.540689, Accuracy 90.419%\n",
      "Epoch 26, Batch 609, LR 1.168487 Loss 4.540561, Accuracy 90.421%\n",
      "Epoch 26, Batch 610, LR 1.168353 Loss 4.542613, Accuracy 90.415%\n",
      "Epoch 26, Batch 611, LR 1.168219 Loss 4.542426, Accuracy 90.419%\n",
      "Epoch 26, Batch 612, LR 1.168086 Loss 4.542314, Accuracy 90.419%\n",
      "Epoch 26, Batch 613, LR 1.167952 Loss 4.543221, Accuracy 90.416%\n",
      "Epoch 26, Batch 614, LR 1.167818 Loss 4.543405, Accuracy 90.414%\n",
      "Epoch 26, Batch 615, LR 1.167685 Loss 4.543463, Accuracy 90.413%\n",
      "Epoch 26, Batch 616, LR 1.167551 Loss 4.543840, Accuracy 90.416%\n",
      "Epoch 26, Batch 617, LR 1.167417 Loss 4.543920, Accuracy 90.414%\n",
      "Epoch 26, Batch 618, LR 1.167284 Loss 4.545506, Accuracy 90.406%\n",
      "Epoch 26, Batch 619, LR 1.167150 Loss 4.545668, Accuracy 90.405%\n",
      "Epoch 26, Batch 620, LR 1.167016 Loss 4.545758, Accuracy 90.410%\n",
      "Epoch 26, Batch 621, LR 1.166883 Loss 4.546579, Accuracy 90.406%\n",
      "Epoch 26, Batch 622, LR 1.166749 Loss 4.546602, Accuracy 90.403%\n",
      "Epoch 26, Batch 623, LR 1.166615 Loss 4.545867, Accuracy 90.402%\n",
      "Epoch 26, Batch 624, LR 1.166482 Loss 4.545083, Accuracy 90.407%\n",
      "Epoch 26, Batch 625, LR 1.166348 Loss 4.544856, Accuracy 90.407%\n",
      "Epoch 26, Batch 626, LR 1.166214 Loss 4.544928, Accuracy 90.404%\n",
      "Epoch 26, Batch 627, LR 1.166081 Loss 4.544921, Accuracy 90.403%\n",
      "Epoch 26, Batch 628, LR 1.165947 Loss 4.544861, Accuracy 90.410%\n",
      "Epoch 26, Batch 629, LR 1.165813 Loss 4.546400, Accuracy 90.401%\n",
      "Epoch 26, Batch 630, LR 1.165680 Loss 4.546304, Accuracy 90.399%\n",
      "Epoch 26, Batch 631, LR 1.165546 Loss 4.545898, Accuracy 90.397%\n",
      "Epoch 26, Batch 632, LR 1.165413 Loss 4.545905, Accuracy 90.398%\n",
      "Epoch 26, Batch 633, LR 1.165279 Loss 4.546341, Accuracy 90.392%\n",
      "Epoch 26, Batch 634, LR 1.165145 Loss 4.546789, Accuracy 90.386%\n",
      "Epoch 26, Batch 635, LR 1.165012 Loss 4.546366, Accuracy 90.386%\n",
      "Epoch 26, Batch 636, LR 1.164878 Loss 4.548041, Accuracy 90.379%\n",
      "Epoch 26, Batch 637, LR 1.164744 Loss 4.548446, Accuracy 90.380%\n",
      "Epoch 26, Batch 638, LR 1.164611 Loss 4.549616, Accuracy 90.373%\n",
      "Epoch 26, Batch 639, LR 1.164477 Loss 4.549190, Accuracy 90.379%\n",
      "Epoch 26, Batch 640, LR 1.164343 Loss 4.549649, Accuracy 90.374%\n",
      "Epoch 26, Batch 641, LR 1.164210 Loss 4.549447, Accuracy 90.373%\n",
      "Epoch 26, Batch 642, LR 1.164076 Loss 4.549135, Accuracy 90.378%\n",
      "Epoch 26, Batch 643, LR 1.163943 Loss 4.548510, Accuracy 90.386%\n",
      "Epoch 26, Batch 644, LR 1.163809 Loss 4.547700, Accuracy 90.388%\n",
      "Epoch 26, Batch 645, LR 1.163675 Loss 4.546776, Accuracy 90.389%\n",
      "Epoch 26, Batch 646, LR 1.163542 Loss 4.547264, Accuracy 90.389%\n",
      "Epoch 26, Batch 647, LR 1.163408 Loss 4.547617, Accuracy 90.391%\n",
      "Epoch 26, Batch 648, LR 1.163274 Loss 4.547714, Accuracy 90.392%\n",
      "Epoch 26, Batch 649, LR 1.163141 Loss 4.547182, Accuracy 90.390%\n",
      "Epoch 26, Batch 650, LR 1.163007 Loss 4.545947, Accuracy 90.394%\n",
      "Epoch 26, Batch 651, LR 1.162873 Loss 4.545626, Accuracy 90.399%\n",
      "Epoch 26, Batch 652, LR 1.162740 Loss 4.545729, Accuracy 90.402%\n",
      "Epoch 26, Batch 653, LR 1.162606 Loss 4.545037, Accuracy 90.405%\n",
      "Epoch 26, Batch 654, LR 1.162473 Loss 4.544999, Accuracy 90.405%\n",
      "Epoch 26, Batch 655, LR 1.162339 Loss 4.544864, Accuracy 90.402%\n",
      "Epoch 26, Batch 656, LR 1.162205 Loss 4.544397, Accuracy 90.408%\n",
      "Epoch 26, Batch 657, LR 1.162072 Loss 4.544598, Accuracy 90.409%\n",
      "Epoch 26, Batch 658, LR 1.161938 Loss 4.544209, Accuracy 90.410%\n",
      "Epoch 26, Batch 659, LR 1.161804 Loss 4.543119, Accuracy 90.415%\n",
      "Epoch 26, Batch 660, LR 1.161671 Loss 4.543910, Accuracy 90.410%\n",
      "Epoch 26, Batch 661, LR 1.161537 Loss 4.543409, Accuracy 90.411%\n",
      "Epoch 26, Batch 662, LR 1.161404 Loss 4.543986, Accuracy 90.408%\n",
      "Epoch 26, Batch 663, LR 1.161270 Loss 4.543909, Accuracy 90.407%\n",
      "Epoch 26, Batch 664, LR 1.161136 Loss 4.543254, Accuracy 90.407%\n",
      "Epoch 26, Batch 665, LR 1.161003 Loss 4.543894, Accuracy 90.401%\n",
      "Epoch 26, Batch 666, LR 1.160869 Loss 4.543851, Accuracy 90.400%\n",
      "Epoch 26, Batch 667, LR 1.160736 Loss 4.543855, Accuracy 90.406%\n",
      "Epoch 26, Batch 668, LR 1.160602 Loss 4.543665, Accuracy 90.407%\n",
      "Epoch 26, Batch 669, LR 1.160468 Loss 4.544176, Accuracy 90.403%\n",
      "Epoch 26, Batch 670, LR 1.160335 Loss 4.545194, Accuracy 90.398%\n",
      "Epoch 26, Batch 671, LR 1.160201 Loss 4.544260, Accuracy 90.403%\n",
      "Epoch 26, Batch 672, LR 1.160068 Loss 4.544220, Accuracy 90.397%\n",
      "Epoch 26, Batch 673, LR 1.159934 Loss 4.544163, Accuracy 90.396%\n",
      "Epoch 26, Batch 674, LR 1.159800 Loss 4.545109, Accuracy 90.390%\n",
      "Epoch 26, Batch 675, LR 1.159667 Loss 4.545696, Accuracy 90.383%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 676, LR 1.159533 Loss 4.546925, Accuracy 90.378%\n",
      "Epoch 26, Batch 677, LR 1.159399 Loss 4.546674, Accuracy 90.376%\n",
      "Epoch 26, Batch 678, LR 1.159266 Loss 4.546430, Accuracy 90.376%\n",
      "Epoch 26, Batch 679, LR 1.159132 Loss 4.546311, Accuracy 90.374%\n",
      "Epoch 26, Batch 680, LR 1.158999 Loss 4.546548, Accuracy 90.371%\n",
      "Epoch 26, Batch 681, LR 1.158865 Loss 4.546366, Accuracy 90.376%\n",
      "Epoch 26, Batch 682, LR 1.158731 Loss 4.547459, Accuracy 90.367%\n",
      "Epoch 26, Batch 683, LR 1.158598 Loss 4.547361, Accuracy 90.369%\n",
      "Epoch 26, Batch 684, LR 1.158464 Loss 4.548458, Accuracy 90.359%\n",
      "Epoch 26, Batch 685, LR 1.158331 Loss 4.549088, Accuracy 90.357%\n",
      "Epoch 26, Batch 686, LR 1.158197 Loss 4.549068, Accuracy 90.353%\n",
      "Epoch 26, Batch 687, LR 1.158064 Loss 4.549289, Accuracy 90.354%\n",
      "Epoch 26, Batch 688, LR 1.157930 Loss 4.549819, Accuracy 90.354%\n",
      "Epoch 26, Batch 689, LR 1.157796 Loss 4.550038, Accuracy 90.359%\n",
      "Epoch 26, Batch 690, LR 1.157663 Loss 4.549981, Accuracy 90.357%\n",
      "Epoch 26, Batch 691, LR 1.157529 Loss 4.549907, Accuracy 90.357%\n",
      "Epoch 26, Batch 692, LR 1.157396 Loss 4.549061, Accuracy 90.360%\n",
      "Epoch 26, Batch 693, LR 1.157262 Loss 4.549424, Accuracy 90.354%\n",
      "Epoch 26, Batch 694, LR 1.157128 Loss 4.550415, Accuracy 90.355%\n",
      "Epoch 26, Batch 695, LR 1.156995 Loss 4.551138, Accuracy 90.348%\n",
      "Epoch 26, Batch 696, LR 1.156861 Loss 4.551598, Accuracy 90.347%\n",
      "Epoch 26, Batch 697, LR 1.156728 Loss 4.551040, Accuracy 90.349%\n",
      "Epoch 26, Batch 698, LR 1.156594 Loss 4.551152, Accuracy 90.346%\n",
      "Epoch 26, Batch 699, LR 1.156461 Loss 4.550483, Accuracy 90.349%\n",
      "Epoch 26, Batch 700, LR 1.156327 Loss 4.550524, Accuracy 90.350%\n",
      "Epoch 26, Batch 701, LR 1.156193 Loss 4.550896, Accuracy 90.345%\n",
      "Epoch 26, Batch 702, LR 1.156060 Loss 4.550371, Accuracy 90.341%\n",
      "Epoch 26, Batch 703, LR 1.155926 Loss 4.549629, Accuracy 90.348%\n",
      "Epoch 26, Batch 704, LR 1.155793 Loss 4.549581, Accuracy 90.351%\n",
      "Epoch 26, Batch 705, LR 1.155659 Loss 4.549982, Accuracy 90.348%\n",
      "Epoch 26, Batch 706, LR 1.155526 Loss 4.550605, Accuracy 90.344%\n",
      "Epoch 26, Batch 707, LR 1.155392 Loss 4.551390, Accuracy 90.344%\n",
      "Epoch 26, Batch 708, LR 1.155258 Loss 4.551721, Accuracy 90.346%\n",
      "Epoch 26, Batch 709, LR 1.155125 Loss 4.551946, Accuracy 90.348%\n",
      "Epoch 26, Batch 710, LR 1.154991 Loss 4.552867, Accuracy 90.341%\n",
      "Epoch 26, Batch 711, LR 1.154858 Loss 4.553325, Accuracy 90.339%\n",
      "Epoch 26, Batch 712, LR 1.154724 Loss 4.552389, Accuracy 90.348%\n",
      "Epoch 26, Batch 713, LR 1.154591 Loss 4.551943, Accuracy 90.350%\n",
      "Epoch 26, Batch 714, LR 1.154457 Loss 4.551803, Accuracy 90.354%\n",
      "Epoch 26, Batch 715, LR 1.154323 Loss 4.552862, Accuracy 90.351%\n",
      "Epoch 26, Batch 716, LR 1.154190 Loss 4.552488, Accuracy 90.354%\n",
      "Epoch 26, Batch 717, LR 1.154056 Loss 4.552129, Accuracy 90.358%\n",
      "Epoch 26, Batch 718, LR 1.153923 Loss 4.552132, Accuracy 90.355%\n",
      "Epoch 26, Batch 719, LR 1.153789 Loss 4.551445, Accuracy 90.359%\n",
      "Epoch 26, Batch 720, LR 1.153656 Loss 4.551460, Accuracy 90.356%\n",
      "Epoch 26, Batch 721, LR 1.153522 Loss 4.551598, Accuracy 90.357%\n",
      "Epoch 26, Batch 722, LR 1.153389 Loss 4.551638, Accuracy 90.361%\n",
      "Epoch 26, Batch 723, LR 1.153255 Loss 4.550975, Accuracy 90.359%\n",
      "Epoch 26, Batch 724, LR 1.153121 Loss 4.550933, Accuracy 90.358%\n",
      "Epoch 26, Batch 725, LR 1.152988 Loss 4.550487, Accuracy 90.361%\n",
      "Epoch 26, Batch 726, LR 1.152854 Loss 4.550812, Accuracy 90.361%\n",
      "Epoch 26, Batch 727, LR 1.152721 Loss 4.550687, Accuracy 90.361%\n",
      "Epoch 26, Batch 728, LR 1.152587 Loss 4.550572, Accuracy 90.363%\n",
      "Epoch 26, Batch 729, LR 1.152454 Loss 4.550999, Accuracy 90.359%\n",
      "Epoch 26, Batch 730, LR 1.152320 Loss 4.550593, Accuracy 90.362%\n",
      "Epoch 26, Batch 731, LR 1.152187 Loss 4.550770, Accuracy 90.362%\n",
      "Epoch 26, Batch 732, LR 1.152053 Loss 4.550472, Accuracy 90.365%\n",
      "Epoch 26, Batch 733, LR 1.151920 Loss 4.551420, Accuracy 90.364%\n",
      "Epoch 26, Batch 734, LR 1.151786 Loss 4.551778, Accuracy 90.362%\n",
      "Epoch 26, Batch 735, LR 1.151652 Loss 4.551630, Accuracy 90.360%\n",
      "Epoch 26, Batch 736, LR 1.151519 Loss 4.551497, Accuracy 90.360%\n",
      "Epoch 26, Batch 737, LR 1.151385 Loss 4.551269, Accuracy 90.360%\n",
      "Epoch 26, Batch 738, LR 1.151252 Loss 4.552057, Accuracy 90.353%\n",
      "Epoch 26, Batch 739, LR 1.151118 Loss 4.551992, Accuracy 90.354%\n",
      "Epoch 26, Batch 740, LR 1.150985 Loss 4.552695, Accuracy 90.349%\n",
      "Epoch 26, Batch 741, LR 1.150851 Loss 4.552638, Accuracy 90.346%\n",
      "Epoch 26, Batch 742, LR 1.150718 Loss 4.552750, Accuracy 90.345%\n",
      "Epoch 26, Batch 743, LR 1.150584 Loss 4.552281, Accuracy 90.346%\n",
      "Epoch 26, Batch 744, LR 1.150451 Loss 4.551989, Accuracy 90.349%\n",
      "Epoch 26, Batch 745, LR 1.150317 Loss 4.552261, Accuracy 90.347%\n",
      "Epoch 26, Batch 746, LR 1.150184 Loss 4.551504, Accuracy 90.351%\n",
      "Epoch 26, Batch 747, LR 1.150050 Loss 4.551003, Accuracy 90.345%\n",
      "Epoch 26, Batch 748, LR 1.149917 Loss 4.550572, Accuracy 90.346%\n",
      "Epoch 26, Batch 749, LR 1.149783 Loss 4.550281, Accuracy 90.348%\n",
      "Epoch 26, Batch 750, LR 1.149649 Loss 4.550837, Accuracy 90.346%\n",
      "Epoch 26, Batch 751, LR 1.149516 Loss 4.549915, Accuracy 90.349%\n",
      "Epoch 26, Batch 752, LR 1.149382 Loss 4.550706, Accuracy 90.346%\n",
      "Epoch 26, Batch 753, LR 1.149249 Loss 4.550626, Accuracy 90.347%\n",
      "Epoch 26, Batch 754, LR 1.149115 Loss 4.550937, Accuracy 90.349%\n",
      "Epoch 26, Batch 755, LR 1.148982 Loss 4.551346, Accuracy 90.351%\n",
      "Epoch 26, Batch 756, LR 1.148848 Loss 4.551322, Accuracy 90.346%\n",
      "Epoch 26, Batch 757, LR 1.148715 Loss 4.551153, Accuracy 90.346%\n",
      "Epoch 26, Batch 758, LR 1.148581 Loss 4.550930, Accuracy 90.349%\n",
      "Epoch 26, Batch 759, LR 1.148448 Loss 4.550100, Accuracy 90.352%\n",
      "Epoch 26, Batch 760, LR 1.148314 Loss 4.550915, Accuracy 90.356%\n",
      "Epoch 26, Batch 761, LR 1.148181 Loss 4.551448, Accuracy 90.351%\n",
      "Epoch 26, Batch 762, LR 1.148047 Loss 4.551604, Accuracy 90.348%\n",
      "Epoch 26, Batch 763, LR 1.147914 Loss 4.551200, Accuracy 90.354%\n",
      "Epoch 26, Batch 764, LR 1.147780 Loss 4.550745, Accuracy 90.355%\n",
      "Epoch 26, Batch 765, LR 1.147647 Loss 4.550915, Accuracy 90.354%\n",
      "Epoch 26, Batch 766, LR 1.147513 Loss 4.551335, Accuracy 90.355%\n",
      "Epoch 26, Batch 767, LR 1.147380 Loss 4.550559, Accuracy 90.359%\n",
      "Epoch 26, Batch 768, LR 1.147246 Loss 4.550941, Accuracy 90.357%\n",
      "Epoch 26, Batch 769, LR 1.147113 Loss 4.551293, Accuracy 90.356%\n",
      "Epoch 26, Batch 770, LR 1.146979 Loss 4.551544, Accuracy 90.357%\n",
      "Epoch 26, Batch 771, LR 1.146846 Loss 4.552218, Accuracy 90.355%\n",
      "Epoch 26, Batch 772, LR 1.146712 Loss 4.552539, Accuracy 90.354%\n",
      "Epoch 26, Batch 773, LR 1.146579 Loss 4.552684, Accuracy 90.356%\n",
      "Epoch 26, Batch 774, LR 1.146445 Loss 4.552059, Accuracy 90.360%\n",
      "Epoch 26, Batch 775, LR 1.146312 Loss 4.551802, Accuracy 90.358%\n",
      "Epoch 26, Batch 776, LR 1.146178 Loss 4.551501, Accuracy 90.359%\n",
      "Epoch 26, Batch 777, LR 1.146045 Loss 4.552122, Accuracy 90.355%\n",
      "Epoch 26, Batch 778, LR 1.145911 Loss 4.552014, Accuracy 90.359%\n",
      "Epoch 26, Batch 779, LR 1.145778 Loss 4.551636, Accuracy 90.364%\n",
      "Epoch 26, Batch 780, LR 1.145644 Loss 4.553013, Accuracy 90.357%\n",
      "Epoch 26, Batch 781, LR 1.145511 Loss 4.552421, Accuracy 90.362%\n",
      "Epoch 26, Batch 782, LR 1.145377 Loss 4.553038, Accuracy 90.362%\n",
      "Epoch 26, Batch 783, LR 1.145244 Loss 4.553166, Accuracy 90.356%\n",
      "Epoch 26, Batch 784, LR 1.145110 Loss 4.553610, Accuracy 90.356%\n",
      "Epoch 26, Batch 785, LR 1.144977 Loss 4.554208, Accuracy 90.359%\n",
      "Epoch 26, Batch 786, LR 1.144844 Loss 4.554040, Accuracy 90.362%\n",
      "Epoch 26, Batch 787, LR 1.144710 Loss 4.552959, Accuracy 90.365%\n",
      "Epoch 26, Batch 788, LR 1.144577 Loss 4.552606, Accuracy 90.366%\n",
      "Epoch 26, Batch 789, LR 1.144443 Loss 4.552658, Accuracy 90.364%\n",
      "Epoch 26, Batch 790, LR 1.144310 Loss 4.552769, Accuracy 90.362%\n",
      "Epoch 26, Batch 791, LR 1.144176 Loss 4.552291, Accuracy 90.362%\n",
      "Epoch 26, Batch 792, LR 1.144043 Loss 4.551913, Accuracy 90.363%\n",
      "Epoch 26, Batch 793, LR 1.143909 Loss 4.551807, Accuracy 90.363%\n",
      "Epoch 26, Batch 794, LR 1.143776 Loss 4.551648, Accuracy 90.367%\n",
      "Epoch 26, Batch 795, LR 1.143642 Loss 4.551024, Accuracy 90.369%\n",
      "Epoch 26, Batch 796, LR 1.143509 Loss 4.550291, Accuracy 90.369%\n",
      "Epoch 26, Batch 797, LR 1.143375 Loss 4.550733, Accuracy 90.372%\n",
      "Epoch 26, Batch 798, LR 1.143242 Loss 4.551119, Accuracy 90.371%\n",
      "Epoch 26, Batch 799, LR 1.143108 Loss 4.551052, Accuracy 90.369%\n",
      "Epoch 26, Batch 800, LR 1.142975 Loss 4.552106, Accuracy 90.369%\n",
      "Epoch 26, Batch 801, LR 1.142841 Loss 4.552129, Accuracy 90.373%\n",
      "Epoch 26, Batch 802, LR 1.142708 Loss 4.551975, Accuracy 90.376%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 803, LR 1.142575 Loss 4.551812, Accuracy 90.378%\n",
      "Epoch 26, Batch 804, LR 1.142441 Loss 4.551464, Accuracy 90.379%\n",
      "Epoch 26, Batch 805, LR 1.142308 Loss 4.551260, Accuracy 90.381%\n",
      "Epoch 26, Batch 806, LR 1.142174 Loss 4.550660, Accuracy 90.385%\n",
      "Epoch 26, Batch 807, LR 1.142041 Loss 4.551247, Accuracy 90.386%\n",
      "Epoch 26, Batch 808, LR 1.141907 Loss 4.550776, Accuracy 90.390%\n",
      "Epoch 26, Batch 809, LR 1.141774 Loss 4.551034, Accuracy 90.386%\n",
      "Epoch 26, Batch 810, LR 1.141640 Loss 4.551617, Accuracy 90.381%\n",
      "Epoch 26, Batch 811, LR 1.141507 Loss 4.552471, Accuracy 90.372%\n",
      "Epoch 26, Batch 812, LR 1.141373 Loss 4.552180, Accuracy 90.371%\n",
      "Epoch 26, Batch 813, LR 1.141240 Loss 4.552902, Accuracy 90.365%\n",
      "Epoch 26, Batch 814, LR 1.141107 Loss 4.553235, Accuracy 90.365%\n",
      "Epoch 26, Batch 815, LR 1.140973 Loss 4.553678, Accuracy 90.363%\n",
      "Epoch 26, Batch 816, LR 1.140840 Loss 4.553852, Accuracy 90.364%\n",
      "Epoch 26, Batch 817, LR 1.140706 Loss 4.553609, Accuracy 90.365%\n",
      "Epoch 26, Batch 818, LR 1.140573 Loss 4.553096, Accuracy 90.363%\n",
      "Epoch 26, Batch 819, LR 1.140439 Loss 4.553118, Accuracy 90.362%\n",
      "Epoch 26, Batch 820, LR 1.140306 Loss 4.552901, Accuracy 90.364%\n",
      "Epoch 26, Batch 821, LR 1.140173 Loss 4.553016, Accuracy 90.369%\n",
      "Epoch 26, Batch 822, LR 1.140039 Loss 4.552470, Accuracy 90.372%\n",
      "Epoch 26, Batch 823, LR 1.139906 Loss 4.552818, Accuracy 90.371%\n",
      "Epoch 26, Batch 824, LR 1.139772 Loss 4.553147, Accuracy 90.366%\n",
      "Epoch 26, Batch 825, LR 1.139639 Loss 4.553168, Accuracy 90.367%\n",
      "Epoch 26, Batch 826, LR 1.139505 Loss 4.552878, Accuracy 90.369%\n",
      "Epoch 26, Batch 827, LR 1.139372 Loss 4.552931, Accuracy 90.369%\n",
      "Epoch 26, Batch 828, LR 1.139239 Loss 4.553120, Accuracy 90.367%\n",
      "Epoch 26, Batch 829, LR 1.139105 Loss 4.552472, Accuracy 90.366%\n",
      "Epoch 26, Batch 830, LR 1.138972 Loss 4.552171, Accuracy 90.371%\n",
      "Epoch 26, Batch 831, LR 1.138838 Loss 4.552733, Accuracy 90.366%\n",
      "Epoch 26, Batch 832, LR 1.138705 Loss 4.552227, Accuracy 90.364%\n",
      "Epoch 26, Batch 833, LR 1.138571 Loss 4.552595, Accuracy 90.362%\n",
      "Epoch 26, Batch 834, LR 1.138438 Loss 4.552357, Accuracy 90.365%\n",
      "Epoch 26, Batch 835, LR 1.138305 Loss 4.552722, Accuracy 90.362%\n",
      "Epoch 26, Batch 836, LR 1.138171 Loss 4.553369, Accuracy 90.357%\n",
      "Epoch 26, Batch 837, LR 1.138038 Loss 4.552810, Accuracy 90.356%\n",
      "Epoch 26, Batch 838, LR 1.137904 Loss 4.552330, Accuracy 90.357%\n",
      "Epoch 26, Batch 839, LR 1.137771 Loss 4.551744, Accuracy 90.358%\n",
      "Epoch 26, Batch 840, LR 1.137637 Loss 4.552706, Accuracy 90.358%\n",
      "Epoch 26, Batch 841, LR 1.137504 Loss 4.552272, Accuracy 90.362%\n",
      "Epoch 26, Batch 842, LR 1.137371 Loss 4.551569, Accuracy 90.363%\n",
      "Epoch 26, Batch 843, LR 1.137237 Loss 4.552010, Accuracy 90.362%\n",
      "Epoch 26, Batch 844, LR 1.137104 Loss 4.551231, Accuracy 90.366%\n",
      "Epoch 26, Batch 845, LR 1.136970 Loss 4.551074, Accuracy 90.364%\n",
      "Epoch 26, Batch 846, LR 1.136837 Loss 4.550663, Accuracy 90.365%\n",
      "Epoch 26, Batch 847, LR 1.136704 Loss 4.551133, Accuracy 90.367%\n",
      "Epoch 26, Batch 848, LR 1.136570 Loss 4.551804, Accuracy 90.366%\n",
      "Epoch 26, Batch 849, LR 1.136437 Loss 4.551452, Accuracy 90.368%\n",
      "Epoch 26, Batch 850, LR 1.136303 Loss 4.551287, Accuracy 90.369%\n",
      "Epoch 26, Batch 851, LR 1.136170 Loss 4.551424, Accuracy 90.371%\n",
      "Epoch 26, Batch 852, LR 1.136037 Loss 4.551169, Accuracy 90.370%\n",
      "Epoch 26, Batch 853, LR 1.135903 Loss 4.550295, Accuracy 90.375%\n",
      "Epoch 26, Batch 854, LR 1.135770 Loss 4.550430, Accuracy 90.378%\n",
      "Epoch 26, Batch 855, LR 1.135636 Loss 4.550681, Accuracy 90.380%\n",
      "Epoch 26, Batch 856, LR 1.135503 Loss 4.550283, Accuracy 90.382%\n",
      "Epoch 26, Batch 857, LR 1.135370 Loss 4.550307, Accuracy 90.382%\n",
      "Epoch 26, Batch 858, LR 1.135236 Loss 4.550442, Accuracy 90.382%\n",
      "Epoch 26, Batch 859, LR 1.135103 Loss 4.550403, Accuracy 90.381%\n",
      "Epoch 26, Batch 860, LR 1.134970 Loss 4.550418, Accuracy 90.382%\n",
      "Epoch 26, Batch 861, LR 1.134836 Loss 4.550148, Accuracy 90.381%\n",
      "Epoch 26, Batch 862, LR 1.134703 Loss 4.550219, Accuracy 90.379%\n",
      "Epoch 26, Batch 863, LR 1.134569 Loss 4.551098, Accuracy 90.376%\n",
      "Epoch 26, Batch 864, LR 1.134436 Loss 4.550974, Accuracy 90.381%\n",
      "Epoch 26, Batch 865, LR 1.134303 Loss 4.551155, Accuracy 90.378%\n",
      "Epoch 26, Batch 866, LR 1.134169 Loss 4.552401, Accuracy 90.371%\n",
      "Epoch 26, Batch 867, LR 1.134036 Loss 4.551815, Accuracy 90.374%\n",
      "Epoch 26, Batch 868, LR 1.133902 Loss 4.550768, Accuracy 90.377%\n",
      "Epoch 26, Batch 869, LR 1.133769 Loss 4.550716, Accuracy 90.380%\n",
      "Epoch 26, Batch 870, LR 1.133636 Loss 4.550082, Accuracy 90.380%\n",
      "Epoch 26, Batch 871, LR 1.133502 Loss 4.550473, Accuracy 90.377%\n",
      "Epoch 26, Batch 872, LR 1.133369 Loss 4.550374, Accuracy 90.375%\n",
      "Epoch 26, Batch 873, LR 1.133236 Loss 4.549883, Accuracy 90.375%\n",
      "Epoch 26, Batch 874, LR 1.133102 Loss 4.549850, Accuracy 90.375%\n",
      "Epoch 26, Batch 875, LR 1.132969 Loss 4.550305, Accuracy 90.372%\n",
      "Epoch 26, Batch 876, LR 1.132836 Loss 4.549675, Accuracy 90.375%\n",
      "Epoch 26, Batch 877, LR 1.132702 Loss 4.550105, Accuracy 90.371%\n",
      "Epoch 26, Batch 878, LR 1.132569 Loss 4.550313, Accuracy 90.370%\n",
      "Epoch 26, Batch 879, LR 1.132435 Loss 4.551036, Accuracy 90.366%\n",
      "Epoch 26, Batch 880, LR 1.132302 Loss 4.550808, Accuracy 90.368%\n",
      "Epoch 26, Batch 881, LR 1.132169 Loss 4.549819, Accuracy 90.373%\n",
      "Epoch 26, Batch 882, LR 1.132035 Loss 4.550374, Accuracy 90.373%\n",
      "Epoch 26, Batch 883, LR 1.131902 Loss 4.551104, Accuracy 90.366%\n",
      "Epoch 26, Batch 884, LR 1.131769 Loss 4.551022, Accuracy 90.364%\n",
      "Epoch 26, Batch 885, LR 1.131635 Loss 4.550872, Accuracy 90.365%\n",
      "Epoch 26, Batch 886, LR 1.131502 Loss 4.551089, Accuracy 90.360%\n",
      "Epoch 26, Batch 887, LR 1.131369 Loss 4.551358, Accuracy 90.359%\n",
      "Epoch 26, Batch 888, LR 1.131235 Loss 4.551210, Accuracy 90.363%\n",
      "Epoch 26, Batch 889, LR 1.131102 Loss 4.550821, Accuracy 90.366%\n",
      "Epoch 26, Batch 890, LR 1.130969 Loss 4.551807, Accuracy 90.361%\n",
      "Epoch 26, Batch 891, LR 1.130835 Loss 4.552159, Accuracy 90.360%\n",
      "Epoch 26, Batch 892, LR 1.130702 Loss 4.552391, Accuracy 90.360%\n",
      "Epoch 26, Batch 893, LR 1.130569 Loss 4.552657, Accuracy 90.358%\n",
      "Epoch 26, Batch 894, LR 1.130435 Loss 4.552395, Accuracy 90.363%\n",
      "Epoch 26, Batch 895, LR 1.130302 Loss 4.552341, Accuracy 90.367%\n",
      "Epoch 26, Batch 896, LR 1.130169 Loss 4.551872, Accuracy 90.370%\n",
      "Epoch 26, Batch 897, LR 1.130035 Loss 4.552622, Accuracy 90.365%\n",
      "Epoch 26, Batch 898, LR 1.129902 Loss 4.552672, Accuracy 90.365%\n",
      "Epoch 26, Batch 899, LR 1.129769 Loss 4.553201, Accuracy 90.359%\n",
      "Epoch 26, Batch 900, LR 1.129635 Loss 4.553270, Accuracy 90.359%\n",
      "Epoch 26, Batch 901, LR 1.129502 Loss 4.552826, Accuracy 90.361%\n",
      "Epoch 26, Batch 902, LR 1.129369 Loss 4.552873, Accuracy 90.361%\n",
      "Epoch 26, Batch 903, LR 1.129235 Loss 4.552257, Accuracy 90.362%\n",
      "Epoch 26, Batch 904, LR 1.129102 Loss 4.552320, Accuracy 90.364%\n",
      "Epoch 26, Batch 905, LR 1.128969 Loss 4.552158, Accuracy 90.369%\n",
      "Epoch 26, Batch 906, LR 1.128835 Loss 4.551680, Accuracy 90.369%\n",
      "Epoch 26, Batch 907, LR 1.128702 Loss 4.552067, Accuracy 90.368%\n",
      "Epoch 26, Batch 908, LR 1.128569 Loss 4.552622, Accuracy 90.368%\n",
      "Epoch 26, Batch 909, LR 1.128435 Loss 4.552609, Accuracy 90.368%\n",
      "Epoch 26, Batch 910, LR 1.128302 Loss 4.553077, Accuracy 90.367%\n",
      "Epoch 26, Batch 911, LR 1.128169 Loss 4.553113, Accuracy 90.364%\n",
      "Epoch 26, Batch 912, LR 1.128035 Loss 4.553110, Accuracy 90.360%\n",
      "Epoch 26, Batch 913, LR 1.127902 Loss 4.553330, Accuracy 90.357%\n",
      "Epoch 26, Batch 914, LR 1.127769 Loss 4.553416, Accuracy 90.355%\n",
      "Epoch 26, Batch 915, LR 1.127635 Loss 4.552961, Accuracy 90.355%\n",
      "Epoch 26, Batch 916, LR 1.127502 Loss 4.552447, Accuracy 90.358%\n",
      "Epoch 26, Batch 917, LR 1.127369 Loss 4.552498, Accuracy 90.359%\n",
      "Epoch 26, Batch 918, LR 1.127235 Loss 4.552387, Accuracy 90.363%\n",
      "Epoch 26, Batch 919, LR 1.127102 Loss 4.552525, Accuracy 90.367%\n",
      "Epoch 26, Batch 920, LR 1.126969 Loss 4.552503, Accuracy 90.366%\n",
      "Epoch 26, Batch 921, LR 1.126836 Loss 4.552579, Accuracy 90.365%\n",
      "Epoch 26, Batch 922, LR 1.126702 Loss 4.552619, Accuracy 90.363%\n",
      "Epoch 26, Batch 923, LR 1.126569 Loss 4.552337, Accuracy 90.364%\n",
      "Epoch 26, Batch 924, LR 1.126436 Loss 4.552572, Accuracy 90.362%\n",
      "Epoch 26, Batch 925, LR 1.126302 Loss 4.553587, Accuracy 90.357%\n",
      "Epoch 26, Batch 926, LR 1.126169 Loss 4.553274, Accuracy 90.359%\n",
      "Epoch 26, Batch 927, LR 1.126036 Loss 4.552164, Accuracy 90.360%\n",
      "Epoch 26, Batch 928, LR 1.125903 Loss 4.551939, Accuracy 90.361%\n",
      "Epoch 26, Batch 929, LR 1.125769 Loss 4.551630, Accuracy 90.367%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 930, LR 1.125636 Loss 4.552453, Accuracy 90.360%\n",
      "Epoch 26, Batch 931, LR 1.125503 Loss 4.552128, Accuracy 90.358%\n",
      "Epoch 26, Batch 932, LR 1.125369 Loss 4.552434, Accuracy 90.353%\n",
      "Epoch 26, Batch 933, LR 1.125236 Loss 4.552274, Accuracy 90.355%\n",
      "Epoch 26, Batch 934, LR 1.125103 Loss 4.552397, Accuracy 90.355%\n",
      "Epoch 26, Batch 935, LR 1.124969 Loss 4.551957, Accuracy 90.354%\n",
      "Epoch 26, Batch 936, LR 1.124836 Loss 4.552214, Accuracy 90.354%\n",
      "Epoch 26, Batch 937, LR 1.124703 Loss 4.552165, Accuracy 90.354%\n",
      "Epoch 26, Batch 938, LR 1.124570 Loss 4.551774, Accuracy 90.358%\n",
      "Epoch 26, Batch 939, LR 1.124436 Loss 4.551435, Accuracy 90.356%\n",
      "Epoch 26, Batch 940, LR 1.124303 Loss 4.551289, Accuracy 90.356%\n",
      "Epoch 26, Batch 941, LR 1.124170 Loss 4.551209, Accuracy 90.356%\n",
      "Epoch 26, Batch 942, LR 1.124037 Loss 4.551625, Accuracy 90.352%\n",
      "Epoch 26, Batch 943, LR 1.123903 Loss 4.551589, Accuracy 90.355%\n",
      "Epoch 26, Batch 944, LR 1.123770 Loss 4.551027, Accuracy 90.357%\n",
      "Epoch 26, Batch 945, LR 1.123637 Loss 4.551037, Accuracy 90.356%\n",
      "Epoch 26, Batch 946, LR 1.123503 Loss 4.551310, Accuracy 90.354%\n",
      "Epoch 26, Batch 947, LR 1.123370 Loss 4.551543, Accuracy 90.354%\n",
      "Epoch 26, Batch 948, LR 1.123237 Loss 4.552304, Accuracy 90.351%\n",
      "Epoch 26, Batch 949, LR 1.123104 Loss 4.551943, Accuracy 90.352%\n",
      "Epoch 26, Batch 950, LR 1.122970 Loss 4.551511, Accuracy 90.357%\n",
      "Epoch 26, Batch 951, LR 1.122837 Loss 4.551630, Accuracy 90.356%\n",
      "Epoch 26, Batch 952, LR 1.122704 Loss 4.551931, Accuracy 90.356%\n",
      "Epoch 26, Batch 953, LR 1.122571 Loss 4.551326, Accuracy 90.357%\n",
      "Epoch 26, Batch 954, LR 1.122437 Loss 4.551002, Accuracy 90.357%\n",
      "Epoch 26, Batch 955, LR 1.122304 Loss 4.550820, Accuracy 90.358%\n",
      "Epoch 26, Batch 956, LR 1.122171 Loss 4.551718, Accuracy 90.354%\n",
      "Epoch 26, Batch 957, LR 1.122038 Loss 4.552127, Accuracy 90.350%\n",
      "Epoch 26, Batch 958, LR 1.121904 Loss 4.551859, Accuracy 90.353%\n",
      "Epoch 26, Batch 959, LR 1.121771 Loss 4.551716, Accuracy 90.353%\n",
      "Epoch 26, Batch 960, LR 1.121638 Loss 4.552000, Accuracy 90.352%\n",
      "Epoch 26, Batch 961, LR 1.121505 Loss 4.552368, Accuracy 90.348%\n",
      "Epoch 26, Batch 962, LR 1.121371 Loss 4.552371, Accuracy 90.346%\n",
      "Epoch 26, Batch 963, LR 1.121238 Loss 4.552525, Accuracy 90.344%\n",
      "Epoch 26, Batch 964, LR 1.121105 Loss 4.552061, Accuracy 90.348%\n",
      "Epoch 26, Batch 965, LR 1.120972 Loss 4.552281, Accuracy 90.345%\n",
      "Epoch 26, Batch 966, LR 1.120838 Loss 4.552500, Accuracy 90.346%\n",
      "Epoch 26, Batch 967, LR 1.120705 Loss 4.552239, Accuracy 90.344%\n",
      "Epoch 26, Batch 968, LR 1.120572 Loss 4.552615, Accuracy 90.344%\n",
      "Epoch 26, Batch 969, LR 1.120439 Loss 4.552094, Accuracy 90.346%\n",
      "Epoch 26, Batch 970, LR 1.120306 Loss 4.552539, Accuracy 90.346%\n",
      "Epoch 26, Batch 971, LR 1.120172 Loss 4.552467, Accuracy 90.348%\n",
      "Epoch 26, Batch 972, LR 1.120039 Loss 4.552890, Accuracy 90.347%\n",
      "Epoch 26, Batch 973, LR 1.119906 Loss 4.552816, Accuracy 90.346%\n",
      "Epoch 26, Batch 974, LR 1.119773 Loss 4.552579, Accuracy 90.347%\n",
      "Epoch 26, Batch 975, LR 1.119639 Loss 4.552096, Accuracy 90.345%\n",
      "Epoch 26, Batch 976, LR 1.119506 Loss 4.552029, Accuracy 90.345%\n",
      "Epoch 26, Batch 977, LR 1.119373 Loss 4.552142, Accuracy 90.346%\n",
      "Epoch 26, Batch 978, LR 1.119240 Loss 4.552640, Accuracy 90.347%\n",
      "Epoch 26, Batch 979, LR 1.119107 Loss 4.552615, Accuracy 90.347%\n",
      "Epoch 26, Batch 980, LR 1.118973 Loss 4.552204, Accuracy 90.354%\n",
      "Epoch 26, Batch 981, LR 1.118840 Loss 4.551781, Accuracy 90.356%\n",
      "Epoch 26, Batch 982, LR 1.118707 Loss 4.551618, Accuracy 90.354%\n",
      "Epoch 26, Batch 983, LR 1.118574 Loss 4.552077, Accuracy 90.352%\n",
      "Epoch 26, Batch 984, LR 1.118440 Loss 4.552534, Accuracy 90.353%\n",
      "Epoch 26, Batch 985, LR 1.118307 Loss 4.553095, Accuracy 90.351%\n",
      "Epoch 26, Batch 986, LR 1.118174 Loss 4.553008, Accuracy 90.351%\n",
      "Epoch 26, Batch 987, LR 1.118041 Loss 4.552621, Accuracy 90.353%\n",
      "Epoch 26, Batch 988, LR 1.117908 Loss 4.553074, Accuracy 90.351%\n",
      "Epoch 26, Batch 989, LR 1.117774 Loss 4.553366, Accuracy 90.352%\n",
      "Epoch 26, Batch 990, LR 1.117641 Loss 4.553119, Accuracy 90.350%\n",
      "Epoch 26, Batch 991, LR 1.117508 Loss 4.552525, Accuracy 90.351%\n",
      "Epoch 26, Batch 992, LR 1.117375 Loss 4.552823, Accuracy 90.350%\n",
      "Epoch 26, Batch 993, LR 1.117242 Loss 4.552709, Accuracy 90.350%\n",
      "Epoch 26, Batch 994, LR 1.117108 Loss 4.553029, Accuracy 90.348%\n",
      "Epoch 26, Batch 995, LR 1.116975 Loss 4.553158, Accuracy 90.345%\n",
      "Epoch 26, Batch 996, LR 1.116842 Loss 4.553472, Accuracy 90.345%\n",
      "Epoch 26, Batch 997, LR 1.116709 Loss 4.553275, Accuracy 90.344%\n",
      "Epoch 26, Batch 998, LR 1.116576 Loss 4.553625, Accuracy 90.342%\n",
      "Epoch 26, Batch 999, LR 1.116443 Loss 4.553127, Accuracy 90.346%\n",
      "Epoch 26, Batch 1000, LR 1.116309 Loss 4.552851, Accuracy 90.348%\n",
      "Epoch 26, Batch 1001, LR 1.116176 Loss 4.552816, Accuracy 90.348%\n",
      "Epoch 26, Batch 1002, LR 1.116043 Loss 4.553669, Accuracy 90.342%\n",
      "Epoch 26, Batch 1003, LR 1.115910 Loss 4.554549, Accuracy 90.336%\n",
      "Epoch 26, Batch 1004, LR 1.115777 Loss 4.554599, Accuracy 90.336%\n",
      "Epoch 26, Batch 1005, LR 1.115643 Loss 4.554481, Accuracy 90.338%\n",
      "Epoch 26, Batch 1006, LR 1.115510 Loss 4.554560, Accuracy 90.339%\n",
      "Epoch 26, Batch 1007, LR 1.115377 Loss 4.554623, Accuracy 90.338%\n",
      "Epoch 26, Batch 1008, LR 1.115244 Loss 4.554163, Accuracy 90.337%\n",
      "Epoch 26, Batch 1009, LR 1.115111 Loss 4.554491, Accuracy 90.332%\n",
      "Epoch 26, Batch 1010, LR 1.114978 Loss 4.553984, Accuracy 90.333%\n",
      "Epoch 26, Batch 1011, LR 1.114844 Loss 4.554259, Accuracy 90.333%\n",
      "Epoch 26, Batch 1012, LR 1.114711 Loss 4.553972, Accuracy 90.337%\n",
      "Epoch 26, Batch 1013, LR 1.114578 Loss 4.553592, Accuracy 90.339%\n",
      "Epoch 26, Batch 1014, LR 1.114445 Loss 4.552795, Accuracy 90.341%\n",
      "Epoch 26, Batch 1015, LR 1.114312 Loss 4.552466, Accuracy 90.343%\n",
      "Epoch 26, Batch 1016, LR 1.114179 Loss 4.552780, Accuracy 90.344%\n",
      "Epoch 26, Batch 1017, LR 1.114045 Loss 4.552785, Accuracy 90.342%\n",
      "Epoch 26, Batch 1018, LR 1.113912 Loss 4.552566, Accuracy 90.344%\n",
      "Epoch 26, Batch 1019, LR 1.113779 Loss 4.552026, Accuracy 90.345%\n",
      "Epoch 26, Batch 1020, LR 1.113646 Loss 4.552056, Accuracy 90.346%\n",
      "Epoch 26, Batch 1021, LR 1.113513 Loss 4.552272, Accuracy 90.347%\n",
      "Epoch 26, Batch 1022, LR 1.113380 Loss 4.551608, Accuracy 90.345%\n",
      "Epoch 26, Batch 1023, LR 1.113246 Loss 4.551471, Accuracy 90.344%\n",
      "Epoch 26, Batch 1024, LR 1.113113 Loss 4.551001, Accuracy 90.347%\n",
      "Epoch 26, Batch 1025, LR 1.112980 Loss 4.550978, Accuracy 90.347%\n",
      "Epoch 26, Batch 1026, LR 1.112847 Loss 4.550791, Accuracy 90.346%\n",
      "Epoch 26, Batch 1027, LR 1.112714 Loss 4.551177, Accuracy 90.344%\n",
      "Epoch 26, Batch 1028, LR 1.112581 Loss 4.551045, Accuracy 90.344%\n",
      "Epoch 26, Batch 1029, LR 1.112448 Loss 4.550528, Accuracy 90.347%\n",
      "Epoch 26, Batch 1030, LR 1.112314 Loss 4.550301, Accuracy 90.347%\n",
      "Epoch 26, Batch 1031, LR 1.112181 Loss 4.550336, Accuracy 90.350%\n",
      "Epoch 26, Batch 1032, LR 1.112048 Loss 4.550481, Accuracy 90.349%\n",
      "Epoch 26, Batch 1033, LR 1.111915 Loss 4.549919, Accuracy 90.351%\n",
      "Epoch 26, Batch 1034, LR 1.111782 Loss 4.550396, Accuracy 90.348%\n",
      "Epoch 26, Batch 1035, LR 1.111649 Loss 4.550497, Accuracy 90.347%\n",
      "Epoch 26, Batch 1036, LR 1.111516 Loss 4.550163, Accuracy 90.350%\n",
      "Epoch 26, Batch 1037, LR 1.111383 Loss 4.550258, Accuracy 90.350%\n",
      "Epoch 26, Batch 1038, LR 1.111249 Loss 4.550142, Accuracy 90.348%\n",
      "Epoch 26, Batch 1039, LR 1.111116 Loss 4.550024, Accuracy 90.350%\n",
      "Epoch 26, Batch 1040, LR 1.110983 Loss 4.549868, Accuracy 90.350%\n",
      "Epoch 26, Batch 1041, LR 1.110850 Loss 4.550172, Accuracy 90.350%\n",
      "Epoch 26, Batch 1042, LR 1.110717 Loss 4.549744, Accuracy 90.348%\n",
      "Epoch 26, Batch 1043, LR 1.110584 Loss 4.550628, Accuracy 90.338%\n",
      "Epoch 26, Batch 1044, LR 1.110451 Loss 4.550415, Accuracy 90.337%\n",
      "Epoch 26, Batch 1045, LR 1.110318 Loss 4.550386, Accuracy 90.338%\n",
      "Epoch 26, Batch 1046, LR 1.110184 Loss 4.550620, Accuracy 90.334%\n",
      "Epoch 26, Batch 1047, LR 1.110051 Loss 4.550463, Accuracy 90.336%\n",
      "Epoch 26, Loss (train set) 4.550463, Accuracy (train set) 90.336%\n",
      "Epoch 27, Batch 1, LR 1.109918 Loss 4.317426, Accuracy 93.750%\n",
      "Epoch 27, Batch 2, LR 1.109785 Loss 4.371525, Accuracy 93.359%\n",
      "Epoch 27, Batch 3, LR 1.109652 Loss 4.395801, Accuracy 92.708%\n",
      "Epoch 27, Batch 4, LR 1.109519 Loss 4.456776, Accuracy 91.797%\n",
      "Epoch 27, Batch 5, LR 1.109386 Loss 4.449359, Accuracy 91.406%\n",
      "Epoch 27, Batch 6, LR 1.109253 Loss 4.349272, Accuracy 91.927%\n",
      "Epoch 27, Batch 7, LR 1.109120 Loss 4.337792, Accuracy 91.853%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 8, LR 1.108987 Loss 4.378124, Accuracy 91.895%\n",
      "Epoch 27, Batch 9, LR 1.108853 Loss 4.418808, Accuracy 91.493%\n",
      "Epoch 27, Batch 10, LR 1.108720 Loss 4.392572, Accuracy 91.562%\n",
      "Epoch 27, Batch 11, LR 1.108587 Loss 4.386147, Accuracy 91.548%\n",
      "Epoch 27, Batch 12, LR 1.108454 Loss 4.413906, Accuracy 91.211%\n",
      "Epoch 27, Batch 13, LR 1.108321 Loss 4.392764, Accuracy 91.166%\n",
      "Epoch 27, Batch 14, LR 1.108188 Loss 4.366240, Accuracy 91.295%\n",
      "Epoch 27, Batch 15, LR 1.108055 Loss 4.382392, Accuracy 91.198%\n",
      "Epoch 27, Batch 16, LR 1.107922 Loss 4.418437, Accuracy 91.211%\n",
      "Epoch 27, Batch 17, LR 1.107789 Loss 4.389971, Accuracy 91.360%\n",
      "Epoch 27, Batch 18, LR 1.107656 Loss 4.358381, Accuracy 91.363%\n",
      "Epoch 27, Batch 19, LR 1.107523 Loss 4.346874, Accuracy 91.406%\n",
      "Epoch 27, Batch 20, LR 1.107389 Loss 4.365594, Accuracy 91.289%\n",
      "Epoch 27, Batch 21, LR 1.107256 Loss 4.396261, Accuracy 91.257%\n",
      "Epoch 27, Batch 22, LR 1.107123 Loss 4.359278, Accuracy 91.442%\n",
      "Epoch 27, Batch 23, LR 1.106990 Loss 4.367949, Accuracy 91.236%\n",
      "Epoch 27, Batch 24, LR 1.106857 Loss 4.355041, Accuracy 91.309%\n",
      "Epoch 27, Batch 25, LR 1.106724 Loss 4.353102, Accuracy 91.219%\n",
      "Epoch 27, Batch 26, LR 1.106591 Loss 4.305255, Accuracy 91.406%\n",
      "Epoch 27, Batch 27, LR 1.106458 Loss 4.316661, Accuracy 91.348%\n",
      "Epoch 27, Batch 28, LR 1.106325 Loss 4.346009, Accuracy 91.127%\n",
      "Epoch 27, Batch 29, LR 1.106192 Loss 4.359172, Accuracy 91.083%\n",
      "Epoch 27, Batch 30, LR 1.106059 Loss 4.384542, Accuracy 91.016%\n",
      "Epoch 27, Batch 31, LR 1.105926 Loss 4.353295, Accuracy 91.205%\n",
      "Epoch 27, Batch 32, LR 1.105793 Loss 4.342756, Accuracy 91.260%\n",
      "Epoch 27, Batch 33, LR 1.105660 Loss 4.356002, Accuracy 91.241%\n",
      "Epoch 27, Batch 34, LR 1.105527 Loss 4.359432, Accuracy 91.314%\n",
      "Epoch 27, Batch 35, LR 1.105393 Loss 4.377126, Accuracy 91.362%\n",
      "Epoch 27, Batch 36, LR 1.105260 Loss 4.388652, Accuracy 91.341%\n",
      "Epoch 27, Batch 37, LR 1.105127 Loss 4.386899, Accuracy 91.343%\n",
      "Epoch 27, Batch 38, LR 1.104994 Loss 4.391347, Accuracy 91.324%\n",
      "Epoch 27, Batch 39, LR 1.104861 Loss 4.379138, Accuracy 91.366%\n",
      "Epoch 27, Batch 40, LR 1.104728 Loss 4.373199, Accuracy 91.426%\n",
      "Epoch 27, Batch 41, LR 1.104595 Loss 4.387252, Accuracy 91.254%\n",
      "Epoch 27, Batch 42, LR 1.104462 Loss 4.368725, Accuracy 91.369%\n",
      "Epoch 27, Batch 43, LR 1.104329 Loss 4.364148, Accuracy 91.334%\n",
      "Epoch 27, Batch 44, LR 1.104196 Loss 4.368191, Accuracy 91.335%\n",
      "Epoch 27, Batch 45, LR 1.104063 Loss 4.370656, Accuracy 91.319%\n",
      "Epoch 27, Batch 46, LR 1.103930 Loss 4.376803, Accuracy 91.270%\n",
      "Epoch 27, Batch 47, LR 1.103797 Loss 4.375585, Accuracy 91.174%\n",
      "Epoch 27, Batch 48, LR 1.103664 Loss 4.370485, Accuracy 91.130%\n",
      "Epoch 27, Batch 49, LR 1.103531 Loss 4.378999, Accuracy 91.135%\n",
      "Epoch 27, Batch 50, LR 1.103398 Loss 4.385114, Accuracy 91.062%\n",
      "Epoch 27, Batch 51, LR 1.103265 Loss 4.386880, Accuracy 91.085%\n",
      "Epoch 27, Batch 52, LR 1.103132 Loss 4.396355, Accuracy 90.986%\n",
      "Epoch 27, Batch 53, LR 1.102999 Loss 4.380764, Accuracy 91.082%\n",
      "Epoch 27, Batch 54, LR 1.102866 Loss 4.372693, Accuracy 91.102%\n",
      "Epoch 27, Batch 55, LR 1.102733 Loss 4.369986, Accuracy 91.122%\n",
      "Epoch 27, Batch 56, LR 1.102600 Loss 4.373170, Accuracy 91.071%\n",
      "Epoch 27, Batch 57, LR 1.102467 Loss 4.387343, Accuracy 91.091%\n",
      "Epoch 27, Batch 58, LR 1.102334 Loss 4.389527, Accuracy 91.083%\n",
      "Epoch 27, Batch 59, LR 1.102201 Loss 4.390536, Accuracy 91.062%\n",
      "Epoch 27, Batch 60, LR 1.102068 Loss 4.378559, Accuracy 91.133%\n",
      "Epoch 27, Batch 61, LR 1.101935 Loss 4.386799, Accuracy 91.086%\n",
      "Epoch 27, Batch 62, LR 1.101802 Loss 4.376919, Accuracy 91.066%\n",
      "Epoch 27, Batch 63, LR 1.101669 Loss 4.363897, Accuracy 91.084%\n",
      "Epoch 27, Batch 64, LR 1.101536 Loss 4.368808, Accuracy 91.028%\n",
      "Epoch 27, Batch 65, LR 1.101403 Loss 4.368473, Accuracy 91.046%\n",
      "Epoch 27, Batch 66, LR 1.101270 Loss 4.368850, Accuracy 91.051%\n",
      "Epoch 27, Batch 67, LR 1.101137 Loss 4.369261, Accuracy 91.045%\n",
      "Epoch 27, Batch 68, LR 1.101004 Loss 4.366270, Accuracy 91.085%\n",
      "Epoch 27, Batch 69, LR 1.100871 Loss 4.364296, Accuracy 91.123%\n",
      "Epoch 27, Batch 70, LR 1.100738 Loss 4.362559, Accuracy 91.150%\n",
      "Epoch 27, Batch 71, LR 1.100605 Loss 4.364583, Accuracy 91.109%\n",
      "Epoch 27, Batch 72, LR 1.100472 Loss 4.356927, Accuracy 91.146%\n",
      "Epoch 27, Batch 73, LR 1.100339 Loss 4.351627, Accuracy 91.160%\n",
      "Epoch 27, Batch 74, LR 1.100206 Loss 4.355191, Accuracy 91.163%\n",
      "Epoch 27, Batch 75, LR 1.100073 Loss 4.356492, Accuracy 91.177%\n",
      "Epoch 27, Batch 76, LR 1.099940 Loss 4.356641, Accuracy 91.149%\n",
      "Epoch 27, Batch 77, LR 1.099807 Loss 4.352522, Accuracy 91.153%\n",
      "Epoch 27, Batch 78, LR 1.099674 Loss 4.364705, Accuracy 91.066%\n",
      "Epoch 27, Batch 79, LR 1.099541 Loss 4.372477, Accuracy 91.021%\n",
      "Epoch 27, Batch 80, LR 1.099408 Loss 4.373335, Accuracy 91.006%\n",
      "Epoch 27, Batch 81, LR 1.099275 Loss 4.374116, Accuracy 91.011%\n",
      "Epoch 27, Batch 82, LR 1.099142 Loss 4.364212, Accuracy 91.054%\n",
      "Epoch 27, Batch 83, LR 1.099009 Loss 4.366758, Accuracy 91.030%\n",
      "Epoch 27, Batch 84, LR 1.098876 Loss 4.372029, Accuracy 91.034%\n",
      "Epoch 27, Batch 85, LR 1.098743 Loss 4.375595, Accuracy 91.020%\n",
      "Epoch 27, Batch 86, LR 1.098610 Loss 4.373398, Accuracy 91.025%\n",
      "Epoch 27, Batch 87, LR 1.098477 Loss 4.379165, Accuracy 91.002%\n",
      "Epoch 27, Batch 88, LR 1.098344 Loss 4.382430, Accuracy 90.989%\n",
      "Epoch 27, Batch 89, LR 1.098211 Loss 4.384697, Accuracy 90.985%\n",
      "Epoch 27, Batch 90, LR 1.098078 Loss 4.385365, Accuracy 90.998%\n",
      "Epoch 27, Batch 91, LR 1.097945 Loss 4.387580, Accuracy 90.943%\n",
      "Epoch 27, Batch 92, LR 1.097812 Loss 4.388985, Accuracy 90.956%\n",
      "Epoch 27, Batch 93, LR 1.097679 Loss 4.387743, Accuracy 90.953%\n",
      "Epoch 27, Batch 94, LR 1.097546 Loss 4.385427, Accuracy 90.974%\n",
      "Epoch 27, Batch 95, LR 1.097413 Loss 4.384399, Accuracy 90.970%\n",
      "Epoch 27, Batch 96, LR 1.097280 Loss 4.383402, Accuracy 90.983%\n",
      "Epoch 27, Batch 97, LR 1.097147 Loss 4.381711, Accuracy 90.963%\n",
      "Epoch 27, Batch 98, LR 1.097014 Loss 4.383043, Accuracy 90.944%\n",
      "Epoch 27, Batch 99, LR 1.096881 Loss 4.383913, Accuracy 90.949%\n",
      "Epoch 27, Batch 100, LR 1.096749 Loss 4.385621, Accuracy 90.969%\n",
      "Epoch 27, Batch 101, LR 1.096616 Loss 4.388808, Accuracy 90.965%\n",
      "Epoch 27, Batch 102, LR 1.096483 Loss 4.392217, Accuracy 90.970%\n",
      "Epoch 27, Batch 103, LR 1.096350 Loss 4.390095, Accuracy 90.981%\n",
      "Epoch 27, Batch 104, LR 1.096217 Loss 4.396167, Accuracy 90.918%\n",
      "Epoch 27, Batch 105, LR 1.096084 Loss 4.399959, Accuracy 90.930%\n",
      "Epoch 27, Batch 106, LR 1.095951 Loss 4.398669, Accuracy 90.912%\n",
      "Epoch 27, Batch 107, LR 1.095818 Loss 4.401556, Accuracy 90.895%\n",
      "Epoch 27, Batch 108, LR 1.095685 Loss 4.397128, Accuracy 90.922%\n",
      "Epoch 27, Batch 109, LR 1.095552 Loss 4.404066, Accuracy 90.897%\n",
      "Epoch 27, Batch 110, LR 1.095419 Loss 4.402239, Accuracy 90.902%\n",
      "Epoch 27, Batch 111, LR 1.095286 Loss 4.396208, Accuracy 90.907%\n",
      "Epoch 27, Batch 112, LR 1.095153 Loss 4.399311, Accuracy 90.911%\n",
      "Epoch 27, Batch 113, LR 1.095020 Loss 4.398570, Accuracy 90.908%\n",
      "Epoch 27, Batch 114, LR 1.094888 Loss 4.400542, Accuracy 90.879%\n",
      "Epoch 27, Batch 115, LR 1.094755 Loss 4.399352, Accuracy 90.883%\n",
      "Epoch 27, Batch 116, LR 1.094622 Loss 4.397094, Accuracy 90.867%\n",
      "Epoch 27, Batch 117, LR 1.094489 Loss 4.396778, Accuracy 90.859%\n",
      "Epoch 27, Batch 118, LR 1.094356 Loss 4.393283, Accuracy 90.863%\n",
      "Epoch 27, Batch 119, LR 1.094223 Loss 4.392601, Accuracy 90.874%\n",
      "Epoch 27, Batch 120, LR 1.094090 Loss 4.396873, Accuracy 90.846%\n",
      "Epoch 27, Batch 121, LR 1.093957 Loss 4.400952, Accuracy 90.806%\n",
      "Epoch 27, Batch 122, LR 1.093824 Loss 4.394661, Accuracy 90.843%\n",
      "Epoch 27, Batch 123, LR 1.093691 Loss 4.397350, Accuracy 90.809%\n",
      "Epoch 27, Batch 124, LR 1.093558 Loss 4.399863, Accuracy 90.820%\n",
      "Epoch 27, Batch 125, LR 1.093426 Loss 4.398258, Accuracy 90.806%\n",
      "Epoch 27, Batch 126, LR 1.093293 Loss 4.395070, Accuracy 90.830%\n",
      "Epoch 27, Batch 127, LR 1.093160 Loss 4.390306, Accuracy 90.865%\n",
      "Epoch 27, Batch 128, LR 1.093027 Loss 4.390258, Accuracy 90.881%\n",
      "Epoch 27, Batch 129, LR 1.092894 Loss 4.394767, Accuracy 90.861%\n",
      "Epoch 27, Batch 130, LR 1.092761 Loss 4.394216, Accuracy 90.829%\n",
      "Epoch 27, Batch 131, LR 1.092628 Loss 4.394731, Accuracy 90.810%\n",
      "Epoch 27, Batch 132, LR 1.092495 Loss 4.391659, Accuracy 90.832%\n",
      "Epoch 27, Batch 133, LR 1.092362 Loss 4.387280, Accuracy 90.848%\n",
      "Epoch 27, Batch 134, LR 1.092230 Loss 4.395017, Accuracy 90.817%\n",
      "Epoch 27, Batch 135, LR 1.092097 Loss 4.394602, Accuracy 90.816%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 136, LR 1.091964 Loss 4.396093, Accuracy 90.797%\n",
      "Epoch 27, Batch 137, LR 1.091831 Loss 4.389694, Accuracy 90.830%\n",
      "Epoch 27, Batch 138, LR 1.091698 Loss 4.387967, Accuracy 90.834%\n",
      "Epoch 27, Batch 139, LR 1.091565 Loss 4.387139, Accuracy 90.833%\n",
      "Epoch 27, Batch 140, LR 1.091432 Loss 4.386011, Accuracy 90.848%\n",
      "Epoch 27, Batch 141, LR 1.091299 Loss 4.385605, Accuracy 90.847%\n",
      "Epoch 27, Batch 142, LR 1.091167 Loss 4.383712, Accuracy 90.856%\n",
      "Epoch 27, Batch 143, LR 1.091034 Loss 4.382037, Accuracy 90.871%\n",
      "Epoch 27, Batch 144, LR 1.090901 Loss 4.387675, Accuracy 90.864%\n",
      "Epoch 27, Batch 145, LR 1.090768 Loss 4.387216, Accuracy 90.862%\n",
      "Epoch 27, Batch 146, LR 1.090635 Loss 4.391269, Accuracy 90.855%\n",
      "Epoch 27, Batch 147, LR 1.090502 Loss 4.391448, Accuracy 90.854%\n",
      "Epoch 27, Batch 148, LR 1.090369 Loss 4.391095, Accuracy 90.852%\n",
      "Epoch 27, Batch 149, LR 1.090237 Loss 4.392073, Accuracy 90.840%\n",
      "Epoch 27, Batch 150, LR 1.090104 Loss 4.391453, Accuracy 90.833%\n",
      "Epoch 27, Batch 151, LR 1.089971 Loss 4.395061, Accuracy 90.827%\n",
      "Epoch 27, Batch 152, LR 1.089838 Loss 4.393975, Accuracy 90.841%\n",
      "Epoch 27, Batch 153, LR 1.089705 Loss 4.399519, Accuracy 90.809%\n",
      "Epoch 27, Batch 154, LR 1.089572 Loss 4.395383, Accuracy 90.808%\n",
      "Epoch 27, Batch 155, LR 1.089439 Loss 4.394627, Accuracy 90.817%\n",
      "Epoch 27, Batch 156, LR 1.089307 Loss 4.390713, Accuracy 90.825%\n",
      "Epoch 27, Batch 157, LR 1.089174 Loss 4.387762, Accuracy 90.814%\n",
      "Epoch 27, Batch 158, LR 1.089041 Loss 4.389456, Accuracy 90.808%\n",
      "Epoch 27, Batch 159, LR 1.088908 Loss 4.391201, Accuracy 90.787%\n",
      "Epoch 27, Batch 160, LR 1.088775 Loss 4.390074, Accuracy 90.791%\n",
      "Epoch 27, Batch 161, LR 1.088642 Loss 4.394418, Accuracy 90.761%\n",
      "Epoch 27, Batch 162, LR 1.088510 Loss 4.394094, Accuracy 90.760%\n",
      "Epoch 27, Batch 163, LR 1.088377 Loss 4.399414, Accuracy 90.730%\n",
      "Epoch 27, Batch 164, LR 1.088244 Loss 4.400929, Accuracy 90.735%\n",
      "Epoch 27, Batch 165, LR 1.088111 Loss 4.397616, Accuracy 90.762%\n",
      "Epoch 27, Batch 166, LR 1.087978 Loss 4.397086, Accuracy 90.752%\n",
      "Epoch 27, Batch 167, LR 1.087845 Loss 4.397525, Accuracy 90.751%\n",
      "Epoch 27, Batch 168, LR 1.087713 Loss 4.394124, Accuracy 90.765%\n",
      "Epoch 27, Batch 169, LR 1.087580 Loss 4.395895, Accuracy 90.759%\n",
      "Epoch 27, Batch 170, LR 1.087447 Loss 4.397208, Accuracy 90.744%\n",
      "Epoch 27, Batch 171, LR 1.087314 Loss 4.397848, Accuracy 90.753%\n",
      "Epoch 27, Batch 172, LR 1.087181 Loss 4.397923, Accuracy 90.752%\n",
      "Epoch 27, Batch 173, LR 1.087049 Loss 4.397974, Accuracy 90.765%\n",
      "Epoch 27, Batch 174, LR 1.086916 Loss 4.397420, Accuracy 90.791%\n",
      "Epoch 27, Batch 175, LR 1.086783 Loss 4.395620, Accuracy 90.804%\n",
      "Epoch 27, Batch 176, LR 1.086650 Loss 4.393479, Accuracy 90.816%\n",
      "Epoch 27, Batch 177, LR 1.086517 Loss 4.393991, Accuracy 90.806%\n",
      "Epoch 27, Batch 178, LR 1.086385 Loss 4.396945, Accuracy 90.783%\n",
      "Epoch 27, Batch 179, LR 1.086252 Loss 4.401678, Accuracy 90.760%\n",
      "Epoch 27, Batch 180, LR 1.086119 Loss 4.404250, Accuracy 90.747%\n",
      "Epoch 27, Batch 181, LR 1.085986 Loss 4.402486, Accuracy 90.754%\n",
      "Epoch 27, Batch 182, LR 1.085853 Loss 4.400623, Accuracy 90.754%\n",
      "Epoch 27, Batch 183, LR 1.085721 Loss 4.399145, Accuracy 90.766%\n",
      "Epoch 27, Batch 184, LR 1.085588 Loss 4.399952, Accuracy 90.765%\n",
      "Epoch 27, Batch 185, LR 1.085455 Loss 4.400761, Accuracy 90.752%\n",
      "Epoch 27, Batch 186, LR 1.085322 Loss 4.401130, Accuracy 90.759%\n",
      "Epoch 27, Batch 187, LR 1.085189 Loss 4.398605, Accuracy 90.780%\n",
      "Epoch 27, Batch 188, LR 1.085057 Loss 4.401282, Accuracy 90.775%\n",
      "Epoch 27, Batch 189, LR 1.084924 Loss 4.403591, Accuracy 90.786%\n",
      "Epoch 27, Batch 190, LR 1.084791 Loss 4.405470, Accuracy 90.798%\n",
      "Epoch 27, Batch 191, LR 1.084658 Loss 4.407504, Accuracy 90.797%\n",
      "Epoch 27, Batch 192, LR 1.084525 Loss 4.412124, Accuracy 90.788%\n",
      "Epoch 27, Batch 193, LR 1.084393 Loss 4.413622, Accuracy 90.767%\n",
      "Epoch 27, Batch 194, LR 1.084260 Loss 4.410990, Accuracy 90.786%\n",
      "Epoch 27, Batch 195, LR 1.084127 Loss 4.409138, Accuracy 90.797%\n",
      "Epoch 27, Batch 196, LR 1.083994 Loss 4.406785, Accuracy 90.804%\n",
      "Epoch 27, Batch 197, LR 1.083862 Loss 4.406011, Accuracy 90.796%\n",
      "Epoch 27, Batch 198, LR 1.083729 Loss 4.405907, Accuracy 90.791%\n",
      "Epoch 27, Batch 199, LR 1.083596 Loss 4.408155, Accuracy 90.770%\n",
      "Epoch 27, Batch 200, LR 1.083463 Loss 4.407429, Accuracy 90.770%\n",
      "Epoch 27, Batch 201, LR 1.083331 Loss 4.409211, Accuracy 90.757%\n",
      "Epoch 27, Batch 202, LR 1.083198 Loss 4.409552, Accuracy 90.756%\n",
      "Epoch 27, Batch 203, LR 1.083065 Loss 4.409764, Accuracy 90.733%\n",
      "Epoch 27, Batch 204, LR 1.082932 Loss 4.410310, Accuracy 90.728%\n",
      "Epoch 27, Batch 205, LR 1.082800 Loss 4.408068, Accuracy 90.736%\n",
      "Epoch 27, Batch 206, LR 1.082667 Loss 4.409292, Accuracy 90.720%\n",
      "Epoch 27, Batch 207, LR 1.082534 Loss 4.409145, Accuracy 90.719%\n",
      "Epoch 27, Batch 208, LR 1.082401 Loss 4.408081, Accuracy 90.726%\n",
      "Epoch 27, Batch 209, LR 1.082269 Loss 4.408889, Accuracy 90.715%\n",
      "Epoch 27, Batch 210, LR 1.082136 Loss 4.409953, Accuracy 90.722%\n",
      "Epoch 27, Batch 211, LR 1.082003 Loss 4.411672, Accuracy 90.703%\n",
      "Epoch 27, Batch 212, LR 1.081870 Loss 4.414539, Accuracy 90.684%\n",
      "Epoch 27, Batch 213, LR 1.081738 Loss 4.413981, Accuracy 90.669%\n",
      "Epoch 27, Batch 214, LR 1.081605 Loss 4.414921, Accuracy 90.672%\n",
      "Epoch 27, Batch 215, LR 1.081472 Loss 4.410713, Accuracy 90.687%\n",
      "Epoch 27, Batch 216, LR 1.081339 Loss 4.410496, Accuracy 90.679%\n",
      "Epoch 27, Batch 217, LR 1.081207 Loss 4.410793, Accuracy 90.686%\n",
      "Epoch 27, Batch 218, LR 1.081074 Loss 4.409971, Accuracy 90.682%\n",
      "Epoch 27, Batch 219, LR 1.080941 Loss 4.407822, Accuracy 90.703%\n",
      "Epoch 27, Batch 220, LR 1.080809 Loss 4.407267, Accuracy 90.710%\n",
      "Epoch 27, Batch 221, LR 1.080676 Loss 4.407121, Accuracy 90.717%\n",
      "Epoch 27, Batch 222, LR 1.080543 Loss 4.410142, Accuracy 90.706%\n",
      "Epoch 27, Batch 223, LR 1.080410 Loss 4.411974, Accuracy 90.702%\n",
      "Epoch 27, Batch 224, LR 1.080278 Loss 4.411589, Accuracy 90.702%\n",
      "Epoch 27, Batch 225, LR 1.080145 Loss 4.408380, Accuracy 90.722%\n",
      "Epoch 27, Batch 226, LR 1.080012 Loss 4.412356, Accuracy 90.698%\n",
      "Epoch 27, Batch 227, LR 1.079880 Loss 4.414184, Accuracy 90.687%\n",
      "Epoch 27, Batch 228, LR 1.079747 Loss 4.413893, Accuracy 90.690%\n",
      "Epoch 27, Batch 229, LR 1.079614 Loss 4.412182, Accuracy 90.693%\n",
      "Epoch 27, Batch 230, LR 1.079481 Loss 4.408666, Accuracy 90.696%\n",
      "Epoch 27, Batch 231, LR 1.079349 Loss 4.406783, Accuracy 90.710%\n",
      "Epoch 27, Batch 232, LR 1.079216 Loss 4.406561, Accuracy 90.719%\n",
      "Epoch 27, Batch 233, LR 1.079083 Loss 4.405279, Accuracy 90.729%\n",
      "Epoch 27, Batch 234, LR 1.078951 Loss 4.405135, Accuracy 90.735%\n",
      "Epoch 27, Batch 235, LR 1.078818 Loss 4.404749, Accuracy 90.745%\n",
      "Epoch 27, Batch 236, LR 1.078685 Loss 4.406908, Accuracy 90.738%\n",
      "Epoch 27, Batch 237, LR 1.078553 Loss 4.407702, Accuracy 90.727%\n",
      "Epoch 27, Batch 238, LR 1.078420 Loss 4.414133, Accuracy 90.687%\n",
      "Epoch 27, Batch 239, LR 1.078287 Loss 4.411725, Accuracy 90.700%\n",
      "Epoch 27, Batch 240, LR 1.078155 Loss 4.411800, Accuracy 90.703%\n",
      "Epoch 27, Batch 241, LR 1.078022 Loss 4.411224, Accuracy 90.719%\n",
      "Epoch 27, Batch 242, LR 1.077889 Loss 4.410577, Accuracy 90.719%\n",
      "Epoch 27, Batch 243, LR 1.077757 Loss 4.409547, Accuracy 90.718%\n",
      "Epoch 27, Batch 244, LR 1.077624 Loss 4.408970, Accuracy 90.718%\n",
      "Epoch 27, Batch 245, LR 1.077491 Loss 4.408829, Accuracy 90.721%\n",
      "Epoch 27, Batch 246, LR 1.077358 Loss 4.407916, Accuracy 90.733%\n",
      "Epoch 27, Batch 247, LR 1.077226 Loss 4.406559, Accuracy 90.745%\n",
      "Epoch 27, Batch 248, LR 1.077093 Loss 4.404530, Accuracy 90.748%\n",
      "Epoch 27, Batch 249, LR 1.076960 Loss 4.404541, Accuracy 90.751%\n",
      "Epoch 27, Batch 250, LR 1.076828 Loss 4.407251, Accuracy 90.737%\n",
      "Epoch 27, Batch 251, LR 1.076695 Loss 4.409420, Accuracy 90.721%\n",
      "Epoch 27, Batch 252, LR 1.076563 Loss 4.411352, Accuracy 90.724%\n",
      "Epoch 27, Batch 253, LR 1.076430 Loss 4.410409, Accuracy 90.730%\n",
      "Epoch 27, Batch 254, LR 1.076297 Loss 4.410901, Accuracy 90.723%\n",
      "Epoch 27, Batch 255, LR 1.076165 Loss 4.414667, Accuracy 90.723%\n",
      "Epoch 27, Batch 256, LR 1.076032 Loss 4.415541, Accuracy 90.735%\n",
      "Epoch 27, Batch 257, LR 1.075899 Loss 4.412615, Accuracy 90.750%\n",
      "Epoch 27, Batch 258, LR 1.075767 Loss 4.411611, Accuracy 90.767%\n",
      "Epoch 27, Batch 259, LR 1.075634 Loss 4.409461, Accuracy 90.764%\n",
      "Epoch 27, Batch 260, LR 1.075501 Loss 4.408921, Accuracy 90.766%\n",
      "Epoch 27, Batch 261, LR 1.075369 Loss 4.411492, Accuracy 90.760%\n",
      "Epoch 27, Batch 262, LR 1.075236 Loss 4.409408, Accuracy 90.762%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 263, LR 1.075103 Loss 4.407304, Accuracy 90.765%\n",
      "Epoch 27, Batch 264, LR 1.074971 Loss 4.407983, Accuracy 90.755%\n",
      "Epoch 27, Batch 265, LR 1.074838 Loss 4.406861, Accuracy 90.752%\n",
      "Epoch 27, Batch 266, LR 1.074706 Loss 4.406327, Accuracy 90.757%\n",
      "Epoch 27, Batch 267, LR 1.074573 Loss 4.407781, Accuracy 90.751%\n",
      "Epoch 27, Batch 268, LR 1.074440 Loss 4.409660, Accuracy 90.742%\n",
      "Epoch 27, Batch 269, LR 1.074308 Loss 4.412086, Accuracy 90.741%\n",
      "Epoch 27, Batch 270, LR 1.074175 Loss 4.414526, Accuracy 90.729%\n",
      "Epoch 27, Batch 271, LR 1.074042 Loss 4.414554, Accuracy 90.732%\n",
      "Epoch 27, Batch 272, LR 1.073910 Loss 4.414967, Accuracy 90.734%\n",
      "Epoch 27, Batch 273, LR 1.073777 Loss 4.413833, Accuracy 90.745%\n",
      "Epoch 27, Batch 274, LR 1.073645 Loss 4.415119, Accuracy 90.745%\n",
      "Epoch 27, Batch 275, LR 1.073512 Loss 4.414378, Accuracy 90.761%\n",
      "Epoch 27, Batch 276, LR 1.073379 Loss 4.415121, Accuracy 90.755%\n",
      "Epoch 27, Batch 277, LR 1.073247 Loss 4.413237, Accuracy 90.769%\n",
      "Epoch 27, Batch 278, LR 1.073114 Loss 4.415759, Accuracy 90.766%\n",
      "Epoch 27, Batch 279, LR 1.072981 Loss 4.413952, Accuracy 90.773%\n",
      "Epoch 27, Batch 280, LR 1.072849 Loss 4.413107, Accuracy 90.781%\n",
      "Epoch 27, Batch 281, LR 1.072716 Loss 4.412557, Accuracy 90.781%\n",
      "Epoch 27, Batch 282, LR 1.072584 Loss 4.411078, Accuracy 90.800%\n",
      "Epoch 27, Batch 283, LR 1.072451 Loss 4.410804, Accuracy 90.793%\n",
      "Epoch 27, Batch 284, LR 1.072319 Loss 4.411897, Accuracy 90.785%\n",
      "Epoch 27, Batch 285, LR 1.072186 Loss 4.413474, Accuracy 90.789%\n",
      "Epoch 27, Batch 286, LR 1.072053 Loss 4.413774, Accuracy 90.794%\n",
      "Epoch 27, Batch 287, LR 1.071921 Loss 4.413197, Accuracy 90.794%\n",
      "Epoch 27, Batch 288, LR 1.071788 Loss 4.413324, Accuracy 90.796%\n",
      "Epoch 27, Batch 289, LR 1.071656 Loss 4.413748, Accuracy 90.798%\n",
      "Epoch 27, Batch 290, LR 1.071523 Loss 4.416067, Accuracy 90.787%\n",
      "Epoch 27, Batch 291, LR 1.071390 Loss 4.414399, Accuracy 90.791%\n",
      "Epoch 27, Batch 292, LR 1.071258 Loss 4.414615, Accuracy 90.786%\n",
      "Epoch 27, Batch 293, LR 1.071125 Loss 4.413483, Accuracy 90.790%\n",
      "Epoch 27, Batch 294, LR 1.070993 Loss 4.411145, Accuracy 90.795%\n",
      "Epoch 27, Batch 295, LR 1.070860 Loss 4.410602, Accuracy 90.802%\n",
      "Epoch 27, Batch 296, LR 1.070728 Loss 4.410019, Accuracy 90.804%\n",
      "Epoch 27, Batch 297, LR 1.070595 Loss 4.407536, Accuracy 90.812%\n",
      "Epoch 27, Batch 298, LR 1.070462 Loss 4.407481, Accuracy 90.809%\n",
      "Epoch 27, Batch 299, LR 1.070330 Loss 4.405954, Accuracy 90.821%\n",
      "Epoch 27, Batch 300, LR 1.070197 Loss 4.405305, Accuracy 90.823%\n",
      "Epoch 27, Batch 301, LR 1.070065 Loss 4.405770, Accuracy 90.817%\n",
      "Epoch 27, Batch 302, LR 1.069932 Loss 4.406967, Accuracy 90.803%\n",
      "Epoch 27, Batch 303, LR 1.069800 Loss 4.406738, Accuracy 90.808%\n",
      "Epoch 27, Batch 304, LR 1.069667 Loss 4.408434, Accuracy 90.795%\n",
      "Epoch 27, Batch 305, LR 1.069535 Loss 4.405268, Accuracy 90.802%\n",
      "Epoch 27, Batch 306, LR 1.069402 Loss 4.405977, Accuracy 90.799%\n",
      "Epoch 27, Batch 307, LR 1.069269 Loss 4.406018, Accuracy 90.793%\n",
      "Epoch 27, Batch 308, LR 1.069137 Loss 4.408653, Accuracy 90.777%\n",
      "Epoch 27, Batch 309, LR 1.069004 Loss 4.406357, Accuracy 90.787%\n",
      "Epoch 27, Batch 310, LR 1.068872 Loss 4.404575, Accuracy 90.794%\n",
      "Epoch 27, Batch 311, LR 1.068739 Loss 4.405125, Accuracy 90.793%\n",
      "Epoch 27, Batch 312, LR 1.068607 Loss 4.406518, Accuracy 90.793%\n",
      "Epoch 27, Batch 313, LR 1.068474 Loss 4.404327, Accuracy 90.790%\n",
      "Epoch 27, Batch 314, LR 1.068342 Loss 4.405332, Accuracy 90.779%\n",
      "Epoch 27, Batch 315, LR 1.068209 Loss 4.404787, Accuracy 90.786%\n",
      "Epoch 27, Batch 316, LR 1.068077 Loss 4.406107, Accuracy 90.776%\n",
      "Epoch 27, Batch 317, LR 1.067944 Loss 4.406091, Accuracy 90.770%\n",
      "Epoch 27, Batch 318, LR 1.067812 Loss 4.408026, Accuracy 90.770%\n",
      "Epoch 27, Batch 319, LR 1.067679 Loss 4.411457, Accuracy 90.760%\n",
      "Epoch 27, Batch 320, LR 1.067546 Loss 4.412715, Accuracy 90.754%\n",
      "Epoch 27, Batch 321, LR 1.067414 Loss 4.411345, Accuracy 90.764%\n",
      "Epoch 27, Batch 322, LR 1.067281 Loss 4.410591, Accuracy 90.766%\n",
      "Epoch 27, Batch 323, LR 1.067149 Loss 4.410372, Accuracy 90.763%\n",
      "Epoch 27, Batch 324, LR 1.067016 Loss 4.411373, Accuracy 90.743%\n",
      "Epoch 27, Batch 325, LR 1.066884 Loss 4.413543, Accuracy 90.726%\n",
      "Epoch 27, Batch 326, LR 1.066751 Loss 4.413355, Accuracy 90.738%\n",
      "Epoch 27, Batch 327, LR 1.066619 Loss 4.412980, Accuracy 90.737%\n",
      "Epoch 27, Batch 328, LR 1.066486 Loss 4.410681, Accuracy 90.751%\n",
      "Epoch 27, Batch 329, LR 1.066354 Loss 4.409406, Accuracy 90.748%\n",
      "Epoch 27, Batch 330, LR 1.066221 Loss 4.408927, Accuracy 90.753%\n",
      "Epoch 27, Batch 331, LR 1.066089 Loss 4.409217, Accuracy 90.750%\n",
      "Epoch 27, Batch 332, LR 1.065956 Loss 4.408506, Accuracy 90.757%\n",
      "Epoch 27, Batch 333, LR 1.065824 Loss 4.408927, Accuracy 90.752%\n",
      "Epoch 27, Batch 334, LR 1.065691 Loss 4.411518, Accuracy 90.730%\n",
      "Epoch 27, Batch 335, LR 1.065559 Loss 4.411438, Accuracy 90.723%\n",
      "Epoch 27, Batch 336, LR 1.065426 Loss 4.408908, Accuracy 90.730%\n",
      "Epoch 27, Batch 337, LR 1.065294 Loss 4.409341, Accuracy 90.725%\n",
      "Epoch 27, Batch 338, LR 1.065162 Loss 4.409708, Accuracy 90.720%\n",
      "Epoch 27, Batch 339, LR 1.065029 Loss 4.410406, Accuracy 90.724%\n",
      "Epoch 27, Batch 340, LR 1.064897 Loss 4.408321, Accuracy 90.738%\n",
      "Epoch 27, Batch 341, LR 1.064764 Loss 4.406946, Accuracy 90.744%\n",
      "Epoch 27, Batch 342, LR 1.064632 Loss 4.410030, Accuracy 90.732%\n",
      "Epoch 27, Batch 343, LR 1.064499 Loss 4.409240, Accuracy 90.730%\n",
      "Epoch 27, Batch 344, LR 1.064367 Loss 4.408940, Accuracy 90.739%\n",
      "Epoch 27, Batch 345, LR 1.064234 Loss 4.410974, Accuracy 90.718%\n",
      "Epoch 27, Batch 346, LR 1.064102 Loss 4.410714, Accuracy 90.727%\n",
      "Epoch 27, Batch 347, LR 1.063969 Loss 4.411647, Accuracy 90.720%\n",
      "Epoch 27, Batch 348, LR 1.063837 Loss 4.409692, Accuracy 90.731%\n",
      "Epoch 27, Batch 349, LR 1.063704 Loss 4.410473, Accuracy 90.732%\n",
      "Epoch 27, Batch 350, LR 1.063572 Loss 4.411834, Accuracy 90.725%\n",
      "Epoch 27, Batch 351, LR 1.063439 Loss 4.413396, Accuracy 90.721%\n",
      "Epoch 27, Batch 352, LR 1.063307 Loss 4.413320, Accuracy 90.725%\n",
      "Epoch 27, Batch 353, LR 1.063175 Loss 4.411920, Accuracy 90.731%\n",
      "Epoch 27, Batch 354, LR 1.063042 Loss 4.411464, Accuracy 90.729%\n",
      "Epoch 27, Batch 355, LR 1.062910 Loss 4.412541, Accuracy 90.724%\n",
      "Epoch 27, Batch 356, LR 1.062777 Loss 4.413661, Accuracy 90.717%\n",
      "Epoch 27, Batch 357, LR 1.062645 Loss 4.414654, Accuracy 90.708%\n",
      "Epoch 27, Batch 358, LR 1.062512 Loss 4.414941, Accuracy 90.710%\n",
      "Epoch 27, Batch 359, LR 1.062380 Loss 4.413173, Accuracy 90.714%\n",
      "Epoch 27, Batch 360, LR 1.062247 Loss 4.413633, Accuracy 90.714%\n",
      "Epoch 27, Batch 361, LR 1.062115 Loss 4.413442, Accuracy 90.707%\n",
      "Epoch 27, Batch 362, LR 1.061983 Loss 4.414766, Accuracy 90.711%\n",
      "Epoch 27, Batch 363, LR 1.061850 Loss 4.415599, Accuracy 90.702%\n",
      "Epoch 27, Batch 364, LR 1.061718 Loss 4.413677, Accuracy 90.711%\n",
      "Epoch 27, Batch 365, LR 1.061585 Loss 4.412037, Accuracy 90.717%\n",
      "Epoch 27, Batch 366, LR 1.061453 Loss 4.411241, Accuracy 90.715%\n",
      "Epoch 27, Batch 367, LR 1.061320 Loss 4.411214, Accuracy 90.714%\n",
      "Epoch 27, Batch 368, LR 1.061188 Loss 4.410657, Accuracy 90.718%\n",
      "Epoch 27, Batch 369, LR 1.061056 Loss 4.410523, Accuracy 90.725%\n",
      "Epoch 27, Batch 370, LR 1.060923 Loss 4.410558, Accuracy 90.720%\n",
      "Epoch 27, Batch 371, LR 1.060791 Loss 4.410824, Accuracy 90.713%\n",
      "Epoch 27, Batch 372, LR 1.060658 Loss 4.411121, Accuracy 90.717%\n",
      "Epoch 27, Batch 373, LR 1.060526 Loss 4.410271, Accuracy 90.717%\n",
      "Epoch 27, Batch 374, LR 1.060394 Loss 4.408949, Accuracy 90.721%\n",
      "Epoch 27, Batch 375, LR 1.060261 Loss 4.408621, Accuracy 90.715%\n",
      "Epoch 27, Batch 376, LR 1.060129 Loss 4.408062, Accuracy 90.721%\n",
      "Epoch 27, Batch 377, LR 1.059996 Loss 4.407256, Accuracy 90.731%\n",
      "Epoch 27, Batch 378, LR 1.059864 Loss 4.408441, Accuracy 90.722%\n",
      "Epoch 27, Batch 379, LR 1.059732 Loss 4.407449, Accuracy 90.732%\n",
      "Epoch 27, Batch 380, LR 1.059599 Loss 4.405241, Accuracy 90.744%\n",
      "Epoch 27, Batch 381, LR 1.059467 Loss 4.405160, Accuracy 90.744%\n",
      "Epoch 27, Batch 382, LR 1.059334 Loss 4.403721, Accuracy 90.748%\n",
      "Epoch 27, Batch 383, LR 1.059202 Loss 4.403937, Accuracy 90.745%\n",
      "Epoch 27, Batch 384, LR 1.059070 Loss 4.405324, Accuracy 90.747%\n",
      "Epoch 27, Batch 385, LR 1.058937 Loss 4.404620, Accuracy 90.749%\n",
      "Epoch 27, Batch 386, LR 1.058805 Loss 4.403105, Accuracy 90.755%\n",
      "Epoch 27, Batch 387, LR 1.058673 Loss 4.405357, Accuracy 90.738%\n",
      "Epoch 27, Batch 388, LR 1.058540 Loss 4.404693, Accuracy 90.742%\n",
      "Epoch 27, Batch 389, LR 1.058408 Loss 4.403933, Accuracy 90.739%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 390, LR 1.058275 Loss 4.402653, Accuracy 90.747%\n",
      "Epoch 27, Batch 391, LR 1.058143 Loss 4.401989, Accuracy 90.749%\n",
      "Epoch 27, Batch 392, LR 1.058011 Loss 4.401608, Accuracy 90.749%\n",
      "Epoch 27, Batch 393, LR 1.057878 Loss 4.402509, Accuracy 90.738%\n",
      "Epoch 27, Batch 394, LR 1.057746 Loss 4.401535, Accuracy 90.742%\n",
      "Epoch 27, Batch 395, LR 1.057614 Loss 4.403069, Accuracy 90.734%\n",
      "Epoch 27, Batch 396, LR 1.057481 Loss 4.404177, Accuracy 90.726%\n",
      "Epoch 27, Batch 397, LR 1.057349 Loss 4.404662, Accuracy 90.729%\n",
      "Epoch 27, Batch 398, LR 1.057217 Loss 4.403913, Accuracy 90.727%\n",
      "Epoch 27, Batch 399, LR 1.057084 Loss 4.406607, Accuracy 90.723%\n",
      "Epoch 27, Batch 400, LR 1.056952 Loss 4.405655, Accuracy 90.729%\n",
      "Epoch 27, Batch 401, LR 1.056820 Loss 4.406194, Accuracy 90.726%\n",
      "Epoch 27, Batch 402, LR 1.056687 Loss 4.406745, Accuracy 90.730%\n",
      "Epoch 27, Batch 403, LR 1.056555 Loss 4.406344, Accuracy 90.730%\n",
      "Epoch 27, Batch 404, LR 1.056422 Loss 4.406929, Accuracy 90.731%\n",
      "Epoch 27, Batch 405, LR 1.056290 Loss 4.406653, Accuracy 90.745%\n",
      "Epoch 27, Batch 406, LR 1.056158 Loss 4.405321, Accuracy 90.752%\n",
      "Epoch 27, Batch 407, LR 1.056025 Loss 4.404135, Accuracy 90.750%\n",
      "Epoch 27, Batch 408, LR 1.055893 Loss 4.402403, Accuracy 90.755%\n",
      "Epoch 27, Batch 409, LR 1.055761 Loss 4.403467, Accuracy 90.753%\n",
      "Epoch 27, Batch 410, LR 1.055629 Loss 4.403468, Accuracy 90.745%\n",
      "Epoch 27, Batch 411, LR 1.055496 Loss 4.403477, Accuracy 90.739%\n",
      "Epoch 27, Batch 412, LR 1.055364 Loss 4.404087, Accuracy 90.743%\n",
      "Epoch 27, Batch 413, LR 1.055232 Loss 4.405019, Accuracy 90.744%\n",
      "Epoch 27, Batch 414, LR 1.055099 Loss 4.404415, Accuracy 90.746%\n",
      "Epoch 27, Batch 415, LR 1.054967 Loss 4.403733, Accuracy 90.751%\n",
      "Epoch 27, Batch 416, LR 1.054835 Loss 4.403937, Accuracy 90.753%\n",
      "Epoch 27, Batch 417, LR 1.054702 Loss 4.404366, Accuracy 90.751%\n",
      "Epoch 27, Batch 418, LR 1.054570 Loss 4.405115, Accuracy 90.748%\n",
      "Epoch 27, Batch 419, LR 1.054438 Loss 4.405083, Accuracy 90.754%\n",
      "Epoch 27, Batch 420, LR 1.054305 Loss 4.403495, Accuracy 90.757%\n",
      "Epoch 27, Batch 421, LR 1.054173 Loss 4.402542, Accuracy 90.768%\n",
      "Epoch 27, Batch 422, LR 1.054041 Loss 4.403344, Accuracy 90.762%\n",
      "Epoch 27, Batch 423, LR 1.053908 Loss 4.403869, Accuracy 90.764%\n",
      "Epoch 27, Batch 424, LR 1.053776 Loss 4.403200, Accuracy 90.772%\n",
      "Epoch 27, Batch 425, LR 1.053644 Loss 4.402361, Accuracy 90.768%\n",
      "Epoch 27, Batch 426, LR 1.053512 Loss 4.403668, Accuracy 90.763%\n",
      "Epoch 27, Batch 427, LR 1.053379 Loss 4.403713, Accuracy 90.760%\n",
      "Epoch 27, Batch 428, LR 1.053247 Loss 4.404263, Accuracy 90.758%\n",
      "Epoch 27, Batch 429, LR 1.053115 Loss 4.405998, Accuracy 90.749%\n",
      "Epoch 27, Batch 430, LR 1.052982 Loss 4.407554, Accuracy 90.741%\n",
      "Epoch 27, Batch 431, LR 1.052850 Loss 4.408211, Accuracy 90.734%\n",
      "Epoch 27, Batch 432, LR 1.052718 Loss 4.408313, Accuracy 90.726%\n",
      "Epoch 27, Batch 433, LR 1.052586 Loss 4.409404, Accuracy 90.722%\n",
      "Epoch 27, Batch 434, LR 1.052453 Loss 4.409490, Accuracy 90.724%\n",
      "Epoch 27, Batch 435, LR 1.052321 Loss 4.410638, Accuracy 90.722%\n",
      "Epoch 27, Batch 436, LR 1.052189 Loss 4.412907, Accuracy 90.720%\n",
      "Epoch 27, Batch 437, LR 1.052057 Loss 4.412405, Accuracy 90.725%\n",
      "Epoch 27, Batch 438, LR 1.051924 Loss 4.412566, Accuracy 90.723%\n",
      "Epoch 27, Batch 439, LR 1.051792 Loss 4.411889, Accuracy 90.726%\n",
      "Epoch 27, Batch 440, LR 1.051660 Loss 4.411742, Accuracy 90.730%\n",
      "Epoch 27, Batch 441, LR 1.051528 Loss 4.411151, Accuracy 90.735%\n",
      "Epoch 27, Batch 442, LR 1.051395 Loss 4.410183, Accuracy 90.743%\n",
      "Epoch 27, Batch 443, LR 1.051263 Loss 4.411305, Accuracy 90.736%\n",
      "Epoch 27, Batch 444, LR 1.051131 Loss 4.409557, Accuracy 90.748%\n",
      "Epoch 27, Batch 445, LR 1.050999 Loss 4.409431, Accuracy 90.748%\n",
      "Epoch 27, Batch 446, LR 1.050866 Loss 4.407836, Accuracy 90.755%\n",
      "Epoch 27, Batch 447, LR 1.050734 Loss 4.408391, Accuracy 90.756%\n",
      "Epoch 27, Batch 448, LR 1.050602 Loss 4.408464, Accuracy 90.759%\n",
      "Epoch 27, Batch 449, LR 1.050470 Loss 4.407225, Accuracy 90.766%\n",
      "Epoch 27, Batch 450, LR 1.050337 Loss 4.406488, Accuracy 90.769%\n",
      "Epoch 27, Batch 451, LR 1.050205 Loss 4.406400, Accuracy 90.776%\n",
      "Epoch 27, Batch 452, LR 1.050073 Loss 4.405955, Accuracy 90.775%\n",
      "Epoch 27, Batch 453, LR 1.049941 Loss 4.406571, Accuracy 90.770%\n",
      "Epoch 27, Batch 454, LR 1.049808 Loss 4.406508, Accuracy 90.771%\n",
      "Epoch 27, Batch 455, LR 1.049676 Loss 4.405166, Accuracy 90.781%\n",
      "Epoch 27, Batch 456, LR 1.049544 Loss 4.404098, Accuracy 90.784%\n",
      "Epoch 27, Batch 457, LR 1.049412 Loss 4.404953, Accuracy 90.782%\n",
      "Epoch 27, Batch 458, LR 1.049280 Loss 4.404293, Accuracy 90.780%\n",
      "Epoch 27, Batch 459, LR 1.049147 Loss 4.404729, Accuracy 90.782%\n",
      "Epoch 27, Batch 460, LR 1.049015 Loss 4.404266, Accuracy 90.790%\n",
      "Epoch 27, Batch 461, LR 1.048883 Loss 4.403699, Accuracy 90.793%\n",
      "Epoch 27, Batch 462, LR 1.048751 Loss 4.402854, Accuracy 90.796%\n",
      "Epoch 27, Batch 463, LR 1.048619 Loss 4.401967, Accuracy 90.795%\n",
      "Epoch 27, Batch 464, LR 1.048486 Loss 4.401978, Accuracy 90.798%\n",
      "Epoch 27, Batch 465, LR 1.048354 Loss 4.401827, Accuracy 90.800%\n",
      "Epoch 27, Batch 466, LR 1.048222 Loss 4.401782, Accuracy 90.803%\n",
      "Epoch 27, Batch 467, LR 1.048090 Loss 4.402200, Accuracy 90.799%\n",
      "Epoch 27, Batch 468, LR 1.047958 Loss 4.402411, Accuracy 90.800%\n",
      "Epoch 27, Batch 469, LR 1.047825 Loss 4.403258, Accuracy 90.798%\n",
      "Epoch 27, Batch 470, LR 1.047693 Loss 4.404278, Accuracy 90.795%\n",
      "Epoch 27, Batch 471, LR 1.047561 Loss 4.405728, Accuracy 90.794%\n",
      "Epoch 27, Batch 472, LR 1.047429 Loss 4.406073, Accuracy 90.794%\n",
      "Epoch 27, Batch 473, LR 1.047297 Loss 4.405293, Accuracy 90.795%\n",
      "Epoch 27, Batch 474, LR 1.047164 Loss 4.405271, Accuracy 90.787%\n",
      "Epoch 27, Batch 475, LR 1.047032 Loss 4.405054, Accuracy 90.789%\n",
      "Epoch 27, Batch 476, LR 1.046900 Loss 4.405730, Accuracy 90.784%\n",
      "Epoch 27, Batch 477, LR 1.046768 Loss 4.404697, Accuracy 90.782%\n",
      "Epoch 27, Batch 478, LR 1.046636 Loss 4.404257, Accuracy 90.787%\n",
      "Epoch 27, Batch 479, LR 1.046504 Loss 4.404403, Accuracy 90.782%\n",
      "Epoch 27, Batch 480, LR 1.046371 Loss 4.404632, Accuracy 90.776%\n",
      "Epoch 27, Batch 481, LR 1.046239 Loss 4.403984, Accuracy 90.776%\n",
      "Epoch 27, Batch 482, LR 1.046107 Loss 4.405180, Accuracy 90.769%\n",
      "Epoch 27, Batch 483, LR 1.045975 Loss 4.405930, Accuracy 90.767%\n",
      "Epoch 27, Batch 484, LR 1.045843 Loss 4.405118, Accuracy 90.765%\n",
      "Epoch 27, Batch 485, LR 1.045711 Loss 4.404483, Accuracy 90.765%\n",
      "Epoch 27, Batch 486, LR 1.045578 Loss 4.404643, Accuracy 90.762%\n",
      "Epoch 27, Batch 487, LR 1.045446 Loss 4.403459, Accuracy 90.769%\n",
      "Epoch 27, Batch 488, LR 1.045314 Loss 4.404121, Accuracy 90.766%\n",
      "Epoch 27, Batch 489, LR 1.045182 Loss 4.403799, Accuracy 90.766%\n",
      "Epoch 27, Batch 490, LR 1.045050 Loss 4.404347, Accuracy 90.762%\n",
      "Epoch 27, Batch 491, LR 1.044918 Loss 4.405292, Accuracy 90.760%\n",
      "Epoch 27, Batch 492, LR 1.044786 Loss 4.405064, Accuracy 90.762%\n",
      "Epoch 27, Batch 493, LR 1.044653 Loss 4.405059, Accuracy 90.763%\n",
      "Epoch 27, Batch 494, LR 1.044521 Loss 4.404022, Accuracy 90.769%\n",
      "Epoch 27, Batch 495, LR 1.044389 Loss 4.404314, Accuracy 90.767%\n",
      "Epoch 27, Batch 496, LR 1.044257 Loss 4.403983, Accuracy 90.773%\n",
      "Epoch 27, Batch 497, LR 1.044125 Loss 4.403396, Accuracy 90.770%\n",
      "Epoch 27, Batch 498, LR 1.043993 Loss 4.403337, Accuracy 90.772%\n",
      "Epoch 27, Batch 499, LR 1.043861 Loss 4.403956, Accuracy 90.772%\n",
      "Epoch 27, Batch 500, LR 1.043729 Loss 4.404460, Accuracy 90.775%\n",
      "Epoch 27, Batch 501, LR 1.043596 Loss 4.404226, Accuracy 90.778%\n",
      "Epoch 27, Batch 502, LR 1.043464 Loss 4.404524, Accuracy 90.776%\n",
      "Epoch 27, Batch 503, LR 1.043332 Loss 4.404502, Accuracy 90.780%\n",
      "Epoch 27, Batch 504, LR 1.043200 Loss 4.404330, Accuracy 90.782%\n",
      "Epoch 27, Batch 505, LR 1.043068 Loss 4.404815, Accuracy 90.774%\n",
      "Epoch 27, Batch 506, LR 1.042936 Loss 4.405007, Accuracy 90.779%\n",
      "Epoch 27, Batch 507, LR 1.042804 Loss 4.406522, Accuracy 90.771%\n",
      "Epoch 27, Batch 508, LR 1.042672 Loss 4.408009, Accuracy 90.765%\n",
      "Epoch 27, Batch 509, LR 1.042540 Loss 4.407379, Accuracy 90.771%\n",
      "Epoch 27, Batch 510, LR 1.042408 Loss 4.407945, Accuracy 90.767%\n",
      "Epoch 27, Batch 511, LR 1.042275 Loss 4.407810, Accuracy 90.767%\n",
      "Epoch 27, Batch 512, LR 1.042143 Loss 4.406594, Accuracy 90.773%\n",
      "Epoch 27, Batch 513, LR 1.042011 Loss 4.406719, Accuracy 90.777%\n",
      "Epoch 27, Batch 514, LR 1.041879 Loss 4.407111, Accuracy 90.775%\n",
      "Epoch 27, Batch 515, LR 1.041747 Loss 4.408773, Accuracy 90.769%\n",
      "Epoch 27, Batch 516, LR 1.041615 Loss 4.410303, Accuracy 90.761%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 517, LR 1.041483 Loss 4.410178, Accuracy 90.766%\n",
      "Epoch 27, Batch 518, LR 1.041351 Loss 4.410505, Accuracy 90.765%\n",
      "Epoch 27, Batch 519, LR 1.041219 Loss 4.409838, Accuracy 90.765%\n",
      "Epoch 27, Batch 520, LR 1.041087 Loss 4.410133, Accuracy 90.763%\n",
      "Epoch 27, Batch 521, LR 1.040955 Loss 4.410695, Accuracy 90.757%\n",
      "Epoch 27, Batch 522, LR 1.040823 Loss 4.410623, Accuracy 90.761%\n",
      "Epoch 27, Batch 523, LR 1.040691 Loss 4.410215, Accuracy 90.764%\n",
      "Epoch 27, Batch 524, LR 1.040558 Loss 4.410394, Accuracy 90.756%\n",
      "Epoch 27, Batch 525, LR 1.040426 Loss 4.410501, Accuracy 90.756%\n",
      "Epoch 27, Batch 526, LR 1.040294 Loss 4.409748, Accuracy 90.757%\n",
      "Epoch 27, Batch 527, LR 1.040162 Loss 4.409495, Accuracy 90.767%\n",
      "Epoch 27, Batch 528, LR 1.040030 Loss 4.410169, Accuracy 90.769%\n",
      "Epoch 27, Batch 529, LR 1.039898 Loss 4.408867, Accuracy 90.770%\n",
      "Epoch 27, Batch 530, LR 1.039766 Loss 4.408285, Accuracy 90.772%\n",
      "Epoch 27, Batch 531, LR 1.039634 Loss 4.408227, Accuracy 90.771%\n",
      "Epoch 27, Batch 532, LR 1.039502 Loss 4.409983, Accuracy 90.762%\n",
      "Epoch 27, Batch 533, LR 1.039370 Loss 4.411108, Accuracy 90.754%\n",
      "Epoch 27, Batch 534, LR 1.039238 Loss 4.411125, Accuracy 90.752%\n",
      "Epoch 27, Batch 535, LR 1.039106 Loss 4.411344, Accuracy 90.752%\n",
      "Epoch 27, Batch 536, LR 1.038974 Loss 4.410443, Accuracy 90.758%\n",
      "Epoch 27, Batch 537, LR 1.038842 Loss 4.409995, Accuracy 90.760%\n",
      "Epoch 27, Batch 538, LR 1.038710 Loss 4.409305, Accuracy 90.767%\n",
      "Epoch 27, Batch 539, LR 1.038578 Loss 4.408717, Accuracy 90.773%\n",
      "Epoch 27, Batch 540, LR 1.038446 Loss 4.407946, Accuracy 90.775%\n",
      "Epoch 27, Batch 541, LR 1.038314 Loss 4.408590, Accuracy 90.775%\n",
      "Epoch 27, Batch 542, LR 1.038182 Loss 4.408317, Accuracy 90.771%\n",
      "Epoch 27, Batch 543, LR 1.038050 Loss 4.408217, Accuracy 90.772%\n",
      "Epoch 27, Batch 544, LR 1.037918 Loss 4.408245, Accuracy 90.771%\n",
      "Epoch 27, Batch 545, LR 1.037786 Loss 4.408355, Accuracy 90.765%\n",
      "Epoch 27, Batch 546, LR 1.037654 Loss 4.408793, Accuracy 90.767%\n",
      "Epoch 27, Batch 547, LR 1.037522 Loss 4.408730, Accuracy 90.772%\n",
      "Epoch 27, Batch 548, LR 1.037390 Loss 4.408521, Accuracy 90.780%\n",
      "Epoch 27, Batch 549, LR 1.037258 Loss 4.407797, Accuracy 90.776%\n",
      "Epoch 27, Batch 550, LR 1.037126 Loss 4.407582, Accuracy 90.776%\n",
      "Epoch 27, Batch 551, LR 1.036994 Loss 4.406367, Accuracy 90.780%\n",
      "Epoch 27, Batch 552, LR 1.036862 Loss 4.407548, Accuracy 90.775%\n",
      "Epoch 27, Batch 553, LR 1.036730 Loss 4.405832, Accuracy 90.782%\n",
      "Epoch 27, Batch 554, LR 1.036598 Loss 4.406527, Accuracy 90.782%\n",
      "Epoch 27, Batch 555, LR 1.036466 Loss 4.406791, Accuracy 90.780%\n",
      "Epoch 27, Batch 556, LR 1.036334 Loss 4.405713, Accuracy 90.791%\n",
      "Epoch 27, Batch 557, LR 1.036202 Loss 4.406843, Accuracy 90.783%\n",
      "Epoch 27, Batch 558, LR 1.036070 Loss 4.406106, Accuracy 90.779%\n",
      "Epoch 27, Batch 559, LR 1.035938 Loss 4.405726, Accuracy 90.782%\n",
      "Epoch 27, Batch 560, LR 1.035806 Loss 4.405767, Accuracy 90.780%\n",
      "Epoch 27, Batch 561, LR 1.035674 Loss 4.406373, Accuracy 90.775%\n",
      "Epoch 27, Batch 562, LR 1.035542 Loss 4.406988, Accuracy 90.775%\n",
      "Epoch 27, Batch 563, LR 1.035410 Loss 4.407066, Accuracy 90.772%\n",
      "Epoch 27, Batch 564, LR 1.035278 Loss 4.407114, Accuracy 90.772%\n",
      "Epoch 27, Batch 565, LR 1.035146 Loss 4.407722, Accuracy 90.766%\n",
      "Epoch 27, Batch 566, LR 1.035014 Loss 4.408393, Accuracy 90.770%\n",
      "Epoch 27, Batch 567, LR 1.034882 Loss 4.407787, Accuracy 90.772%\n",
      "Epoch 27, Batch 568, LR 1.034750 Loss 4.409051, Accuracy 90.761%\n",
      "Epoch 27, Batch 569, LR 1.034618 Loss 4.408895, Accuracy 90.758%\n",
      "Epoch 27, Batch 570, LR 1.034486 Loss 4.408556, Accuracy 90.765%\n",
      "Epoch 27, Batch 571, LR 1.034354 Loss 4.408473, Accuracy 90.760%\n",
      "Epoch 27, Batch 572, LR 1.034222 Loss 4.406671, Accuracy 90.770%\n",
      "Epoch 27, Batch 573, LR 1.034090 Loss 4.406230, Accuracy 90.770%\n",
      "Epoch 27, Batch 574, LR 1.033959 Loss 4.405971, Accuracy 90.771%\n",
      "Epoch 27, Batch 575, LR 1.033827 Loss 4.404881, Accuracy 90.772%\n",
      "Epoch 27, Batch 576, LR 1.033695 Loss 4.403723, Accuracy 90.776%\n",
      "Epoch 27, Batch 577, LR 1.033563 Loss 4.403739, Accuracy 90.775%\n",
      "Epoch 27, Batch 578, LR 1.033431 Loss 4.403628, Accuracy 90.776%\n",
      "Epoch 27, Batch 579, LR 1.033299 Loss 4.403356, Accuracy 90.780%\n",
      "Epoch 27, Batch 580, LR 1.033167 Loss 4.403454, Accuracy 90.776%\n",
      "Epoch 27, Batch 581, LR 1.033035 Loss 4.403455, Accuracy 90.772%\n",
      "Epoch 27, Batch 582, LR 1.032903 Loss 4.404539, Accuracy 90.769%\n",
      "Epoch 27, Batch 583, LR 1.032771 Loss 4.403158, Accuracy 90.775%\n",
      "Epoch 27, Batch 584, LR 1.032639 Loss 4.403584, Accuracy 90.773%\n",
      "Epoch 27, Batch 585, LR 1.032507 Loss 4.403418, Accuracy 90.769%\n",
      "Epoch 27, Batch 586, LR 1.032375 Loss 4.403458, Accuracy 90.769%\n",
      "Epoch 27, Batch 587, LR 1.032244 Loss 4.403543, Accuracy 90.767%\n",
      "Epoch 27, Batch 588, LR 1.032112 Loss 4.403407, Accuracy 90.763%\n",
      "Epoch 27, Batch 589, LR 1.031980 Loss 4.404371, Accuracy 90.754%\n",
      "Epoch 27, Batch 590, LR 1.031848 Loss 4.404849, Accuracy 90.748%\n",
      "Epoch 27, Batch 591, LR 1.031716 Loss 4.404605, Accuracy 90.752%\n",
      "Epoch 27, Batch 592, LR 1.031584 Loss 4.404194, Accuracy 90.753%\n",
      "Epoch 27, Batch 593, LR 1.031452 Loss 4.403629, Accuracy 90.755%\n",
      "Epoch 27, Batch 594, LR 1.031320 Loss 4.403125, Accuracy 90.758%\n",
      "Epoch 27, Batch 595, LR 1.031188 Loss 4.403166, Accuracy 90.755%\n",
      "Epoch 27, Batch 596, LR 1.031057 Loss 4.402411, Accuracy 90.763%\n",
      "Epoch 27, Batch 597, LR 1.030925 Loss 4.402181, Accuracy 90.765%\n",
      "Epoch 27, Batch 598, LR 1.030793 Loss 4.402462, Accuracy 90.766%\n",
      "Epoch 27, Batch 599, LR 1.030661 Loss 4.401627, Accuracy 90.766%\n",
      "Epoch 27, Batch 600, LR 1.030529 Loss 4.402149, Accuracy 90.766%\n",
      "Epoch 27, Batch 601, LR 1.030397 Loss 4.402228, Accuracy 90.767%\n",
      "Epoch 27, Batch 602, LR 1.030265 Loss 4.401736, Accuracy 90.770%\n",
      "Epoch 27, Batch 603, LR 1.030133 Loss 4.401838, Accuracy 90.766%\n",
      "Epoch 27, Batch 604, LR 1.030002 Loss 4.402513, Accuracy 90.767%\n",
      "Epoch 27, Batch 605, LR 1.029870 Loss 4.401447, Accuracy 90.771%\n",
      "Epoch 27, Batch 606, LR 1.029738 Loss 4.401975, Accuracy 90.769%\n",
      "Epoch 27, Batch 607, LR 1.029606 Loss 4.401970, Accuracy 90.769%\n",
      "Epoch 27, Batch 608, LR 1.029474 Loss 4.403112, Accuracy 90.761%\n",
      "Epoch 27, Batch 609, LR 1.029342 Loss 4.402171, Accuracy 90.765%\n",
      "Epoch 27, Batch 610, LR 1.029210 Loss 4.401200, Accuracy 90.768%\n",
      "Epoch 27, Batch 611, LR 1.029079 Loss 4.400295, Accuracy 90.771%\n",
      "Epoch 27, Batch 612, LR 1.028947 Loss 4.400452, Accuracy 90.768%\n",
      "Epoch 27, Batch 613, LR 1.028815 Loss 4.400494, Accuracy 90.761%\n",
      "Epoch 27, Batch 614, LR 1.028683 Loss 4.400238, Accuracy 90.760%\n",
      "Epoch 27, Batch 615, LR 1.028551 Loss 4.401038, Accuracy 90.757%\n",
      "Epoch 27, Batch 616, LR 1.028419 Loss 4.400771, Accuracy 90.758%\n",
      "Epoch 27, Batch 617, LR 1.028288 Loss 4.399707, Accuracy 90.766%\n",
      "Epoch 27, Batch 618, LR 1.028156 Loss 4.399484, Accuracy 90.770%\n",
      "Epoch 27, Batch 619, LR 1.028024 Loss 4.399738, Accuracy 90.775%\n",
      "Epoch 27, Batch 620, LR 1.027892 Loss 4.400069, Accuracy 90.772%\n",
      "Epoch 27, Batch 621, LR 1.027760 Loss 4.401509, Accuracy 90.761%\n",
      "Epoch 27, Batch 622, LR 1.027628 Loss 4.401772, Accuracy 90.761%\n",
      "Epoch 27, Batch 623, LR 1.027497 Loss 4.401788, Accuracy 90.759%\n",
      "Epoch 27, Batch 624, LR 1.027365 Loss 4.401330, Accuracy 90.760%\n",
      "Epoch 27, Batch 625, LR 1.027233 Loss 4.401842, Accuracy 90.761%\n",
      "Epoch 27, Batch 626, LR 1.027101 Loss 4.401865, Accuracy 90.766%\n",
      "Epoch 27, Batch 627, LR 1.026969 Loss 4.401940, Accuracy 90.770%\n",
      "Epoch 27, Batch 628, LR 1.026838 Loss 4.402752, Accuracy 90.771%\n",
      "Epoch 27, Batch 629, LR 1.026706 Loss 4.402394, Accuracy 90.772%\n",
      "Epoch 27, Batch 630, LR 1.026574 Loss 4.402612, Accuracy 90.769%\n",
      "Epoch 27, Batch 631, LR 1.026442 Loss 4.403489, Accuracy 90.770%\n",
      "Epoch 27, Batch 632, LR 1.026310 Loss 4.402907, Accuracy 90.773%\n",
      "Epoch 27, Batch 633, LR 1.026179 Loss 4.403046, Accuracy 90.777%\n",
      "Epoch 27, Batch 634, LR 1.026047 Loss 4.402144, Accuracy 90.781%\n",
      "Epoch 27, Batch 635, LR 1.025915 Loss 4.402498, Accuracy 90.779%\n",
      "Epoch 27, Batch 636, LR 1.025783 Loss 4.402823, Accuracy 90.774%\n",
      "Epoch 27, Batch 637, LR 1.025652 Loss 4.402840, Accuracy 90.775%\n",
      "Epoch 27, Batch 638, LR 1.025520 Loss 4.403457, Accuracy 90.776%\n",
      "Epoch 27, Batch 639, LR 1.025388 Loss 4.404130, Accuracy 90.774%\n",
      "Epoch 27, Batch 640, LR 1.025256 Loss 4.404623, Accuracy 90.774%\n",
      "Epoch 27, Batch 641, LR 1.025124 Loss 4.404736, Accuracy 90.774%\n",
      "Epoch 27, Batch 642, LR 1.024993 Loss 4.405278, Accuracy 90.773%\n",
      "Epoch 27, Batch 643, LR 1.024861 Loss 4.404314, Accuracy 90.778%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 644, LR 1.024729 Loss 4.404041, Accuracy 90.778%\n",
      "Epoch 27, Batch 645, LR 1.024597 Loss 4.403935, Accuracy 90.781%\n",
      "Epoch 27, Batch 646, LR 1.024466 Loss 4.404161, Accuracy 90.782%\n",
      "Epoch 27, Batch 647, LR 1.024334 Loss 4.403924, Accuracy 90.781%\n",
      "Epoch 27, Batch 648, LR 1.024202 Loss 4.403516, Accuracy 90.778%\n",
      "Epoch 27, Batch 649, LR 1.024070 Loss 4.402841, Accuracy 90.778%\n",
      "Epoch 27, Batch 650, LR 1.023939 Loss 4.402389, Accuracy 90.776%\n",
      "Epoch 27, Batch 651, LR 1.023807 Loss 4.402841, Accuracy 90.773%\n",
      "Epoch 27, Batch 652, LR 1.023675 Loss 4.402186, Accuracy 90.775%\n",
      "Epoch 27, Batch 653, LR 1.023543 Loss 4.402978, Accuracy 90.770%\n",
      "Epoch 27, Batch 654, LR 1.023412 Loss 4.402778, Accuracy 90.770%\n",
      "Epoch 27, Batch 655, LR 1.023280 Loss 4.403398, Accuracy 90.768%\n",
      "Epoch 27, Batch 656, LR 1.023148 Loss 4.403250, Accuracy 90.770%\n",
      "Epoch 27, Batch 657, LR 1.023016 Loss 4.401974, Accuracy 90.775%\n",
      "Epoch 27, Batch 658, LR 1.022885 Loss 4.402031, Accuracy 90.773%\n",
      "Epoch 27, Batch 659, LR 1.022753 Loss 4.401662, Accuracy 90.776%\n",
      "Epoch 27, Batch 660, LR 1.022621 Loss 4.401747, Accuracy 90.778%\n",
      "Epoch 27, Batch 661, LR 1.022490 Loss 4.402055, Accuracy 90.780%\n",
      "Epoch 27, Batch 662, LR 1.022358 Loss 4.402993, Accuracy 90.776%\n",
      "Epoch 27, Batch 663, LR 1.022226 Loss 4.403106, Accuracy 90.775%\n",
      "Epoch 27, Batch 664, LR 1.022094 Loss 4.403116, Accuracy 90.776%\n",
      "Epoch 27, Batch 665, LR 1.021963 Loss 4.403510, Accuracy 90.773%\n",
      "Epoch 27, Batch 666, LR 1.021831 Loss 4.403910, Accuracy 90.773%\n",
      "Epoch 27, Batch 667, LR 1.021699 Loss 4.403460, Accuracy 90.770%\n",
      "Epoch 27, Batch 668, LR 1.021568 Loss 4.404099, Accuracy 90.768%\n",
      "Epoch 27, Batch 669, LR 1.021436 Loss 4.403761, Accuracy 90.770%\n",
      "Epoch 27, Batch 670, LR 1.021304 Loss 4.403275, Accuracy 90.774%\n",
      "Epoch 27, Batch 671, LR 1.021173 Loss 4.402333, Accuracy 90.781%\n",
      "Epoch 27, Batch 672, LR 1.021041 Loss 4.402325, Accuracy 90.781%\n",
      "Epoch 27, Batch 673, LR 1.020909 Loss 4.402927, Accuracy 90.774%\n",
      "Epoch 27, Batch 674, LR 1.020777 Loss 4.402932, Accuracy 90.775%\n",
      "Epoch 27, Batch 675, LR 1.020646 Loss 4.403806, Accuracy 90.771%\n",
      "Epoch 27, Batch 676, LR 1.020514 Loss 4.404263, Accuracy 90.766%\n",
      "Epoch 27, Batch 677, LR 1.020382 Loss 4.403315, Accuracy 90.769%\n",
      "Epoch 27, Batch 678, LR 1.020251 Loss 4.402649, Accuracy 90.770%\n",
      "Epoch 27, Batch 679, LR 1.020119 Loss 4.403030, Accuracy 90.772%\n",
      "Epoch 27, Batch 680, LR 1.019987 Loss 4.403399, Accuracy 90.772%\n",
      "Epoch 27, Batch 681, LR 1.019856 Loss 4.403789, Accuracy 90.763%\n",
      "Epoch 27, Batch 682, LR 1.019724 Loss 4.403646, Accuracy 90.761%\n",
      "Epoch 27, Batch 683, LR 1.019592 Loss 4.403566, Accuracy 90.760%\n",
      "Epoch 27, Batch 684, LR 1.019461 Loss 4.403536, Accuracy 90.762%\n",
      "Epoch 27, Batch 685, LR 1.019329 Loss 4.404007, Accuracy 90.760%\n",
      "Epoch 27, Batch 686, LR 1.019198 Loss 4.405401, Accuracy 90.750%\n",
      "Epoch 27, Batch 687, LR 1.019066 Loss 4.405685, Accuracy 90.748%\n",
      "Epoch 27, Batch 688, LR 1.018934 Loss 4.405572, Accuracy 90.744%\n",
      "Epoch 27, Batch 689, LR 1.018803 Loss 4.404888, Accuracy 90.745%\n",
      "Epoch 27, Batch 690, LR 1.018671 Loss 4.405003, Accuracy 90.743%\n",
      "Epoch 27, Batch 691, LR 1.018539 Loss 4.403952, Accuracy 90.744%\n",
      "Epoch 27, Batch 692, LR 1.018408 Loss 4.404535, Accuracy 90.745%\n",
      "Epoch 27, Batch 693, LR 1.018276 Loss 4.405042, Accuracy 90.746%\n",
      "Epoch 27, Batch 694, LR 1.018144 Loss 4.404803, Accuracy 90.750%\n",
      "Epoch 27, Batch 695, LR 1.018013 Loss 4.404794, Accuracy 90.752%\n",
      "Epoch 27, Batch 696, LR 1.017881 Loss 4.405062, Accuracy 90.754%\n",
      "Epoch 27, Batch 697, LR 1.017750 Loss 4.405325, Accuracy 90.756%\n",
      "Epoch 27, Batch 698, LR 1.017618 Loss 4.404639, Accuracy 90.762%\n",
      "Epoch 27, Batch 699, LR 1.017486 Loss 4.403722, Accuracy 90.762%\n",
      "Epoch 27, Batch 700, LR 1.017355 Loss 4.403854, Accuracy 90.761%\n",
      "Epoch 27, Batch 701, LR 1.017223 Loss 4.403658, Accuracy 90.762%\n",
      "Epoch 27, Batch 702, LR 1.017091 Loss 4.404368, Accuracy 90.760%\n",
      "Epoch 27, Batch 703, LR 1.016960 Loss 4.404849, Accuracy 90.756%\n",
      "Epoch 27, Batch 704, LR 1.016828 Loss 4.404334, Accuracy 90.758%\n",
      "Epoch 27, Batch 705, LR 1.016697 Loss 4.403790, Accuracy 90.762%\n",
      "Epoch 27, Batch 706, LR 1.016565 Loss 4.403677, Accuracy 90.762%\n",
      "Epoch 27, Batch 707, LR 1.016433 Loss 4.403391, Accuracy 90.766%\n",
      "Epoch 27, Batch 708, LR 1.016302 Loss 4.402949, Accuracy 90.767%\n",
      "Epoch 27, Batch 709, LR 1.016170 Loss 4.403093, Accuracy 90.766%\n",
      "Epoch 27, Batch 710, LR 1.016039 Loss 4.403164, Accuracy 90.767%\n",
      "Epoch 27, Batch 711, LR 1.015907 Loss 4.402905, Accuracy 90.770%\n",
      "Epoch 27, Batch 712, LR 1.015776 Loss 4.402552, Accuracy 90.772%\n",
      "Epoch 27, Batch 713, LR 1.015644 Loss 4.403684, Accuracy 90.766%\n",
      "Epoch 27, Batch 714, LR 1.015512 Loss 4.404014, Accuracy 90.765%\n",
      "Epoch 27, Batch 715, LR 1.015381 Loss 4.404632, Accuracy 90.763%\n",
      "Epoch 27, Batch 716, LR 1.015249 Loss 4.405557, Accuracy 90.762%\n",
      "Epoch 27, Batch 717, LR 1.015118 Loss 4.405184, Accuracy 90.762%\n",
      "Epoch 27, Batch 718, LR 1.014986 Loss 4.404893, Accuracy 90.764%\n",
      "Epoch 27, Batch 719, LR 1.014855 Loss 4.404207, Accuracy 90.765%\n",
      "Epoch 27, Batch 720, LR 1.014723 Loss 4.404264, Accuracy 90.765%\n",
      "Epoch 27, Batch 721, LR 1.014591 Loss 4.403965, Accuracy 90.765%\n",
      "Epoch 27, Batch 722, LR 1.014460 Loss 4.403908, Accuracy 90.767%\n",
      "Epoch 27, Batch 723, LR 1.014328 Loss 4.403364, Accuracy 90.770%\n",
      "Epoch 27, Batch 724, LR 1.014197 Loss 4.403798, Accuracy 90.769%\n",
      "Epoch 27, Batch 725, LR 1.014065 Loss 4.404142, Accuracy 90.767%\n",
      "Epoch 27, Batch 726, LR 1.013934 Loss 4.402864, Accuracy 90.774%\n",
      "Epoch 27, Batch 727, LR 1.013802 Loss 4.403272, Accuracy 90.775%\n",
      "Epoch 27, Batch 728, LR 1.013671 Loss 4.402184, Accuracy 90.777%\n",
      "Epoch 27, Batch 729, LR 1.013539 Loss 4.402081, Accuracy 90.777%\n",
      "Epoch 27, Batch 730, LR 1.013408 Loss 4.401755, Accuracy 90.777%\n",
      "Epoch 27, Batch 731, LR 1.013276 Loss 4.401638, Accuracy 90.779%\n",
      "Epoch 27, Batch 732, LR 1.013144 Loss 4.401646, Accuracy 90.779%\n",
      "Epoch 27, Batch 733, LR 1.013013 Loss 4.402355, Accuracy 90.776%\n",
      "Epoch 27, Batch 734, LR 1.012881 Loss 4.402149, Accuracy 90.781%\n",
      "Epoch 27, Batch 735, LR 1.012750 Loss 4.402497, Accuracy 90.787%\n",
      "Epoch 27, Batch 736, LR 1.012618 Loss 4.402502, Accuracy 90.784%\n",
      "Epoch 27, Batch 737, LR 1.012487 Loss 4.402128, Accuracy 90.787%\n",
      "Epoch 27, Batch 738, LR 1.012355 Loss 4.402284, Accuracy 90.784%\n",
      "Epoch 27, Batch 739, LR 1.012224 Loss 4.402542, Accuracy 90.788%\n",
      "Epoch 27, Batch 740, LR 1.012092 Loss 4.401954, Accuracy 90.792%\n",
      "Epoch 27, Batch 741, LR 1.011961 Loss 4.400969, Accuracy 90.797%\n",
      "Epoch 27, Batch 742, LR 1.011829 Loss 4.401985, Accuracy 90.792%\n",
      "Epoch 27, Batch 743, LR 1.011698 Loss 4.402944, Accuracy 90.786%\n",
      "Epoch 27, Batch 744, LR 1.011566 Loss 4.402861, Accuracy 90.786%\n",
      "Epoch 27, Batch 745, LR 1.011435 Loss 4.402318, Accuracy 90.788%\n",
      "Epoch 27, Batch 746, LR 1.011303 Loss 4.402207, Accuracy 90.790%\n",
      "Epoch 27, Batch 747, LR 1.011172 Loss 4.402836, Accuracy 90.787%\n",
      "Epoch 27, Batch 748, LR 1.011040 Loss 4.402963, Accuracy 90.783%\n",
      "Epoch 27, Batch 749, LR 1.010909 Loss 4.402862, Accuracy 90.781%\n",
      "Epoch 27, Batch 750, LR 1.010777 Loss 4.402602, Accuracy 90.782%\n",
      "Epoch 27, Batch 751, LR 1.010646 Loss 4.403443, Accuracy 90.780%\n",
      "Epoch 27, Batch 752, LR 1.010514 Loss 4.403443, Accuracy 90.780%\n",
      "Epoch 27, Batch 753, LR 1.010383 Loss 4.403146, Accuracy 90.782%\n",
      "Epoch 27, Batch 754, LR 1.010252 Loss 4.403433, Accuracy 90.780%\n",
      "Epoch 27, Batch 755, LR 1.010120 Loss 4.402798, Accuracy 90.787%\n",
      "Epoch 27, Batch 756, LR 1.009989 Loss 4.402836, Accuracy 90.789%\n",
      "Epoch 27, Batch 757, LR 1.009857 Loss 4.401604, Accuracy 90.795%\n",
      "Epoch 27, Batch 758, LR 1.009726 Loss 4.402496, Accuracy 90.793%\n",
      "Epoch 27, Batch 759, LR 1.009594 Loss 4.402508, Accuracy 90.796%\n",
      "Epoch 27, Batch 760, LR 1.009463 Loss 4.403279, Accuracy 90.786%\n",
      "Epoch 27, Batch 761, LR 1.009331 Loss 4.403174, Accuracy 90.789%\n",
      "Epoch 27, Batch 762, LR 1.009200 Loss 4.403225, Accuracy 90.786%\n",
      "Epoch 27, Batch 763, LR 1.009068 Loss 4.403309, Accuracy 90.784%\n",
      "Epoch 27, Batch 764, LR 1.008937 Loss 4.403048, Accuracy 90.780%\n",
      "Epoch 27, Batch 765, LR 1.008806 Loss 4.403375, Accuracy 90.779%\n",
      "Epoch 27, Batch 766, LR 1.008674 Loss 4.403102, Accuracy 90.780%\n",
      "Epoch 27, Batch 767, LR 1.008543 Loss 4.403098, Accuracy 90.779%\n",
      "Epoch 27, Batch 768, LR 1.008411 Loss 4.403571, Accuracy 90.776%\n",
      "Epoch 27, Batch 769, LR 1.008280 Loss 4.404659, Accuracy 90.767%\n",
      "Epoch 27, Batch 770, LR 1.008148 Loss 4.404678, Accuracy 90.767%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 771, LR 1.008017 Loss 4.404678, Accuracy 90.766%\n",
      "Epoch 27, Batch 772, LR 1.007886 Loss 4.405035, Accuracy 90.759%\n",
      "Epoch 27, Batch 773, LR 1.007754 Loss 4.404896, Accuracy 90.755%\n",
      "Epoch 27, Batch 774, LR 1.007623 Loss 4.404193, Accuracy 90.758%\n",
      "Epoch 27, Batch 775, LR 1.007491 Loss 4.403998, Accuracy 90.758%\n",
      "Epoch 27, Batch 776, LR 1.007360 Loss 4.404450, Accuracy 90.754%\n",
      "Epoch 27, Batch 777, LR 1.007229 Loss 4.404926, Accuracy 90.749%\n",
      "Epoch 27, Batch 778, LR 1.007097 Loss 4.405721, Accuracy 90.738%\n",
      "Epoch 27, Batch 779, LR 1.006966 Loss 4.405365, Accuracy 90.741%\n",
      "Epoch 27, Batch 780, LR 1.006834 Loss 4.403723, Accuracy 90.748%\n",
      "Epoch 27, Batch 781, LR 1.006703 Loss 4.402953, Accuracy 90.749%\n",
      "Epoch 27, Batch 782, LR 1.006572 Loss 4.402507, Accuracy 90.753%\n",
      "Epoch 27, Batch 783, LR 1.006440 Loss 4.403185, Accuracy 90.747%\n",
      "Epoch 27, Batch 784, LR 1.006309 Loss 4.404211, Accuracy 90.742%\n",
      "Epoch 27, Batch 785, LR 1.006177 Loss 4.403372, Accuracy 90.744%\n",
      "Epoch 27, Batch 786, LR 1.006046 Loss 4.403158, Accuracy 90.745%\n",
      "Epoch 27, Batch 787, LR 1.005915 Loss 4.403496, Accuracy 90.739%\n",
      "Epoch 27, Batch 788, LR 1.005783 Loss 4.404349, Accuracy 90.738%\n",
      "Epoch 27, Batch 789, LR 1.005652 Loss 4.404958, Accuracy 90.736%\n",
      "Epoch 27, Batch 790, LR 1.005521 Loss 4.404912, Accuracy 90.735%\n",
      "Epoch 27, Batch 791, LR 1.005389 Loss 4.404658, Accuracy 90.735%\n",
      "Epoch 27, Batch 792, LR 1.005258 Loss 4.404409, Accuracy 90.736%\n",
      "Epoch 27, Batch 793, LR 1.005126 Loss 4.404099, Accuracy 90.735%\n",
      "Epoch 27, Batch 794, LR 1.004995 Loss 4.404479, Accuracy 90.733%\n",
      "Epoch 27, Batch 795, LR 1.004864 Loss 4.404349, Accuracy 90.734%\n",
      "Epoch 27, Batch 796, LR 1.004732 Loss 4.404402, Accuracy 90.734%\n",
      "Epoch 27, Batch 797, LR 1.004601 Loss 4.403342, Accuracy 90.738%\n",
      "Epoch 27, Batch 798, LR 1.004470 Loss 4.402548, Accuracy 90.742%\n",
      "Epoch 27, Batch 799, LR 1.004338 Loss 4.403094, Accuracy 90.741%\n",
      "Epoch 27, Batch 800, LR 1.004207 Loss 4.402873, Accuracy 90.742%\n",
      "Epoch 27, Batch 801, LR 1.004076 Loss 4.402149, Accuracy 90.743%\n",
      "Epoch 27, Batch 802, LR 1.003944 Loss 4.401400, Accuracy 90.750%\n",
      "Epoch 27, Batch 803, LR 1.003813 Loss 4.400680, Accuracy 90.754%\n",
      "Epoch 27, Batch 804, LR 1.003682 Loss 4.400799, Accuracy 90.756%\n",
      "Epoch 27, Batch 805, LR 1.003550 Loss 4.400338, Accuracy 90.755%\n",
      "Epoch 27, Batch 806, LR 1.003419 Loss 4.400593, Accuracy 90.756%\n",
      "Epoch 27, Batch 807, LR 1.003288 Loss 4.400308, Accuracy 90.761%\n",
      "Epoch 27, Batch 808, LR 1.003156 Loss 4.400653, Accuracy 90.758%\n",
      "Epoch 27, Batch 809, LR 1.003025 Loss 4.400238, Accuracy 90.761%\n",
      "Epoch 27, Batch 810, LR 1.002894 Loss 4.400072, Accuracy 90.768%\n",
      "Epoch 27, Batch 811, LR 1.002762 Loss 4.400205, Accuracy 90.769%\n",
      "Epoch 27, Batch 812, LR 1.002631 Loss 4.400286, Accuracy 90.766%\n",
      "Epoch 27, Batch 813, LR 1.002500 Loss 4.400259, Accuracy 90.763%\n",
      "Epoch 27, Batch 814, LR 1.002369 Loss 4.400567, Accuracy 90.762%\n",
      "Epoch 27, Batch 815, LR 1.002237 Loss 4.400360, Accuracy 90.764%\n",
      "Epoch 27, Batch 816, LR 1.002106 Loss 4.400388, Accuracy 90.764%\n",
      "Epoch 27, Batch 817, LR 1.001975 Loss 4.400358, Accuracy 90.763%\n",
      "Epoch 27, Batch 818, LR 1.001843 Loss 4.400077, Accuracy 90.766%\n",
      "Epoch 27, Batch 819, LR 1.001712 Loss 4.400961, Accuracy 90.756%\n",
      "Epoch 27, Batch 820, LR 1.001581 Loss 4.401595, Accuracy 90.753%\n",
      "Epoch 27, Batch 821, LR 1.001450 Loss 4.401998, Accuracy 90.750%\n",
      "Epoch 27, Batch 822, LR 1.001318 Loss 4.401549, Accuracy 90.752%\n",
      "Epoch 27, Batch 823, LR 1.001187 Loss 4.401387, Accuracy 90.751%\n",
      "Epoch 27, Batch 824, LR 1.001056 Loss 4.401142, Accuracy 90.753%\n",
      "Epoch 27, Batch 825, LR 1.000924 Loss 4.403046, Accuracy 90.742%\n",
      "Epoch 27, Batch 826, LR 1.000793 Loss 4.402756, Accuracy 90.739%\n",
      "Epoch 27, Batch 827, LR 1.000662 Loss 4.403274, Accuracy 90.738%\n",
      "Epoch 27, Batch 828, LR 1.000531 Loss 4.403410, Accuracy 90.737%\n",
      "Epoch 27, Batch 829, LR 1.000399 Loss 4.403333, Accuracy 90.735%\n",
      "Epoch 27, Batch 830, LR 1.000268 Loss 4.403431, Accuracy 90.732%\n",
      "Epoch 27, Batch 831, LR 1.000137 Loss 4.403179, Accuracy 90.733%\n",
      "Epoch 27, Batch 832, LR 1.000006 Loss 4.402637, Accuracy 90.734%\n",
      "Epoch 27, Batch 833, LR 0.999874 Loss 4.402233, Accuracy 90.738%\n",
      "Epoch 27, Batch 834, LR 0.999743 Loss 4.401627, Accuracy 90.738%\n",
      "Epoch 27, Batch 835, LR 0.999612 Loss 4.402349, Accuracy 90.740%\n",
      "Epoch 27, Batch 836, LR 0.999481 Loss 4.401419, Accuracy 90.746%\n",
      "Epoch 27, Batch 837, LR 0.999349 Loss 4.401697, Accuracy 90.743%\n",
      "Epoch 27, Batch 838, LR 0.999218 Loss 4.401777, Accuracy 90.742%\n",
      "Epoch 27, Batch 839, LR 0.999087 Loss 4.401908, Accuracy 90.740%\n",
      "Epoch 27, Batch 840, LR 0.998956 Loss 4.402085, Accuracy 90.740%\n",
      "Epoch 27, Batch 841, LR 0.998825 Loss 4.402807, Accuracy 90.736%\n",
      "Epoch 27, Batch 842, LR 0.998693 Loss 4.402444, Accuracy 90.737%\n",
      "Epoch 27, Batch 843, LR 0.998562 Loss 4.402966, Accuracy 90.738%\n",
      "Epoch 27, Batch 844, LR 0.998431 Loss 4.401931, Accuracy 90.743%\n",
      "Epoch 27, Batch 845, LR 0.998300 Loss 4.401462, Accuracy 90.743%\n",
      "Epoch 27, Batch 846, LR 0.998168 Loss 4.402704, Accuracy 90.738%\n",
      "Epoch 27, Batch 847, LR 0.998037 Loss 4.401876, Accuracy 90.743%\n",
      "Epoch 27, Batch 848, LR 0.997906 Loss 4.401801, Accuracy 90.745%\n",
      "Epoch 27, Batch 849, LR 0.997775 Loss 4.401692, Accuracy 90.747%\n",
      "Epoch 27, Batch 850, LR 0.997644 Loss 4.401706, Accuracy 90.748%\n",
      "Epoch 27, Batch 851, LR 0.997513 Loss 4.401555, Accuracy 90.746%\n",
      "Epoch 27, Batch 852, LR 0.997381 Loss 4.402120, Accuracy 90.741%\n",
      "Epoch 27, Batch 853, LR 0.997250 Loss 4.402508, Accuracy 90.741%\n",
      "Epoch 27, Batch 854, LR 0.997119 Loss 4.402742, Accuracy 90.738%\n",
      "Epoch 27, Batch 855, LR 0.996988 Loss 4.402779, Accuracy 90.738%\n",
      "Epoch 27, Batch 856, LR 0.996857 Loss 4.403239, Accuracy 90.735%\n",
      "Epoch 27, Batch 857, LR 0.996725 Loss 4.403202, Accuracy 90.735%\n",
      "Epoch 27, Batch 858, LR 0.996594 Loss 4.403116, Accuracy 90.738%\n",
      "Epoch 27, Batch 859, LR 0.996463 Loss 4.402521, Accuracy 90.742%\n",
      "Epoch 27, Batch 860, LR 0.996332 Loss 4.401704, Accuracy 90.747%\n",
      "Epoch 27, Batch 861, LR 0.996201 Loss 4.401612, Accuracy 90.749%\n",
      "Epoch 27, Batch 862, LR 0.996070 Loss 4.401652, Accuracy 90.750%\n",
      "Epoch 27, Batch 863, LR 0.995938 Loss 4.401584, Accuracy 90.748%\n",
      "Epoch 27, Batch 864, LR 0.995807 Loss 4.402291, Accuracy 90.743%\n",
      "Epoch 27, Batch 865, LR 0.995676 Loss 4.401707, Accuracy 90.743%\n",
      "Epoch 27, Batch 866, LR 0.995545 Loss 4.401752, Accuracy 90.742%\n",
      "Epoch 27, Batch 867, LR 0.995414 Loss 4.401783, Accuracy 90.741%\n",
      "Epoch 27, Batch 868, LR 0.995283 Loss 4.401598, Accuracy 90.742%\n",
      "Epoch 27, Batch 869, LR 0.995152 Loss 4.401351, Accuracy 90.741%\n",
      "Epoch 27, Batch 870, LR 0.995020 Loss 4.401297, Accuracy 90.742%\n",
      "Epoch 27, Batch 871, LR 0.994889 Loss 4.401490, Accuracy 90.741%\n",
      "Epoch 27, Batch 872, LR 0.994758 Loss 4.401758, Accuracy 90.739%\n",
      "Epoch 27, Batch 873, LR 0.994627 Loss 4.401280, Accuracy 90.740%\n",
      "Epoch 27, Batch 874, LR 0.994496 Loss 4.400220, Accuracy 90.746%\n",
      "Epoch 27, Batch 875, LR 0.994365 Loss 4.400007, Accuracy 90.748%\n",
      "Epoch 27, Batch 876, LR 0.994234 Loss 4.400573, Accuracy 90.744%\n",
      "Epoch 27, Batch 877, LR 0.994102 Loss 4.400282, Accuracy 90.742%\n",
      "Epoch 27, Batch 878, LR 0.993971 Loss 4.400253, Accuracy 90.742%\n",
      "Epoch 27, Batch 879, LR 0.993840 Loss 4.399761, Accuracy 90.741%\n",
      "Epoch 27, Batch 880, LR 0.993709 Loss 4.399272, Accuracy 90.743%\n",
      "Epoch 27, Batch 881, LR 0.993578 Loss 4.399928, Accuracy 90.742%\n",
      "Epoch 27, Batch 882, LR 0.993447 Loss 4.400342, Accuracy 90.739%\n",
      "Epoch 27, Batch 883, LR 0.993316 Loss 4.400651, Accuracy 90.735%\n",
      "Epoch 27, Batch 884, LR 0.993185 Loss 4.400496, Accuracy 90.737%\n",
      "Epoch 27, Batch 885, LR 0.993054 Loss 4.401000, Accuracy 90.737%\n",
      "Epoch 27, Batch 886, LR 0.992923 Loss 4.400530, Accuracy 90.740%\n",
      "Epoch 27, Batch 887, LR 0.992791 Loss 4.400788, Accuracy 90.739%\n",
      "Epoch 27, Batch 888, LR 0.992660 Loss 4.400654, Accuracy 90.738%\n",
      "Epoch 27, Batch 889, LR 0.992529 Loss 4.399621, Accuracy 90.741%\n",
      "Epoch 27, Batch 890, LR 0.992398 Loss 4.399577, Accuracy 90.739%\n",
      "Epoch 27, Batch 891, LR 0.992267 Loss 4.399409, Accuracy 90.740%\n",
      "Epoch 27, Batch 892, LR 0.992136 Loss 4.400387, Accuracy 90.737%\n",
      "Epoch 27, Batch 893, LR 0.992005 Loss 4.400299, Accuracy 90.740%\n",
      "Epoch 27, Batch 894, LR 0.991874 Loss 4.400375, Accuracy 90.739%\n",
      "Epoch 27, Batch 895, LR 0.991743 Loss 4.400241, Accuracy 90.736%\n",
      "Epoch 27, Batch 896, LR 0.991612 Loss 4.400297, Accuracy 90.737%\n",
      "Epoch 27, Batch 897, LR 0.991481 Loss 4.400324, Accuracy 90.740%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 898, LR 0.991350 Loss 4.400134, Accuracy 90.741%\n",
      "Epoch 27, Batch 899, LR 0.991219 Loss 4.400168, Accuracy 90.737%\n",
      "Epoch 27, Batch 900, LR 0.991088 Loss 4.401073, Accuracy 90.729%\n",
      "Epoch 27, Batch 901, LR 0.990957 Loss 4.400601, Accuracy 90.734%\n",
      "Epoch 27, Batch 902, LR 0.990826 Loss 4.401216, Accuracy 90.733%\n",
      "Epoch 27, Batch 903, LR 0.990694 Loss 4.402422, Accuracy 90.728%\n",
      "Epoch 27, Batch 904, LR 0.990563 Loss 4.402173, Accuracy 90.730%\n",
      "Epoch 27, Batch 905, LR 0.990432 Loss 4.402384, Accuracy 90.727%\n",
      "Epoch 27, Batch 906, LR 0.990301 Loss 4.402612, Accuracy 90.728%\n",
      "Epoch 27, Batch 907, LR 0.990170 Loss 4.402353, Accuracy 90.726%\n",
      "Epoch 27, Batch 908, LR 0.990039 Loss 4.402404, Accuracy 90.727%\n",
      "Epoch 27, Batch 909, LR 0.989908 Loss 4.403633, Accuracy 90.724%\n",
      "Epoch 27, Batch 910, LR 0.989777 Loss 4.403437, Accuracy 90.726%\n",
      "Epoch 27, Batch 911, LR 0.989646 Loss 4.403574, Accuracy 90.727%\n",
      "Epoch 27, Batch 912, LR 0.989515 Loss 4.403138, Accuracy 90.730%\n",
      "Epoch 27, Batch 913, LR 0.989384 Loss 4.403131, Accuracy 90.730%\n",
      "Epoch 27, Batch 914, LR 0.989253 Loss 4.402822, Accuracy 90.727%\n",
      "Epoch 27, Batch 915, LR 0.989122 Loss 4.403117, Accuracy 90.725%\n",
      "Epoch 27, Batch 916, LR 0.988991 Loss 4.403120, Accuracy 90.722%\n",
      "Epoch 27, Batch 917, LR 0.988860 Loss 4.403023, Accuracy 90.721%\n",
      "Epoch 27, Batch 918, LR 0.988729 Loss 4.403117, Accuracy 90.721%\n",
      "Epoch 27, Batch 919, LR 0.988598 Loss 4.403521, Accuracy 90.721%\n",
      "Epoch 27, Batch 920, LR 0.988467 Loss 4.403851, Accuracy 90.720%\n",
      "Epoch 27, Batch 921, LR 0.988336 Loss 4.403823, Accuracy 90.719%\n",
      "Epoch 27, Batch 922, LR 0.988205 Loss 4.403563, Accuracy 90.723%\n",
      "Epoch 27, Batch 923, LR 0.988074 Loss 4.403375, Accuracy 90.724%\n",
      "Epoch 27, Batch 924, LR 0.987943 Loss 4.403678, Accuracy 90.722%\n",
      "Epoch 27, Batch 925, LR 0.987812 Loss 4.403082, Accuracy 90.727%\n",
      "Epoch 27, Batch 926, LR 0.987681 Loss 4.403166, Accuracy 90.725%\n",
      "Epoch 27, Batch 927, LR 0.987550 Loss 4.402218, Accuracy 90.730%\n",
      "Epoch 27, Batch 928, LR 0.987419 Loss 4.402386, Accuracy 90.729%\n",
      "Epoch 27, Batch 929, LR 0.987288 Loss 4.402390, Accuracy 90.732%\n",
      "Epoch 27, Batch 930, LR 0.987158 Loss 4.401510, Accuracy 90.733%\n",
      "Epoch 27, Batch 931, LR 0.987027 Loss 4.402030, Accuracy 90.732%\n",
      "Epoch 27, Batch 932, LR 0.986896 Loss 4.402216, Accuracy 90.731%\n",
      "Epoch 27, Batch 933, LR 0.986765 Loss 4.402682, Accuracy 90.727%\n",
      "Epoch 27, Batch 934, LR 0.986634 Loss 4.402432, Accuracy 90.729%\n",
      "Epoch 27, Batch 935, LR 0.986503 Loss 4.402911, Accuracy 90.727%\n",
      "Epoch 27, Batch 936, LR 0.986372 Loss 4.402377, Accuracy 90.732%\n",
      "Epoch 27, Batch 937, LR 0.986241 Loss 4.402153, Accuracy 90.733%\n",
      "Epoch 27, Batch 938, LR 0.986110 Loss 4.401814, Accuracy 90.734%\n",
      "Epoch 27, Batch 939, LR 0.985979 Loss 4.401632, Accuracy 90.734%\n",
      "Epoch 27, Batch 940, LR 0.985848 Loss 4.401587, Accuracy 90.732%\n",
      "Epoch 27, Batch 941, LR 0.985717 Loss 4.401172, Accuracy 90.735%\n",
      "Epoch 27, Batch 942, LR 0.985586 Loss 4.401490, Accuracy 90.734%\n",
      "Epoch 27, Batch 943, LR 0.985455 Loss 4.401323, Accuracy 90.736%\n",
      "Epoch 27, Batch 944, LR 0.985324 Loss 4.401441, Accuracy 90.733%\n",
      "Epoch 27, Batch 945, LR 0.985193 Loss 4.401162, Accuracy 90.735%\n",
      "Epoch 27, Batch 946, LR 0.985063 Loss 4.401817, Accuracy 90.731%\n",
      "Epoch 27, Batch 947, LR 0.984932 Loss 4.402316, Accuracy 90.726%\n",
      "Epoch 27, Batch 948, LR 0.984801 Loss 4.401920, Accuracy 90.729%\n",
      "Epoch 27, Batch 949, LR 0.984670 Loss 4.402111, Accuracy 90.727%\n",
      "Epoch 27, Batch 950, LR 0.984539 Loss 4.401962, Accuracy 90.730%\n",
      "Epoch 27, Batch 951, LR 0.984408 Loss 4.403220, Accuracy 90.725%\n",
      "Epoch 27, Batch 952, LR 0.984277 Loss 4.403890, Accuracy 90.722%\n",
      "Epoch 27, Batch 953, LR 0.984146 Loss 4.404147, Accuracy 90.718%\n",
      "Epoch 27, Batch 954, LR 0.984015 Loss 4.403944, Accuracy 90.718%\n",
      "Epoch 27, Batch 955, LR 0.983885 Loss 4.404113, Accuracy 90.717%\n",
      "Epoch 27, Batch 956, LR 0.983754 Loss 4.403679, Accuracy 90.717%\n",
      "Epoch 27, Batch 957, LR 0.983623 Loss 4.404580, Accuracy 90.712%\n",
      "Epoch 27, Batch 958, LR 0.983492 Loss 4.404307, Accuracy 90.713%\n",
      "Epoch 27, Batch 959, LR 0.983361 Loss 4.405012, Accuracy 90.707%\n",
      "Epoch 27, Batch 960, LR 0.983230 Loss 4.405382, Accuracy 90.707%\n",
      "Epoch 27, Batch 961, LR 0.983099 Loss 4.405507, Accuracy 90.703%\n",
      "Epoch 27, Batch 962, LR 0.982968 Loss 4.405089, Accuracy 90.704%\n",
      "Epoch 27, Batch 963, LR 0.982838 Loss 4.405414, Accuracy 90.702%\n",
      "Epoch 27, Batch 964, LR 0.982707 Loss 4.405761, Accuracy 90.700%\n",
      "Epoch 27, Batch 965, LR 0.982576 Loss 4.405257, Accuracy 90.699%\n",
      "Epoch 27, Batch 966, LR 0.982445 Loss 4.405213, Accuracy 90.699%\n",
      "Epoch 27, Batch 967, LR 0.982314 Loss 4.405234, Accuracy 90.696%\n",
      "Epoch 27, Batch 968, LR 0.982183 Loss 4.404754, Accuracy 90.699%\n",
      "Epoch 27, Batch 969, LR 0.982052 Loss 4.405171, Accuracy 90.698%\n",
      "Epoch 27, Batch 970, LR 0.981922 Loss 4.405200, Accuracy 90.699%\n",
      "Epoch 27, Batch 971, LR 0.981791 Loss 4.405655, Accuracy 90.700%\n",
      "Epoch 27, Batch 972, LR 0.981660 Loss 4.405875, Accuracy 90.700%\n",
      "Epoch 27, Batch 973, LR 0.981529 Loss 4.405341, Accuracy 90.703%\n",
      "Epoch 27, Batch 974, LR 0.981398 Loss 4.405269, Accuracy 90.704%\n",
      "Epoch 27, Batch 975, LR 0.981267 Loss 4.404472, Accuracy 90.706%\n",
      "Epoch 27, Batch 976, LR 0.981137 Loss 4.403929, Accuracy 90.710%\n",
      "Epoch 27, Batch 977, LR 0.981006 Loss 4.403212, Accuracy 90.711%\n",
      "Epoch 27, Batch 978, LR 0.980875 Loss 4.403261, Accuracy 90.710%\n",
      "Epoch 27, Batch 979, LR 0.980744 Loss 4.403097, Accuracy 90.713%\n",
      "Epoch 27, Batch 980, LR 0.980613 Loss 4.403217, Accuracy 90.713%\n",
      "Epoch 27, Batch 981, LR 0.980483 Loss 4.402710, Accuracy 90.717%\n",
      "Epoch 27, Batch 982, LR 0.980352 Loss 4.402706, Accuracy 90.717%\n",
      "Epoch 27, Batch 983, LR 0.980221 Loss 4.402874, Accuracy 90.720%\n",
      "Epoch 27, Batch 984, LR 0.980090 Loss 4.402194, Accuracy 90.723%\n",
      "Epoch 27, Batch 985, LR 0.979959 Loss 4.402296, Accuracy 90.721%\n",
      "Epoch 27, Batch 986, LR 0.979829 Loss 4.401813, Accuracy 90.722%\n",
      "Epoch 27, Batch 987, LR 0.979698 Loss 4.401739, Accuracy 90.725%\n",
      "Epoch 27, Batch 988, LR 0.979567 Loss 4.401297, Accuracy 90.726%\n",
      "Epoch 27, Batch 989, LR 0.979436 Loss 4.401424, Accuracy 90.728%\n",
      "Epoch 27, Batch 990, LR 0.979306 Loss 4.401982, Accuracy 90.723%\n",
      "Epoch 27, Batch 991, LR 0.979175 Loss 4.402784, Accuracy 90.718%\n",
      "Epoch 27, Batch 992, LR 0.979044 Loss 4.402670, Accuracy 90.717%\n",
      "Epoch 27, Batch 993, LR 0.978913 Loss 4.402541, Accuracy 90.716%\n",
      "Epoch 27, Batch 994, LR 0.978782 Loss 4.402315, Accuracy 90.718%\n",
      "Epoch 27, Batch 995, LR 0.978652 Loss 4.402120, Accuracy 90.719%\n",
      "Epoch 27, Batch 996, LR 0.978521 Loss 4.402443, Accuracy 90.714%\n",
      "Epoch 27, Batch 997, LR 0.978390 Loss 4.402177, Accuracy 90.714%\n",
      "Epoch 27, Batch 998, LR 0.978259 Loss 4.402667, Accuracy 90.707%\n",
      "Epoch 27, Batch 999, LR 0.978129 Loss 4.402483, Accuracy 90.709%\n",
      "Epoch 27, Batch 1000, LR 0.977998 Loss 4.402925, Accuracy 90.704%\n",
      "Epoch 27, Batch 1001, LR 0.977867 Loss 4.402479, Accuracy 90.707%\n",
      "Epoch 27, Batch 1002, LR 0.977736 Loss 4.402550, Accuracy 90.708%\n",
      "Epoch 27, Batch 1003, LR 0.977606 Loss 4.402715, Accuracy 90.710%\n",
      "Epoch 27, Batch 1004, LR 0.977475 Loss 4.403006, Accuracy 90.708%\n",
      "Epoch 27, Batch 1005, LR 0.977344 Loss 4.402352, Accuracy 90.708%\n",
      "Epoch 27, Batch 1006, LR 0.977214 Loss 4.402503, Accuracy 90.704%\n",
      "Epoch 27, Batch 1007, LR 0.977083 Loss 4.403098, Accuracy 90.699%\n",
      "Epoch 27, Batch 1008, LR 0.976952 Loss 4.403097, Accuracy 90.701%\n",
      "Epoch 27, Batch 1009, LR 0.976821 Loss 4.403854, Accuracy 90.694%\n",
      "Epoch 27, Batch 1010, LR 0.976691 Loss 4.403974, Accuracy 90.695%\n",
      "Epoch 27, Batch 1011, LR 0.976560 Loss 4.404100, Accuracy 90.694%\n",
      "Epoch 27, Batch 1012, LR 0.976429 Loss 4.404243, Accuracy 90.692%\n",
      "Epoch 27, Batch 1013, LR 0.976299 Loss 4.404320, Accuracy 90.691%\n",
      "Epoch 27, Batch 1014, LR 0.976168 Loss 4.404756, Accuracy 90.688%\n",
      "Epoch 27, Batch 1015, LR 0.976037 Loss 4.404148, Accuracy 90.690%\n",
      "Epoch 27, Batch 1016, LR 0.975906 Loss 4.403841, Accuracy 90.693%\n",
      "Epoch 27, Batch 1017, LR 0.975776 Loss 4.402914, Accuracy 90.694%\n",
      "Epoch 27, Batch 1018, LR 0.975645 Loss 4.402778, Accuracy 90.696%\n",
      "Epoch 27, Batch 1019, LR 0.975514 Loss 4.403243, Accuracy 90.692%\n",
      "Epoch 27, Batch 1020, LR 0.975384 Loss 4.402800, Accuracy 90.693%\n",
      "Epoch 27, Batch 1021, LR 0.975253 Loss 4.403106, Accuracy 90.690%\n",
      "Epoch 27, Batch 1022, LR 0.975122 Loss 4.402987, Accuracy 90.688%\n",
      "Epoch 27, Batch 1023, LR 0.974992 Loss 4.403677, Accuracy 90.684%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 1024, LR 0.974861 Loss 4.402875, Accuracy 90.685%\n",
      "Epoch 27, Batch 1025, LR 0.974730 Loss 4.402998, Accuracy 90.684%\n",
      "Epoch 27, Batch 1026, LR 0.974600 Loss 4.402838, Accuracy 90.684%\n",
      "Epoch 27, Batch 1027, LR 0.974469 Loss 4.402696, Accuracy 90.683%\n",
      "Epoch 27, Batch 1028, LR 0.974338 Loss 4.402393, Accuracy 90.684%\n",
      "Epoch 27, Batch 1029, LR 0.974208 Loss 4.402257, Accuracy 90.686%\n",
      "Epoch 27, Batch 1030, LR 0.974077 Loss 4.402288, Accuracy 90.686%\n",
      "Epoch 27, Batch 1031, LR 0.973946 Loss 4.401764, Accuracy 90.688%\n",
      "Epoch 27, Batch 1032, LR 0.973816 Loss 4.401394, Accuracy 90.688%\n",
      "Epoch 27, Batch 1033, LR 0.973685 Loss 4.400896, Accuracy 90.689%\n",
      "Epoch 27, Batch 1034, LR 0.973554 Loss 4.400801, Accuracy 90.689%\n",
      "Epoch 27, Batch 1035, LR 0.973424 Loss 4.400412, Accuracy 90.691%\n",
      "Epoch 27, Batch 1036, LR 0.973293 Loss 4.399810, Accuracy 90.694%\n",
      "Epoch 27, Batch 1037, LR 0.973163 Loss 4.399536, Accuracy 90.694%\n",
      "Epoch 27, Batch 1038, LR 0.973032 Loss 4.399449, Accuracy 90.696%\n",
      "Epoch 27, Batch 1039, LR 0.972901 Loss 4.399500, Accuracy 90.697%\n",
      "Epoch 27, Batch 1040, LR 0.972771 Loss 4.400123, Accuracy 90.693%\n",
      "Epoch 27, Batch 1041, LR 0.972640 Loss 4.399969, Accuracy 90.692%\n",
      "Epoch 27, Batch 1042, LR 0.972509 Loss 4.399624, Accuracy 90.695%\n",
      "Epoch 27, Batch 1043, LR 0.972379 Loss 4.399644, Accuracy 90.698%\n",
      "Epoch 27, Batch 1044, LR 0.972248 Loss 4.399305, Accuracy 90.701%\n",
      "Epoch 27, Batch 1045, LR 0.972118 Loss 4.399239, Accuracy 90.703%\n",
      "Epoch 27, Batch 1046, LR 0.971987 Loss 4.398548, Accuracy 90.703%\n",
      "Epoch 27, Batch 1047, LR 0.971856 Loss 4.398879, Accuracy 90.703%\n",
      "Epoch 27, Loss (train set) 4.398879, Accuracy (train set) 90.703%\n",
      "Epoch 28, Batch 1, LR 0.971726 Loss 4.450074, Accuracy 89.062%\n",
      "Epoch 28, Batch 2, LR 0.971595 Loss 4.292588, Accuracy 89.453%\n",
      "Epoch 28, Batch 3, LR 0.971465 Loss 4.339963, Accuracy 90.625%\n",
      "Epoch 28, Batch 4, LR 0.971334 Loss 4.359126, Accuracy 90.820%\n",
      "Epoch 28, Batch 5, LR 0.971204 Loss 4.335403, Accuracy 91.094%\n",
      "Epoch 28, Batch 6, LR 0.971073 Loss 4.406076, Accuracy 91.016%\n",
      "Epoch 28, Batch 7, LR 0.970942 Loss 4.405424, Accuracy 90.960%\n",
      "Epoch 28, Batch 8, LR 0.970812 Loss 4.343545, Accuracy 91.211%\n",
      "Epoch 28, Batch 9, LR 0.970681 Loss 4.305546, Accuracy 91.059%\n",
      "Epoch 28, Batch 10, LR 0.970551 Loss 4.314454, Accuracy 91.406%\n",
      "Epoch 28, Batch 11, LR 0.970420 Loss 4.328419, Accuracy 91.335%\n",
      "Epoch 28, Batch 12, LR 0.970290 Loss 4.300934, Accuracy 91.471%\n",
      "Epoch 28, Batch 13, LR 0.970159 Loss 4.304662, Accuracy 91.406%\n",
      "Epoch 28, Batch 14, LR 0.970028 Loss 4.275751, Accuracy 91.574%\n",
      "Epoch 28, Batch 15, LR 0.969898 Loss 4.299742, Accuracy 91.510%\n",
      "Epoch 28, Batch 16, LR 0.969767 Loss 4.282221, Accuracy 91.748%\n",
      "Epoch 28, Batch 17, LR 0.969637 Loss 4.284194, Accuracy 91.590%\n",
      "Epoch 28, Batch 18, LR 0.969506 Loss 4.278452, Accuracy 91.623%\n",
      "Epoch 28, Batch 19, LR 0.969376 Loss 4.273086, Accuracy 91.612%\n",
      "Epoch 28, Batch 20, LR 0.969245 Loss 4.267813, Accuracy 91.523%\n",
      "Epoch 28, Batch 21, LR 0.969115 Loss 4.276897, Accuracy 91.555%\n",
      "Epoch 28, Batch 22, LR 0.968984 Loss 4.264580, Accuracy 91.548%\n",
      "Epoch 28, Batch 23, LR 0.968854 Loss 4.264403, Accuracy 91.542%\n",
      "Epoch 28, Batch 24, LR 0.968723 Loss 4.252432, Accuracy 91.536%\n",
      "Epoch 28, Batch 25, LR 0.968593 Loss 4.240676, Accuracy 91.625%\n",
      "Epoch 28, Batch 26, LR 0.968462 Loss 4.225443, Accuracy 91.587%\n",
      "Epoch 28, Batch 27, LR 0.968332 Loss 4.212627, Accuracy 91.725%\n",
      "Epoch 28, Batch 28, LR 0.968201 Loss 4.226920, Accuracy 91.629%\n",
      "Epoch 28, Batch 29, LR 0.968071 Loss 4.243617, Accuracy 91.514%\n",
      "Epoch 28, Batch 30, LR 0.967940 Loss 4.253485, Accuracy 91.484%\n",
      "Epoch 28, Batch 31, LR 0.967810 Loss 4.256566, Accuracy 91.356%\n",
      "Epoch 28, Batch 32, LR 0.967679 Loss 4.237668, Accuracy 91.406%\n",
      "Epoch 28, Batch 33, LR 0.967549 Loss 4.226930, Accuracy 91.477%\n",
      "Epoch 28, Batch 34, LR 0.967418 Loss 4.235801, Accuracy 91.360%\n",
      "Epoch 28, Batch 35, LR 0.967288 Loss 4.237163, Accuracy 91.339%\n",
      "Epoch 28, Batch 36, LR 0.967157 Loss 4.226093, Accuracy 91.428%\n",
      "Epoch 28, Batch 37, LR 0.967027 Loss 4.235137, Accuracy 91.406%\n",
      "Epoch 28, Batch 38, LR 0.966896 Loss 4.220173, Accuracy 91.468%\n",
      "Epoch 28, Batch 39, LR 0.966766 Loss 4.218833, Accuracy 91.426%\n",
      "Epoch 28, Batch 40, LR 0.966635 Loss 4.201712, Accuracy 91.504%\n",
      "Epoch 28, Batch 41, LR 0.966505 Loss 4.196419, Accuracy 91.559%\n",
      "Epoch 28, Batch 42, LR 0.966374 Loss 4.195443, Accuracy 91.611%\n",
      "Epoch 28, Batch 43, LR 0.966244 Loss 4.192901, Accuracy 91.679%\n",
      "Epoch 28, Batch 44, LR 0.966113 Loss 4.207234, Accuracy 91.673%\n",
      "Epoch 28, Batch 45, LR 0.965983 Loss 4.215395, Accuracy 91.580%\n",
      "Epoch 28, Batch 46, LR 0.965853 Loss 4.212421, Accuracy 91.576%\n",
      "Epoch 28, Batch 47, LR 0.965722 Loss 4.208577, Accuracy 91.606%\n",
      "Epoch 28, Batch 48, LR 0.965592 Loss 4.218708, Accuracy 91.553%\n",
      "Epoch 28, Batch 49, LR 0.965461 Loss 4.220256, Accuracy 91.486%\n",
      "Epoch 28, Batch 50, LR 0.965331 Loss 4.221169, Accuracy 91.531%\n",
      "Epoch 28, Batch 51, LR 0.965200 Loss 4.242738, Accuracy 91.452%\n",
      "Epoch 28, Batch 52, LR 0.965070 Loss 4.236531, Accuracy 91.436%\n",
      "Epoch 28, Batch 53, LR 0.964939 Loss 4.241393, Accuracy 91.362%\n",
      "Epoch 28, Batch 54, LR 0.964809 Loss 4.243480, Accuracy 91.319%\n",
      "Epoch 28, Batch 55, LR 0.964679 Loss 4.233371, Accuracy 91.392%\n",
      "Epoch 28, Batch 56, LR 0.964548 Loss 4.232996, Accuracy 91.406%\n",
      "Epoch 28, Batch 57, LR 0.964418 Loss 4.235568, Accuracy 91.365%\n",
      "Epoch 28, Batch 58, LR 0.964287 Loss 4.233281, Accuracy 91.352%\n",
      "Epoch 28, Batch 59, LR 0.964157 Loss 4.234623, Accuracy 91.380%\n",
      "Epoch 28, Batch 60, LR 0.964027 Loss 4.229956, Accuracy 91.380%\n",
      "Epoch 28, Batch 61, LR 0.963896 Loss 4.222853, Accuracy 91.406%\n",
      "Epoch 28, Batch 62, LR 0.963766 Loss 4.221275, Accuracy 91.419%\n",
      "Epoch 28, Batch 63, LR 0.963635 Loss 4.214474, Accuracy 91.443%\n",
      "Epoch 28, Batch 64, LR 0.963505 Loss 4.226880, Accuracy 91.345%\n",
      "Epoch 28, Batch 65, LR 0.963375 Loss 4.232621, Accuracy 91.310%\n",
      "Epoch 28, Batch 66, LR 0.963244 Loss 4.245874, Accuracy 91.252%\n",
      "Epoch 28, Batch 67, LR 0.963114 Loss 4.243838, Accuracy 91.220%\n",
      "Epoch 28, Batch 68, LR 0.962983 Loss 4.240781, Accuracy 91.280%\n",
      "Epoch 28, Batch 69, LR 0.962853 Loss 4.248618, Accuracy 91.270%\n",
      "Epoch 28, Batch 70, LR 0.962723 Loss 4.256333, Accuracy 91.250%\n",
      "Epoch 28, Batch 71, LR 0.962592 Loss 4.258154, Accuracy 91.252%\n",
      "Epoch 28, Batch 72, LR 0.962462 Loss 4.251564, Accuracy 91.298%\n",
      "Epoch 28, Batch 73, LR 0.962332 Loss 4.250110, Accuracy 91.299%\n",
      "Epoch 28, Batch 74, LR 0.962201 Loss 4.254179, Accuracy 91.290%\n",
      "Epoch 28, Batch 75, LR 0.962071 Loss 4.254925, Accuracy 91.302%\n",
      "Epoch 28, Batch 76, LR 0.961941 Loss 4.251911, Accuracy 91.314%\n",
      "Epoch 28, Batch 77, LR 0.961810 Loss 4.246566, Accuracy 91.335%\n",
      "Epoch 28, Batch 78, LR 0.961680 Loss 4.249204, Accuracy 91.316%\n",
      "Epoch 28, Batch 79, LR 0.961550 Loss 4.247359, Accuracy 91.347%\n",
      "Epoch 28, Batch 80, LR 0.961419 Loss 4.243400, Accuracy 91.338%\n",
      "Epoch 28, Batch 81, LR 0.961289 Loss 4.254151, Accuracy 91.300%\n",
      "Epoch 28, Batch 82, LR 0.961159 Loss 4.256281, Accuracy 91.254%\n",
      "Epoch 28, Batch 83, LR 0.961028 Loss 4.264555, Accuracy 91.227%\n",
      "Epoch 28, Batch 84, LR 0.960898 Loss 4.258613, Accuracy 91.230%\n",
      "Epoch 28, Batch 85, LR 0.960768 Loss 4.254137, Accuracy 91.278%\n",
      "Epoch 28, Batch 86, LR 0.960637 Loss 4.253668, Accuracy 91.297%\n",
      "Epoch 28, Batch 87, LR 0.960507 Loss 4.255358, Accuracy 91.290%\n",
      "Epoch 28, Batch 88, LR 0.960377 Loss 4.250337, Accuracy 91.255%\n",
      "Epoch 28, Batch 89, LR 0.960246 Loss 4.250269, Accuracy 91.239%\n",
      "Epoch 28, Batch 90, LR 0.960116 Loss 4.248242, Accuracy 91.198%\n",
      "Epoch 28, Batch 91, LR 0.959986 Loss 4.247577, Accuracy 91.226%\n",
      "Epoch 28, Batch 92, LR 0.959855 Loss 4.250543, Accuracy 91.211%\n",
      "Epoch 28, Batch 93, LR 0.959725 Loss 4.248054, Accuracy 91.230%\n",
      "Epoch 28, Batch 94, LR 0.959595 Loss 4.242187, Accuracy 91.265%\n",
      "Epoch 28, Batch 95, LR 0.959465 Loss 4.230362, Accuracy 91.332%\n",
      "Epoch 28, Batch 96, LR 0.959334 Loss 4.228675, Accuracy 91.341%\n",
      "Epoch 28, Batch 97, LR 0.959204 Loss 4.232396, Accuracy 91.318%\n",
      "Epoch 28, Batch 98, LR 0.959074 Loss 4.226019, Accuracy 91.335%\n",
      "Epoch 28, Batch 99, LR 0.958943 Loss 4.226032, Accuracy 91.335%\n",
      "Epoch 28, Batch 100, LR 0.958813 Loss 4.223953, Accuracy 91.352%\n",
      "Epoch 28, Batch 101, LR 0.958683 Loss 4.224203, Accuracy 91.337%\n",
      "Epoch 28, Batch 102, LR 0.958553 Loss 4.219636, Accuracy 91.368%\n",
      "Epoch 28, Batch 103, LR 0.958422 Loss 4.224331, Accuracy 91.376%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 104, LR 0.958292 Loss 4.220680, Accuracy 91.384%\n",
      "Epoch 28, Batch 105, LR 0.958162 Loss 4.223321, Accuracy 91.369%\n",
      "Epoch 28, Batch 106, LR 0.958032 Loss 4.227735, Accuracy 91.362%\n",
      "Epoch 28, Batch 107, LR 0.957901 Loss 4.229070, Accuracy 91.341%\n",
      "Epoch 28, Batch 108, LR 0.957771 Loss 4.231409, Accuracy 91.312%\n",
      "Epoch 28, Batch 109, LR 0.957641 Loss 4.237294, Accuracy 91.292%\n",
      "Epoch 28, Batch 110, LR 0.957511 Loss 4.239151, Accuracy 91.278%\n",
      "Epoch 28, Batch 111, LR 0.957380 Loss 4.241814, Accuracy 91.273%\n",
      "Epoch 28, Batch 112, LR 0.957250 Loss 4.239329, Accuracy 91.267%\n",
      "Epoch 28, Batch 113, LR 0.957120 Loss 4.237738, Accuracy 91.275%\n",
      "Epoch 28, Batch 114, LR 0.956990 Loss 4.232326, Accuracy 91.303%\n",
      "Epoch 28, Batch 115, LR 0.956860 Loss 4.233686, Accuracy 91.284%\n",
      "Epoch 28, Batch 116, LR 0.956729 Loss 4.237260, Accuracy 91.278%\n",
      "Epoch 28, Batch 117, LR 0.956599 Loss 4.240249, Accuracy 91.239%\n",
      "Epoch 28, Batch 118, LR 0.956469 Loss 4.235898, Accuracy 91.247%\n",
      "Epoch 28, Batch 119, LR 0.956339 Loss 4.238091, Accuracy 91.222%\n",
      "Epoch 28, Batch 120, LR 0.956209 Loss 4.236052, Accuracy 91.243%\n",
      "Epoch 28, Batch 121, LR 0.956078 Loss 4.240751, Accuracy 91.219%\n",
      "Epoch 28, Batch 122, LR 0.955948 Loss 4.240430, Accuracy 91.240%\n",
      "Epoch 28, Batch 123, LR 0.955818 Loss 4.239004, Accuracy 91.241%\n",
      "Epoch 28, Batch 124, LR 0.955688 Loss 4.238494, Accuracy 91.211%\n",
      "Epoch 28, Batch 125, LR 0.955558 Loss 4.238945, Accuracy 91.213%\n",
      "Epoch 28, Batch 126, LR 0.955427 Loss 4.238239, Accuracy 91.189%\n",
      "Epoch 28, Batch 127, LR 0.955297 Loss 4.241302, Accuracy 91.191%\n",
      "Epoch 28, Batch 128, LR 0.955167 Loss 4.240581, Accuracy 91.174%\n",
      "Epoch 28, Batch 129, LR 0.955037 Loss 4.235218, Accuracy 91.212%\n",
      "Epoch 28, Batch 130, LR 0.954907 Loss 4.241989, Accuracy 91.172%\n",
      "Epoch 28, Batch 131, LR 0.954777 Loss 4.244416, Accuracy 91.180%\n",
      "Epoch 28, Batch 132, LR 0.954646 Loss 4.242025, Accuracy 91.193%\n",
      "Epoch 28, Batch 133, LR 0.954516 Loss 4.246114, Accuracy 91.171%\n",
      "Epoch 28, Batch 134, LR 0.954386 Loss 4.246241, Accuracy 91.208%\n",
      "Epoch 28, Batch 135, LR 0.954256 Loss 4.244915, Accuracy 91.227%\n",
      "Epoch 28, Batch 136, LR 0.954126 Loss 4.244027, Accuracy 91.228%\n",
      "Epoch 28, Batch 137, LR 0.953996 Loss 4.241706, Accuracy 91.241%\n",
      "Epoch 28, Batch 138, LR 0.953865 Loss 4.241832, Accuracy 91.242%\n",
      "Epoch 28, Batch 139, LR 0.953735 Loss 4.239797, Accuracy 91.260%\n",
      "Epoch 28, Batch 140, LR 0.953605 Loss 4.239108, Accuracy 91.261%\n",
      "Epoch 28, Batch 141, LR 0.953475 Loss 4.242515, Accuracy 91.246%\n",
      "Epoch 28, Batch 142, LR 0.953345 Loss 4.248881, Accuracy 91.203%\n",
      "Epoch 28, Batch 143, LR 0.953215 Loss 4.247896, Accuracy 91.220%\n",
      "Epoch 28, Batch 144, LR 0.953085 Loss 4.250848, Accuracy 91.206%\n",
      "Epoch 28, Batch 145, LR 0.952955 Loss 4.251749, Accuracy 91.191%\n",
      "Epoch 28, Batch 146, LR 0.952824 Loss 4.251875, Accuracy 91.176%\n",
      "Epoch 28, Batch 147, LR 0.952694 Loss 4.251843, Accuracy 91.156%\n",
      "Epoch 28, Batch 148, LR 0.952564 Loss 4.252031, Accuracy 91.158%\n",
      "Epoch 28, Batch 149, LR 0.952434 Loss 4.250116, Accuracy 91.165%\n",
      "Epoch 28, Batch 150, LR 0.952304 Loss 4.252453, Accuracy 91.156%\n",
      "Epoch 28, Batch 151, LR 0.952174 Loss 4.250962, Accuracy 91.179%\n",
      "Epoch 28, Batch 152, LR 0.952044 Loss 4.252351, Accuracy 91.160%\n",
      "Epoch 28, Batch 153, LR 0.951914 Loss 4.252630, Accuracy 91.161%\n",
      "Epoch 28, Batch 154, LR 0.951784 Loss 4.254900, Accuracy 91.148%\n",
      "Epoch 28, Batch 155, LR 0.951654 Loss 4.256215, Accuracy 91.144%\n",
      "Epoch 28, Batch 156, LR 0.951524 Loss 4.255782, Accuracy 91.166%\n",
      "Epoch 28, Batch 157, LR 0.951393 Loss 4.259438, Accuracy 91.152%\n",
      "Epoch 28, Batch 158, LR 0.951263 Loss 4.257634, Accuracy 91.174%\n",
      "Epoch 28, Batch 159, LR 0.951133 Loss 4.255287, Accuracy 91.200%\n",
      "Epoch 28, Batch 160, LR 0.951003 Loss 4.254260, Accuracy 91.191%\n",
      "Epoch 28, Batch 161, LR 0.950873 Loss 4.255395, Accuracy 91.198%\n",
      "Epoch 28, Batch 162, LR 0.950743 Loss 4.259172, Accuracy 91.199%\n",
      "Epoch 28, Batch 163, LR 0.950613 Loss 4.259536, Accuracy 91.186%\n",
      "Epoch 28, Batch 164, LR 0.950483 Loss 4.262718, Accuracy 91.168%\n",
      "Epoch 28, Batch 165, LR 0.950353 Loss 4.262194, Accuracy 91.165%\n",
      "Epoch 28, Batch 166, LR 0.950223 Loss 4.261207, Accuracy 91.180%\n",
      "Epoch 28, Batch 167, LR 0.950093 Loss 4.262331, Accuracy 91.168%\n",
      "Epoch 28, Batch 168, LR 0.949963 Loss 4.263502, Accuracy 91.155%\n",
      "Epoch 28, Batch 169, LR 0.949833 Loss 4.263819, Accuracy 91.147%\n",
      "Epoch 28, Batch 170, LR 0.949703 Loss 4.261279, Accuracy 91.163%\n",
      "Epoch 28, Batch 171, LR 0.949573 Loss 4.259600, Accuracy 91.187%\n",
      "Epoch 28, Batch 172, LR 0.949443 Loss 4.260967, Accuracy 91.188%\n",
      "Epoch 28, Batch 173, LR 0.949313 Loss 4.264937, Accuracy 91.189%\n",
      "Epoch 28, Batch 174, LR 0.949183 Loss 4.267663, Accuracy 91.177%\n",
      "Epoch 28, Batch 175, LR 0.949053 Loss 4.267912, Accuracy 91.188%\n",
      "Epoch 28, Batch 176, LR 0.948923 Loss 4.266729, Accuracy 91.175%\n",
      "Epoch 28, Batch 177, LR 0.948793 Loss 4.267132, Accuracy 91.168%\n",
      "Epoch 28, Batch 178, LR 0.948663 Loss 4.268279, Accuracy 91.165%\n",
      "Epoch 28, Batch 179, LR 0.948533 Loss 4.266910, Accuracy 91.171%\n",
      "Epoch 28, Batch 180, LR 0.948403 Loss 4.266990, Accuracy 91.168%\n",
      "Epoch 28, Batch 181, LR 0.948273 Loss 4.262524, Accuracy 91.199%\n",
      "Epoch 28, Batch 182, LR 0.948143 Loss 4.263095, Accuracy 91.187%\n",
      "Epoch 28, Batch 183, LR 0.948013 Loss 4.266050, Accuracy 91.167%\n",
      "Epoch 28, Batch 184, LR 0.947883 Loss 4.265343, Accuracy 91.156%\n",
      "Epoch 28, Batch 185, LR 0.947753 Loss 4.265226, Accuracy 91.170%\n",
      "Epoch 28, Batch 186, LR 0.947623 Loss 4.264680, Accuracy 91.163%\n",
      "Epoch 28, Batch 187, LR 0.947493 Loss 4.265151, Accuracy 91.164%\n",
      "Epoch 28, Batch 188, LR 0.947363 Loss 4.269772, Accuracy 91.128%\n",
      "Epoch 28, Batch 189, LR 0.947233 Loss 4.271021, Accuracy 91.129%\n",
      "Epoch 28, Batch 190, LR 0.947103 Loss 4.268233, Accuracy 91.151%\n",
      "Epoch 28, Batch 191, LR 0.946973 Loss 4.270753, Accuracy 91.124%\n",
      "Epoch 28, Batch 192, LR 0.946843 Loss 4.268955, Accuracy 91.142%\n",
      "Epoch 28, Batch 193, LR 0.946713 Loss 4.269118, Accuracy 91.135%\n",
      "Epoch 28, Batch 194, LR 0.946583 Loss 4.268759, Accuracy 91.144%\n",
      "Epoch 28, Batch 195, LR 0.946453 Loss 4.266610, Accuracy 91.158%\n",
      "Epoch 28, Batch 196, LR 0.946323 Loss 4.269445, Accuracy 91.151%\n",
      "Epoch 28, Batch 197, LR 0.946193 Loss 4.269246, Accuracy 91.156%\n",
      "Epoch 28, Batch 198, LR 0.946063 Loss 4.266448, Accuracy 91.173%\n",
      "Epoch 28, Batch 199, LR 0.945933 Loss 4.267490, Accuracy 91.171%\n",
      "Epoch 28, Batch 200, LR 0.945803 Loss 4.267083, Accuracy 91.168%\n",
      "Epoch 28, Batch 201, LR 0.945673 Loss 4.269942, Accuracy 91.150%\n",
      "Epoch 28, Batch 202, LR 0.945544 Loss 4.271839, Accuracy 91.147%\n",
      "Epoch 28, Batch 203, LR 0.945414 Loss 4.270866, Accuracy 91.152%\n",
      "Epoch 28, Batch 204, LR 0.945284 Loss 4.268343, Accuracy 91.157%\n",
      "Epoch 28, Batch 205, LR 0.945154 Loss 4.270389, Accuracy 91.143%\n",
      "Epoch 28, Batch 206, LR 0.945024 Loss 4.270516, Accuracy 91.137%\n",
      "Epoch 28, Batch 207, LR 0.944894 Loss 4.267771, Accuracy 91.146%\n",
      "Epoch 28, Batch 208, LR 0.944764 Loss 4.263988, Accuracy 91.151%\n",
      "Epoch 28, Batch 209, LR 0.944634 Loss 4.262160, Accuracy 91.163%\n",
      "Epoch 28, Batch 210, LR 0.944504 Loss 4.262476, Accuracy 91.153%\n",
      "Epoch 28, Batch 211, LR 0.944374 Loss 4.265249, Accuracy 91.143%\n",
      "Epoch 28, Batch 212, LR 0.944245 Loss 4.266561, Accuracy 91.152%\n",
      "Epoch 28, Batch 213, LR 0.944115 Loss 4.267634, Accuracy 91.138%\n",
      "Epoch 28, Batch 214, LR 0.943985 Loss 4.265862, Accuracy 91.151%\n",
      "Epoch 28, Batch 215, LR 0.943855 Loss 4.267029, Accuracy 91.137%\n",
      "Epoch 28, Batch 216, LR 0.943725 Loss 4.270536, Accuracy 91.113%\n",
      "Epoch 28, Batch 217, LR 0.943595 Loss 4.270966, Accuracy 91.100%\n",
      "Epoch 28, Batch 218, LR 0.943465 Loss 4.269525, Accuracy 91.102%\n",
      "Epoch 28, Batch 219, LR 0.943335 Loss 4.268917, Accuracy 91.096%\n",
      "Epoch 28, Batch 220, LR 0.943206 Loss 4.270786, Accuracy 91.087%\n",
      "Epoch 28, Batch 221, LR 0.943076 Loss 4.269556, Accuracy 91.109%\n",
      "Epoch 28, Batch 222, LR 0.942946 Loss 4.269500, Accuracy 91.104%\n",
      "Epoch 28, Batch 223, LR 0.942816 Loss 4.267586, Accuracy 91.115%\n",
      "Epoch 28, Batch 224, LR 0.942686 Loss 4.269181, Accuracy 91.110%\n",
      "Epoch 28, Batch 225, LR 0.942556 Loss 4.270921, Accuracy 91.104%\n",
      "Epoch 28, Batch 226, LR 0.942427 Loss 4.270551, Accuracy 91.085%\n",
      "Epoch 28, Batch 227, LR 0.942297 Loss 4.270022, Accuracy 91.076%\n",
      "Epoch 28, Batch 228, LR 0.942167 Loss 4.270607, Accuracy 91.077%\n",
      "Epoch 28, Batch 229, LR 0.942037 Loss 4.273283, Accuracy 91.072%\n",
      "Epoch 28, Batch 230, LR 0.941907 Loss 4.273103, Accuracy 91.084%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 231, LR 0.941777 Loss 4.272465, Accuracy 91.082%\n",
      "Epoch 28, Batch 232, LR 0.941648 Loss 4.272951, Accuracy 91.090%\n",
      "Epoch 28, Batch 233, LR 0.941518 Loss 4.271034, Accuracy 91.098%\n",
      "Epoch 28, Batch 234, LR 0.941388 Loss 4.273377, Accuracy 91.112%\n",
      "Epoch 28, Batch 235, LR 0.941258 Loss 4.274728, Accuracy 91.114%\n",
      "Epoch 28, Batch 236, LR 0.941128 Loss 4.275888, Accuracy 91.092%\n",
      "Epoch 28, Batch 237, LR 0.940999 Loss 4.273008, Accuracy 91.110%\n",
      "Epoch 28, Batch 238, LR 0.940869 Loss 4.273081, Accuracy 91.114%\n",
      "Epoch 28, Batch 239, LR 0.940739 Loss 4.271415, Accuracy 91.128%\n",
      "Epoch 28, Batch 240, LR 0.940609 Loss 4.272600, Accuracy 91.126%\n",
      "Epoch 28, Batch 241, LR 0.940479 Loss 4.275330, Accuracy 91.114%\n",
      "Epoch 28, Batch 242, LR 0.940350 Loss 4.275027, Accuracy 91.119%\n",
      "Epoch 28, Batch 243, LR 0.940220 Loss 4.273409, Accuracy 91.120%\n",
      "Epoch 28, Batch 244, LR 0.940090 Loss 4.275270, Accuracy 91.102%\n",
      "Epoch 28, Batch 245, LR 0.939960 Loss 4.275213, Accuracy 91.107%\n",
      "Epoch 28, Batch 246, LR 0.939831 Loss 4.276136, Accuracy 91.098%\n",
      "Epoch 28, Batch 247, LR 0.939701 Loss 4.279589, Accuracy 91.071%\n",
      "Epoch 28, Batch 248, LR 0.939571 Loss 4.275956, Accuracy 91.082%\n",
      "Epoch 28, Batch 249, LR 0.939441 Loss 4.276771, Accuracy 91.074%\n",
      "Epoch 28, Batch 250, LR 0.939312 Loss 4.277227, Accuracy 91.066%\n",
      "Epoch 28, Batch 251, LR 0.939182 Loss 4.276071, Accuracy 91.086%\n",
      "Epoch 28, Batch 252, LR 0.939052 Loss 4.276108, Accuracy 91.078%\n",
      "Epoch 28, Batch 253, LR 0.938922 Loss 4.275358, Accuracy 91.070%\n",
      "Epoch 28, Batch 254, LR 0.938793 Loss 4.274506, Accuracy 91.080%\n",
      "Epoch 28, Batch 255, LR 0.938663 Loss 4.277845, Accuracy 91.054%\n",
      "Epoch 28, Batch 256, LR 0.938533 Loss 4.278024, Accuracy 91.046%\n",
      "Epoch 28, Batch 257, LR 0.938403 Loss 4.278212, Accuracy 91.063%\n",
      "Epoch 28, Batch 258, LR 0.938274 Loss 4.276407, Accuracy 91.082%\n",
      "Epoch 28, Batch 259, LR 0.938144 Loss 4.277280, Accuracy 91.071%\n",
      "Epoch 28, Batch 260, LR 0.938014 Loss 4.277610, Accuracy 91.070%\n",
      "Epoch 28, Batch 261, LR 0.937884 Loss 4.275120, Accuracy 91.089%\n",
      "Epoch 28, Batch 262, LR 0.937755 Loss 4.277988, Accuracy 91.081%\n",
      "Epoch 28, Batch 263, LR 0.937625 Loss 4.278125, Accuracy 91.088%\n",
      "Epoch 28, Batch 264, LR 0.937495 Loss 4.279167, Accuracy 91.081%\n",
      "Epoch 28, Batch 265, LR 0.937366 Loss 4.278131, Accuracy 91.082%\n",
      "Epoch 28, Batch 266, LR 0.937236 Loss 4.276226, Accuracy 91.092%\n",
      "Epoch 28, Batch 267, LR 0.937106 Loss 4.277661, Accuracy 91.084%\n",
      "Epoch 28, Batch 268, LR 0.936977 Loss 4.279159, Accuracy 91.077%\n",
      "Epoch 28, Batch 269, LR 0.936847 Loss 4.278308, Accuracy 91.090%\n",
      "Epoch 28, Batch 270, LR 0.936717 Loss 4.279088, Accuracy 91.091%\n",
      "Epoch 28, Batch 271, LR 0.936588 Loss 4.281890, Accuracy 91.083%\n",
      "Epoch 28, Batch 272, LR 0.936458 Loss 4.284420, Accuracy 91.085%\n",
      "Epoch 28, Batch 273, LR 0.936328 Loss 4.285914, Accuracy 91.094%\n",
      "Epoch 28, Batch 274, LR 0.936199 Loss 4.286859, Accuracy 91.084%\n",
      "Epoch 28, Batch 275, LR 0.936069 Loss 4.285435, Accuracy 91.088%\n",
      "Epoch 28, Batch 276, LR 0.935939 Loss 4.285691, Accuracy 91.086%\n",
      "Epoch 28, Batch 277, LR 0.935810 Loss 4.285595, Accuracy 91.088%\n",
      "Epoch 28, Batch 278, LR 0.935680 Loss 4.284928, Accuracy 91.097%\n",
      "Epoch 28, Batch 279, LR 0.935550 Loss 4.284541, Accuracy 91.093%\n",
      "Epoch 28, Batch 280, LR 0.935421 Loss 4.285705, Accuracy 91.094%\n",
      "Epoch 28, Batch 281, LR 0.935291 Loss 4.284615, Accuracy 91.089%\n",
      "Epoch 28, Batch 282, LR 0.935161 Loss 4.285242, Accuracy 91.096%\n",
      "Epoch 28, Batch 283, LR 0.935032 Loss 4.285969, Accuracy 91.089%\n",
      "Epoch 28, Batch 284, LR 0.934902 Loss 4.286004, Accuracy 91.101%\n",
      "Epoch 28, Batch 285, LR 0.934772 Loss 4.286307, Accuracy 91.099%\n",
      "Epoch 28, Batch 286, LR 0.934643 Loss 4.289603, Accuracy 91.087%\n",
      "Epoch 28, Batch 287, LR 0.934513 Loss 4.289367, Accuracy 91.085%\n",
      "Epoch 28, Batch 288, LR 0.934384 Loss 4.288575, Accuracy 91.089%\n",
      "Epoch 28, Batch 289, LR 0.934254 Loss 4.290231, Accuracy 91.101%\n",
      "Epoch 28, Batch 290, LR 0.934124 Loss 4.290163, Accuracy 91.105%\n",
      "Epoch 28, Batch 291, LR 0.933995 Loss 4.291086, Accuracy 91.103%\n",
      "Epoch 28, Batch 292, LR 0.933865 Loss 4.293727, Accuracy 91.077%\n",
      "Epoch 28, Batch 293, LR 0.933736 Loss 4.291868, Accuracy 91.086%\n",
      "Epoch 28, Batch 294, LR 0.933606 Loss 4.291759, Accuracy 91.090%\n",
      "Epoch 28, Batch 295, LR 0.933476 Loss 4.291545, Accuracy 91.099%\n",
      "Epoch 28, Batch 296, LR 0.933347 Loss 4.292492, Accuracy 91.097%\n",
      "Epoch 28, Batch 297, LR 0.933217 Loss 4.293320, Accuracy 91.104%\n",
      "Epoch 28, Batch 298, LR 0.933088 Loss 4.291276, Accuracy 91.113%\n",
      "Epoch 28, Batch 299, LR 0.932958 Loss 4.289191, Accuracy 91.119%\n",
      "Epoch 28, Batch 300, LR 0.932829 Loss 4.288233, Accuracy 91.128%\n",
      "Epoch 28, Batch 301, LR 0.932699 Loss 4.288946, Accuracy 91.113%\n",
      "Epoch 28, Batch 302, LR 0.932569 Loss 4.288282, Accuracy 91.124%\n",
      "Epoch 28, Batch 303, LR 0.932440 Loss 4.287770, Accuracy 91.123%\n",
      "Epoch 28, Batch 304, LR 0.932310 Loss 4.286556, Accuracy 91.134%\n",
      "Epoch 28, Batch 305, LR 0.932181 Loss 4.286180, Accuracy 91.137%\n",
      "Epoch 28, Batch 306, LR 0.932051 Loss 4.284848, Accuracy 91.148%\n",
      "Epoch 28, Batch 307, LR 0.931922 Loss 4.284959, Accuracy 91.137%\n",
      "Epoch 28, Batch 308, LR 0.931792 Loss 4.285917, Accuracy 91.135%\n",
      "Epoch 28, Batch 309, LR 0.931663 Loss 4.286653, Accuracy 91.138%\n",
      "Epoch 28, Batch 310, LR 0.931533 Loss 4.286013, Accuracy 91.149%\n",
      "Epoch 28, Batch 311, LR 0.931404 Loss 4.284840, Accuracy 91.150%\n",
      "Epoch 28, Batch 312, LR 0.931274 Loss 4.282568, Accuracy 91.158%\n",
      "Epoch 28, Batch 313, LR 0.931144 Loss 4.281306, Accuracy 91.174%\n",
      "Epoch 28, Batch 314, LR 0.931015 Loss 4.281324, Accuracy 91.172%\n",
      "Epoch 28, Batch 315, LR 0.930885 Loss 4.281123, Accuracy 91.168%\n",
      "Epoch 28, Batch 316, LR 0.930756 Loss 4.283991, Accuracy 91.147%\n",
      "Epoch 28, Batch 317, LR 0.930626 Loss 4.285227, Accuracy 91.150%\n",
      "Epoch 28, Batch 318, LR 0.930497 Loss 4.286135, Accuracy 91.146%\n",
      "Epoch 28, Batch 319, LR 0.930367 Loss 4.288059, Accuracy 91.137%\n",
      "Epoch 28, Batch 320, LR 0.930238 Loss 4.286607, Accuracy 91.140%\n",
      "Epoch 28, Batch 321, LR 0.930108 Loss 4.285031, Accuracy 91.148%\n",
      "Epoch 28, Batch 322, LR 0.929979 Loss 4.284871, Accuracy 91.154%\n",
      "Epoch 28, Batch 323, LR 0.929849 Loss 4.287527, Accuracy 91.140%\n",
      "Epoch 28, Batch 324, LR 0.929720 Loss 4.289088, Accuracy 91.129%\n",
      "Epoch 28, Batch 325, LR 0.929590 Loss 4.290223, Accuracy 91.118%\n",
      "Epoch 28, Batch 326, LR 0.929461 Loss 4.290526, Accuracy 91.119%\n",
      "Epoch 28, Batch 327, LR 0.929332 Loss 4.290012, Accuracy 91.122%\n",
      "Epoch 28, Batch 328, LR 0.929202 Loss 4.289924, Accuracy 91.118%\n",
      "Epoch 28, Batch 329, LR 0.929073 Loss 4.287716, Accuracy 91.114%\n",
      "Epoch 28, Batch 330, LR 0.928943 Loss 4.288351, Accuracy 91.115%\n",
      "Epoch 28, Batch 331, LR 0.928814 Loss 4.290582, Accuracy 91.097%\n",
      "Epoch 28, Batch 332, LR 0.928684 Loss 4.288377, Accuracy 91.107%\n",
      "Epoch 28, Batch 333, LR 0.928555 Loss 4.289276, Accuracy 91.106%\n",
      "Epoch 28, Batch 334, LR 0.928425 Loss 4.287616, Accuracy 91.119%\n",
      "Epoch 28, Batch 335, LR 0.928296 Loss 4.287427, Accuracy 91.122%\n",
      "Epoch 28, Batch 336, LR 0.928166 Loss 4.287803, Accuracy 91.113%\n",
      "Epoch 28, Batch 337, LR 0.928037 Loss 4.289811, Accuracy 91.103%\n",
      "Epoch 28, Batch 338, LR 0.927908 Loss 4.288406, Accuracy 91.110%\n",
      "Epoch 28, Batch 339, LR 0.927778 Loss 4.288414, Accuracy 91.118%\n",
      "Epoch 28, Batch 340, LR 0.927649 Loss 4.287735, Accuracy 91.117%\n",
      "Epoch 28, Batch 341, LR 0.927519 Loss 4.288333, Accuracy 91.108%\n",
      "Epoch 28, Batch 342, LR 0.927390 Loss 4.288065, Accuracy 91.109%\n",
      "Epoch 28, Batch 343, LR 0.927260 Loss 4.287079, Accuracy 91.115%\n",
      "Epoch 28, Batch 344, LR 0.927131 Loss 4.286529, Accuracy 91.125%\n",
      "Epoch 28, Batch 345, LR 0.927002 Loss 4.286790, Accuracy 91.130%\n",
      "Epoch 28, Batch 346, LR 0.926872 Loss 4.286695, Accuracy 91.138%\n",
      "Epoch 28, Batch 347, LR 0.926743 Loss 4.288201, Accuracy 91.125%\n",
      "Epoch 28, Batch 348, LR 0.926613 Loss 4.287343, Accuracy 91.135%\n",
      "Epoch 28, Batch 349, LR 0.926484 Loss 4.287771, Accuracy 91.129%\n",
      "Epoch 28, Batch 350, LR 0.926355 Loss 4.288281, Accuracy 91.132%\n",
      "Epoch 28, Batch 351, LR 0.926225 Loss 4.287701, Accuracy 91.139%\n",
      "Epoch 28, Batch 352, LR 0.926096 Loss 4.288247, Accuracy 91.144%\n",
      "Epoch 28, Batch 353, LR 0.925967 Loss 4.288485, Accuracy 91.150%\n",
      "Epoch 28, Batch 354, LR 0.925837 Loss 4.289980, Accuracy 91.139%\n",
      "Epoch 28, Batch 355, LR 0.925708 Loss 4.289170, Accuracy 91.136%\n",
      "Epoch 28, Batch 356, LR 0.925578 Loss 4.290781, Accuracy 91.130%\n",
      "Epoch 28, Batch 357, LR 0.925449 Loss 4.289918, Accuracy 91.137%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 358, LR 0.925320 Loss 4.291037, Accuracy 91.136%\n",
      "Epoch 28, Batch 359, LR 0.925190 Loss 4.290564, Accuracy 91.139%\n",
      "Epoch 28, Batch 360, LR 0.925061 Loss 4.289947, Accuracy 91.139%\n",
      "Epoch 28, Batch 361, LR 0.924932 Loss 4.291376, Accuracy 91.138%\n",
      "Epoch 28, Batch 362, LR 0.924802 Loss 4.291933, Accuracy 91.139%\n",
      "Epoch 28, Batch 363, LR 0.924673 Loss 4.293004, Accuracy 91.135%\n",
      "Epoch 28, Batch 364, LR 0.924544 Loss 4.292592, Accuracy 91.132%\n",
      "Epoch 28, Batch 365, LR 0.924414 Loss 4.293386, Accuracy 91.132%\n",
      "Epoch 28, Batch 366, LR 0.924285 Loss 4.293091, Accuracy 91.133%\n",
      "Epoch 28, Batch 367, LR 0.924156 Loss 4.292686, Accuracy 91.136%\n",
      "Epoch 28, Batch 368, LR 0.924026 Loss 4.293783, Accuracy 91.130%\n",
      "Epoch 28, Batch 369, LR 0.923897 Loss 4.294556, Accuracy 91.125%\n",
      "Epoch 28, Batch 370, LR 0.923768 Loss 4.296747, Accuracy 91.109%\n",
      "Epoch 28, Batch 371, LR 0.923638 Loss 4.297761, Accuracy 91.101%\n",
      "Epoch 28, Batch 372, LR 0.923509 Loss 4.297750, Accuracy 91.098%\n",
      "Epoch 28, Batch 373, LR 0.923380 Loss 4.297410, Accuracy 91.098%\n",
      "Epoch 28, Batch 374, LR 0.923251 Loss 4.297395, Accuracy 91.103%\n",
      "Epoch 28, Batch 375, LR 0.923121 Loss 4.297328, Accuracy 91.102%\n",
      "Epoch 28, Batch 376, LR 0.922992 Loss 4.298130, Accuracy 91.093%\n",
      "Epoch 28, Batch 377, LR 0.922863 Loss 4.297647, Accuracy 91.102%\n",
      "Epoch 28, Batch 378, LR 0.922733 Loss 4.296373, Accuracy 91.104%\n",
      "Epoch 28, Batch 379, LR 0.922604 Loss 4.295623, Accuracy 91.107%\n",
      "Epoch 28, Batch 380, LR 0.922475 Loss 4.295078, Accuracy 91.096%\n",
      "Epoch 28, Batch 381, LR 0.922346 Loss 4.296491, Accuracy 91.082%\n",
      "Epoch 28, Batch 382, LR 0.922216 Loss 4.296235, Accuracy 91.087%\n",
      "Epoch 28, Batch 383, LR 0.922087 Loss 4.295079, Accuracy 91.092%\n",
      "Epoch 28, Batch 384, LR 0.921958 Loss 4.295751, Accuracy 91.095%\n",
      "Epoch 28, Batch 385, LR 0.921828 Loss 4.295936, Accuracy 91.098%\n",
      "Epoch 28, Batch 386, LR 0.921699 Loss 4.295291, Accuracy 91.101%\n",
      "Epoch 28, Batch 387, LR 0.921570 Loss 4.295798, Accuracy 91.099%\n",
      "Epoch 28, Batch 388, LR 0.921441 Loss 4.297472, Accuracy 91.094%\n",
      "Epoch 28, Batch 389, LR 0.921311 Loss 4.295912, Accuracy 91.095%\n",
      "Epoch 28, Batch 390, LR 0.921182 Loss 4.295893, Accuracy 91.100%\n",
      "Epoch 28, Batch 391, LR 0.921053 Loss 4.295010, Accuracy 91.113%\n",
      "Epoch 28, Batch 392, LR 0.920924 Loss 4.294143, Accuracy 91.117%\n",
      "Epoch 28, Batch 393, LR 0.920795 Loss 4.292542, Accuracy 91.122%\n",
      "Epoch 28, Batch 394, LR 0.920665 Loss 4.292359, Accuracy 91.125%\n",
      "Epoch 28, Batch 395, LR 0.920536 Loss 4.293507, Accuracy 91.114%\n",
      "Epoch 28, Batch 396, LR 0.920407 Loss 4.293101, Accuracy 91.114%\n",
      "Epoch 28, Batch 397, LR 0.920278 Loss 4.292343, Accuracy 91.117%\n",
      "Epoch 28, Batch 398, LR 0.920148 Loss 4.291392, Accuracy 91.118%\n",
      "Epoch 28, Batch 399, LR 0.920019 Loss 4.290528, Accuracy 91.130%\n",
      "Epoch 28, Batch 400, LR 0.919890 Loss 4.290253, Accuracy 91.137%\n",
      "Epoch 28, Batch 401, LR 0.919761 Loss 4.288279, Accuracy 91.149%\n",
      "Epoch 28, Batch 402, LR 0.919632 Loss 4.289439, Accuracy 91.152%\n",
      "Epoch 28, Batch 403, LR 0.919503 Loss 4.289117, Accuracy 91.148%\n",
      "Epoch 28, Batch 404, LR 0.919373 Loss 4.288771, Accuracy 91.153%\n",
      "Epoch 28, Batch 405, LR 0.919244 Loss 4.287917, Accuracy 91.159%\n",
      "Epoch 28, Batch 406, LR 0.919115 Loss 4.286117, Accuracy 91.170%\n",
      "Epoch 28, Batch 407, LR 0.918986 Loss 4.285443, Accuracy 91.170%\n",
      "Epoch 28, Batch 408, LR 0.918857 Loss 4.286220, Accuracy 91.159%\n",
      "Epoch 28, Batch 409, LR 0.918727 Loss 4.287559, Accuracy 91.158%\n",
      "Epoch 28, Batch 410, LR 0.918598 Loss 4.288028, Accuracy 91.160%\n",
      "Epoch 28, Batch 411, LR 0.918469 Loss 4.286264, Accuracy 91.165%\n",
      "Epoch 28, Batch 412, LR 0.918340 Loss 4.287002, Accuracy 91.160%\n",
      "Epoch 28, Batch 413, LR 0.918211 Loss 4.286312, Accuracy 91.160%\n",
      "Epoch 28, Batch 414, LR 0.918082 Loss 4.286560, Accuracy 91.161%\n",
      "Epoch 28, Batch 415, LR 0.917953 Loss 4.286666, Accuracy 91.158%\n",
      "Epoch 28, Batch 416, LR 0.917823 Loss 4.286886, Accuracy 91.149%\n",
      "Epoch 28, Batch 417, LR 0.917694 Loss 4.287554, Accuracy 91.148%\n",
      "Epoch 28, Batch 418, LR 0.917565 Loss 4.287025, Accuracy 91.154%\n",
      "Epoch 28, Batch 419, LR 0.917436 Loss 4.286368, Accuracy 91.158%\n",
      "Epoch 28, Batch 420, LR 0.917307 Loss 4.286711, Accuracy 91.157%\n",
      "Epoch 28, Batch 421, LR 0.917178 Loss 4.286107, Accuracy 91.156%\n",
      "Epoch 28, Batch 422, LR 0.917049 Loss 4.285801, Accuracy 91.162%\n",
      "Epoch 28, Batch 423, LR 0.916920 Loss 4.286277, Accuracy 91.168%\n",
      "Epoch 28, Batch 424, LR 0.916790 Loss 4.286277, Accuracy 91.165%\n",
      "Epoch 28, Batch 425, LR 0.916661 Loss 4.286322, Accuracy 91.153%\n",
      "Epoch 28, Batch 426, LR 0.916532 Loss 4.285137, Accuracy 91.161%\n",
      "Epoch 28, Batch 427, LR 0.916403 Loss 4.284462, Accuracy 91.159%\n",
      "Epoch 28, Batch 428, LR 0.916274 Loss 4.285318, Accuracy 91.160%\n",
      "Epoch 28, Batch 429, LR 0.916145 Loss 4.283314, Accuracy 91.168%\n",
      "Epoch 28, Batch 430, LR 0.916016 Loss 4.283788, Accuracy 91.166%\n",
      "Epoch 28, Batch 431, LR 0.915887 Loss 4.284489, Accuracy 91.163%\n",
      "Epoch 28, Batch 432, LR 0.915758 Loss 4.282505, Accuracy 91.169%\n",
      "Epoch 28, Batch 433, LR 0.915629 Loss 4.282184, Accuracy 91.164%\n",
      "Epoch 28, Batch 434, LR 0.915500 Loss 4.282838, Accuracy 91.163%\n",
      "Epoch 28, Batch 435, LR 0.915371 Loss 4.283715, Accuracy 91.157%\n",
      "Epoch 28, Batch 436, LR 0.915241 Loss 4.284017, Accuracy 91.155%\n",
      "Epoch 28, Batch 437, LR 0.915112 Loss 4.283521, Accuracy 91.158%\n",
      "Epoch 28, Batch 438, LR 0.914983 Loss 4.282899, Accuracy 91.158%\n",
      "Epoch 28, Batch 439, LR 0.914854 Loss 4.283506, Accuracy 91.154%\n",
      "Epoch 28, Batch 440, LR 0.914725 Loss 4.283249, Accuracy 91.156%\n",
      "Epoch 28, Batch 441, LR 0.914596 Loss 4.283698, Accuracy 91.160%\n",
      "Epoch 28, Batch 442, LR 0.914467 Loss 4.283361, Accuracy 91.159%\n",
      "Epoch 28, Batch 443, LR 0.914338 Loss 4.284567, Accuracy 91.149%\n",
      "Epoch 28, Batch 444, LR 0.914209 Loss 4.284309, Accuracy 91.149%\n",
      "Epoch 28, Batch 445, LR 0.914080 Loss 4.283930, Accuracy 91.157%\n",
      "Epoch 28, Batch 446, LR 0.913951 Loss 4.284079, Accuracy 91.159%\n",
      "Epoch 28, Batch 447, LR 0.913822 Loss 4.283278, Accuracy 91.165%\n",
      "Epoch 28, Batch 448, LR 0.913693 Loss 4.282256, Accuracy 91.171%\n",
      "Epoch 28, Batch 449, LR 0.913564 Loss 4.282417, Accuracy 91.170%\n",
      "Epoch 28, Batch 450, LR 0.913435 Loss 4.282446, Accuracy 91.167%\n",
      "Epoch 28, Batch 451, LR 0.913306 Loss 4.283135, Accuracy 91.162%\n",
      "Epoch 28, Batch 452, LR 0.913177 Loss 4.284771, Accuracy 91.156%\n",
      "Epoch 28, Batch 453, LR 0.913048 Loss 4.284138, Accuracy 91.156%\n",
      "Epoch 28, Batch 454, LR 0.912919 Loss 4.284237, Accuracy 91.162%\n",
      "Epoch 28, Batch 455, LR 0.912790 Loss 4.282951, Accuracy 91.164%\n",
      "Epoch 28, Batch 456, LR 0.912661 Loss 4.283473, Accuracy 91.165%\n",
      "Epoch 28, Batch 457, LR 0.912532 Loss 4.283739, Accuracy 91.169%\n",
      "Epoch 28, Batch 458, LR 0.912403 Loss 4.283517, Accuracy 91.167%\n",
      "Epoch 28, Batch 459, LR 0.912274 Loss 4.283018, Accuracy 91.171%\n",
      "Epoch 28, Batch 460, LR 0.912145 Loss 4.283063, Accuracy 91.168%\n",
      "Epoch 28, Batch 461, LR 0.912016 Loss 4.282273, Accuracy 91.171%\n",
      "Epoch 28, Batch 462, LR 0.911887 Loss 4.282567, Accuracy 91.170%\n",
      "Epoch 28, Batch 463, LR 0.911758 Loss 4.283602, Accuracy 91.167%\n",
      "Epoch 28, Batch 464, LR 0.911629 Loss 4.283405, Accuracy 91.171%\n",
      "Epoch 28, Batch 465, LR 0.911500 Loss 4.283452, Accuracy 91.173%\n",
      "Epoch 28, Batch 466, LR 0.911371 Loss 4.282849, Accuracy 91.177%\n",
      "Epoch 28, Batch 467, LR 0.911242 Loss 4.283749, Accuracy 91.177%\n",
      "Epoch 28, Batch 468, LR 0.911114 Loss 4.284518, Accuracy 91.173%\n",
      "Epoch 28, Batch 469, LR 0.910985 Loss 4.283537, Accuracy 91.178%\n",
      "Epoch 28, Batch 470, LR 0.910856 Loss 4.282813, Accuracy 91.179%\n",
      "Epoch 28, Batch 471, LR 0.910727 Loss 4.283951, Accuracy 91.169%\n",
      "Epoch 28, Batch 472, LR 0.910598 Loss 4.283445, Accuracy 91.170%\n",
      "Epoch 28, Batch 473, LR 0.910469 Loss 4.284470, Accuracy 91.163%\n",
      "Epoch 28, Batch 474, LR 0.910340 Loss 4.283683, Accuracy 91.161%\n",
      "Epoch 28, Batch 475, LR 0.910211 Loss 4.284626, Accuracy 91.166%\n",
      "Epoch 28, Batch 476, LR 0.910082 Loss 4.285005, Accuracy 91.160%\n",
      "Epoch 28, Batch 477, LR 0.909953 Loss 4.286242, Accuracy 91.157%\n",
      "Epoch 28, Batch 478, LR 0.909824 Loss 4.284450, Accuracy 91.161%\n",
      "Epoch 28, Batch 479, LR 0.909695 Loss 4.284720, Accuracy 91.166%\n",
      "Epoch 28, Batch 480, LR 0.909567 Loss 4.284385, Accuracy 91.172%\n",
      "Epoch 28, Batch 481, LR 0.909438 Loss 4.284752, Accuracy 91.167%\n",
      "Epoch 28, Batch 482, LR 0.909309 Loss 4.284550, Accuracy 91.165%\n",
      "Epoch 28, Batch 483, LR 0.909180 Loss 4.283992, Accuracy 91.175%\n",
      "Epoch 28, Batch 484, LR 0.909051 Loss 4.283279, Accuracy 91.177%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 485, LR 0.908922 Loss 4.282959, Accuracy 91.179%\n",
      "Epoch 28, Batch 486, LR 0.908793 Loss 4.283061, Accuracy 91.180%\n",
      "Epoch 28, Batch 487, LR 0.908664 Loss 4.283441, Accuracy 91.174%\n",
      "Epoch 28, Batch 488, LR 0.908536 Loss 4.283569, Accuracy 91.174%\n",
      "Epoch 28, Batch 489, LR 0.908407 Loss 4.283119, Accuracy 91.181%\n",
      "Epoch 28, Batch 490, LR 0.908278 Loss 4.281478, Accuracy 91.185%\n",
      "Epoch 28, Batch 491, LR 0.908149 Loss 4.280951, Accuracy 91.182%\n",
      "Epoch 28, Batch 492, LR 0.908020 Loss 4.280740, Accuracy 91.181%\n",
      "Epoch 28, Batch 493, LR 0.907891 Loss 4.281335, Accuracy 91.176%\n",
      "Epoch 28, Batch 494, LR 0.907762 Loss 4.282635, Accuracy 91.171%\n",
      "Epoch 28, Batch 495, LR 0.907634 Loss 4.282967, Accuracy 91.171%\n",
      "Epoch 28, Batch 496, LR 0.907505 Loss 4.283290, Accuracy 91.172%\n",
      "Epoch 28, Batch 497, LR 0.907376 Loss 4.282350, Accuracy 91.175%\n",
      "Epoch 28, Batch 498, LR 0.907247 Loss 4.282558, Accuracy 91.169%\n",
      "Epoch 28, Batch 499, LR 0.907118 Loss 4.283312, Accuracy 91.164%\n",
      "Epoch 28, Batch 500, LR 0.906990 Loss 4.282614, Accuracy 91.169%\n",
      "Epoch 28, Batch 501, LR 0.906861 Loss 4.282995, Accuracy 91.172%\n",
      "Epoch 28, Batch 502, LR 0.906732 Loss 4.283176, Accuracy 91.176%\n",
      "Epoch 28, Batch 503, LR 0.906603 Loss 4.283061, Accuracy 91.175%\n",
      "Epoch 28, Batch 504, LR 0.906474 Loss 4.283685, Accuracy 91.172%\n",
      "Epoch 28, Batch 505, LR 0.906346 Loss 4.283126, Accuracy 91.179%\n",
      "Epoch 28, Batch 506, LR 0.906217 Loss 4.283300, Accuracy 91.182%\n",
      "Epoch 28, Batch 507, LR 0.906088 Loss 4.284497, Accuracy 91.172%\n",
      "Epoch 28, Batch 508, LR 0.905959 Loss 4.286258, Accuracy 91.165%\n",
      "Epoch 28, Batch 509, LR 0.905830 Loss 4.286653, Accuracy 91.161%\n",
      "Epoch 28, Batch 510, LR 0.905702 Loss 4.286558, Accuracy 91.161%\n",
      "Epoch 28, Batch 511, LR 0.905573 Loss 4.286641, Accuracy 91.162%\n",
      "Epoch 28, Batch 512, LR 0.905444 Loss 4.287381, Accuracy 91.154%\n",
      "Epoch 28, Batch 513, LR 0.905315 Loss 4.286342, Accuracy 91.155%\n",
      "Epoch 28, Batch 514, LR 0.905187 Loss 4.286499, Accuracy 91.151%\n",
      "Epoch 28, Batch 515, LR 0.905058 Loss 4.286273, Accuracy 91.153%\n",
      "Epoch 28, Batch 516, LR 0.904929 Loss 4.286263, Accuracy 91.156%\n",
      "Epoch 28, Batch 517, LR 0.904800 Loss 4.286033, Accuracy 91.155%\n",
      "Epoch 28, Batch 518, LR 0.904672 Loss 4.285622, Accuracy 91.159%\n",
      "Epoch 28, Batch 519, LR 0.904543 Loss 4.285809, Accuracy 91.156%\n",
      "Epoch 28, Batch 520, LR 0.904414 Loss 4.285038, Accuracy 91.160%\n",
      "Epoch 28, Batch 521, LR 0.904285 Loss 4.284857, Accuracy 91.160%\n",
      "Epoch 28, Batch 522, LR 0.904157 Loss 4.286054, Accuracy 91.158%\n",
      "Epoch 28, Batch 523, LR 0.904028 Loss 4.286325, Accuracy 91.155%\n",
      "Epoch 28, Batch 524, LR 0.903899 Loss 4.285686, Accuracy 91.153%\n",
      "Epoch 28, Batch 525, LR 0.903771 Loss 4.285144, Accuracy 91.155%\n",
      "Epoch 28, Batch 526, LR 0.903642 Loss 4.286264, Accuracy 91.152%\n",
      "Epoch 28, Batch 527, LR 0.903513 Loss 4.286913, Accuracy 91.145%\n",
      "Epoch 28, Batch 528, LR 0.903384 Loss 4.287818, Accuracy 91.137%\n",
      "Epoch 28, Batch 529, LR 0.903256 Loss 4.289215, Accuracy 91.133%\n",
      "Epoch 28, Batch 530, LR 0.903127 Loss 4.290116, Accuracy 91.131%\n",
      "Epoch 28, Batch 531, LR 0.902998 Loss 4.290271, Accuracy 91.134%\n",
      "Epoch 28, Batch 532, LR 0.902870 Loss 4.290022, Accuracy 91.136%\n",
      "Epoch 28, Batch 533, LR 0.902741 Loss 4.289132, Accuracy 91.142%\n",
      "Epoch 28, Batch 534, LR 0.902612 Loss 4.289370, Accuracy 91.136%\n",
      "Epoch 28, Batch 535, LR 0.902484 Loss 4.288332, Accuracy 91.142%\n",
      "Epoch 28, Batch 536, LR 0.902355 Loss 4.287880, Accuracy 91.147%\n",
      "Epoch 28, Batch 537, LR 0.902226 Loss 4.288707, Accuracy 91.137%\n",
      "Epoch 28, Batch 538, LR 0.902098 Loss 4.289675, Accuracy 91.133%\n",
      "Epoch 28, Batch 539, LR 0.901969 Loss 4.290101, Accuracy 91.128%\n",
      "Epoch 28, Batch 540, LR 0.901840 Loss 4.290586, Accuracy 91.133%\n",
      "Epoch 28, Batch 541, LR 0.901712 Loss 4.291527, Accuracy 91.128%\n",
      "Epoch 28, Batch 542, LR 0.901583 Loss 4.292990, Accuracy 91.124%\n",
      "Epoch 28, Batch 543, LR 0.901454 Loss 4.292710, Accuracy 91.127%\n",
      "Epoch 28, Batch 544, LR 0.901326 Loss 4.292667, Accuracy 91.119%\n",
      "Epoch 28, Batch 545, LR 0.901197 Loss 4.292316, Accuracy 91.121%\n",
      "Epoch 28, Batch 546, LR 0.901068 Loss 4.292483, Accuracy 91.122%\n",
      "Epoch 28, Batch 547, LR 0.900940 Loss 4.292090, Accuracy 91.121%\n",
      "Epoch 28, Batch 548, LR 0.900811 Loss 4.292872, Accuracy 91.120%\n",
      "Epoch 28, Batch 549, LR 0.900683 Loss 4.292441, Accuracy 91.120%\n",
      "Epoch 28, Batch 550, LR 0.900554 Loss 4.291726, Accuracy 91.124%\n",
      "Epoch 28, Batch 551, LR 0.900425 Loss 4.291767, Accuracy 91.117%\n",
      "Epoch 28, Batch 552, LR 0.900297 Loss 4.290983, Accuracy 91.125%\n",
      "Epoch 28, Batch 553, LR 0.900168 Loss 4.291973, Accuracy 91.114%\n",
      "Epoch 28, Batch 554, LR 0.900040 Loss 4.292124, Accuracy 91.113%\n",
      "Epoch 28, Batch 555, LR 0.899911 Loss 4.292494, Accuracy 91.104%\n",
      "Epoch 28, Batch 556, LR 0.899782 Loss 4.292863, Accuracy 91.103%\n",
      "Epoch 28, Batch 557, LR 0.899654 Loss 4.292588, Accuracy 91.105%\n",
      "Epoch 28, Batch 558, LR 0.899525 Loss 4.292300, Accuracy 91.108%\n",
      "Epoch 28, Batch 559, LR 0.899397 Loss 4.292098, Accuracy 91.109%\n",
      "Epoch 28, Batch 560, LR 0.899268 Loss 4.291722, Accuracy 91.110%\n",
      "Epoch 28, Batch 561, LR 0.899139 Loss 4.291906, Accuracy 91.103%\n",
      "Epoch 28, Batch 562, LR 0.899011 Loss 4.292138, Accuracy 91.102%\n",
      "Epoch 28, Batch 563, LR 0.898882 Loss 4.292309, Accuracy 91.098%\n",
      "Epoch 28, Batch 564, LR 0.898754 Loss 4.292257, Accuracy 91.097%\n",
      "Epoch 28, Batch 565, LR 0.898625 Loss 4.291122, Accuracy 91.101%\n",
      "Epoch 28, Batch 566, LR 0.898497 Loss 4.292081, Accuracy 91.092%\n",
      "Epoch 28, Batch 567, LR 0.898368 Loss 4.291473, Accuracy 91.089%\n",
      "Epoch 28, Batch 568, LR 0.898240 Loss 4.290880, Accuracy 91.090%\n",
      "Epoch 28, Batch 569, LR 0.898111 Loss 4.291083, Accuracy 91.089%\n",
      "Epoch 28, Batch 570, LR 0.897983 Loss 4.292005, Accuracy 91.083%\n",
      "Epoch 28, Batch 571, LR 0.897854 Loss 4.292060, Accuracy 91.086%\n",
      "Epoch 28, Batch 572, LR 0.897725 Loss 4.291513, Accuracy 91.091%\n",
      "Epoch 28, Batch 573, LR 0.897597 Loss 4.291673, Accuracy 91.089%\n",
      "Epoch 28, Batch 574, LR 0.897468 Loss 4.290587, Accuracy 91.095%\n",
      "Epoch 28, Batch 575, LR 0.897340 Loss 4.290273, Accuracy 91.096%\n",
      "Epoch 28, Batch 576, LR 0.897211 Loss 4.289888, Accuracy 91.100%\n",
      "Epoch 28, Batch 577, LR 0.897083 Loss 4.289515, Accuracy 91.104%\n",
      "Epoch 28, Batch 578, LR 0.896954 Loss 4.288829, Accuracy 91.106%\n",
      "Epoch 28, Batch 579, LR 0.896826 Loss 4.289940, Accuracy 91.103%\n",
      "Epoch 28, Batch 580, LR 0.896697 Loss 4.289628, Accuracy 91.099%\n",
      "Epoch 28, Batch 581, LR 0.896569 Loss 4.289621, Accuracy 91.096%\n",
      "Epoch 28, Batch 582, LR 0.896440 Loss 4.289176, Accuracy 91.098%\n",
      "Epoch 28, Batch 583, LR 0.896312 Loss 4.289632, Accuracy 91.093%\n",
      "Epoch 28, Batch 584, LR 0.896183 Loss 4.290244, Accuracy 91.097%\n",
      "Epoch 28, Batch 585, LR 0.896055 Loss 4.290461, Accuracy 91.088%\n",
      "Epoch 28, Batch 586, LR 0.895927 Loss 4.291043, Accuracy 91.092%\n",
      "Epoch 28, Batch 587, LR 0.895798 Loss 4.289991, Accuracy 91.100%\n",
      "Epoch 28, Batch 588, LR 0.895670 Loss 4.289774, Accuracy 91.102%\n",
      "Epoch 28, Batch 589, LR 0.895541 Loss 4.289666, Accuracy 91.108%\n",
      "Epoch 28, Batch 590, LR 0.895413 Loss 4.290366, Accuracy 91.102%\n",
      "Epoch 28, Batch 591, LR 0.895284 Loss 4.290111, Accuracy 91.101%\n",
      "Epoch 28, Batch 592, LR 0.895156 Loss 4.290842, Accuracy 91.101%\n",
      "Epoch 28, Batch 593, LR 0.895027 Loss 4.290924, Accuracy 91.099%\n",
      "Epoch 28, Batch 594, LR 0.894899 Loss 4.289648, Accuracy 91.108%\n",
      "Epoch 28, Batch 595, LR 0.894771 Loss 4.289317, Accuracy 91.107%\n",
      "Epoch 28, Batch 596, LR 0.894642 Loss 4.289679, Accuracy 91.106%\n",
      "Epoch 28, Batch 597, LR 0.894514 Loss 4.289489, Accuracy 91.108%\n",
      "Epoch 28, Batch 598, LR 0.894385 Loss 4.289771, Accuracy 91.110%\n",
      "Epoch 28, Batch 599, LR 0.894257 Loss 4.289087, Accuracy 91.111%\n",
      "Epoch 28, Batch 600, LR 0.894128 Loss 4.289642, Accuracy 91.113%\n",
      "Epoch 28, Batch 601, LR 0.894000 Loss 4.289620, Accuracy 91.116%\n",
      "Epoch 28, Batch 602, LR 0.893872 Loss 4.289687, Accuracy 91.114%\n",
      "Epoch 28, Batch 603, LR 0.893743 Loss 4.288791, Accuracy 91.119%\n",
      "Epoch 28, Batch 604, LR 0.893615 Loss 4.288140, Accuracy 91.122%\n",
      "Epoch 28, Batch 605, LR 0.893486 Loss 4.287485, Accuracy 91.121%\n",
      "Epoch 28, Batch 606, LR 0.893358 Loss 4.286824, Accuracy 91.123%\n",
      "Epoch 28, Batch 607, LR 0.893230 Loss 4.286710, Accuracy 91.122%\n",
      "Epoch 28, Batch 608, LR 0.893101 Loss 4.287844, Accuracy 91.117%\n",
      "Epoch 28, Batch 609, LR 0.892973 Loss 4.287316, Accuracy 91.120%\n",
      "Epoch 28, Batch 610, LR 0.892845 Loss 4.287980, Accuracy 91.121%\n",
      "Epoch 28, Batch 611, LR 0.892716 Loss 4.287785, Accuracy 91.117%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 612, LR 0.892588 Loss 4.288134, Accuracy 91.115%\n",
      "Epoch 28, Batch 613, LR 0.892459 Loss 4.286835, Accuracy 91.123%\n",
      "Epoch 28, Batch 614, LR 0.892331 Loss 4.286044, Accuracy 91.125%\n",
      "Epoch 28, Batch 615, LR 0.892203 Loss 4.286562, Accuracy 91.123%\n",
      "Epoch 28, Batch 616, LR 0.892074 Loss 4.285155, Accuracy 91.135%\n",
      "Epoch 28, Batch 617, LR 0.891946 Loss 4.284925, Accuracy 91.138%\n",
      "Epoch 28, Batch 618, LR 0.891818 Loss 4.284487, Accuracy 91.141%\n",
      "Epoch 28, Batch 619, LR 0.891689 Loss 4.284875, Accuracy 91.141%\n",
      "Epoch 28, Batch 620, LR 0.891561 Loss 4.285718, Accuracy 91.138%\n",
      "Epoch 28, Batch 621, LR 0.891433 Loss 4.284583, Accuracy 91.145%\n",
      "Epoch 28, Batch 622, LR 0.891304 Loss 4.283952, Accuracy 91.148%\n",
      "Epoch 28, Batch 623, LR 0.891176 Loss 4.283870, Accuracy 91.154%\n",
      "Epoch 28, Batch 624, LR 0.891048 Loss 4.283156, Accuracy 91.156%\n",
      "Epoch 28, Batch 625, LR 0.890919 Loss 4.284017, Accuracy 91.146%\n",
      "Epoch 28, Batch 626, LR 0.890791 Loss 4.285380, Accuracy 91.144%\n",
      "Epoch 28, Batch 627, LR 0.890663 Loss 4.284730, Accuracy 91.148%\n",
      "Epoch 28, Batch 628, LR 0.890535 Loss 4.284918, Accuracy 91.147%\n",
      "Epoch 28, Batch 629, LR 0.890406 Loss 4.284329, Accuracy 91.148%\n",
      "Epoch 28, Batch 630, LR 0.890278 Loss 4.283227, Accuracy 91.151%\n",
      "Epoch 28, Batch 631, LR 0.890150 Loss 4.282964, Accuracy 91.152%\n",
      "Epoch 28, Batch 632, LR 0.890021 Loss 4.283140, Accuracy 91.149%\n",
      "Epoch 28, Batch 633, LR 0.889893 Loss 4.282432, Accuracy 91.150%\n",
      "Epoch 28, Batch 634, LR 0.889765 Loss 4.282528, Accuracy 91.155%\n",
      "Epoch 28, Batch 635, LR 0.889637 Loss 4.282277, Accuracy 91.155%\n",
      "Epoch 28, Batch 636, LR 0.889508 Loss 4.281853, Accuracy 91.154%\n",
      "Epoch 28, Batch 637, LR 0.889380 Loss 4.282294, Accuracy 91.154%\n",
      "Epoch 28, Batch 638, LR 0.889252 Loss 4.282144, Accuracy 91.153%\n",
      "Epoch 28, Batch 639, LR 0.889124 Loss 4.282011, Accuracy 91.158%\n",
      "Epoch 28, Batch 640, LR 0.888995 Loss 4.282373, Accuracy 91.163%\n",
      "Epoch 28, Batch 641, LR 0.888867 Loss 4.281563, Accuracy 91.171%\n",
      "Epoch 28, Batch 642, LR 0.888739 Loss 4.281344, Accuracy 91.174%\n",
      "Epoch 28, Batch 643, LR 0.888611 Loss 4.282393, Accuracy 91.167%\n",
      "Epoch 28, Batch 644, LR 0.888482 Loss 4.282406, Accuracy 91.166%\n",
      "Epoch 28, Batch 645, LR 0.888354 Loss 4.281612, Accuracy 91.170%\n",
      "Epoch 28, Batch 646, LR 0.888226 Loss 4.282040, Accuracy 91.175%\n",
      "Epoch 28, Batch 647, LR 0.888098 Loss 4.281228, Accuracy 91.182%\n",
      "Epoch 28, Batch 648, LR 0.887969 Loss 4.281143, Accuracy 91.181%\n",
      "Epoch 28, Batch 649, LR 0.887841 Loss 4.281564, Accuracy 91.178%\n",
      "Epoch 28, Batch 650, LR 0.887713 Loss 4.281601, Accuracy 91.180%\n",
      "Epoch 28, Batch 651, LR 0.887585 Loss 4.281621, Accuracy 91.181%\n",
      "Epoch 28, Batch 652, LR 0.887457 Loss 4.282209, Accuracy 91.177%\n",
      "Epoch 28, Batch 653, LR 0.887328 Loss 4.281644, Accuracy 91.186%\n",
      "Epoch 28, Batch 654, LR 0.887200 Loss 4.280778, Accuracy 91.192%\n",
      "Epoch 28, Batch 655, LR 0.887072 Loss 4.280889, Accuracy 91.195%\n",
      "Epoch 28, Batch 656, LR 0.886944 Loss 4.281064, Accuracy 91.190%\n",
      "Epoch 28, Batch 657, LR 0.886816 Loss 4.280412, Accuracy 91.186%\n",
      "Epoch 28, Batch 658, LR 0.886688 Loss 4.280053, Accuracy 91.190%\n",
      "Epoch 28, Batch 659, LR 0.886559 Loss 4.280591, Accuracy 91.187%\n",
      "Epoch 28, Batch 660, LR 0.886431 Loss 4.280872, Accuracy 91.188%\n",
      "Epoch 28, Batch 661, LR 0.886303 Loss 4.281183, Accuracy 91.183%\n",
      "Epoch 28, Batch 662, LR 0.886175 Loss 4.280617, Accuracy 91.184%\n",
      "Epoch 28, Batch 663, LR 0.886047 Loss 4.279504, Accuracy 91.191%\n",
      "Epoch 28, Batch 664, LR 0.885919 Loss 4.279409, Accuracy 91.194%\n",
      "Epoch 28, Batch 665, LR 0.885790 Loss 4.279010, Accuracy 91.196%\n",
      "Epoch 28, Batch 666, LR 0.885662 Loss 4.278135, Accuracy 91.204%\n",
      "Epoch 28, Batch 667, LR 0.885534 Loss 4.278741, Accuracy 91.202%\n",
      "Epoch 28, Batch 668, LR 0.885406 Loss 4.278402, Accuracy 91.202%\n",
      "Epoch 28, Batch 669, LR 0.885278 Loss 4.278051, Accuracy 91.197%\n",
      "Epoch 28, Batch 670, LR 0.885150 Loss 4.277941, Accuracy 91.200%\n",
      "Epoch 28, Batch 671, LR 0.885022 Loss 4.278358, Accuracy 91.198%\n",
      "Epoch 28, Batch 672, LR 0.884894 Loss 4.277705, Accuracy 91.205%\n",
      "Epoch 28, Batch 673, LR 0.884765 Loss 4.276883, Accuracy 91.208%\n",
      "Epoch 28, Batch 674, LR 0.884637 Loss 4.276861, Accuracy 91.207%\n",
      "Epoch 28, Batch 675, LR 0.884509 Loss 4.275569, Accuracy 91.208%\n",
      "Epoch 28, Batch 676, LR 0.884381 Loss 4.276473, Accuracy 91.205%\n",
      "Epoch 28, Batch 677, LR 0.884253 Loss 4.276266, Accuracy 91.200%\n",
      "Epoch 28, Batch 678, LR 0.884125 Loss 4.275316, Accuracy 91.205%\n",
      "Epoch 28, Batch 679, LR 0.883997 Loss 4.274768, Accuracy 91.205%\n",
      "Epoch 28, Batch 680, LR 0.883869 Loss 4.275707, Accuracy 91.198%\n",
      "Epoch 28, Batch 681, LR 0.883741 Loss 4.276003, Accuracy 91.196%\n",
      "Epoch 28, Batch 682, LR 0.883613 Loss 4.276377, Accuracy 91.191%\n",
      "Epoch 28, Batch 683, LR 0.883485 Loss 4.277084, Accuracy 91.190%\n",
      "Epoch 28, Batch 684, LR 0.883357 Loss 4.277322, Accuracy 91.186%\n",
      "Epoch 28, Batch 685, LR 0.883229 Loss 4.277735, Accuracy 91.184%\n",
      "Epoch 28, Batch 686, LR 0.883100 Loss 4.278431, Accuracy 91.180%\n",
      "Epoch 28, Batch 687, LR 0.882972 Loss 4.278361, Accuracy 91.179%\n",
      "Epoch 28, Batch 688, LR 0.882844 Loss 4.276655, Accuracy 91.184%\n",
      "Epoch 28, Batch 689, LR 0.882716 Loss 4.275545, Accuracy 91.185%\n",
      "Epoch 28, Batch 690, LR 0.882588 Loss 4.274718, Accuracy 91.191%\n",
      "Epoch 28, Batch 691, LR 0.882460 Loss 4.275046, Accuracy 91.188%\n",
      "Epoch 28, Batch 692, LR 0.882332 Loss 4.275458, Accuracy 91.185%\n",
      "Epoch 28, Batch 693, LR 0.882204 Loss 4.275924, Accuracy 91.183%\n",
      "Epoch 28, Batch 694, LR 0.882076 Loss 4.276658, Accuracy 91.182%\n",
      "Epoch 28, Batch 695, LR 0.881948 Loss 4.276712, Accuracy 91.176%\n",
      "Epoch 28, Batch 696, LR 0.881820 Loss 4.277316, Accuracy 91.174%\n",
      "Epoch 28, Batch 697, LR 0.881692 Loss 4.277640, Accuracy 91.174%\n",
      "Epoch 28, Batch 698, LR 0.881564 Loss 4.277845, Accuracy 91.172%\n",
      "Epoch 28, Batch 699, LR 0.881436 Loss 4.278860, Accuracy 91.165%\n",
      "Epoch 28, Batch 700, LR 0.881308 Loss 4.278780, Accuracy 91.164%\n",
      "Epoch 28, Batch 701, LR 0.881180 Loss 4.279531, Accuracy 91.161%\n",
      "Epoch 28, Batch 702, LR 0.881052 Loss 4.280088, Accuracy 91.163%\n",
      "Epoch 28, Batch 703, LR 0.880924 Loss 4.280222, Accuracy 91.157%\n",
      "Epoch 28, Batch 704, LR 0.880796 Loss 4.279948, Accuracy 91.159%\n",
      "Epoch 28, Batch 705, LR 0.880668 Loss 4.280579, Accuracy 91.159%\n",
      "Epoch 28, Batch 706, LR 0.880540 Loss 4.280945, Accuracy 91.161%\n",
      "Epoch 28, Batch 707, LR 0.880412 Loss 4.281255, Accuracy 91.164%\n",
      "Epoch 28, Batch 708, LR 0.880284 Loss 4.281791, Accuracy 91.162%\n",
      "Epoch 28, Batch 709, LR 0.880156 Loss 4.281813, Accuracy 91.163%\n",
      "Epoch 28, Batch 710, LR 0.880028 Loss 4.280742, Accuracy 91.167%\n",
      "Epoch 28, Batch 711, LR 0.879900 Loss 4.279520, Accuracy 91.171%\n",
      "Epoch 28, Batch 712, LR 0.879773 Loss 4.279298, Accuracy 91.170%\n",
      "Epoch 28, Batch 713, LR 0.879645 Loss 4.278930, Accuracy 91.173%\n",
      "Epoch 28, Batch 714, LR 0.879517 Loss 4.278642, Accuracy 91.176%\n",
      "Epoch 28, Batch 715, LR 0.879389 Loss 4.278959, Accuracy 91.177%\n",
      "Epoch 28, Batch 716, LR 0.879261 Loss 4.278275, Accuracy 91.177%\n",
      "Epoch 28, Batch 717, LR 0.879133 Loss 4.278710, Accuracy 91.173%\n",
      "Epoch 28, Batch 718, LR 0.879005 Loss 4.278883, Accuracy 91.172%\n",
      "Epoch 28, Batch 719, LR 0.878877 Loss 4.278980, Accuracy 91.173%\n",
      "Epoch 28, Batch 720, LR 0.878749 Loss 4.278875, Accuracy 91.173%\n",
      "Epoch 28, Batch 721, LR 0.878621 Loss 4.277948, Accuracy 91.178%\n",
      "Epoch 28, Batch 722, LR 0.878493 Loss 4.277528, Accuracy 91.178%\n",
      "Epoch 28, Batch 723, LR 0.878365 Loss 4.277400, Accuracy 91.179%\n",
      "Epoch 28, Batch 724, LR 0.878238 Loss 4.276993, Accuracy 91.177%\n",
      "Epoch 28, Batch 725, LR 0.878110 Loss 4.276957, Accuracy 91.181%\n",
      "Epoch 28, Batch 726, LR 0.877982 Loss 4.277420, Accuracy 91.179%\n",
      "Epoch 28, Batch 727, LR 0.877854 Loss 4.277561, Accuracy 91.178%\n",
      "Epoch 28, Batch 728, LR 0.877726 Loss 4.278074, Accuracy 91.178%\n",
      "Epoch 28, Batch 729, LR 0.877598 Loss 4.277048, Accuracy 91.182%\n",
      "Epoch 28, Batch 730, LR 0.877470 Loss 4.277059, Accuracy 91.182%\n",
      "Epoch 28, Batch 731, LR 0.877342 Loss 4.277139, Accuracy 91.181%\n",
      "Epoch 28, Batch 732, LR 0.877215 Loss 4.276468, Accuracy 91.185%\n",
      "Epoch 28, Batch 733, LR 0.877087 Loss 4.275789, Accuracy 91.187%\n",
      "Epoch 28, Batch 734, LR 0.876959 Loss 4.275427, Accuracy 91.182%\n",
      "Epoch 28, Batch 735, LR 0.876831 Loss 4.275529, Accuracy 91.181%\n",
      "Epoch 28, Batch 736, LR 0.876703 Loss 4.276018, Accuracy 91.181%\n",
      "Epoch 28, Batch 737, LR 0.876575 Loss 4.275828, Accuracy 91.180%\n",
      "Epoch 28, Batch 738, LR 0.876447 Loss 4.275829, Accuracy 91.183%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 739, LR 0.876320 Loss 4.276377, Accuracy 91.181%\n",
      "Epoch 28, Batch 740, LR 0.876192 Loss 4.275506, Accuracy 91.187%\n",
      "Epoch 28, Batch 741, LR 0.876064 Loss 4.274887, Accuracy 91.190%\n",
      "Epoch 28, Batch 742, LR 0.875936 Loss 4.274634, Accuracy 91.190%\n",
      "Epoch 28, Batch 743, LR 0.875808 Loss 4.274776, Accuracy 91.191%\n",
      "Epoch 28, Batch 744, LR 0.875681 Loss 4.274648, Accuracy 91.193%\n",
      "Epoch 28, Batch 745, LR 0.875553 Loss 4.273564, Accuracy 91.197%\n",
      "Epoch 28, Batch 746, LR 0.875425 Loss 4.273456, Accuracy 91.197%\n",
      "Epoch 28, Batch 747, LR 0.875297 Loss 4.273059, Accuracy 91.195%\n",
      "Epoch 28, Batch 748, LR 0.875169 Loss 4.273126, Accuracy 91.194%\n",
      "Epoch 28, Batch 749, LR 0.875042 Loss 4.273623, Accuracy 91.189%\n",
      "Epoch 28, Batch 750, LR 0.874914 Loss 4.273800, Accuracy 91.188%\n",
      "Epoch 28, Batch 751, LR 0.874786 Loss 4.274368, Accuracy 91.186%\n",
      "Epoch 28, Batch 752, LR 0.874658 Loss 4.274234, Accuracy 91.185%\n",
      "Epoch 28, Batch 753, LR 0.874530 Loss 4.273792, Accuracy 91.188%\n",
      "Epoch 28, Batch 754, LR 0.874403 Loss 4.273819, Accuracy 91.186%\n",
      "Epoch 28, Batch 755, LR 0.874275 Loss 4.273299, Accuracy 91.190%\n",
      "Epoch 28, Batch 756, LR 0.874147 Loss 4.273608, Accuracy 91.186%\n",
      "Epoch 28, Batch 757, LR 0.874019 Loss 4.272744, Accuracy 91.186%\n",
      "Epoch 28, Batch 758, LR 0.873892 Loss 4.272457, Accuracy 91.189%\n",
      "Epoch 28, Batch 759, LR 0.873764 Loss 4.271940, Accuracy 91.190%\n",
      "Epoch 28, Batch 760, LR 0.873636 Loss 4.271561, Accuracy 91.192%\n",
      "Epoch 28, Batch 761, LR 0.873508 Loss 4.271956, Accuracy 91.192%\n",
      "Epoch 28, Batch 762, LR 0.873381 Loss 4.271360, Accuracy 91.190%\n",
      "Epoch 28, Batch 763, LR 0.873253 Loss 4.271115, Accuracy 91.194%\n",
      "Epoch 28, Batch 764, LR 0.873125 Loss 4.271243, Accuracy 91.190%\n",
      "Epoch 28, Batch 765, LR 0.872998 Loss 4.271526, Accuracy 91.191%\n",
      "Epoch 28, Batch 766, LR 0.872870 Loss 4.271578, Accuracy 91.187%\n",
      "Epoch 28, Batch 767, LR 0.872742 Loss 4.271906, Accuracy 91.186%\n",
      "Epoch 28, Batch 768, LR 0.872614 Loss 4.272021, Accuracy 91.186%\n",
      "Epoch 28, Batch 769, LR 0.872487 Loss 4.272293, Accuracy 91.185%\n",
      "Epoch 28, Batch 770, LR 0.872359 Loss 4.272402, Accuracy 91.182%\n",
      "Epoch 28, Batch 771, LR 0.872231 Loss 4.273157, Accuracy 91.177%\n",
      "Epoch 28, Batch 772, LR 0.872104 Loss 4.272594, Accuracy 91.182%\n",
      "Epoch 28, Batch 773, LR 0.871976 Loss 4.272017, Accuracy 91.183%\n",
      "Epoch 28, Batch 774, LR 0.871848 Loss 4.271777, Accuracy 91.182%\n",
      "Epoch 28, Batch 775, LR 0.871721 Loss 4.272100, Accuracy 91.184%\n",
      "Epoch 28, Batch 776, LR 0.871593 Loss 4.272048, Accuracy 91.186%\n",
      "Epoch 28, Batch 777, LR 0.871465 Loss 4.272782, Accuracy 91.184%\n",
      "Epoch 28, Batch 778, LR 0.871338 Loss 4.272630, Accuracy 91.186%\n",
      "Epoch 28, Batch 779, LR 0.871210 Loss 4.272317, Accuracy 91.187%\n",
      "Epoch 28, Batch 780, LR 0.871082 Loss 4.271388, Accuracy 91.191%\n",
      "Epoch 28, Batch 781, LR 0.870955 Loss 4.271009, Accuracy 91.193%\n",
      "Epoch 28, Batch 782, LR 0.870827 Loss 4.271334, Accuracy 91.188%\n",
      "Epoch 28, Batch 783, LR 0.870699 Loss 4.270657, Accuracy 91.187%\n",
      "Epoch 28, Batch 784, LR 0.870572 Loss 4.270950, Accuracy 91.185%\n",
      "Epoch 28, Batch 785, LR 0.870444 Loss 4.270939, Accuracy 91.185%\n",
      "Epoch 28, Batch 786, LR 0.870316 Loss 4.270390, Accuracy 91.187%\n",
      "Epoch 28, Batch 787, LR 0.870189 Loss 4.270245, Accuracy 91.193%\n",
      "Epoch 28, Batch 788, LR 0.870061 Loss 4.270683, Accuracy 91.191%\n",
      "Epoch 28, Batch 789, LR 0.869934 Loss 4.270204, Accuracy 91.191%\n",
      "Epoch 28, Batch 790, LR 0.869806 Loss 4.269847, Accuracy 91.190%\n",
      "Epoch 28, Batch 791, LR 0.869678 Loss 4.271046, Accuracy 91.181%\n",
      "Epoch 28, Batch 792, LR 0.869551 Loss 4.271639, Accuracy 91.178%\n",
      "Epoch 28, Batch 793, LR 0.869423 Loss 4.271519, Accuracy 91.177%\n",
      "Epoch 28, Batch 794, LR 0.869296 Loss 4.271176, Accuracy 91.180%\n",
      "Epoch 28, Batch 795, LR 0.869168 Loss 4.271599, Accuracy 91.181%\n",
      "Epoch 28, Batch 796, LR 0.869040 Loss 4.271789, Accuracy 91.183%\n",
      "Epoch 28, Batch 797, LR 0.868913 Loss 4.271276, Accuracy 91.187%\n",
      "Epoch 28, Batch 798, LR 0.868785 Loss 4.270135, Accuracy 91.192%\n",
      "Epoch 28, Batch 799, LR 0.868658 Loss 4.269522, Accuracy 91.196%\n",
      "Epoch 28, Batch 800, LR 0.868530 Loss 4.269913, Accuracy 91.195%\n",
      "Epoch 28, Batch 801, LR 0.868403 Loss 4.269700, Accuracy 91.195%\n",
      "Epoch 28, Batch 802, LR 0.868275 Loss 4.268948, Accuracy 91.196%\n",
      "Epoch 28, Batch 803, LR 0.868148 Loss 4.268165, Accuracy 91.196%\n",
      "Epoch 28, Batch 804, LR 0.868020 Loss 4.267925, Accuracy 91.198%\n",
      "Epoch 28, Batch 805, LR 0.867892 Loss 4.268031, Accuracy 91.199%\n",
      "Epoch 28, Batch 806, LR 0.867765 Loss 4.267910, Accuracy 91.201%\n",
      "Epoch 28, Batch 807, LR 0.867637 Loss 4.267750, Accuracy 91.201%\n",
      "Epoch 28, Batch 808, LR 0.867510 Loss 4.266942, Accuracy 91.201%\n",
      "Epoch 28, Batch 809, LR 0.867382 Loss 4.266320, Accuracy 91.202%\n",
      "Epoch 28, Batch 810, LR 0.867255 Loss 4.266226, Accuracy 91.203%\n",
      "Epoch 28, Batch 811, LR 0.867127 Loss 4.266122, Accuracy 91.203%\n",
      "Epoch 28, Batch 812, LR 0.867000 Loss 4.266234, Accuracy 91.198%\n",
      "Epoch 28, Batch 813, LR 0.866872 Loss 4.265898, Accuracy 91.198%\n",
      "Epoch 28, Batch 814, LR 0.866745 Loss 4.265742, Accuracy 91.197%\n",
      "Epoch 28, Batch 815, LR 0.866617 Loss 4.265582, Accuracy 91.196%\n",
      "Epoch 28, Batch 816, LR 0.866490 Loss 4.265053, Accuracy 91.195%\n",
      "Epoch 28, Batch 817, LR 0.866362 Loss 4.265103, Accuracy 91.191%\n",
      "Epoch 28, Batch 818, LR 0.866235 Loss 4.264993, Accuracy 91.194%\n",
      "Epoch 28, Batch 819, LR 0.866107 Loss 4.265509, Accuracy 91.191%\n",
      "Epoch 28, Batch 820, LR 0.865980 Loss 4.265440, Accuracy 91.195%\n",
      "Epoch 28, Batch 821, LR 0.865852 Loss 4.266660, Accuracy 91.191%\n",
      "Epoch 28, Batch 822, LR 0.865725 Loss 4.266982, Accuracy 91.188%\n",
      "Epoch 28, Batch 823, LR 0.865597 Loss 4.265910, Accuracy 91.195%\n",
      "Epoch 28, Batch 824, LR 0.865470 Loss 4.266068, Accuracy 91.191%\n",
      "Epoch 28, Batch 825, LR 0.865342 Loss 4.265887, Accuracy 91.194%\n",
      "Epoch 28, Batch 826, LR 0.865215 Loss 4.265460, Accuracy 91.194%\n",
      "Epoch 28, Batch 827, LR 0.865088 Loss 4.265174, Accuracy 91.197%\n",
      "Epoch 28, Batch 828, LR 0.864960 Loss 4.264319, Accuracy 91.202%\n",
      "Epoch 28, Batch 829, LR 0.864833 Loss 4.264082, Accuracy 91.201%\n",
      "Epoch 28, Batch 830, LR 0.864705 Loss 4.264201, Accuracy 91.199%\n",
      "Epoch 28, Batch 831, LR 0.864578 Loss 4.263992, Accuracy 91.198%\n",
      "Epoch 28, Batch 832, LR 0.864450 Loss 4.263992, Accuracy 91.201%\n",
      "Epoch 28, Batch 833, LR 0.864323 Loss 4.264253, Accuracy 91.198%\n",
      "Epoch 28, Batch 834, LR 0.864196 Loss 4.265222, Accuracy 91.189%\n",
      "Epoch 28, Batch 835, LR 0.864068 Loss 4.264331, Accuracy 91.191%\n",
      "Epoch 28, Batch 836, LR 0.863941 Loss 4.264106, Accuracy 91.191%\n",
      "Epoch 28, Batch 837, LR 0.863813 Loss 4.263056, Accuracy 91.195%\n",
      "Epoch 28, Batch 838, LR 0.863686 Loss 4.262757, Accuracy 91.195%\n",
      "Epoch 28, Batch 839, LR 0.863559 Loss 4.262554, Accuracy 91.196%\n",
      "Epoch 28, Batch 840, LR 0.863431 Loss 4.262479, Accuracy 91.195%\n",
      "Epoch 28, Batch 841, LR 0.863304 Loss 4.262998, Accuracy 91.190%\n",
      "Epoch 28, Batch 842, LR 0.863176 Loss 4.262953, Accuracy 91.190%\n",
      "Epoch 28, Batch 843, LR 0.863049 Loss 4.262813, Accuracy 91.188%\n",
      "Epoch 28, Batch 844, LR 0.862922 Loss 4.263092, Accuracy 91.185%\n",
      "Epoch 28, Batch 845, LR 0.862794 Loss 4.263540, Accuracy 91.181%\n",
      "Epoch 28, Batch 846, LR 0.862667 Loss 4.263043, Accuracy 91.186%\n",
      "Epoch 28, Batch 847, LR 0.862540 Loss 4.263225, Accuracy 91.189%\n",
      "Epoch 28, Batch 848, LR 0.862412 Loss 4.262911, Accuracy 91.189%\n",
      "Epoch 28, Batch 849, LR 0.862285 Loss 4.262786, Accuracy 91.192%\n",
      "Epoch 28, Batch 850, LR 0.862158 Loss 4.262704, Accuracy 91.194%\n",
      "Epoch 28, Batch 851, LR 0.862030 Loss 4.262572, Accuracy 91.195%\n",
      "Epoch 28, Batch 852, LR 0.861903 Loss 4.262763, Accuracy 91.190%\n",
      "Epoch 28, Batch 853, LR 0.861776 Loss 4.262538, Accuracy 91.194%\n",
      "Epoch 28, Batch 854, LR 0.861648 Loss 4.262592, Accuracy 91.193%\n",
      "Epoch 28, Batch 855, LR 0.861521 Loss 4.263565, Accuracy 91.185%\n",
      "Epoch 28, Batch 856, LR 0.861394 Loss 4.264124, Accuracy 91.180%\n",
      "Epoch 28, Batch 857, LR 0.861266 Loss 4.264172, Accuracy 91.182%\n",
      "Epoch 28, Batch 858, LR 0.861139 Loss 4.264274, Accuracy 91.186%\n",
      "Epoch 28, Batch 859, LR 0.861012 Loss 4.264718, Accuracy 91.185%\n",
      "Epoch 28, Batch 860, LR 0.860884 Loss 4.265280, Accuracy 91.184%\n",
      "Epoch 28, Batch 861, LR 0.860757 Loss 4.265463, Accuracy 91.184%\n",
      "Epoch 28, Batch 862, LR 0.860630 Loss 4.265628, Accuracy 91.180%\n",
      "Epoch 28, Batch 863, LR 0.860502 Loss 4.266179, Accuracy 91.177%\n",
      "Epoch 28, Batch 864, LR 0.860375 Loss 4.266310, Accuracy 91.174%\n",
      "Epoch 28, Batch 865, LR 0.860248 Loss 4.266249, Accuracy 91.171%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 866, LR 0.860121 Loss 4.266452, Accuracy 91.169%\n",
      "Epoch 28, Batch 867, LR 0.859993 Loss 4.266200, Accuracy 91.170%\n",
      "Epoch 28, Batch 868, LR 0.859866 Loss 4.266452, Accuracy 91.166%\n",
      "Epoch 28, Batch 869, LR 0.859739 Loss 4.266235, Accuracy 91.168%\n",
      "Epoch 28, Batch 870, LR 0.859612 Loss 4.265396, Accuracy 91.168%\n",
      "Epoch 28, Batch 871, LR 0.859484 Loss 4.264925, Accuracy 91.172%\n",
      "Epoch 28, Batch 872, LR 0.859357 Loss 4.265377, Accuracy 91.167%\n",
      "Epoch 28, Batch 873, LR 0.859230 Loss 4.265191, Accuracy 91.171%\n",
      "Epoch 28, Batch 874, LR 0.859103 Loss 4.264599, Accuracy 91.176%\n",
      "Epoch 28, Batch 875, LR 0.858975 Loss 4.264868, Accuracy 91.172%\n",
      "Epoch 28, Batch 876, LR 0.858848 Loss 4.265142, Accuracy 91.170%\n",
      "Epoch 28, Batch 877, LR 0.858721 Loss 4.264471, Accuracy 91.170%\n",
      "Epoch 28, Batch 878, LR 0.858594 Loss 4.264818, Accuracy 91.170%\n",
      "Epoch 28, Batch 879, LR 0.858467 Loss 4.264494, Accuracy 91.172%\n",
      "Epoch 28, Batch 880, LR 0.858339 Loss 4.265378, Accuracy 91.168%\n",
      "Epoch 28, Batch 881, LR 0.858212 Loss 4.266452, Accuracy 91.165%\n",
      "Epoch 28, Batch 882, LR 0.858085 Loss 4.267136, Accuracy 91.161%\n",
      "Epoch 28, Batch 883, LR 0.857958 Loss 4.267645, Accuracy 91.159%\n",
      "Epoch 28, Batch 884, LR 0.857831 Loss 4.266866, Accuracy 91.162%\n",
      "Epoch 28, Batch 885, LR 0.857703 Loss 4.266971, Accuracy 91.161%\n",
      "Epoch 28, Batch 886, LR 0.857576 Loss 4.267568, Accuracy 91.158%\n",
      "Epoch 28, Batch 887, LR 0.857449 Loss 4.267909, Accuracy 91.158%\n",
      "Epoch 28, Batch 888, LR 0.857322 Loss 4.267848, Accuracy 91.159%\n",
      "Epoch 28, Batch 889, LR 0.857195 Loss 4.268256, Accuracy 91.158%\n",
      "Epoch 28, Batch 890, LR 0.857067 Loss 4.268094, Accuracy 91.159%\n",
      "Epoch 28, Batch 891, LR 0.856940 Loss 4.267781, Accuracy 91.158%\n",
      "Epoch 28, Batch 892, LR 0.856813 Loss 4.268535, Accuracy 91.156%\n",
      "Epoch 28, Batch 893, LR 0.856686 Loss 4.268046, Accuracy 91.155%\n",
      "Epoch 28, Batch 894, LR 0.856559 Loss 4.268116, Accuracy 91.156%\n",
      "Epoch 28, Batch 895, LR 0.856432 Loss 4.268462, Accuracy 91.155%\n",
      "Epoch 28, Batch 896, LR 0.856305 Loss 4.268183, Accuracy 91.154%\n",
      "Epoch 28, Batch 897, LR 0.856177 Loss 4.268270, Accuracy 91.152%\n",
      "Epoch 28, Batch 898, LR 0.856050 Loss 4.268493, Accuracy 91.150%\n",
      "Epoch 28, Batch 899, LR 0.855923 Loss 4.268703, Accuracy 91.148%\n",
      "Epoch 28, Batch 900, LR 0.855796 Loss 4.268945, Accuracy 91.144%\n",
      "Epoch 28, Batch 901, LR 0.855669 Loss 4.268335, Accuracy 91.144%\n",
      "Epoch 28, Batch 902, LR 0.855542 Loss 4.268370, Accuracy 91.143%\n",
      "Epoch 28, Batch 903, LR 0.855415 Loss 4.268084, Accuracy 91.147%\n",
      "Epoch 28, Batch 904, LR 0.855288 Loss 4.267764, Accuracy 91.150%\n",
      "Epoch 28, Batch 905, LR 0.855161 Loss 4.266345, Accuracy 91.156%\n",
      "Epoch 28, Batch 906, LR 0.855033 Loss 4.266198, Accuracy 91.156%\n",
      "Epoch 28, Batch 907, LR 0.854906 Loss 4.266651, Accuracy 91.154%\n",
      "Epoch 28, Batch 908, LR 0.854779 Loss 4.266751, Accuracy 91.152%\n",
      "Epoch 28, Batch 909, LR 0.854652 Loss 4.266785, Accuracy 91.150%\n",
      "Epoch 28, Batch 910, LR 0.854525 Loss 4.266304, Accuracy 91.152%\n",
      "Epoch 28, Batch 911, LR 0.854398 Loss 4.266453, Accuracy 91.151%\n",
      "Epoch 28, Batch 912, LR 0.854271 Loss 4.266854, Accuracy 91.152%\n",
      "Epoch 28, Batch 913, LR 0.854144 Loss 4.266739, Accuracy 91.154%\n",
      "Epoch 28, Batch 914, LR 0.854017 Loss 4.267147, Accuracy 91.150%\n",
      "Epoch 28, Batch 915, LR 0.853890 Loss 4.266638, Accuracy 91.154%\n",
      "Epoch 28, Batch 916, LR 0.853763 Loss 4.266316, Accuracy 91.155%\n",
      "Epoch 28, Batch 917, LR 0.853636 Loss 4.266462, Accuracy 91.157%\n",
      "Epoch 28, Batch 918, LR 0.853509 Loss 4.267236, Accuracy 91.155%\n",
      "Epoch 28, Batch 919, LR 0.853382 Loss 4.267616, Accuracy 91.153%\n",
      "Epoch 28, Batch 920, LR 0.853255 Loss 4.267404, Accuracy 91.154%\n",
      "Epoch 28, Batch 921, LR 0.853128 Loss 4.266965, Accuracy 91.153%\n",
      "Epoch 28, Batch 922, LR 0.853001 Loss 4.266716, Accuracy 91.152%\n",
      "Epoch 28, Batch 923, LR 0.852874 Loss 4.266405, Accuracy 91.158%\n",
      "Epoch 28, Batch 924, LR 0.852747 Loss 4.266098, Accuracy 91.159%\n",
      "Epoch 28, Batch 925, LR 0.852620 Loss 4.265956, Accuracy 91.160%\n",
      "Epoch 28, Batch 926, LR 0.852493 Loss 4.266197, Accuracy 91.158%\n",
      "Epoch 28, Batch 927, LR 0.852366 Loss 4.266415, Accuracy 91.158%\n",
      "Epoch 28, Batch 928, LR 0.852239 Loss 4.266971, Accuracy 91.154%\n",
      "Epoch 28, Batch 929, LR 0.852112 Loss 4.266739, Accuracy 91.154%\n",
      "Epoch 28, Batch 930, LR 0.851985 Loss 4.266277, Accuracy 91.154%\n",
      "Epoch 28, Batch 931, LR 0.851858 Loss 4.266955, Accuracy 91.149%\n",
      "Epoch 28, Batch 932, LR 0.851731 Loss 4.266253, Accuracy 91.151%\n",
      "Epoch 28, Batch 933, LR 0.851604 Loss 4.266332, Accuracy 91.149%\n",
      "Epoch 28, Batch 934, LR 0.851477 Loss 4.266311, Accuracy 91.146%\n",
      "Epoch 28, Batch 935, LR 0.851350 Loss 4.266482, Accuracy 91.144%\n",
      "Epoch 28, Batch 936, LR 0.851223 Loss 4.266073, Accuracy 91.148%\n",
      "Epoch 28, Batch 937, LR 0.851096 Loss 4.266581, Accuracy 91.148%\n",
      "Epoch 28, Batch 938, LR 0.850969 Loss 4.266758, Accuracy 91.150%\n",
      "Epoch 28, Batch 939, LR 0.850842 Loss 4.266768, Accuracy 91.151%\n",
      "Epoch 28, Batch 940, LR 0.850715 Loss 4.266436, Accuracy 91.150%\n",
      "Epoch 28, Batch 941, LR 0.850588 Loss 4.266425, Accuracy 91.151%\n",
      "Epoch 28, Batch 942, LR 0.850461 Loss 4.266472, Accuracy 91.152%\n",
      "Epoch 28, Batch 943, LR 0.850334 Loss 4.266703, Accuracy 91.151%\n",
      "Epoch 28, Batch 944, LR 0.850207 Loss 4.266778, Accuracy 91.155%\n",
      "Epoch 28, Batch 945, LR 0.850080 Loss 4.266640, Accuracy 91.157%\n",
      "Epoch 28, Batch 946, LR 0.849953 Loss 4.266854, Accuracy 91.158%\n",
      "Epoch 28, Batch 947, LR 0.849827 Loss 4.266764, Accuracy 91.163%\n",
      "Epoch 28, Batch 948, LR 0.849700 Loss 4.267244, Accuracy 91.161%\n",
      "Epoch 28, Batch 949, LR 0.849573 Loss 4.267206, Accuracy 91.162%\n",
      "Epoch 28, Batch 950, LR 0.849446 Loss 4.267272, Accuracy 91.160%\n",
      "Epoch 28, Batch 951, LR 0.849319 Loss 4.266714, Accuracy 91.165%\n",
      "Epoch 28, Batch 952, LR 0.849192 Loss 4.266505, Accuracy 91.169%\n",
      "Epoch 28, Batch 953, LR 0.849065 Loss 4.266049, Accuracy 91.170%\n",
      "Epoch 28, Batch 954, LR 0.848938 Loss 4.266015, Accuracy 91.168%\n",
      "Epoch 28, Batch 955, LR 0.848811 Loss 4.265700, Accuracy 91.171%\n",
      "Epoch 28, Batch 956, LR 0.848685 Loss 4.265686, Accuracy 91.173%\n",
      "Epoch 28, Batch 957, LR 0.848558 Loss 4.265774, Accuracy 91.171%\n",
      "Epoch 28, Batch 958, LR 0.848431 Loss 4.265869, Accuracy 91.168%\n",
      "Epoch 28, Batch 959, LR 0.848304 Loss 4.265500, Accuracy 91.169%\n",
      "Epoch 28, Batch 960, LR 0.848177 Loss 4.265883, Accuracy 91.168%\n",
      "Epoch 28, Batch 961, LR 0.848050 Loss 4.266205, Accuracy 91.169%\n",
      "Epoch 28, Batch 962, LR 0.847924 Loss 4.265307, Accuracy 91.172%\n",
      "Epoch 28, Batch 963, LR 0.847797 Loss 4.265355, Accuracy 91.173%\n",
      "Epoch 28, Batch 964, LR 0.847670 Loss 4.265039, Accuracy 91.174%\n",
      "Epoch 28, Batch 965, LR 0.847543 Loss 4.265347, Accuracy 91.173%\n",
      "Epoch 28, Batch 966, LR 0.847416 Loss 4.264825, Accuracy 91.173%\n",
      "Epoch 28, Batch 967, LR 0.847289 Loss 4.264733, Accuracy 91.173%\n",
      "Epoch 28, Batch 968, LR 0.847163 Loss 4.264710, Accuracy 91.172%\n",
      "Epoch 28, Batch 969, LR 0.847036 Loss 4.265269, Accuracy 91.172%\n",
      "Epoch 28, Batch 970, LR 0.846909 Loss 4.265304, Accuracy 91.169%\n",
      "Epoch 28, Batch 971, LR 0.846782 Loss 4.265316, Accuracy 91.167%\n",
      "Epoch 28, Batch 972, LR 0.846655 Loss 4.265794, Accuracy 91.166%\n",
      "Epoch 28, Batch 973, LR 0.846529 Loss 4.266043, Accuracy 91.167%\n",
      "Epoch 28, Batch 974, LR 0.846402 Loss 4.266324, Accuracy 91.164%\n",
      "Epoch 28, Batch 975, LR 0.846275 Loss 4.266387, Accuracy 91.165%\n",
      "Epoch 28, Batch 976, LR 0.846148 Loss 4.266627, Accuracy 91.166%\n",
      "Epoch 28, Batch 977, LR 0.846022 Loss 4.267058, Accuracy 91.162%\n",
      "Epoch 28, Batch 978, LR 0.845895 Loss 4.266852, Accuracy 91.167%\n",
      "Epoch 28, Batch 979, LR 0.845768 Loss 4.266403, Accuracy 91.171%\n",
      "Epoch 28, Batch 980, LR 0.845641 Loss 4.266429, Accuracy 91.172%\n",
      "Epoch 28, Batch 981, LR 0.845515 Loss 4.266900, Accuracy 91.171%\n",
      "Epoch 28, Batch 982, LR 0.845388 Loss 4.267077, Accuracy 91.168%\n",
      "Epoch 28, Batch 983, LR 0.845261 Loss 4.267024, Accuracy 91.169%\n",
      "Epoch 28, Batch 984, LR 0.845134 Loss 4.266802, Accuracy 91.171%\n",
      "Epoch 28, Batch 985, LR 0.845008 Loss 4.266871, Accuracy 91.171%\n",
      "Epoch 28, Batch 986, LR 0.844881 Loss 4.266037, Accuracy 91.173%\n",
      "Epoch 28, Batch 987, LR 0.844754 Loss 4.265522, Accuracy 91.174%\n",
      "Epoch 28, Batch 988, LR 0.844627 Loss 4.265056, Accuracy 91.175%\n",
      "Epoch 28, Batch 989, LR 0.844501 Loss 4.264964, Accuracy 91.171%\n",
      "Epoch 28, Batch 990, LR 0.844374 Loss 4.265397, Accuracy 91.170%\n",
      "Epoch 28, Batch 991, LR 0.844247 Loss 4.265620, Accuracy 91.171%\n",
      "Epoch 28, Batch 992, LR 0.844121 Loss 4.265262, Accuracy 91.174%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 993, LR 0.843994 Loss 4.264488, Accuracy 91.179%\n",
      "Epoch 28, Batch 994, LR 0.843867 Loss 4.264225, Accuracy 91.181%\n",
      "Epoch 28, Batch 995, LR 0.843741 Loss 4.264289, Accuracy 91.185%\n",
      "Epoch 28, Batch 996, LR 0.843614 Loss 4.264453, Accuracy 91.187%\n",
      "Epoch 28, Batch 997, LR 0.843487 Loss 4.264813, Accuracy 91.185%\n",
      "Epoch 28, Batch 998, LR 0.843361 Loss 4.264922, Accuracy 91.184%\n",
      "Epoch 28, Batch 999, LR 0.843234 Loss 4.265655, Accuracy 91.181%\n",
      "Epoch 28, Batch 1000, LR 0.843107 Loss 4.265918, Accuracy 91.182%\n",
      "Epoch 28, Batch 1001, LR 0.842981 Loss 4.265307, Accuracy 91.185%\n",
      "Epoch 28, Batch 1002, LR 0.842854 Loss 4.265200, Accuracy 91.183%\n",
      "Epoch 28, Batch 1003, LR 0.842727 Loss 4.265650, Accuracy 91.183%\n",
      "Epoch 28, Batch 1004, LR 0.842601 Loss 4.265944, Accuracy 91.181%\n",
      "Epoch 28, Batch 1005, LR 0.842474 Loss 4.266256, Accuracy 91.180%\n",
      "Epoch 28, Batch 1006, LR 0.842347 Loss 4.266358, Accuracy 91.181%\n",
      "Epoch 28, Batch 1007, LR 0.842221 Loss 4.266510, Accuracy 91.180%\n",
      "Epoch 28, Batch 1008, LR 0.842094 Loss 4.266291, Accuracy 91.184%\n",
      "Epoch 28, Batch 1009, LR 0.841967 Loss 4.267357, Accuracy 91.180%\n",
      "Epoch 28, Batch 1010, LR 0.841841 Loss 4.266991, Accuracy 91.182%\n",
      "Epoch 28, Batch 1011, LR 0.841714 Loss 4.266888, Accuracy 91.181%\n",
      "Epoch 28, Batch 1012, LR 0.841588 Loss 4.266830, Accuracy 91.180%\n",
      "Epoch 28, Batch 1013, LR 0.841461 Loss 4.266767, Accuracy 91.180%\n",
      "Epoch 28, Batch 1014, LR 0.841334 Loss 4.266715, Accuracy 91.178%\n",
      "Epoch 28, Batch 1015, LR 0.841208 Loss 4.267622, Accuracy 91.175%\n",
      "Epoch 28, Batch 1016, LR 0.841081 Loss 4.267659, Accuracy 91.174%\n",
      "Epoch 28, Batch 1017, LR 0.840955 Loss 4.268019, Accuracy 91.173%\n",
      "Epoch 28, Batch 1018, LR 0.840828 Loss 4.268692, Accuracy 91.170%\n",
      "Epoch 28, Batch 1019, LR 0.840702 Loss 4.268367, Accuracy 91.169%\n",
      "Epoch 28, Batch 1020, LR 0.840575 Loss 4.268484, Accuracy 91.168%\n",
      "Epoch 28, Batch 1021, LR 0.840448 Loss 4.268545, Accuracy 91.165%\n",
      "Epoch 28, Batch 1022, LR 0.840322 Loss 4.268515, Accuracy 91.165%\n",
      "Epoch 28, Batch 1023, LR 0.840195 Loss 4.268374, Accuracy 91.164%\n",
      "Epoch 28, Batch 1024, LR 0.840069 Loss 4.269236, Accuracy 91.157%\n",
      "Epoch 28, Batch 1025, LR 0.839942 Loss 4.268760, Accuracy 91.155%\n",
      "Epoch 28, Batch 1026, LR 0.839816 Loss 4.268538, Accuracy 91.154%\n",
      "Epoch 28, Batch 1027, LR 0.839689 Loss 4.267871, Accuracy 91.157%\n",
      "Epoch 28, Batch 1028, LR 0.839563 Loss 4.267911, Accuracy 91.159%\n",
      "Epoch 28, Batch 1029, LR 0.839436 Loss 4.268437, Accuracy 91.156%\n",
      "Epoch 28, Batch 1030, LR 0.839310 Loss 4.268675, Accuracy 91.154%\n",
      "Epoch 28, Batch 1031, LR 0.839183 Loss 4.268873, Accuracy 91.153%\n",
      "Epoch 28, Batch 1032, LR 0.839057 Loss 4.269029, Accuracy 91.157%\n",
      "Epoch 28, Batch 1033, LR 0.838930 Loss 4.269252, Accuracy 91.154%\n",
      "Epoch 28, Batch 1034, LR 0.838804 Loss 4.268800, Accuracy 91.155%\n",
      "Epoch 28, Batch 1035, LR 0.838677 Loss 4.268685, Accuracy 91.157%\n",
      "Epoch 28, Batch 1036, LR 0.838551 Loss 4.269263, Accuracy 91.151%\n",
      "Epoch 28, Batch 1037, LR 0.838424 Loss 4.268773, Accuracy 91.155%\n",
      "Epoch 28, Batch 1038, LR 0.838298 Loss 4.268335, Accuracy 91.156%\n",
      "Epoch 28, Batch 1039, LR 0.838171 Loss 4.268851, Accuracy 91.151%\n",
      "Epoch 28, Batch 1040, LR 0.838045 Loss 4.269152, Accuracy 91.149%\n",
      "Epoch 28, Batch 1041, LR 0.837918 Loss 4.269207, Accuracy 91.150%\n",
      "Epoch 28, Batch 1042, LR 0.837792 Loss 4.269734, Accuracy 91.150%\n",
      "Epoch 28, Batch 1043, LR 0.837665 Loss 4.269451, Accuracy 91.156%\n",
      "Epoch 28, Batch 1044, LR 0.837539 Loss 4.269680, Accuracy 91.157%\n",
      "Epoch 28, Batch 1045, LR 0.837412 Loss 4.269891, Accuracy 91.157%\n",
      "Epoch 28, Batch 1046, LR 0.837286 Loss 4.270128, Accuracy 91.158%\n",
      "Epoch 28, Batch 1047, LR 0.837159 Loss 4.270602, Accuracy 91.156%\n",
      "Epoch 28, Loss (train set) 4.270602, Accuracy (train set) 91.156%\n",
      "Epoch 29, Batch 1, LR 0.837033 Loss 4.282664, Accuracy 92.969%\n",
      "Epoch 29, Batch 2, LR 0.836907 Loss 3.815860, Accuracy 94.531%\n",
      "Epoch 29, Batch 3, LR 0.836780 Loss 3.881108, Accuracy 92.969%\n",
      "Epoch 29, Batch 4, LR 0.836654 Loss 3.928905, Accuracy 92.969%\n",
      "Epoch 29, Batch 5, LR 0.836527 Loss 4.002431, Accuracy 92.188%\n",
      "Epoch 29, Batch 6, LR 0.836401 Loss 4.063955, Accuracy 92.318%\n",
      "Epoch 29, Batch 7, LR 0.836275 Loss 4.062134, Accuracy 92.299%\n",
      "Epoch 29, Batch 8, LR 0.836148 Loss 4.036081, Accuracy 92.480%\n",
      "Epoch 29, Batch 9, LR 0.836022 Loss 4.068853, Accuracy 92.188%\n",
      "Epoch 29, Batch 10, LR 0.835895 Loss 4.094768, Accuracy 92.031%\n",
      "Epoch 29, Batch 11, LR 0.835769 Loss 4.103146, Accuracy 92.045%\n",
      "Epoch 29, Batch 12, LR 0.835643 Loss 4.193562, Accuracy 91.602%\n",
      "Epoch 29, Batch 13, LR 0.835516 Loss 4.214203, Accuracy 91.587%\n",
      "Epoch 29, Batch 14, LR 0.835390 Loss 4.179165, Accuracy 91.685%\n",
      "Epoch 29, Batch 15, LR 0.835263 Loss 4.191747, Accuracy 91.510%\n",
      "Epoch 29, Batch 16, LR 0.835137 Loss 4.140794, Accuracy 91.846%\n",
      "Epoch 29, Batch 17, LR 0.835011 Loss 4.146743, Accuracy 91.866%\n",
      "Epoch 29, Batch 18, LR 0.834884 Loss 4.169671, Accuracy 91.884%\n",
      "Epoch 29, Batch 19, LR 0.834758 Loss 4.170263, Accuracy 92.064%\n",
      "Epoch 29, Batch 20, LR 0.834632 Loss 4.175148, Accuracy 91.953%\n",
      "Epoch 29, Batch 21, LR 0.834505 Loss 4.153592, Accuracy 92.076%\n",
      "Epoch 29, Batch 22, LR 0.834379 Loss 4.145051, Accuracy 92.116%\n",
      "Epoch 29, Batch 23, LR 0.834253 Loss 4.142368, Accuracy 92.120%\n",
      "Epoch 29, Batch 24, LR 0.834126 Loss 4.158471, Accuracy 91.960%\n",
      "Epoch 29, Batch 25, LR 0.834000 Loss 4.160003, Accuracy 91.781%\n",
      "Epoch 29, Batch 26, LR 0.833874 Loss 4.160281, Accuracy 91.707%\n",
      "Epoch 29, Batch 27, LR 0.833747 Loss 4.171616, Accuracy 91.580%\n",
      "Epoch 29, Batch 28, LR 0.833621 Loss 4.144902, Accuracy 91.685%\n",
      "Epoch 29, Batch 29, LR 0.833495 Loss 4.154678, Accuracy 91.703%\n",
      "Epoch 29, Batch 30, LR 0.833369 Loss 4.140471, Accuracy 91.745%\n",
      "Epoch 29, Batch 31, LR 0.833242 Loss 4.134520, Accuracy 91.835%\n",
      "Epoch 29, Batch 32, LR 0.833116 Loss 4.123729, Accuracy 91.846%\n",
      "Epoch 29, Batch 33, LR 0.832990 Loss 4.122093, Accuracy 91.832%\n",
      "Epoch 29, Batch 34, LR 0.832863 Loss 4.123196, Accuracy 91.797%\n",
      "Epoch 29, Batch 35, LR 0.832737 Loss 4.105474, Accuracy 91.853%\n",
      "Epoch 29, Batch 36, LR 0.832611 Loss 4.103873, Accuracy 91.905%\n",
      "Epoch 29, Batch 37, LR 0.832485 Loss 4.105602, Accuracy 91.871%\n",
      "Epoch 29, Batch 38, LR 0.832358 Loss 4.098134, Accuracy 91.941%\n",
      "Epoch 29, Batch 39, LR 0.832232 Loss 4.096605, Accuracy 91.947%\n",
      "Epoch 29, Batch 40, LR 0.832106 Loss 4.096933, Accuracy 91.895%\n",
      "Epoch 29, Batch 41, LR 0.831980 Loss 4.087428, Accuracy 91.902%\n",
      "Epoch 29, Batch 42, LR 0.831853 Loss 4.082014, Accuracy 91.908%\n",
      "Epoch 29, Batch 43, LR 0.831727 Loss 4.076442, Accuracy 91.897%\n",
      "Epoch 29, Batch 44, LR 0.831601 Loss 4.062878, Accuracy 91.939%\n",
      "Epoch 29, Batch 45, LR 0.831475 Loss 4.068640, Accuracy 91.875%\n",
      "Epoch 29, Batch 46, LR 0.831348 Loss 4.071577, Accuracy 91.831%\n",
      "Epoch 29, Batch 47, LR 0.831222 Loss 4.064180, Accuracy 91.888%\n",
      "Epoch 29, Batch 48, LR 0.831096 Loss 4.061976, Accuracy 91.862%\n",
      "Epoch 29, Batch 49, LR 0.830970 Loss 4.061428, Accuracy 91.853%\n",
      "Epoch 29, Batch 50, LR 0.830844 Loss 4.051719, Accuracy 91.906%\n",
      "Epoch 29, Batch 51, LR 0.830717 Loss 4.071964, Accuracy 91.805%\n",
      "Epoch 29, Batch 52, LR 0.830591 Loss 4.076203, Accuracy 91.722%\n",
      "Epoch 29, Batch 53, LR 0.830465 Loss 4.083395, Accuracy 91.657%\n",
      "Epoch 29, Batch 54, LR 0.830339 Loss 4.082560, Accuracy 91.638%\n",
      "Epoch 29, Batch 55, LR 0.830213 Loss 4.084043, Accuracy 91.662%\n",
      "Epoch 29, Batch 56, LR 0.830087 Loss 4.081812, Accuracy 91.671%\n",
      "Epoch 29, Batch 57, LR 0.829960 Loss 4.087879, Accuracy 91.680%\n",
      "Epoch 29, Batch 58, LR 0.829834 Loss 4.091401, Accuracy 91.608%\n",
      "Epoch 29, Batch 59, LR 0.829708 Loss 4.101646, Accuracy 91.592%\n",
      "Epoch 29, Batch 60, LR 0.829582 Loss 4.109080, Accuracy 91.549%\n",
      "Epoch 29, Batch 61, LR 0.829456 Loss 4.103593, Accuracy 91.611%\n",
      "Epoch 29, Batch 62, LR 0.829330 Loss 4.102276, Accuracy 91.658%\n",
      "Epoch 29, Batch 63, LR 0.829203 Loss 4.096639, Accuracy 91.729%\n",
      "Epoch 29, Batch 64, LR 0.829077 Loss 4.098322, Accuracy 91.675%\n",
      "Epoch 29, Batch 65, LR 0.828951 Loss 4.100289, Accuracy 91.695%\n",
      "Epoch 29, Batch 66, LR 0.828825 Loss 4.103425, Accuracy 91.714%\n",
      "Epoch 29, Batch 67, LR 0.828699 Loss 4.104580, Accuracy 91.709%\n",
      "Epoch 29, Batch 68, LR 0.828573 Loss 4.107363, Accuracy 91.659%\n",
      "Epoch 29, Batch 69, LR 0.828447 Loss 4.112189, Accuracy 91.644%\n",
      "Epoch 29, Batch 70, LR 0.828321 Loss 4.124085, Accuracy 91.652%\n",
      "Epoch 29, Batch 71, LR 0.828195 Loss 4.122138, Accuracy 91.615%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 72, LR 0.828068 Loss 4.126969, Accuracy 91.612%\n",
      "Epoch 29, Batch 73, LR 0.827942 Loss 4.126991, Accuracy 91.620%\n",
      "Epoch 29, Batch 74, LR 0.827816 Loss 4.128657, Accuracy 91.586%\n",
      "Epoch 29, Batch 75, LR 0.827690 Loss 4.122827, Accuracy 91.625%\n",
      "Epoch 29, Batch 76, LR 0.827564 Loss 4.117849, Accuracy 91.694%\n",
      "Epoch 29, Batch 77, LR 0.827438 Loss 4.120446, Accuracy 91.640%\n",
      "Epoch 29, Batch 78, LR 0.827312 Loss 4.124933, Accuracy 91.657%\n",
      "Epoch 29, Batch 79, LR 0.827186 Loss 4.121134, Accuracy 91.663%\n",
      "Epoch 29, Batch 80, LR 0.827060 Loss 4.120898, Accuracy 91.641%\n",
      "Epoch 29, Batch 81, LR 0.826934 Loss 4.118482, Accuracy 91.676%\n",
      "Epoch 29, Batch 82, LR 0.826808 Loss 4.113185, Accuracy 91.692%\n",
      "Epoch 29, Batch 83, LR 0.826682 Loss 4.117008, Accuracy 91.660%\n",
      "Epoch 29, Batch 84, LR 0.826556 Loss 4.121818, Accuracy 91.657%\n",
      "Epoch 29, Batch 85, LR 0.826430 Loss 4.118579, Accuracy 91.682%\n",
      "Epoch 29, Batch 86, LR 0.826304 Loss 4.123639, Accuracy 91.661%\n",
      "Epoch 29, Batch 87, LR 0.826178 Loss 4.120494, Accuracy 91.730%\n",
      "Epoch 29, Batch 88, LR 0.826052 Loss 4.121207, Accuracy 91.726%\n",
      "Epoch 29, Batch 89, LR 0.825926 Loss 4.120596, Accuracy 91.722%\n",
      "Epoch 29, Batch 90, LR 0.825800 Loss 4.116205, Accuracy 91.736%\n",
      "Epoch 29, Batch 91, LR 0.825674 Loss 4.114847, Accuracy 91.732%\n",
      "Epoch 29, Batch 92, LR 0.825548 Loss 4.118595, Accuracy 91.729%\n",
      "Epoch 29, Batch 93, LR 0.825422 Loss 4.121990, Accuracy 91.700%\n",
      "Epoch 29, Batch 94, LR 0.825296 Loss 4.125411, Accuracy 91.714%\n",
      "Epoch 29, Batch 95, LR 0.825170 Loss 4.120127, Accuracy 91.735%\n",
      "Epoch 29, Batch 96, LR 0.825044 Loss 4.127633, Accuracy 91.675%\n",
      "Epoch 29, Batch 97, LR 0.824918 Loss 4.126482, Accuracy 91.672%\n",
      "Epoch 29, Batch 98, LR 0.824792 Loss 4.123815, Accuracy 91.653%\n",
      "Epoch 29, Batch 99, LR 0.824666 Loss 4.120917, Accuracy 91.651%\n",
      "Epoch 29, Batch 100, LR 0.824540 Loss 4.127935, Accuracy 91.625%\n",
      "Epoch 29, Batch 101, LR 0.824414 Loss 4.123970, Accuracy 91.631%\n",
      "Epoch 29, Batch 102, LR 0.824288 Loss 4.124264, Accuracy 91.636%\n",
      "Epoch 29, Batch 103, LR 0.824162 Loss 4.123898, Accuracy 91.626%\n",
      "Epoch 29, Batch 104, LR 0.824036 Loss 4.126055, Accuracy 91.602%\n",
      "Epoch 29, Batch 105, LR 0.823910 Loss 4.129768, Accuracy 91.570%\n",
      "Epoch 29, Batch 106, LR 0.823784 Loss 4.129214, Accuracy 91.591%\n",
      "Epoch 29, Batch 107, LR 0.823658 Loss 4.125177, Accuracy 91.581%\n",
      "Epoch 29, Batch 108, LR 0.823532 Loss 4.119746, Accuracy 91.594%\n",
      "Epoch 29, Batch 109, LR 0.823406 Loss 4.120955, Accuracy 91.593%\n",
      "Epoch 29, Batch 110, LR 0.823281 Loss 4.121477, Accuracy 91.598%\n",
      "Epoch 29, Batch 111, LR 0.823155 Loss 4.127079, Accuracy 91.568%\n",
      "Epoch 29, Batch 112, LR 0.823029 Loss 4.127381, Accuracy 91.595%\n",
      "Epoch 29, Batch 113, LR 0.822903 Loss 4.125801, Accuracy 91.593%\n",
      "Epoch 29, Batch 114, LR 0.822777 Loss 4.130575, Accuracy 91.591%\n",
      "Epoch 29, Batch 115, LR 0.822651 Loss 4.128262, Accuracy 91.610%\n",
      "Epoch 29, Batch 116, LR 0.822525 Loss 4.123937, Accuracy 91.622%\n",
      "Epoch 29, Batch 117, LR 0.822399 Loss 4.122475, Accuracy 91.620%\n",
      "Epoch 29, Batch 118, LR 0.822273 Loss 4.117612, Accuracy 91.651%\n",
      "Epoch 29, Batch 119, LR 0.822148 Loss 4.116601, Accuracy 91.636%\n",
      "Epoch 29, Batch 120, LR 0.822022 Loss 4.120158, Accuracy 91.641%\n",
      "Epoch 29, Batch 121, LR 0.821896 Loss 4.117737, Accuracy 91.645%\n",
      "Epoch 29, Batch 122, LR 0.821770 Loss 4.117717, Accuracy 91.662%\n",
      "Epoch 29, Batch 123, LR 0.821644 Loss 4.120408, Accuracy 91.654%\n",
      "Epoch 29, Batch 124, LR 0.821518 Loss 4.124109, Accuracy 91.614%\n",
      "Epoch 29, Batch 125, LR 0.821392 Loss 4.123689, Accuracy 91.631%\n",
      "Epoch 29, Batch 126, LR 0.821267 Loss 4.126151, Accuracy 91.623%\n",
      "Epoch 29, Batch 127, LR 0.821141 Loss 4.122276, Accuracy 91.640%\n",
      "Epoch 29, Batch 128, LR 0.821015 Loss 4.120785, Accuracy 91.675%\n",
      "Epoch 29, Batch 129, LR 0.820889 Loss 4.116605, Accuracy 91.703%\n",
      "Epoch 29, Batch 130, LR 0.820763 Loss 4.113281, Accuracy 91.731%\n",
      "Epoch 29, Batch 131, LR 0.820638 Loss 4.113084, Accuracy 91.722%\n",
      "Epoch 29, Batch 132, LR 0.820512 Loss 4.113647, Accuracy 91.738%\n",
      "Epoch 29, Batch 133, LR 0.820386 Loss 4.114467, Accuracy 91.753%\n",
      "Epoch 29, Batch 134, LR 0.820260 Loss 4.113902, Accuracy 91.750%\n",
      "Epoch 29, Batch 135, LR 0.820134 Loss 4.112804, Accuracy 91.736%\n",
      "Epoch 29, Batch 136, LR 0.820009 Loss 4.112797, Accuracy 91.728%\n",
      "Epoch 29, Batch 137, LR 0.819883 Loss 4.115204, Accuracy 91.714%\n",
      "Epoch 29, Batch 138, LR 0.819757 Loss 4.113969, Accuracy 91.740%\n",
      "Epoch 29, Batch 139, LR 0.819631 Loss 4.114633, Accuracy 91.743%\n",
      "Epoch 29, Batch 140, LR 0.819506 Loss 4.113955, Accuracy 91.741%\n",
      "Epoch 29, Batch 141, LR 0.819380 Loss 4.113342, Accuracy 91.733%\n",
      "Epoch 29, Batch 142, LR 0.819254 Loss 4.114463, Accuracy 91.720%\n",
      "Epoch 29, Batch 143, LR 0.819128 Loss 4.114768, Accuracy 91.734%\n",
      "Epoch 29, Batch 144, LR 0.819003 Loss 4.120479, Accuracy 91.705%\n",
      "Epoch 29, Batch 145, LR 0.818877 Loss 4.119636, Accuracy 91.713%\n",
      "Epoch 29, Batch 146, LR 0.818751 Loss 4.119968, Accuracy 91.690%\n",
      "Epoch 29, Batch 147, LR 0.818625 Loss 4.118130, Accuracy 91.672%\n",
      "Epoch 29, Batch 148, LR 0.818500 Loss 4.117664, Accuracy 91.686%\n",
      "Epoch 29, Batch 149, LR 0.818374 Loss 4.119998, Accuracy 91.658%\n",
      "Epoch 29, Batch 150, LR 0.818248 Loss 4.118653, Accuracy 91.661%\n",
      "Epoch 29, Batch 151, LR 0.818123 Loss 4.116812, Accuracy 91.655%\n",
      "Epoch 29, Batch 152, LR 0.817997 Loss 4.117073, Accuracy 91.653%\n",
      "Epoch 29, Batch 153, LR 0.817871 Loss 4.114041, Accuracy 91.662%\n",
      "Epoch 29, Batch 154, LR 0.817745 Loss 4.108752, Accuracy 91.665%\n",
      "Epoch 29, Batch 155, LR 0.817620 Loss 4.108232, Accuracy 91.663%\n",
      "Epoch 29, Batch 156, LR 0.817494 Loss 4.110370, Accuracy 91.652%\n",
      "Epoch 29, Batch 157, LR 0.817368 Loss 4.110484, Accuracy 91.645%\n",
      "Epoch 29, Batch 158, LR 0.817243 Loss 4.112743, Accuracy 91.639%\n",
      "Epoch 29, Batch 159, LR 0.817117 Loss 4.111482, Accuracy 91.637%\n",
      "Epoch 29, Batch 160, LR 0.816991 Loss 4.112251, Accuracy 91.636%\n",
      "Epoch 29, Batch 161, LR 0.816866 Loss 4.112011, Accuracy 91.634%\n",
      "Epoch 29, Batch 162, LR 0.816740 Loss 4.113861, Accuracy 91.652%\n",
      "Epoch 29, Batch 163, LR 0.816614 Loss 4.112283, Accuracy 91.665%\n",
      "Epoch 29, Batch 164, LR 0.816489 Loss 4.115035, Accuracy 91.649%\n",
      "Epoch 29, Batch 165, LR 0.816363 Loss 4.119290, Accuracy 91.638%\n",
      "Epoch 29, Batch 166, LR 0.816238 Loss 4.118826, Accuracy 91.637%\n",
      "Epoch 29, Batch 167, LR 0.816112 Loss 4.121938, Accuracy 91.631%\n",
      "Epoch 29, Batch 168, LR 0.815986 Loss 4.122795, Accuracy 91.606%\n",
      "Epoch 29, Batch 169, LR 0.815861 Loss 4.125221, Accuracy 91.591%\n",
      "Epoch 29, Batch 170, LR 0.815735 Loss 4.120874, Accuracy 91.618%\n",
      "Epoch 29, Batch 171, LR 0.815609 Loss 4.119628, Accuracy 91.626%\n",
      "Epoch 29, Batch 172, LR 0.815484 Loss 4.118499, Accuracy 91.620%\n",
      "Epoch 29, Batch 173, LR 0.815358 Loss 4.120515, Accuracy 91.618%\n",
      "Epoch 29, Batch 174, LR 0.815233 Loss 4.119333, Accuracy 91.622%\n",
      "Epoch 29, Batch 175, LR 0.815107 Loss 4.121693, Accuracy 91.616%\n",
      "Epoch 29, Batch 176, LR 0.814981 Loss 4.123140, Accuracy 91.610%\n",
      "Epoch 29, Batch 177, LR 0.814856 Loss 4.121242, Accuracy 91.614%\n",
      "Epoch 29, Batch 178, LR 0.814730 Loss 4.121333, Accuracy 91.617%\n",
      "Epoch 29, Batch 179, LR 0.814605 Loss 4.121131, Accuracy 91.620%\n",
      "Epoch 29, Batch 180, LR 0.814479 Loss 4.122692, Accuracy 91.628%\n",
      "Epoch 29, Batch 181, LR 0.814354 Loss 4.118878, Accuracy 91.644%\n",
      "Epoch 29, Batch 182, LR 0.814228 Loss 4.115330, Accuracy 91.642%\n",
      "Epoch 29, Batch 183, LR 0.814103 Loss 4.117092, Accuracy 91.654%\n",
      "Epoch 29, Batch 184, LR 0.813977 Loss 4.119123, Accuracy 91.640%\n",
      "Epoch 29, Batch 185, LR 0.813851 Loss 4.115640, Accuracy 91.655%\n",
      "Epoch 29, Batch 186, LR 0.813726 Loss 4.114063, Accuracy 91.654%\n",
      "Epoch 29, Batch 187, LR 0.813600 Loss 4.114562, Accuracy 91.653%\n",
      "Epoch 29, Batch 188, LR 0.813475 Loss 4.113584, Accuracy 91.656%\n",
      "Epoch 29, Batch 189, LR 0.813349 Loss 4.116532, Accuracy 91.650%\n",
      "Epoch 29, Batch 190, LR 0.813224 Loss 4.118895, Accuracy 91.632%\n",
      "Epoch 29, Batch 191, LR 0.813098 Loss 4.119383, Accuracy 91.627%\n",
      "Epoch 29, Batch 192, LR 0.812973 Loss 4.120483, Accuracy 91.626%\n",
      "Epoch 29, Batch 193, LR 0.812847 Loss 4.121230, Accuracy 91.633%\n",
      "Epoch 29, Batch 194, LR 0.812722 Loss 4.119455, Accuracy 91.644%\n",
      "Epoch 29, Batch 195, LR 0.812596 Loss 4.119043, Accuracy 91.639%\n",
      "Epoch 29, Batch 196, LR 0.812471 Loss 4.116027, Accuracy 91.657%\n",
      "Epoch 29, Batch 197, LR 0.812345 Loss 4.115619, Accuracy 91.656%\n",
      "Epoch 29, Batch 198, LR 0.812220 Loss 4.115763, Accuracy 91.659%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 199, LR 0.812094 Loss 4.116452, Accuracy 91.654%\n",
      "Epoch 29, Batch 200, LR 0.811969 Loss 4.114020, Accuracy 91.676%\n",
      "Epoch 29, Batch 201, LR 0.811844 Loss 4.111589, Accuracy 91.698%\n",
      "Epoch 29, Batch 202, LR 0.811718 Loss 4.110574, Accuracy 91.692%\n",
      "Epoch 29, Batch 203, LR 0.811593 Loss 4.111669, Accuracy 91.683%\n",
      "Epoch 29, Batch 204, LR 0.811467 Loss 4.113698, Accuracy 91.686%\n",
      "Epoch 29, Batch 205, LR 0.811342 Loss 4.115500, Accuracy 91.684%\n",
      "Epoch 29, Batch 206, LR 0.811216 Loss 4.115349, Accuracy 91.687%\n",
      "Epoch 29, Batch 207, LR 0.811091 Loss 4.114099, Accuracy 91.689%\n",
      "Epoch 29, Batch 208, LR 0.810965 Loss 4.115825, Accuracy 91.658%\n",
      "Epoch 29, Batch 209, LR 0.810840 Loss 4.120065, Accuracy 91.642%\n",
      "Epoch 29, Batch 210, LR 0.810715 Loss 4.120192, Accuracy 91.644%\n",
      "Epoch 29, Batch 211, LR 0.810589 Loss 4.119844, Accuracy 91.647%\n",
      "Epoch 29, Batch 212, LR 0.810464 Loss 4.120004, Accuracy 91.627%\n",
      "Epoch 29, Batch 213, LR 0.810338 Loss 4.120337, Accuracy 91.634%\n",
      "Epoch 29, Batch 214, LR 0.810213 Loss 4.119781, Accuracy 91.633%\n",
      "Epoch 29, Batch 215, LR 0.810088 Loss 4.118281, Accuracy 91.635%\n",
      "Epoch 29, Batch 216, LR 0.809962 Loss 4.117893, Accuracy 91.641%\n",
      "Epoch 29, Batch 217, LR 0.809837 Loss 4.115421, Accuracy 91.655%\n",
      "Epoch 29, Batch 218, LR 0.809712 Loss 4.116392, Accuracy 91.657%\n",
      "Epoch 29, Batch 219, LR 0.809586 Loss 4.118916, Accuracy 91.645%\n",
      "Epoch 29, Batch 220, LR 0.809461 Loss 4.119883, Accuracy 91.630%\n",
      "Epoch 29, Batch 221, LR 0.809335 Loss 4.119680, Accuracy 91.622%\n",
      "Epoch 29, Batch 222, LR 0.809210 Loss 4.125255, Accuracy 91.600%\n",
      "Epoch 29, Batch 223, LR 0.809085 Loss 4.127345, Accuracy 91.595%\n",
      "Epoch 29, Batch 224, LR 0.808959 Loss 4.123140, Accuracy 91.622%\n",
      "Epoch 29, Batch 225, LR 0.808834 Loss 4.124477, Accuracy 91.622%\n",
      "Epoch 29, Batch 226, LR 0.808709 Loss 4.128213, Accuracy 91.614%\n",
      "Epoch 29, Batch 227, LR 0.808583 Loss 4.126955, Accuracy 91.630%\n",
      "Epoch 29, Batch 228, LR 0.808458 Loss 4.126825, Accuracy 91.626%\n",
      "Epoch 29, Batch 229, LR 0.808333 Loss 4.123137, Accuracy 91.635%\n",
      "Epoch 29, Batch 230, LR 0.808208 Loss 4.128547, Accuracy 91.607%\n",
      "Epoch 29, Batch 231, LR 0.808082 Loss 4.128583, Accuracy 91.613%\n",
      "Epoch 29, Batch 232, LR 0.807957 Loss 4.126699, Accuracy 91.615%\n",
      "Epoch 29, Batch 233, LR 0.807832 Loss 4.125571, Accuracy 91.607%\n",
      "Epoch 29, Batch 234, LR 0.807706 Loss 4.125640, Accuracy 91.617%\n",
      "Epoch 29, Batch 235, LR 0.807581 Loss 4.126305, Accuracy 91.606%\n",
      "Epoch 29, Batch 236, LR 0.807456 Loss 4.127737, Accuracy 91.602%\n",
      "Epoch 29, Batch 237, LR 0.807330 Loss 4.127902, Accuracy 91.597%\n",
      "Epoch 29, Batch 238, LR 0.807205 Loss 4.128872, Accuracy 91.580%\n",
      "Epoch 29, Batch 239, LR 0.807080 Loss 4.128884, Accuracy 91.566%\n",
      "Epoch 29, Batch 240, LR 0.806955 Loss 4.130066, Accuracy 91.566%\n",
      "Epoch 29, Batch 241, LR 0.806829 Loss 4.131037, Accuracy 91.572%\n",
      "Epoch 29, Batch 242, LR 0.806704 Loss 4.134134, Accuracy 91.558%\n",
      "Epoch 29, Batch 243, LR 0.806579 Loss 4.138496, Accuracy 91.535%\n",
      "Epoch 29, Batch 244, LR 0.806454 Loss 4.139415, Accuracy 91.541%\n",
      "Epoch 29, Batch 245, LR 0.806328 Loss 4.139602, Accuracy 91.534%\n",
      "Epoch 29, Batch 246, LR 0.806203 Loss 4.137309, Accuracy 91.540%\n",
      "Epoch 29, Batch 247, LR 0.806078 Loss 4.136108, Accuracy 91.539%\n",
      "Epoch 29, Batch 248, LR 0.805953 Loss 4.137012, Accuracy 91.539%\n",
      "Epoch 29, Batch 249, LR 0.805828 Loss 4.136378, Accuracy 91.554%\n",
      "Epoch 29, Batch 250, LR 0.805702 Loss 4.138418, Accuracy 91.547%\n",
      "Epoch 29, Batch 251, LR 0.805577 Loss 4.137661, Accuracy 91.549%\n",
      "Epoch 29, Batch 252, LR 0.805452 Loss 4.137177, Accuracy 91.561%\n",
      "Epoch 29, Batch 253, LR 0.805327 Loss 4.138846, Accuracy 91.542%\n",
      "Epoch 29, Batch 254, LR 0.805202 Loss 4.138335, Accuracy 91.539%\n",
      "Epoch 29, Batch 255, LR 0.805076 Loss 4.139617, Accuracy 91.538%\n",
      "Epoch 29, Batch 256, LR 0.804951 Loss 4.142106, Accuracy 91.519%\n",
      "Epoch 29, Batch 257, LR 0.804826 Loss 4.142548, Accuracy 91.522%\n",
      "Epoch 29, Batch 258, LR 0.804701 Loss 4.142341, Accuracy 91.521%\n",
      "Epoch 29, Batch 259, LR 0.804576 Loss 4.141751, Accuracy 91.524%\n",
      "Epoch 29, Batch 260, LR 0.804451 Loss 4.140693, Accuracy 91.535%\n",
      "Epoch 29, Batch 261, LR 0.804325 Loss 4.139571, Accuracy 91.544%\n",
      "Epoch 29, Batch 262, LR 0.804200 Loss 4.141461, Accuracy 91.537%\n",
      "Epoch 29, Batch 263, LR 0.804075 Loss 4.146099, Accuracy 91.522%\n",
      "Epoch 29, Batch 264, LR 0.803950 Loss 4.149180, Accuracy 91.504%\n",
      "Epoch 29, Batch 265, LR 0.803825 Loss 4.149946, Accuracy 91.506%\n",
      "Epoch 29, Batch 266, LR 0.803700 Loss 4.152776, Accuracy 91.503%\n",
      "Epoch 29, Batch 267, LR 0.803575 Loss 4.154417, Accuracy 91.494%\n",
      "Epoch 29, Batch 268, LR 0.803450 Loss 4.152191, Accuracy 91.505%\n",
      "Epoch 29, Batch 269, LR 0.803324 Loss 4.152965, Accuracy 91.499%\n",
      "Epoch 29, Batch 270, LR 0.803199 Loss 4.153811, Accuracy 91.502%\n",
      "Epoch 29, Batch 271, LR 0.803074 Loss 4.154556, Accuracy 91.507%\n",
      "Epoch 29, Batch 272, LR 0.802949 Loss 4.154366, Accuracy 91.504%\n",
      "Epoch 29, Batch 273, LR 0.802824 Loss 4.156843, Accuracy 91.484%\n",
      "Epoch 29, Batch 274, LR 0.802699 Loss 4.156050, Accuracy 91.489%\n",
      "Epoch 29, Batch 275, LR 0.802574 Loss 4.159664, Accuracy 91.483%\n",
      "Epoch 29, Batch 276, LR 0.802449 Loss 4.160434, Accuracy 91.486%\n",
      "Epoch 29, Batch 277, LR 0.802324 Loss 4.160421, Accuracy 91.482%\n",
      "Epoch 29, Batch 278, LR 0.802199 Loss 4.159623, Accuracy 91.491%\n",
      "Epoch 29, Batch 279, LR 0.802074 Loss 4.160960, Accuracy 91.479%\n",
      "Epoch 29, Batch 280, LR 0.801949 Loss 4.160035, Accuracy 91.476%\n",
      "Epoch 29, Batch 281, LR 0.801823 Loss 4.161026, Accuracy 91.465%\n",
      "Epoch 29, Batch 282, LR 0.801698 Loss 4.160956, Accuracy 91.462%\n",
      "Epoch 29, Batch 283, LR 0.801573 Loss 4.162475, Accuracy 91.453%\n",
      "Epoch 29, Batch 284, LR 0.801448 Loss 4.164055, Accuracy 91.439%\n",
      "Epoch 29, Batch 285, LR 0.801323 Loss 4.163677, Accuracy 91.447%\n",
      "Epoch 29, Batch 286, LR 0.801198 Loss 4.164756, Accuracy 91.455%\n",
      "Epoch 29, Batch 287, LR 0.801073 Loss 4.164126, Accuracy 91.447%\n",
      "Epoch 29, Batch 288, LR 0.800948 Loss 4.163766, Accuracy 91.461%\n",
      "Epoch 29, Batch 289, LR 0.800823 Loss 4.162898, Accuracy 91.458%\n",
      "Epoch 29, Batch 290, LR 0.800698 Loss 4.163823, Accuracy 91.447%\n",
      "Epoch 29, Batch 291, LR 0.800573 Loss 4.164852, Accuracy 91.441%\n",
      "Epoch 29, Batch 292, LR 0.800448 Loss 4.166727, Accuracy 91.430%\n",
      "Epoch 29, Batch 293, LR 0.800323 Loss 4.164792, Accuracy 91.446%\n",
      "Epoch 29, Batch 294, LR 0.800198 Loss 4.165011, Accuracy 91.441%\n",
      "Epoch 29, Batch 295, LR 0.800073 Loss 4.163130, Accuracy 91.446%\n",
      "Epoch 29, Batch 296, LR 0.799948 Loss 4.163123, Accuracy 91.448%\n",
      "Epoch 29, Batch 297, LR 0.799823 Loss 4.163286, Accuracy 91.440%\n",
      "Epoch 29, Batch 298, LR 0.799698 Loss 4.164926, Accuracy 91.430%\n",
      "Epoch 29, Batch 299, LR 0.799573 Loss 4.165167, Accuracy 91.435%\n",
      "Epoch 29, Batch 300, LR 0.799449 Loss 4.163935, Accuracy 91.445%\n",
      "Epoch 29, Batch 301, LR 0.799324 Loss 4.163070, Accuracy 91.448%\n",
      "Epoch 29, Batch 302, LR 0.799199 Loss 4.162836, Accuracy 91.437%\n",
      "Epoch 29, Batch 303, LR 0.799074 Loss 4.160555, Accuracy 91.442%\n",
      "Epoch 29, Batch 304, LR 0.798949 Loss 4.162298, Accuracy 91.440%\n",
      "Epoch 29, Batch 305, LR 0.798824 Loss 4.162050, Accuracy 91.437%\n",
      "Epoch 29, Batch 306, LR 0.798699 Loss 4.161801, Accuracy 91.439%\n",
      "Epoch 29, Batch 307, LR 0.798574 Loss 4.162800, Accuracy 91.429%\n",
      "Epoch 29, Batch 308, LR 0.798449 Loss 4.161496, Accuracy 91.442%\n",
      "Epoch 29, Batch 309, LR 0.798324 Loss 4.160941, Accuracy 91.452%\n",
      "Epoch 29, Batch 310, LR 0.798199 Loss 4.160253, Accuracy 91.444%\n",
      "Epoch 29, Batch 311, LR 0.798074 Loss 4.160289, Accuracy 91.444%\n",
      "Epoch 29, Batch 312, LR 0.797949 Loss 4.161071, Accuracy 91.446%\n",
      "Epoch 29, Batch 313, LR 0.797825 Loss 4.162615, Accuracy 91.446%\n",
      "Epoch 29, Batch 314, LR 0.797700 Loss 4.161236, Accuracy 91.454%\n",
      "Epoch 29, Batch 315, LR 0.797575 Loss 4.163236, Accuracy 91.434%\n",
      "Epoch 29, Batch 316, LR 0.797450 Loss 4.164615, Accuracy 91.429%\n",
      "Epoch 29, Batch 317, LR 0.797325 Loss 4.164300, Accuracy 91.421%\n",
      "Epoch 29, Batch 318, LR 0.797200 Loss 4.164870, Accuracy 91.421%\n",
      "Epoch 29, Batch 319, LR 0.797075 Loss 4.165812, Accuracy 91.423%\n",
      "Epoch 29, Batch 320, LR 0.796951 Loss 4.166222, Accuracy 91.426%\n",
      "Epoch 29, Batch 321, LR 0.796826 Loss 4.166619, Accuracy 91.416%\n",
      "Epoch 29, Batch 322, LR 0.796701 Loss 4.167941, Accuracy 91.416%\n",
      "Epoch 29, Batch 323, LR 0.796576 Loss 4.168180, Accuracy 91.416%\n",
      "Epoch 29, Batch 324, LR 0.796451 Loss 4.167782, Accuracy 91.423%\n",
      "Epoch 29, Batch 325, LR 0.796326 Loss 4.166411, Accuracy 91.430%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 326, LR 0.796202 Loss 4.166043, Accuracy 91.437%\n",
      "Epoch 29, Batch 327, LR 0.796077 Loss 4.165974, Accuracy 91.433%\n",
      "Epoch 29, Batch 328, LR 0.795952 Loss 4.165684, Accuracy 91.425%\n",
      "Epoch 29, Batch 329, LR 0.795827 Loss 4.166637, Accuracy 91.425%\n",
      "Epoch 29, Batch 330, LR 0.795702 Loss 4.167580, Accuracy 91.416%\n",
      "Epoch 29, Batch 331, LR 0.795578 Loss 4.165532, Accuracy 91.423%\n",
      "Epoch 29, Batch 332, LR 0.795453 Loss 4.165330, Accuracy 91.427%\n",
      "Epoch 29, Batch 333, LR 0.795328 Loss 4.165924, Accuracy 91.420%\n",
      "Epoch 29, Batch 334, LR 0.795203 Loss 4.167552, Accuracy 91.416%\n",
      "Epoch 29, Batch 335, LR 0.795078 Loss 4.166389, Accuracy 91.416%\n",
      "Epoch 29, Batch 336, LR 0.794954 Loss 4.165211, Accuracy 91.425%\n",
      "Epoch 29, Batch 337, LR 0.794829 Loss 4.167094, Accuracy 91.411%\n",
      "Epoch 29, Batch 338, LR 0.794704 Loss 4.166024, Accuracy 91.418%\n",
      "Epoch 29, Batch 339, LR 0.794579 Loss 4.165676, Accuracy 91.427%\n",
      "Epoch 29, Batch 340, LR 0.794455 Loss 4.166412, Accuracy 91.425%\n",
      "Epoch 29, Batch 341, LR 0.794330 Loss 4.167855, Accuracy 91.404%\n",
      "Epoch 29, Batch 342, LR 0.794205 Loss 4.170620, Accuracy 91.395%\n",
      "Epoch 29, Batch 343, LR 0.794081 Loss 4.170362, Accuracy 91.388%\n",
      "Epoch 29, Batch 344, LR 0.793956 Loss 4.171425, Accuracy 91.381%\n",
      "Epoch 29, Batch 345, LR 0.793831 Loss 4.170212, Accuracy 91.384%\n",
      "Epoch 29, Batch 346, LR 0.793706 Loss 4.171395, Accuracy 91.388%\n",
      "Epoch 29, Batch 347, LR 0.793582 Loss 4.171321, Accuracy 91.390%\n",
      "Epoch 29, Batch 348, LR 0.793457 Loss 4.171965, Accuracy 91.395%\n",
      "Epoch 29, Batch 349, LR 0.793332 Loss 4.173214, Accuracy 91.393%\n",
      "Epoch 29, Batch 350, LR 0.793208 Loss 4.173410, Accuracy 91.395%\n",
      "Epoch 29, Batch 351, LR 0.793083 Loss 4.172996, Accuracy 91.393%\n",
      "Epoch 29, Batch 352, LR 0.792958 Loss 4.172385, Accuracy 91.400%\n",
      "Epoch 29, Batch 353, LR 0.792834 Loss 4.171613, Accuracy 91.402%\n",
      "Epoch 29, Batch 354, LR 0.792709 Loss 4.171930, Accuracy 91.393%\n",
      "Epoch 29, Batch 355, LR 0.792584 Loss 4.172171, Accuracy 91.386%\n",
      "Epoch 29, Batch 356, LR 0.792460 Loss 4.172516, Accuracy 91.386%\n",
      "Epoch 29, Batch 357, LR 0.792335 Loss 4.172449, Accuracy 91.384%\n",
      "Epoch 29, Batch 358, LR 0.792210 Loss 4.173133, Accuracy 91.391%\n",
      "Epoch 29, Batch 359, LR 0.792086 Loss 4.174604, Accuracy 91.378%\n",
      "Epoch 29, Batch 360, LR 0.791961 Loss 4.174104, Accuracy 91.391%\n",
      "Epoch 29, Batch 361, LR 0.791836 Loss 4.174844, Accuracy 91.391%\n",
      "Epoch 29, Batch 362, LR 0.791712 Loss 4.174457, Accuracy 91.395%\n",
      "Epoch 29, Batch 363, LR 0.791587 Loss 4.174216, Accuracy 91.391%\n",
      "Epoch 29, Batch 364, LR 0.791462 Loss 4.173749, Accuracy 91.391%\n",
      "Epoch 29, Batch 365, LR 0.791338 Loss 4.171997, Accuracy 91.391%\n",
      "Epoch 29, Batch 366, LR 0.791213 Loss 4.172494, Accuracy 91.396%\n",
      "Epoch 29, Batch 367, LR 0.791089 Loss 4.172570, Accuracy 91.391%\n",
      "Epoch 29, Batch 368, LR 0.790964 Loss 4.172262, Accuracy 91.385%\n",
      "Epoch 29, Batch 369, LR 0.790839 Loss 4.173575, Accuracy 91.385%\n",
      "Epoch 29, Batch 370, LR 0.790715 Loss 4.170142, Accuracy 91.404%\n",
      "Epoch 29, Batch 371, LR 0.790590 Loss 4.171617, Accuracy 91.394%\n",
      "Epoch 29, Batch 372, LR 0.790466 Loss 4.172261, Accuracy 91.402%\n",
      "Epoch 29, Batch 373, LR 0.790341 Loss 4.173229, Accuracy 91.396%\n",
      "Epoch 29, Batch 374, LR 0.790217 Loss 4.174389, Accuracy 91.383%\n",
      "Epoch 29, Batch 375, LR 0.790092 Loss 4.176255, Accuracy 91.373%\n",
      "Epoch 29, Batch 376, LR 0.789967 Loss 4.176453, Accuracy 91.379%\n",
      "Epoch 29, Batch 377, LR 0.789843 Loss 4.176244, Accuracy 91.388%\n",
      "Epoch 29, Batch 378, LR 0.789718 Loss 4.176481, Accuracy 91.379%\n",
      "Epoch 29, Batch 379, LR 0.789594 Loss 4.176534, Accuracy 91.384%\n",
      "Epoch 29, Batch 380, LR 0.789469 Loss 4.178964, Accuracy 91.365%\n",
      "Epoch 29, Batch 381, LR 0.789345 Loss 4.180247, Accuracy 91.365%\n",
      "Epoch 29, Batch 382, LR 0.789220 Loss 4.179737, Accuracy 91.361%\n",
      "Epoch 29, Batch 383, LR 0.789096 Loss 4.180081, Accuracy 91.367%\n",
      "Epoch 29, Batch 384, LR 0.788971 Loss 4.181000, Accuracy 91.368%\n",
      "Epoch 29, Batch 385, LR 0.788847 Loss 4.180841, Accuracy 91.372%\n",
      "Epoch 29, Batch 386, LR 0.788722 Loss 4.180958, Accuracy 91.368%\n",
      "Epoch 29, Batch 387, LR 0.788598 Loss 4.180181, Accuracy 91.378%\n",
      "Epoch 29, Batch 388, LR 0.788473 Loss 4.179330, Accuracy 91.376%\n",
      "Epoch 29, Batch 389, LR 0.788349 Loss 4.178099, Accuracy 91.386%\n",
      "Epoch 29, Batch 390, LR 0.788224 Loss 4.178842, Accuracy 91.386%\n",
      "Epoch 29, Batch 391, LR 0.788100 Loss 4.179862, Accuracy 91.382%\n",
      "Epoch 29, Batch 392, LR 0.787975 Loss 4.180578, Accuracy 91.376%\n",
      "Epoch 29, Batch 393, LR 0.787851 Loss 4.182010, Accuracy 91.372%\n",
      "Epoch 29, Batch 394, LR 0.787726 Loss 4.181040, Accuracy 91.375%\n",
      "Epoch 29, Batch 395, LR 0.787602 Loss 4.182042, Accuracy 91.373%\n",
      "Epoch 29, Batch 396, LR 0.787477 Loss 4.179807, Accuracy 91.381%\n",
      "Epoch 29, Batch 397, LR 0.787353 Loss 4.179809, Accuracy 91.377%\n",
      "Epoch 29, Batch 398, LR 0.787229 Loss 4.178041, Accuracy 91.383%\n",
      "Epoch 29, Batch 399, LR 0.787104 Loss 4.177088, Accuracy 91.387%\n",
      "Epoch 29, Batch 400, LR 0.786980 Loss 4.177219, Accuracy 91.385%\n",
      "Epoch 29, Batch 401, LR 0.786855 Loss 4.178086, Accuracy 91.371%\n",
      "Epoch 29, Batch 402, LR 0.786731 Loss 4.177622, Accuracy 91.377%\n",
      "Epoch 29, Batch 403, LR 0.786606 Loss 4.178626, Accuracy 91.366%\n",
      "Epoch 29, Batch 404, LR 0.786482 Loss 4.178649, Accuracy 91.368%\n",
      "Epoch 29, Batch 405, LR 0.786358 Loss 4.180042, Accuracy 91.348%\n",
      "Epoch 29, Batch 406, LR 0.786233 Loss 4.181421, Accuracy 91.341%\n",
      "Epoch 29, Batch 407, LR 0.786109 Loss 4.182010, Accuracy 91.335%\n",
      "Epoch 29, Batch 408, LR 0.785985 Loss 4.181669, Accuracy 91.333%\n",
      "Epoch 29, Batch 409, LR 0.785860 Loss 4.181261, Accuracy 91.334%\n",
      "Epoch 29, Batch 410, LR 0.785736 Loss 4.179659, Accuracy 91.341%\n",
      "Epoch 29, Batch 411, LR 0.785611 Loss 4.181581, Accuracy 91.334%\n",
      "Epoch 29, Batch 412, LR 0.785487 Loss 4.181963, Accuracy 91.336%\n",
      "Epoch 29, Batch 413, LR 0.785363 Loss 4.181912, Accuracy 91.331%\n",
      "Epoch 29, Batch 414, LR 0.785238 Loss 4.181187, Accuracy 91.329%\n",
      "Epoch 29, Batch 415, LR 0.785114 Loss 4.181196, Accuracy 91.331%\n",
      "Epoch 29, Batch 416, LR 0.784990 Loss 4.179383, Accuracy 91.342%\n",
      "Epoch 29, Batch 417, LR 0.784865 Loss 4.178178, Accuracy 91.350%\n",
      "Epoch 29, Batch 418, LR 0.784741 Loss 4.177656, Accuracy 91.350%\n",
      "Epoch 29, Batch 419, LR 0.784617 Loss 4.177627, Accuracy 91.341%\n",
      "Epoch 29, Batch 420, LR 0.784492 Loss 4.179339, Accuracy 91.334%\n",
      "Epoch 29, Batch 421, LR 0.784368 Loss 4.180162, Accuracy 91.330%\n",
      "Epoch 29, Batch 422, LR 0.784244 Loss 4.180056, Accuracy 91.332%\n",
      "Epoch 29, Batch 423, LR 0.784119 Loss 4.179650, Accuracy 91.332%\n",
      "Epoch 29, Batch 424, LR 0.783995 Loss 4.179531, Accuracy 91.334%\n",
      "Epoch 29, Batch 425, LR 0.783871 Loss 4.178161, Accuracy 91.338%\n",
      "Epoch 29, Batch 426, LR 0.783747 Loss 4.177791, Accuracy 91.338%\n",
      "Epoch 29, Batch 427, LR 0.783622 Loss 4.178059, Accuracy 91.342%\n",
      "Epoch 29, Batch 428, LR 0.783498 Loss 4.177149, Accuracy 91.353%\n",
      "Epoch 29, Batch 429, LR 0.783374 Loss 4.177756, Accuracy 91.346%\n",
      "Epoch 29, Batch 430, LR 0.783249 Loss 4.179169, Accuracy 91.337%\n",
      "Epoch 29, Batch 431, LR 0.783125 Loss 4.179450, Accuracy 91.341%\n",
      "Epoch 29, Batch 432, LR 0.783001 Loss 4.178808, Accuracy 91.347%\n",
      "Epoch 29, Batch 433, LR 0.782877 Loss 4.178911, Accuracy 91.338%\n",
      "Epoch 29, Batch 434, LR 0.782752 Loss 4.179155, Accuracy 91.329%\n",
      "Epoch 29, Batch 435, LR 0.782628 Loss 4.180302, Accuracy 91.320%\n",
      "Epoch 29, Batch 436, LR 0.782504 Loss 4.180568, Accuracy 91.324%\n",
      "Epoch 29, Batch 437, LR 0.782380 Loss 4.180228, Accuracy 91.328%\n",
      "Epoch 29, Batch 438, LR 0.782255 Loss 4.180702, Accuracy 91.326%\n",
      "Epoch 29, Batch 439, LR 0.782131 Loss 4.179825, Accuracy 91.326%\n",
      "Epoch 29, Batch 440, LR 0.782007 Loss 4.182328, Accuracy 91.314%\n",
      "Epoch 29, Batch 441, LR 0.781883 Loss 4.181990, Accuracy 91.312%\n",
      "Epoch 29, Batch 442, LR 0.781759 Loss 4.182277, Accuracy 91.307%\n",
      "Epoch 29, Batch 443, LR 0.781634 Loss 4.183974, Accuracy 91.304%\n",
      "Epoch 29, Batch 444, LR 0.781510 Loss 4.184814, Accuracy 91.302%\n",
      "Epoch 29, Batch 445, LR 0.781386 Loss 4.184991, Accuracy 91.306%\n",
      "Epoch 29, Batch 446, LR 0.781262 Loss 4.186743, Accuracy 91.303%\n",
      "Epoch 29, Batch 447, LR 0.781138 Loss 4.186202, Accuracy 91.301%\n",
      "Epoch 29, Batch 448, LR 0.781014 Loss 4.185932, Accuracy 91.295%\n",
      "Epoch 29, Batch 449, LR 0.780889 Loss 4.186281, Accuracy 91.295%\n",
      "Epoch 29, Batch 450, LR 0.780765 Loss 4.186659, Accuracy 91.285%\n",
      "Epoch 29, Batch 451, LR 0.780641 Loss 4.187950, Accuracy 91.285%\n",
      "Epoch 29, Batch 452, LR 0.780517 Loss 4.187918, Accuracy 91.285%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 453, LR 0.780393 Loss 4.188214, Accuracy 91.284%\n",
      "Epoch 29, Batch 454, LR 0.780269 Loss 4.188030, Accuracy 91.289%\n",
      "Epoch 29, Batch 455, LR 0.780144 Loss 4.189037, Accuracy 91.286%\n",
      "Epoch 29, Batch 456, LR 0.780020 Loss 4.188297, Accuracy 91.290%\n",
      "Epoch 29, Batch 457, LR 0.779896 Loss 4.186411, Accuracy 91.304%\n",
      "Epoch 29, Batch 458, LR 0.779772 Loss 4.185912, Accuracy 91.311%\n",
      "Epoch 29, Batch 459, LR 0.779648 Loss 4.185413, Accuracy 91.314%\n",
      "Epoch 29, Batch 460, LR 0.779524 Loss 4.185429, Accuracy 91.315%\n",
      "Epoch 29, Batch 461, LR 0.779400 Loss 4.185360, Accuracy 91.316%\n",
      "Epoch 29, Batch 462, LR 0.779276 Loss 4.185559, Accuracy 91.313%\n",
      "Epoch 29, Batch 463, LR 0.779152 Loss 4.186234, Accuracy 91.312%\n",
      "Epoch 29, Batch 464, LR 0.779028 Loss 4.186820, Accuracy 91.305%\n",
      "Epoch 29, Batch 465, LR 0.778903 Loss 4.186034, Accuracy 91.304%\n",
      "Epoch 29, Batch 466, LR 0.778779 Loss 4.187219, Accuracy 91.304%\n",
      "Epoch 29, Batch 467, LR 0.778655 Loss 4.188368, Accuracy 91.292%\n",
      "Epoch 29, Batch 468, LR 0.778531 Loss 4.188679, Accuracy 91.291%\n",
      "Epoch 29, Batch 469, LR 0.778407 Loss 4.187857, Accuracy 91.300%\n",
      "Epoch 29, Batch 470, LR 0.778283 Loss 4.189927, Accuracy 91.290%\n",
      "Epoch 29, Batch 471, LR 0.778159 Loss 4.190921, Accuracy 91.285%\n",
      "Epoch 29, Batch 472, LR 0.778035 Loss 4.189504, Accuracy 91.294%\n",
      "Epoch 29, Batch 473, LR 0.777911 Loss 4.189627, Accuracy 91.284%\n",
      "Epoch 29, Batch 474, LR 0.777787 Loss 4.188880, Accuracy 91.278%\n",
      "Epoch 29, Batch 475, LR 0.777663 Loss 4.189584, Accuracy 91.270%\n",
      "Epoch 29, Batch 476, LR 0.777539 Loss 4.189252, Accuracy 91.268%\n",
      "Epoch 29, Batch 477, LR 0.777415 Loss 4.190026, Accuracy 91.267%\n",
      "Epoch 29, Batch 478, LR 0.777291 Loss 4.189226, Accuracy 91.271%\n",
      "Epoch 29, Batch 479, LR 0.777167 Loss 4.187632, Accuracy 91.276%\n",
      "Epoch 29, Batch 480, LR 0.777043 Loss 4.188821, Accuracy 91.270%\n",
      "Epoch 29, Batch 481, LR 0.776919 Loss 4.189643, Accuracy 91.265%\n",
      "Epoch 29, Batch 482, LR 0.776795 Loss 4.189519, Accuracy 91.265%\n",
      "Epoch 29, Batch 483, LR 0.776671 Loss 4.188992, Accuracy 91.272%\n",
      "Epoch 29, Batch 484, LR 0.776547 Loss 4.188200, Accuracy 91.274%\n",
      "Epoch 29, Batch 485, LR 0.776423 Loss 4.189184, Accuracy 91.269%\n",
      "Epoch 29, Batch 486, LR 0.776299 Loss 4.188389, Accuracy 91.270%\n",
      "Epoch 29, Batch 487, LR 0.776175 Loss 4.187979, Accuracy 91.278%\n",
      "Epoch 29, Batch 488, LR 0.776051 Loss 4.187462, Accuracy 91.275%\n",
      "Epoch 29, Batch 489, LR 0.775927 Loss 4.188068, Accuracy 91.274%\n",
      "Epoch 29, Batch 490, LR 0.775803 Loss 4.188018, Accuracy 91.279%\n",
      "Epoch 29, Batch 491, LR 0.775679 Loss 4.186991, Accuracy 91.287%\n",
      "Epoch 29, Batch 492, LR 0.775555 Loss 4.188000, Accuracy 91.286%\n",
      "Epoch 29, Batch 493, LR 0.775431 Loss 4.188600, Accuracy 91.279%\n",
      "Epoch 29, Batch 494, LR 0.775308 Loss 4.188454, Accuracy 91.280%\n",
      "Epoch 29, Batch 495, LR 0.775184 Loss 4.188911, Accuracy 91.285%\n",
      "Epoch 29, Batch 496, LR 0.775060 Loss 4.189137, Accuracy 91.280%\n",
      "Epoch 29, Batch 497, LR 0.774936 Loss 4.189661, Accuracy 91.280%\n",
      "Epoch 29, Batch 498, LR 0.774812 Loss 4.189602, Accuracy 91.278%\n",
      "Epoch 29, Batch 499, LR 0.774688 Loss 4.189326, Accuracy 91.276%\n",
      "Epoch 29, Batch 500, LR 0.774564 Loss 4.190098, Accuracy 91.275%\n",
      "Epoch 29, Batch 501, LR 0.774440 Loss 4.189791, Accuracy 91.275%\n",
      "Epoch 29, Batch 502, LR 0.774316 Loss 4.189443, Accuracy 91.271%\n",
      "Epoch 29, Batch 503, LR 0.774193 Loss 4.188330, Accuracy 91.274%\n",
      "Epoch 29, Batch 504, LR 0.774069 Loss 4.187223, Accuracy 91.279%\n",
      "Epoch 29, Batch 505, LR 0.773945 Loss 4.188043, Accuracy 91.273%\n",
      "Epoch 29, Batch 506, LR 0.773821 Loss 4.188350, Accuracy 91.272%\n",
      "Epoch 29, Batch 507, LR 0.773697 Loss 4.188580, Accuracy 91.272%\n",
      "Epoch 29, Batch 508, LR 0.773573 Loss 4.187610, Accuracy 91.274%\n",
      "Epoch 29, Batch 509, LR 0.773449 Loss 4.187351, Accuracy 91.270%\n",
      "Epoch 29, Batch 510, LR 0.773326 Loss 4.187594, Accuracy 91.259%\n",
      "Epoch 29, Batch 511, LR 0.773202 Loss 4.186690, Accuracy 91.258%\n",
      "Epoch 29, Batch 512, LR 0.773078 Loss 4.188518, Accuracy 91.249%\n",
      "Epoch 29, Batch 513, LR 0.772954 Loss 4.190292, Accuracy 91.245%\n",
      "Epoch 29, Batch 514, LR 0.772830 Loss 4.189643, Accuracy 91.250%\n",
      "Epoch 29, Batch 515, LR 0.772706 Loss 4.190001, Accuracy 91.253%\n",
      "Epoch 29, Batch 516, LR 0.772583 Loss 4.191802, Accuracy 91.247%\n",
      "Epoch 29, Batch 517, LR 0.772459 Loss 4.192041, Accuracy 91.245%\n",
      "Epoch 29, Batch 518, LR 0.772335 Loss 4.192058, Accuracy 91.239%\n",
      "Epoch 29, Batch 519, LR 0.772211 Loss 4.191790, Accuracy 91.241%\n",
      "Epoch 29, Batch 520, LR 0.772088 Loss 4.192798, Accuracy 91.235%\n",
      "Epoch 29, Batch 521, LR 0.771964 Loss 4.194596, Accuracy 91.225%\n",
      "Epoch 29, Batch 522, LR 0.771840 Loss 4.194633, Accuracy 91.219%\n",
      "Epoch 29, Batch 523, LR 0.771716 Loss 4.194088, Accuracy 91.218%\n",
      "Epoch 29, Batch 524, LR 0.771592 Loss 4.193964, Accuracy 91.217%\n",
      "Epoch 29, Batch 525, LR 0.771469 Loss 4.192965, Accuracy 91.219%\n",
      "Epoch 29, Batch 526, LR 0.771345 Loss 4.192941, Accuracy 91.219%\n",
      "Epoch 29, Batch 527, LR 0.771221 Loss 4.192957, Accuracy 91.215%\n",
      "Epoch 29, Batch 528, LR 0.771098 Loss 4.193274, Accuracy 91.209%\n",
      "Epoch 29, Batch 529, LR 0.770974 Loss 4.192469, Accuracy 91.217%\n",
      "Epoch 29, Batch 530, LR 0.770850 Loss 4.193218, Accuracy 91.215%\n",
      "Epoch 29, Batch 531, LR 0.770726 Loss 4.193554, Accuracy 91.216%\n",
      "Epoch 29, Batch 532, LR 0.770603 Loss 4.194094, Accuracy 91.215%\n",
      "Epoch 29, Batch 533, LR 0.770479 Loss 4.194425, Accuracy 91.220%\n",
      "Epoch 29, Batch 534, LR 0.770355 Loss 4.195071, Accuracy 91.213%\n",
      "Epoch 29, Batch 535, LR 0.770232 Loss 4.193102, Accuracy 91.222%\n",
      "Epoch 29, Batch 536, LR 0.770108 Loss 4.193422, Accuracy 91.227%\n",
      "Epoch 29, Batch 537, LR 0.769984 Loss 4.193411, Accuracy 91.233%\n",
      "Epoch 29, Batch 538, LR 0.769860 Loss 4.193468, Accuracy 91.236%\n",
      "Epoch 29, Batch 539, LR 0.769737 Loss 4.194206, Accuracy 91.229%\n",
      "Epoch 29, Batch 540, LR 0.769613 Loss 4.194011, Accuracy 91.225%\n",
      "Epoch 29, Batch 541, LR 0.769489 Loss 4.193019, Accuracy 91.230%\n",
      "Epoch 29, Batch 542, LR 0.769366 Loss 4.192974, Accuracy 91.229%\n",
      "Epoch 29, Batch 543, LR 0.769242 Loss 4.192464, Accuracy 91.232%\n",
      "Epoch 29, Batch 544, LR 0.769118 Loss 4.191918, Accuracy 91.237%\n",
      "Epoch 29, Batch 545, LR 0.768995 Loss 4.192057, Accuracy 91.239%\n",
      "Epoch 29, Batch 546, LR 0.768871 Loss 4.192324, Accuracy 91.237%\n",
      "Epoch 29, Batch 547, LR 0.768748 Loss 4.191941, Accuracy 91.243%\n",
      "Epoch 29, Batch 548, LR 0.768624 Loss 4.192171, Accuracy 91.245%\n",
      "Epoch 29, Batch 549, LR 0.768500 Loss 4.194470, Accuracy 91.233%\n",
      "Epoch 29, Batch 550, LR 0.768377 Loss 4.194123, Accuracy 91.236%\n",
      "Epoch 29, Batch 551, LR 0.768253 Loss 4.193266, Accuracy 91.242%\n",
      "Epoch 29, Batch 552, LR 0.768130 Loss 4.193901, Accuracy 91.238%\n",
      "Epoch 29, Batch 553, LR 0.768006 Loss 4.194049, Accuracy 91.241%\n",
      "Epoch 29, Batch 554, LR 0.767882 Loss 4.194805, Accuracy 91.238%\n",
      "Epoch 29, Batch 555, LR 0.767759 Loss 4.194135, Accuracy 91.242%\n",
      "Epoch 29, Batch 556, LR 0.767635 Loss 4.194495, Accuracy 91.233%\n",
      "Epoch 29, Batch 557, LR 0.767512 Loss 4.192846, Accuracy 91.241%\n",
      "Epoch 29, Batch 558, LR 0.767388 Loss 4.192203, Accuracy 91.247%\n",
      "Epoch 29, Batch 559, LR 0.767264 Loss 4.192460, Accuracy 91.243%\n",
      "Epoch 29, Batch 560, LR 0.767141 Loss 4.192108, Accuracy 91.243%\n",
      "Epoch 29, Batch 561, LR 0.767017 Loss 4.191215, Accuracy 91.247%\n",
      "Epoch 29, Batch 562, LR 0.766894 Loss 4.190102, Accuracy 91.251%\n",
      "Epoch 29, Batch 563, LR 0.766770 Loss 4.189696, Accuracy 91.245%\n",
      "Epoch 29, Batch 564, LR 0.766647 Loss 4.189124, Accuracy 91.247%\n",
      "Epoch 29, Batch 565, LR 0.766523 Loss 4.189533, Accuracy 91.238%\n",
      "Epoch 29, Batch 566, LR 0.766400 Loss 4.188901, Accuracy 91.239%\n",
      "Epoch 29, Batch 567, LR 0.766276 Loss 4.189140, Accuracy 91.238%\n",
      "Epoch 29, Batch 568, LR 0.766153 Loss 4.190380, Accuracy 91.232%\n",
      "Epoch 29, Batch 569, LR 0.766029 Loss 4.190347, Accuracy 91.232%\n",
      "Epoch 29, Batch 570, LR 0.765906 Loss 4.189224, Accuracy 91.239%\n",
      "Epoch 29, Batch 571, LR 0.765782 Loss 4.189705, Accuracy 91.237%\n",
      "Epoch 29, Batch 572, LR 0.765659 Loss 4.189587, Accuracy 91.234%\n",
      "Epoch 29, Batch 573, LR 0.765535 Loss 4.188118, Accuracy 91.240%\n",
      "Epoch 29, Batch 574, LR 0.765412 Loss 4.189254, Accuracy 91.237%\n",
      "Epoch 29, Batch 575, LR 0.765288 Loss 4.188841, Accuracy 91.238%\n",
      "Epoch 29, Batch 576, LR 0.765165 Loss 4.188511, Accuracy 91.239%\n",
      "Epoch 29, Batch 577, LR 0.765041 Loss 4.188345, Accuracy 91.241%\n",
      "Epoch 29, Batch 578, LR 0.764918 Loss 4.187863, Accuracy 91.240%\n",
      "Epoch 29, Batch 579, LR 0.764794 Loss 4.188329, Accuracy 91.240%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 580, LR 0.764671 Loss 4.188133, Accuracy 91.241%\n",
      "Epoch 29, Batch 581, LR 0.764547 Loss 4.188434, Accuracy 91.240%\n",
      "Epoch 29, Batch 582, LR 0.764424 Loss 4.188151, Accuracy 91.241%\n",
      "Epoch 29, Batch 583, LR 0.764301 Loss 4.187059, Accuracy 91.241%\n",
      "Epoch 29, Batch 584, LR 0.764177 Loss 4.187637, Accuracy 91.238%\n",
      "Epoch 29, Batch 585, LR 0.764054 Loss 4.187493, Accuracy 91.241%\n",
      "Epoch 29, Batch 586, LR 0.763930 Loss 4.187005, Accuracy 91.240%\n",
      "Epoch 29, Batch 587, LR 0.763807 Loss 4.187834, Accuracy 91.236%\n",
      "Epoch 29, Batch 588, LR 0.763684 Loss 4.187816, Accuracy 91.244%\n",
      "Epoch 29, Batch 589, LR 0.763560 Loss 4.188878, Accuracy 91.247%\n",
      "Epoch 29, Batch 590, LR 0.763437 Loss 4.188202, Accuracy 91.247%\n",
      "Epoch 29, Batch 591, LR 0.763313 Loss 4.187736, Accuracy 91.248%\n",
      "Epoch 29, Batch 592, LR 0.763190 Loss 4.187110, Accuracy 91.249%\n",
      "Epoch 29, Batch 593, LR 0.763067 Loss 4.187010, Accuracy 91.249%\n",
      "Epoch 29, Batch 594, LR 0.762943 Loss 4.187166, Accuracy 91.248%\n",
      "Epoch 29, Batch 595, LR 0.762820 Loss 4.187799, Accuracy 91.245%\n",
      "Epoch 29, Batch 596, LR 0.762697 Loss 4.188461, Accuracy 91.246%\n",
      "Epoch 29, Batch 597, LR 0.762573 Loss 4.188478, Accuracy 91.248%\n",
      "Epoch 29, Batch 598, LR 0.762450 Loss 4.188486, Accuracy 91.252%\n",
      "Epoch 29, Batch 599, LR 0.762326 Loss 4.187405, Accuracy 91.254%\n",
      "Epoch 29, Batch 600, LR 0.762203 Loss 4.187940, Accuracy 91.253%\n",
      "Epoch 29, Batch 601, LR 0.762080 Loss 4.188570, Accuracy 91.250%\n",
      "Epoch 29, Batch 602, LR 0.761956 Loss 4.188477, Accuracy 91.257%\n",
      "Epoch 29, Batch 603, LR 0.761833 Loss 4.187157, Accuracy 91.265%\n",
      "Epoch 29, Batch 604, LR 0.761710 Loss 4.186556, Accuracy 91.264%\n",
      "Epoch 29, Batch 605, LR 0.761587 Loss 4.186822, Accuracy 91.263%\n",
      "Epoch 29, Batch 606, LR 0.761463 Loss 4.187326, Accuracy 91.257%\n",
      "Epoch 29, Batch 607, LR 0.761340 Loss 4.187620, Accuracy 91.253%\n",
      "Epoch 29, Batch 608, LR 0.761217 Loss 4.187453, Accuracy 91.252%\n",
      "Epoch 29, Batch 609, LR 0.761093 Loss 4.187161, Accuracy 91.254%\n",
      "Epoch 29, Batch 610, LR 0.760970 Loss 4.186950, Accuracy 91.255%\n",
      "Epoch 29, Batch 611, LR 0.760847 Loss 4.187434, Accuracy 91.248%\n",
      "Epoch 29, Batch 612, LR 0.760724 Loss 4.186436, Accuracy 91.253%\n",
      "Epoch 29, Batch 613, LR 0.760600 Loss 4.186603, Accuracy 91.253%\n",
      "Epoch 29, Batch 614, LR 0.760477 Loss 4.186647, Accuracy 91.252%\n",
      "Epoch 29, Batch 615, LR 0.760354 Loss 4.186824, Accuracy 91.253%\n",
      "Epoch 29, Batch 616, LR 0.760231 Loss 4.186329, Accuracy 91.255%\n",
      "Epoch 29, Batch 617, LR 0.760107 Loss 4.187031, Accuracy 91.251%\n",
      "Epoch 29, Batch 618, LR 0.759984 Loss 4.187528, Accuracy 91.252%\n",
      "Epoch 29, Batch 619, LR 0.759861 Loss 4.188415, Accuracy 91.248%\n",
      "Epoch 29, Batch 620, LR 0.759738 Loss 4.188602, Accuracy 91.244%\n",
      "Epoch 29, Batch 621, LR 0.759614 Loss 4.188077, Accuracy 91.241%\n",
      "Epoch 29, Batch 622, LR 0.759491 Loss 4.187756, Accuracy 91.245%\n",
      "Epoch 29, Batch 623, LR 0.759368 Loss 4.187723, Accuracy 91.248%\n",
      "Epoch 29, Batch 624, LR 0.759245 Loss 4.187421, Accuracy 91.246%\n",
      "Epoch 29, Batch 625, LR 0.759122 Loss 4.188072, Accuracy 91.243%\n",
      "Epoch 29, Batch 626, LR 0.758998 Loss 4.188298, Accuracy 91.240%\n",
      "Epoch 29, Batch 627, LR 0.758875 Loss 4.188107, Accuracy 91.242%\n",
      "Epoch 29, Batch 628, LR 0.758752 Loss 4.187182, Accuracy 91.242%\n",
      "Epoch 29, Batch 629, LR 0.758629 Loss 4.187359, Accuracy 91.244%\n",
      "Epoch 29, Batch 630, LR 0.758506 Loss 4.187445, Accuracy 91.248%\n",
      "Epoch 29, Batch 631, LR 0.758383 Loss 4.188377, Accuracy 91.240%\n",
      "Epoch 29, Batch 632, LR 0.758259 Loss 4.189041, Accuracy 91.234%\n",
      "Epoch 29, Batch 633, LR 0.758136 Loss 4.188658, Accuracy 91.233%\n",
      "Epoch 29, Batch 634, LR 0.758013 Loss 4.189116, Accuracy 91.235%\n",
      "Epoch 29, Batch 635, LR 0.757890 Loss 4.189119, Accuracy 91.234%\n",
      "Epoch 29, Batch 636, LR 0.757767 Loss 4.188897, Accuracy 91.237%\n",
      "Epoch 29, Batch 637, LR 0.757644 Loss 4.187467, Accuracy 91.242%\n",
      "Epoch 29, Batch 638, LR 0.757521 Loss 4.187014, Accuracy 91.240%\n",
      "Epoch 29, Batch 639, LR 0.757397 Loss 4.187864, Accuracy 91.234%\n",
      "Epoch 29, Batch 640, LR 0.757274 Loss 4.187230, Accuracy 91.238%\n",
      "Epoch 29, Batch 641, LR 0.757151 Loss 4.186856, Accuracy 91.240%\n",
      "Epoch 29, Batch 642, LR 0.757028 Loss 4.187764, Accuracy 91.242%\n",
      "Epoch 29, Batch 643, LR 0.756905 Loss 4.188451, Accuracy 91.240%\n",
      "Epoch 29, Batch 644, LR 0.756782 Loss 4.188833, Accuracy 91.239%\n",
      "Epoch 29, Batch 645, LR 0.756659 Loss 4.189242, Accuracy 91.238%\n",
      "Epoch 29, Batch 646, LR 0.756536 Loss 4.189272, Accuracy 91.236%\n",
      "Epoch 29, Batch 647, LR 0.756413 Loss 4.189812, Accuracy 91.235%\n",
      "Epoch 29, Batch 648, LR 0.756290 Loss 4.189339, Accuracy 91.239%\n",
      "Epoch 29, Batch 649, LR 0.756167 Loss 4.188657, Accuracy 91.239%\n",
      "Epoch 29, Batch 650, LR 0.756044 Loss 4.187986, Accuracy 91.240%\n",
      "Epoch 29, Batch 651, LR 0.755921 Loss 4.188522, Accuracy 91.236%\n",
      "Epoch 29, Batch 652, LR 0.755797 Loss 4.188010, Accuracy 91.237%\n",
      "Epoch 29, Batch 653, LR 0.755674 Loss 4.187958, Accuracy 91.236%\n",
      "Epoch 29, Batch 654, LR 0.755551 Loss 4.187489, Accuracy 91.235%\n",
      "Epoch 29, Batch 655, LR 0.755428 Loss 4.188984, Accuracy 91.232%\n",
      "Epoch 29, Batch 656, LR 0.755305 Loss 4.189586, Accuracy 91.234%\n",
      "Epoch 29, Batch 657, LR 0.755182 Loss 4.191164, Accuracy 91.223%\n",
      "Epoch 29, Batch 658, LR 0.755059 Loss 4.190010, Accuracy 91.225%\n",
      "Epoch 29, Batch 659, LR 0.754936 Loss 4.190684, Accuracy 91.224%\n",
      "Epoch 29, Batch 660, LR 0.754813 Loss 4.190164, Accuracy 91.226%\n",
      "Epoch 29, Batch 661, LR 0.754690 Loss 4.190339, Accuracy 91.220%\n",
      "Epoch 29, Batch 662, LR 0.754567 Loss 4.190572, Accuracy 91.221%\n",
      "Epoch 29, Batch 663, LR 0.754444 Loss 4.190356, Accuracy 91.222%\n",
      "Epoch 29, Batch 664, LR 0.754321 Loss 4.189886, Accuracy 91.223%\n",
      "Epoch 29, Batch 665, LR 0.754198 Loss 4.189260, Accuracy 91.228%\n",
      "Epoch 29, Batch 666, LR 0.754075 Loss 4.188844, Accuracy 91.229%\n",
      "Epoch 29, Batch 667, LR 0.753953 Loss 4.189161, Accuracy 91.228%\n",
      "Epoch 29, Batch 668, LR 0.753830 Loss 4.187840, Accuracy 91.231%\n",
      "Epoch 29, Batch 669, LR 0.753707 Loss 4.189202, Accuracy 91.223%\n",
      "Epoch 29, Batch 670, LR 0.753584 Loss 4.189298, Accuracy 91.226%\n",
      "Epoch 29, Batch 671, LR 0.753461 Loss 4.189189, Accuracy 91.222%\n",
      "Epoch 29, Batch 672, LR 0.753338 Loss 4.188821, Accuracy 91.225%\n",
      "Epoch 29, Batch 673, LR 0.753215 Loss 4.188736, Accuracy 91.224%\n",
      "Epoch 29, Batch 674, LR 0.753092 Loss 4.189280, Accuracy 91.222%\n",
      "Epoch 29, Batch 675, LR 0.752969 Loss 4.189963, Accuracy 91.218%\n",
      "Epoch 29, Batch 676, LR 0.752846 Loss 4.189426, Accuracy 91.220%\n",
      "Epoch 29, Batch 677, LR 0.752723 Loss 4.188082, Accuracy 91.230%\n",
      "Epoch 29, Batch 678, LR 0.752600 Loss 4.187233, Accuracy 91.236%\n",
      "Epoch 29, Batch 679, LR 0.752478 Loss 4.187111, Accuracy 91.243%\n",
      "Epoch 29, Batch 680, LR 0.752355 Loss 4.186614, Accuracy 91.248%\n",
      "Epoch 29, Batch 681, LR 0.752232 Loss 4.186305, Accuracy 91.247%\n",
      "Epoch 29, Batch 682, LR 0.752109 Loss 4.185725, Accuracy 91.248%\n",
      "Epoch 29, Batch 683, LR 0.751986 Loss 4.183918, Accuracy 91.253%\n",
      "Epoch 29, Batch 684, LR 0.751863 Loss 4.183425, Accuracy 91.253%\n",
      "Epoch 29, Batch 685, LR 0.751740 Loss 4.183346, Accuracy 91.252%\n",
      "Epoch 29, Batch 686, LR 0.751617 Loss 4.182072, Accuracy 91.255%\n",
      "Epoch 29, Batch 687, LR 0.751495 Loss 4.182095, Accuracy 91.255%\n",
      "Epoch 29, Batch 688, LR 0.751372 Loss 4.181920, Accuracy 91.252%\n",
      "Epoch 29, Batch 689, LR 0.751249 Loss 4.181871, Accuracy 91.251%\n",
      "Epoch 29, Batch 690, LR 0.751126 Loss 4.181626, Accuracy 91.251%\n",
      "Epoch 29, Batch 691, LR 0.751003 Loss 4.181323, Accuracy 91.251%\n",
      "Epoch 29, Batch 692, LR 0.750880 Loss 4.180484, Accuracy 91.255%\n",
      "Epoch 29, Batch 693, LR 0.750758 Loss 4.180314, Accuracy 91.261%\n",
      "Epoch 29, Batch 694, LR 0.750635 Loss 4.180777, Accuracy 91.259%\n",
      "Epoch 29, Batch 695, LR 0.750512 Loss 4.181311, Accuracy 91.257%\n",
      "Epoch 29, Batch 696, LR 0.750389 Loss 4.181322, Accuracy 91.258%\n",
      "Epoch 29, Batch 697, LR 0.750266 Loss 4.180674, Accuracy 91.262%\n",
      "Epoch 29, Batch 698, LR 0.750144 Loss 4.180845, Accuracy 91.263%\n",
      "Epoch 29, Batch 699, LR 0.750021 Loss 4.180121, Accuracy 91.267%\n",
      "Epoch 29, Batch 700, LR 0.749898 Loss 4.180462, Accuracy 91.262%\n",
      "Epoch 29, Batch 701, LR 0.749775 Loss 4.179771, Accuracy 91.267%\n",
      "Epoch 29, Batch 702, LR 0.749653 Loss 4.179450, Accuracy 91.266%\n",
      "Epoch 29, Batch 703, LR 0.749530 Loss 4.180969, Accuracy 91.262%\n",
      "Epoch 29, Batch 704, LR 0.749407 Loss 4.180232, Accuracy 91.266%\n",
      "Epoch 29, Batch 705, LR 0.749284 Loss 4.180279, Accuracy 91.263%\n",
      "Epoch 29, Batch 706, LR 0.749162 Loss 4.179938, Accuracy 91.265%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 707, LR 0.749039 Loss 4.181123, Accuracy 91.260%\n",
      "Epoch 29, Batch 708, LR 0.748916 Loss 4.180093, Accuracy 91.265%\n",
      "Epoch 29, Batch 709, LR 0.748794 Loss 4.179545, Accuracy 91.267%\n",
      "Epoch 29, Batch 710, LR 0.748671 Loss 4.179652, Accuracy 91.268%\n",
      "Epoch 29, Batch 711, LR 0.748548 Loss 4.179954, Accuracy 91.267%\n",
      "Epoch 29, Batch 712, LR 0.748425 Loss 4.179912, Accuracy 91.267%\n",
      "Epoch 29, Batch 713, LR 0.748303 Loss 4.179083, Accuracy 91.270%\n",
      "Epoch 29, Batch 714, LR 0.748180 Loss 4.179200, Accuracy 91.269%\n",
      "Epoch 29, Batch 715, LR 0.748057 Loss 4.179258, Accuracy 91.269%\n",
      "Epoch 29, Batch 716, LR 0.747935 Loss 4.178958, Accuracy 91.271%\n",
      "Epoch 29, Batch 717, LR 0.747812 Loss 4.180083, Accuracy 91.272%\n",
      "Epoch 29, Batch 718, LR 0.747689 Loss 4.179537, Accuracy 91.271%\n",
      "Epoch 29, Batch 719, LR 0.747567 Loss 4.179486, Accuracy 91.274%\n",
      "Epoch 29, Batch 720, LR 0.747444 Loss 4.180531, Accuracy 91.268%\n",
      "Epoch 29, Batch 721, LR 0.747321 Loss 4.180007, Accuracy 91.272%\n",
      "Epoch 29, Batch 722, LR 0.747199 Loss 4.180373, Accuracy 91.270%\n",
      "Epoch 29, Batch 723, LR 0.747076 Loss 4.179631, Accuracy 91.272%\n",
      "Epoch 29, Batch 724, LR 0.746953 Loss 4.177764, Accuracy 91.279%\n",
      "Epoch 29, Batch 725, LR 0.746831 Loss 4.177815, Accuracy 91.277%\n",
      "Epoch 29, Batch 726, LR 0.746708 Loss 4.177374, Accuracy 91.276%\n",
      "Epoch 29, Batch 727, LR 0.746586 Loss 4.177022, Accuracy 91.278%\n",
      "Epoch 29, Batch 728, LR 0.746463 Loss 4.177426, Accuracy 91.276%\n",
      "Epoch 29, Batch 729, LR 0.746340 Loss 4.177584, Accuracy 91.278%\n",
      "Epoch 29, Batch 730, LR 0.746218 Loss 4.177480, Accuracy 91.279%\n",
      "Epoch 29, Batch 731, LR 0.746095 Loss 4.178139, Accuracy 91.273%\n",
      "Epoch 29, Batch 732, LR 0.745973 Loss 4.177528, Accuracy 91.277%\n",
      "Epoch 29, Batch 733, LR 0.745850 Loss 4.176633, Accuracy 91.280%\n",
      "Epoch 29, Batch 734, LR 0.745727 Loss 4.176005, Accuracy 91.283%\n",
      "Epoch 29, Batch 735, LR 0.745605 Loss 4.175850, Accuracy 91.285%\n",
      "Epoch 29, Batch 736, LR 0.745482 Loss 4.175842, Accuracy 91.285%\n",
      "Epoch 29, Batch 737, LR 0.745360 Loss 4.176139, Accuracy 91.285%\n",
      "Epoch 29, Batch 738, LR 0.745237 Loss 4.175668, Accuracy 91.287%\n",
      "Epoch 29, Batch 739, LR 0.745115 Loss 4.176333, Accuracy 91.284%\n",
      "Epoch 29, Batch 740, LR 0.744992 Loss 4.176829, Accuracy 91.284%\n",
      "Epoch 29, Batch 741, LR 0.744870 Loss 4.176239, Accuracy 91.285%\n",
      "Epoch 29, Batch 742, LR 0.744747 Loss 4.176427, Accuracy 91.281%\n",
      "Epoch 29, Batch 743, LR 0.744625 Loss 4.176265, Accuracy 91.285%\n",
      "Epoch 29, Batch 744, LR 0.744502 Loss 4.175444, Accuracy 91.290%\n",
      "Epoch 29, Batch 745, LR 0.744380 Loss 4.175320, Accuracy 91.292%\n",
      "Epoch 29, Batch 746, LR 0.744257 Loss 4.175438, Accuracy 91.291%\n",
      "Epoch 29, Batch 747, LR 0.744135 Loss 4.175642, Accuracy 91.289%\n",
      "Epoch 29, Batch 748, LR 0.744012 Loss 4.174837, Accuracy 91.290%\n",
      "Epoch 29, Batch 749, LR 0.743890 Loss 4.174497, Accuracy 91.290%\n",
      "Epoch 29, Batch 750, LR 0.743767 Loss 4.174110, Accuracy 91.294%\n",
      "Epoch 29, Batch 751, LR 0.743645 Loss 4.173924, Accuracy 91.292%\n",
      "Epoch 29, Batch 752, LR 0.743522 Loss 4.174176, Accuracy 91.291%\n",
      "Epoch 29, Batch 753, LR 0.743400 Loss 4.174050, Accuracy 91.291%\n",
      "Epoch 29, Batch 754, LR 0.743277 Loss 4.173998, Accuracy 91.290%\n",
      "Epoch 29, Batch 755, LR 0.743155 Loss 4.174143, Accuracy 91.284%\n",
      "Epoch 29, Batch 756, LR 0.743032 Loss 4.173505, Accuracy 91.287%\n",
      "Epoch 29, Batch 757, LR 0.742910 Loss 4.172675, Accuracy 91.293%\n",
      "Epoch 29, Batch 758, LR 0.742788 Loss 4.172607, Accuracy 91.296%\n",
      "Epoch 29, Batch 759, LR 0.742665 Loss 4.173448, Accuracy 91.292%\n",
      "Epoch 29, Batch 760, LR 0.742543 Loss 4.172782, Accuracy 91.293%\n",
      "Epoch 29, Batch 761, LR 0.742420 Loss 4.173222, Accuracy 91.291%\n",
      "Epoch 29, Batch 762, LR 0.742298 Loss 4.172687, Accuracy 91.296%\n",
      "Epoch 29, Batch 763, LR 0.742175 Loss 4.172987, Accuracy 91.296%\n",
      "Epoch 29, Batch 764, LR 0.742053 Loss 4.172199, Accuracy 91.297%\n",
      "Epoch 29, Batch 765, LR 0.741931 Loss 4.172094, Accuracy 91.298%\n",
      "Epoch 29, Batch 766, LR 0.741808 Loss 4.172621, Accuracy 91.295%\n",
      "Epoch 29, Batch 767, LR 0.741686 Loss 4.173226, Accuracy 91.295%\n",
      "Epoch 29, Batch 768, LR 0.741564 Loss 4.173185, Accuracy 91.299%\n",
      "Epoch 29, Batch 769, LR 0.741441 Loss 4.172709, Accuracy 91.299%\n",
      "Epoch 29, Batch 770, LR 0.741319 Loss 4.173183, Accuracy 91.298%\n",
      "Epoch 29, Batch 771, LR 0.741196 Loss 4.172952, Accuracy 91.300%\n",
      "Epoch 29, Batch 772, LR 0.741074 Loss 4.173549, Accuracy 91.296%\n",
      "Epoch 29, Batch 773, LR 0.740952 Loss 4.173164, Accuracy 91.296%\n",
      "Epoch 29, Batch 774, LR 0.740829 Loss 4.172315, Accuracy 91.303%\n",
      "Epoch 29, Batch 775, LR 0.740707 Loss 4.172377, Accuracy 91.299%\n",
      "Epoch 29, Batch 776, LR 0.740585 Loss 4.171944, Accuracy 91.301%\n",
      "Epoch 29, Batch 777, LR 0.740462 Loss 4.172498, Accuracy 91.297%\n",
      "Epoch 29, Batch 778, LR 0.740340 Loss 4.173718, Accuracy 91.290%\n",
      "Epoch 29, Batch 779, LR 0.740218 Loss 4.173792, Accuracy 91.284%\n",
      "Epoch 29, Batch 780, LR 0.740095 Loss 4.173041, Accuracy 91.288%\n",
      "Epoch 29, Batch 781, LR 0.739973 Loss 4.172457, Accuracy 91.293%\n",
      "Epoch 29, Batch 782, LR 0.739851 Loss 4.171133, Accuracy 91.298%\n",
      "Epoch 29, Batch 783, LR 0.739729 Loss 4.171090, Accuracy 91.295%\n",
      "Epoch 29, Batch 784, LR 0.739606 Loss 4.171164, Accuracy 91.291%\n",
      "Epoch 29, Batch 785, LR 0.739484 Loss 4.171488, Accuracy 91.288%\n",
      "Epoch 29, Batch 786, LR 0.739362 Loss 4.171493, Accuracy 91.290%\n",
      "Epoch 29, Batch 787, LR 0.739240 Loss 4.170706, Accuracy 91.294%\n",
      "Epoch 29, Batch 788, LR 0.739117 Loss 4.170612, Accuracy 91.295%\n",
      "Epoch 29, Batch 789, LR 0.738995 Loss 4.170071, Accuracy 91.297%\n",
      "Epoch 29, Batch 790, LR 0.738873 Loss 4.170739, Accuracy 91.293%\n",
      "Epoch 29, Batch 791, LR 0.738751 Loss 4.170992, Accuracy 91.294%\n",
      "Epoch 29, Batch 792, LR 0.738628 Loss 4.170821, Accuracy 91.298%\n",
      "Epoch 29, Batch 793, LR 0.738506 Loss 4.170522, Accuracy 91.303%\n",
      "Epoch 29, Batch 794, LR 0.738384 Loss 4.170872, Accuracy 91.304%\n",
      "Epoch 29, Batch 795, LR 0.738262 Loss 4.171170, Accuracy 91.300%\n",
      "Epoch 29, Batch 796, LR 0.738139 Loss 4.170426, Accuracy 91.305%\n",
      "Epoch 29, Batch 797, LR 0.738017 Loss 4.171015, Accuracy 91.302%\n",
      "Epoch 29, Batch 798, LR 0.737895 Loss 4.170729, Accuracy 91.305%\n",
      "Epoch 29, Batch 799, LR 0.737773 Loss 4.170866, Accuracy 91.305%\n",
      "Epoch 29, Batch 800, LR 0.737651 Loss 4.171175, Accuracy 91.307%\n",
      "Epoch 29, Batch 801, LR 0.737528 Loss 4.171889, Accuracy 91.300%\n",
      "Epoch 29, Batch 802, LR 0.737406 Loss 4.171002, Accuracy 91.306%\n",
      "Epoch 29, Batch 803, LR 0.737284 Loss 4.172221, Accuracy 91.298%\n",
      "Epoch 29, Batch 804, LR 0.737162 Loss 4.172454, Accuracy 91.296%\n",
      "Epoch 29, Batch 805, LR 0.737040 Loss 4.172765, Accuracy 91.296%\n",
      "Epoch 29, Batch 806, LR 0.736918 Loss 4.172843, Accuracy 91.295%\n",
      "Epoch 29, Batch 807, LR 0.736795 Loss 4.173294, Accuracy 91.292%\n",
      "Epoch 29, Batch 808, LR 0.736673 Loss 4.173395, Accuracy 91.291%\n",
      "Epoch 29, Batch 809, LR 0.736551 Loss 4.172809, Accuracy 91.293%\n",
      "Epoch 29, Batch 810, LR 0.736429 Loss 4.174067, Accuracy 91.291%\n",
      "Epoch 29, Batch 811, LR 0.736307 Loss 4.173989, Accuracy 91.293%\n",
      "Epoch 29, Batch 812, LR 0.736185 Loss 4.173957, Accuracy 91.292%\n",
      "Epoch 29, Batch 813, LR 0.736063 Loss 4.174293, Accuracy 91.285%\n",
      "Epoch 29, Batch 814, LR 0.735941 Loss 4.173682, Accuracy 91.288%\n",
      "Epoch 29, Batch 815, LR 0.735819 Loss 4.172661, Accuracy 91.290%\n",
      "Epoch 29, Batch 816, LR 0.735696 Loss 4.172263, Accuracy 91.291%\n",
      "Epoch 29, Batch 817, LR 0.735574 Loss 4.173248, Accuracy 91.287%\n",
      "Epoch 29, Batch 818, LR 0.735452 Loss 4.173525, Accuracy 91.288%\n",
      "Epoch 29, Batch 819, LR 0.735330 Loss 4.173559, Accuracy 91.288%\n",
      "Epoch 29, Batch 820, LR 0.735208 Loss 4.174338, Accuracy 91.285%\n",
      "Epoch 29, Batch 821, LR 0.735086 Loss 4.173796, Accuracy 91.289%\n",
      "Epoch 29, Batch 822, LR 0.734964 Loss 4.174172, Accuracy 91.285%\n",
      "Epoch 29, Batch 823, LR 0.734842 Loss 4.174393, Accuracy 91.280%\n",
      "Epoch 29, Batch 824, LR 0.734720 Loss 4.174843, Accuracy 91.278%\n",
      "Epoch 29, Batch 825, LR 0.734598 Loss 4.175169, Accuracy 91.275%\n",
      "Epoch 29, Batch 826, LR 0.734476 Loss 4.175534, Accuracy 91.272%\n",
      "Epoch 29, Batch 827, LR 0.734354 Loss 4.175505, Accuracy 91.272%\n",
      "Epoch 29, Batch 828, LR 0.734232 Loss 4.175264, Accuracy 91.270%\n",
      "Epoch 29, Batch 829, LR 0.734110 Loss 4.174777, Accuracy 91.271%\n",
      "Epoch 29, Batch 830, LR 0.733988 Loss 4.174840, Accuracy 91.268%\n",
      "Epoch 29, Batch 831, LR 0.733866 Loss 4.174820, Accuracy 91.268%\n",
      "Epoch 29, Batch 832, LR 0.733744 Loss 4.173618, Accuracy 91.274%\n",
      "Epoch 29, Batch 833, LR 0.733622 Loss 4.174233, Accuracy 91.270%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 834, LR 0.733500 Loss 4.174398, Accuracy 91.271%\n",
      "Epoch 29, Batch 835, LR 0.733378 Loss 4.175006, Accuracy 91.267%\n",
      "Epoch 29, Batch 836, LR 0.733256 Loss 4.175977, Accuracy 91.264%\n",
      "Epoch 29, Batch 837, LR 0.733134 Loss 4.175790, Accuracy 91.265%\n",
      "Epoch 29, Batch 838, LR 0.733012 Loss 4.175891, Accuracy 91.266%\n",
      "Epoch 29, Batch 839, LR 0.732890 Loss 4.176542, Accuracy 91.268%\n",
      "Epoch 29, Batch 840, LR 0.732768 Loss 4.175839, Accuracy 91.275%\n",
      "Epoch 29, Batch 841, LR 0.732646 Loss 4.175528, Accuracy 91.274%\n",
      "Epoch 29, Batch 842, LR 0.732524 Loss 4.174908, Accuracy 91.277%\n",
      "Epoch 29, Batch 843, LR 0.732402 Loss 4.175041, Accuracy 91.277%\n",
      "Epoch 29, Batch 844, LR 0.732280 Loss 4.174843, Accuracy 91.273%\n",
      "Epoch 29, Batch 845, LR 0.732158 Loss 4.175996, Accuracy 91.264%\n",
      "Epoch 29, Batch 846, LR 0.732036 Loss 4.176482, Accuracy 91.260%\n",
      "Epoch 29, Batch 847, LR 0.731915 Loss 4.176119, Accuracy 91.265%\n",
      "Epoch 29, Batch 848, LR 0.731793 Loss 4.176438, Accuracy 91.265%\n",
      "Epoch 29, Batch 849, LR 0.731671 Loss 4.176813, Accuracy 91.266%\n",
      "Epoch 29, Batch 850, LR 0.731549 Loss 4.177424, Accuracy 91.263%\n",
      "Epoch 29, Batch 851, LR 0.731427 Loss 4.176889, Accuracy 91.265%\n",
      "Epoch 29, Batch 852, LR 0.731305 Loss 4.176077, Accuracy 91.267%\n",
      "Epoch 29, Batch 853, LR 0.731183 Loss 4.176363, Accuracy 91.267%\n",
      "Epoch 29, Batch 854, LR 0.731061 Loss 4.176694, Accuracy 91.265%\n",
      "Epoch 29, Batch 855, LR 0.730939 Loss 4.176294, Accuracy 91.265%\n",
      "Epoch 29, Batch 856, LR 0.730818 Loss 4.176192, Accuracy 91.268%\n",
      "Epoch 29, Batch 857, LR 0.730696 Loss 4.176195, Accuracy 91.270%\n",
      "Epoch 29, Batch 858, LR 0.730574 Loss 4.175715, Accuracy 91.271%\n",
      "Epoch 29, Batch 859, LR 0.730452 Loss 4.176588, Accuracy 91.269%\n",
      "Epoch 29, Batch 860, LR 0.730330 Loss 4.176169, Accuracy 91.273%\n",
      "Epoch 29, Batch 861, LR 0.730208 Loss 4.176658, Accuracy 91.269%\n",
      "Epoch 29, Batch 862, LR 0.730087 Loss 4.177355, Accuracy 91.266%\n",
      "Epoch 29, Batch 863, LR 0.729965 Loss 4.177224, Accuracy 91.267%\n",
      "Epoch 29, Batch 864, LR 0.729843 Loss 4.177720, Accuracy 91.265%\n",
      "Epoch 29, Batch 865, LR 0.729721 Loss 4.177723, Accuracy 91.267%\n",
      "Epoch 29, Batch 866, LR 0.729599 Loss 4.178013, Accuracy 91.266%\n",
      "Epoch 29, Batch 867, LR 0.729478 Loss 4.178044, Accuracy 91.266%\n",
      "Epoch 29, Batch 868, LR 0.729356 Loss 4.177913, Accuracy 91.268%\n",
      "Epoch 29, Batch 869, LR 0.729234 Loss 4.177373, Accuracy 91.269%\n",
      "Epoch 29, Batch 870, LR 0.729112 Loss 4.176688, Accuracy 91.269%\n",
      "Epoch 29, Batch 871, LR 0.728991 Loss 4.175944, Accuracy 91.274%\n",
      "Epoch 29, Batch 872, LR 0.728869 Loss 4.175834, Accuracy 91.274%\n",
      "Epoch 29, Batch 873, LR 0.728747 Loss 4.176141, Accuracy 91.272%\n",
      "Epoch 29, Batch 874, LR 0.728625 Loss 4.175300, Accuracy 91.276%\n",
      "Epoch 29, Batch 875, LR 0.728504 Loss 4.175825, Accuracy 91.272%\n",
      "Epoch 29, Batch 876, LR 0.728382 Loss 4.175632, Accuracy 91.273%\n",
      "Epoch 29, Batch 877, LR 0.728260 Loss 4.175247, Accuracy 91.275%\n",
      "Epoch 29, Batch 878, LR 0.728138 Loss 4.174905, Accuracy 91.276%\n",
      "Epoch 29, Batch 879, LR 0.728017 Loss 4.174811, Accuracy 91.274%\n",
      "Epoch 29, Batch 880, LR 0.727895 Loss 4.174569, Accuracy 91.275%\n",
      "Epoch 29, Batch 881, LR 0.727773 Loss 4.174600, Accuracy 91.276%\n",
      "Epoch 29, Batch 882, LR 0.727651 Loss 4.174518, Accuracy 91.277%\n",
      "Epoch 29, Batch 883, LR 0.727530 Loss 4.174607, Accuracy 91.276%\n",
      "Epoch 29, Batch 884, LR 0.727408 Loss 4.174918, Accuracy 91.281%\n",
      "Epoch 29, Batch 885, LR 0.727286 Loss 4.174599, Accuracy 91.283%\n",
      "Epoch 29, Batch 886, LR 0.727165 Loss 4.174308, Accuracy 91.287%\n",
      "Epoch 29, Batch 887, LR 0.727043 Loss 4.174169, Accuracy 91.290%\n",
      "Epoch 29, Batch 888, LR 0.726921 Loss 4.174241, Accuracy 91.293%\n",
      "Epoch 29, Batch 889, LR 0.726800 Loss 4.174417, Accuracy 91.294%\n",
      "Epoch 29, Batch 890, LR 0.726678 Loss 4.174249, Accuracy 91.291%\n",
      "Epoch 29, Batch 891, LR 0.726556 Loss 4.174281, Accuracy 91.293%\n",
      "Epoch 29, Batch 892, LR 0.726435 Loss 4.174527, Accuracy 91.292%\n",
      "Epoch 29, Batch 893, LR 0.726313 Loss 4.175436, Accuracy 91.287%\n",
      "Epoch 29, Batch 894, LR 0.726192 Loss 4.175620, Accuracy 91.285%\n",
      "Epoch 29, Batch 895, LR 0.726070 Loss 4.175915, Accuracy 91.284%\n",
      "Epoch 29, Batch 896, LR 0.725948 Loss 4.176732, Accuracy 91.280%\n",
      "Epoch 29, Batch 897, LR 0.725827 Loss 4.176069, Accuracy 91.278%\n",
      "Epoch 29, Batch 898, LR 0.725705 Loss 4.175858, Accuracy 91.280%\n",
      "Epoch 29, Batch 899, LR 0.725584 Loss 4.175594, Accuracy 91.283%\n",
      "Epoch 29, Batch 900, LR 0.725462 Loss 4.176408, Accuracy 91.280%\n",
      "Epoch 29, Batch 901, LR 0.725340 Loss 4.176845, Accuracy 91.283%\n",
      "Epoch 29, Batch 902, LR 0.725219 Loss 4.177133, Accuracy 91.282%\n",
      "Epoch 29, Batch 903, LR 0.725097 Loss 4.176549, Accuracy 91.283%\n",
      "Epoch 29, Batch 904, LR 0.724976 Loss 4.176159, Accuracy 91.284%\n",
      "Epoch 29, Batch 905, LR 0.724854 Loss 4.176233, Accuracy 91.282%\n",
      "Epoch 29, Batch 906, LR 0.724733 Loss 4.176137, Accuracy 91.279%\n",
      "Epoch 29, Batch 907, LR 0.724611 Loss 4.175920, Accuracy 91.280%\n",
      "Epoch 29, Batch 908, LR 0.724489 Loss 4.175663, Accuracy 91.281%\n",
      "Epoch 29, Batch 909, LR 0.724368 Loss 4.176384, Accuracy 91.278%\n",
      "Epoch 29, Batch 910, LR 0.724246 Loss 4.176492, Accuracy 91.277%\n",
      "Epoch 29, Batch 911, LR 0.724125 Loss 4.176963, Accuracy 91.277%\n",
      "Epoch 29, Batch 912, LR 0.724003 Loss 4.176995, Accuracy 91.277%\n",
      "Epoch 29, Batch 913, LR 0.723882 Loss 4.176623, Accuracy 91.276%\n",
      "Epoch 29, Batch 914, LR 0.723760 Loss 4.175732, Accuracy 91.281%\n",
      "Epoch 29, Batch 915, LR 0.723639 Loss 4.175568, Accuracy 91.286%\n",
      "Epoch 29, Batch 916, LR 0.723517 Loss 4.175340, Accuracy 91.289%\n",
      "Epoch 29, Batch 917, LR 0.723396 Loss 4.175339, Accuracy 91.290%\n",
      "Epoch 29, Batch 918, LR 0.723274 Loss 4.175344, Accuracy 91.292%\n",
      "Epoch 29, Batch 919, LR 0.723153 Loss 4.175117, Accuracy 91.294%\n",
      "Epoch 29, Batch 920, LR 0.723031 Loss 4.175216, Accuracy 91.297%\n",
      "Epoch 29, Batch 921, LR 0.722910 Loss 4.175119, Accuracy 91.300%\n",
      "Epoch 29, Batch 922, LR 0.722788 Loss 4.175126, Accuracy 91.302%\n",
      "Epoch 29, Batch 923, LR 0.722667 Loss 4.175054, Accuracy 91.302%\n",
      "Epoch 29, Batch 924, LR 0.722546 Loss 4.175187, Accuracy 91.301%\n",
      "Epoch 29, Batch 925, LR 0.722424 Loss 4.175267, Accuracy 91.298%\n",
      "Epoch 29, Batch 926, LR 0.722303 Loss 4.175194, Accuracy 91.299%\n",
      "Epoch 29, Batch 927, LR 0.722181 Loss 4.175300, Accuracy 91.300%\n",
      "Epoch 29, Batch 928, LR 0.722060 Loss 4.175200, Accuracy 91.303%\n",
      "Epoch 29, Batch 929, LR 0.721938 Loss 4.175223, Accuracy 91.301%\n",
      "Epoch 29, Batch 930, LR 0.721817 Loss 4.175614, Accuracy 91.296%\n",
      "Epoch 29, Batch 931, LR 0.721696 Loss 4.175499, Accuracy 91.301%\n",
      "Epoch 29, Batch 932, LR 0.721574 Loss 4.175761, Accuracy 91.300%\n",
      "Epoch 29, Batch 933, LR 0.721453 Loss 4.175767, Accuracy 91.305%\n",
      "Epoch 29, Batch 934, LR 0.721331 Loss 4.175808, Accuracy 91.307%\n",
      "Epoch 29, Batch 935, LR 0.721210 Loss 4.176022, Accuracy 91.308%\n",
      "Epoch 29, Batch 936, LR 0.721089 Loss 4.175967, Accuracy 91.307%\n",
      "Epoch 29, Batch 937, LR 0.720967 Loss 4.176724, Accuracy 91.306%\n",
      "Epoch 29, Batch 938, LR 0.720846 Loss 4.176681, Accuracy 91.309%\n",
      "Epoch 29, Batch 939, LR 0.720725 Loss 4.176626, Accuracy 91.310%\n",
      "Epoch 29, Batch 940, LR 0.720603 Loss 4.176507, Accuracy 91.309%\n",
      "Epoch 29, Batch 941, LR 0.720482 Loss 4.176632, Accuracy 91.309%\n",
      "Epoch 29, Batch 942, LR 0.720361 Loss 4.176196, Accuracy 91.311%\n",
      "Epoch 29, Batch 943, LR 0.720239 Loss 4.176405, Accuracy 91.310%\n",
      "Epoch 29, Batch 944, LR 0.720118 Loss 4.176944, Accuracy 91.308%\n",
      "Epoch 29, Batch 945, LR 0.719997 Loss 4.177695, Accuracy 91.302%\n",
      "Epoch 29, Batch 946, LR 0.719875 Loss 4.177032, Accuracy 91.310%\n",
      "Epoch 29, Batch 947, LR 0.719754 Loss 4.176812, Accuracy 91.311%\n",
      "Epoch 29, Batch 948, LR 0.719633 Loss 4.176612, Accuracy 91.310%\n",
      "Epoch 29, Batch 949, LR 0.719511 Loss 4.176456, Accuracy 91.311%\n",
      "Epoch 29, Batch 950, LR 0.719390 Loss 4.175701, Accuracy 91.313%\n",
      "Epoch 29, Batch 951, LR 0.719269 Loss 4.175658, Accuracy 91.312%\n",
      "Epoch 29, Batch 952, LR 0.719148 Loss 4.175804, Accuracy 91.310%\n",
      "Epoch 29, Batch 953, LR 0.719026 Loss 4.176199, Accuracy 91.311%\n",
      "Epoch 29, Batch 954, LR 0.718905 Loss 4.176571, Accuracy 91.311%\n",
      "Epoch 29, Batch 955, LR 0.718784 Loss 4.176357, Accuracy 91.309%\n",
      "Epoch 29, Batch 956, LR 0.718662 Loss 4.176305, Accuracy 91.311%\n",
      "Epoch 29, Batch 957, LR 0.718541 Loss 4.176583, Accuracy 91.309%\n",
      "Epoch 29, Batch 958, LR 0.718420 Loss 4.176622, Accuracy 91.311%\n",
      "Epoch 29, Batch 959, LR 0.718299 Loss 4.176690, Accuracy 91.313%\n",
      "Epoch 29, Batch 960, LR 0.718178 Loss 4.176793, Accuracy 91.314%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 961, LR 0.718056 Loss 4.176756, Accuracy 91.310%\n",
      "Epoch 29, Batch 962, LR 0.717935 Loss 4.177196, Accuracy 91.310%\n",
      "Epoch 29, Batch 963, LR 0.717814 Loss 4.177458, Accuracy 91.311%\n",
      "Epoch 29, Batch 964, LR 0.717693 Loss 4.176662, Accuracy 91.315%\n",
      "Epoch 29, Batch 965, LR 0.717571 Loss 4.176014, Accuracy 91.320%\n",
      "Epoch 29, Batch 966, LR 0.717450 Loss 4.176396, Accuracy 91.320%\n",
      "Epoch 29, Batch 967, LR 0.717329 Loss 4.176091, Accuracy 91.319%\n",
      "Epoch 29, Batch 968, LR 0.717208 Loss 4.176038, Accuracy 91.318%\n",
      "Epoch 29, Batch 969, LR 0.717087 Loss 4.176123, Accuracy 91.320%\n",
      "Epoch 29, Batch 970, LR 0.716966 Loss 4.176719, Accuracy 91.316%\n",
      "Epoch 29, Batch 971, LR 0.716844 Loss 4.176823, Accuracy 91.313%\n",
      "Epoch 29, Batch 972, LR 0.716723 Loss 4.177712, Accuracy 91.310%\n",
      "Epoch 29, Batch 973, LR 0.716602 Loss 4.177214, Accuracy 91.308%\n",
      "Epoch 29, Batch 974, LR 0.716481 Loss 4.177206, Accuracy 91.307%\n",
      "Epoch 29, Batch 975, LR 0.716360 Loss 4.177558, Accuracy 91.304%\n",
      "Epoch 29, Batch 976, LR 0.716239 Loss 4.177213, Accuracy 91.309%\n",
      "Epoch 29, Batch 977, LR 0.716118 Loss 4.177070, Accuracy 91.309%\n",
      "Epoch 29, Batch 978, LR 0.715996 Loss 4.176950, Accuracy 91.312%\n",
      "Epoch 29, Batch 979, LR 0.715875 Loss 4.176987, Accuracy 91.310%\n",
      "Epoch 29, Batch 980, LR 0.715754 Loss 4.177077, Accuracy 91.309%\n",
      "Epoch 29, Batch 981, LR 0.715633 Loss 4.176520, Accuracy 91.312%\n",
      "Epoch 29, Batch 982, LR 0.715512 Loss 4.176464, Accuracy 91.312%\n",
      "Epoch 29, Batch 983, LR 0.715391 Loss 4.175849, Accuracy 91.314%\n",
      "Epoch 29, Batch 984, LR 0.715270 Loss 4.175387, Accuracy 91.313%\n",
      "Epoch 29, Batch 985, LR 0.715149 Loss 4.176096, Accuracy 91.309%\n",
      "Epoch 29, Batch 986, LR 0.715028 Loss 4.175961, Accuracy 91.310%\n",
      "Epoch 29, Batch 987, LR 0.714907 Loss 4.176140, Accuracy 91.308%\n",
      "Epoch 29, Batch 988, LR 0.714786 Loss 4.176348, Accuracy 91.308%\n",
      "Epoch 29, Batch 989, LR 0.714665 Loss 4.176555, Accuracy 91.309%\n",
      "Epoch 29, Batch 990, LR 0.714544 Loss 4.176093, Accuracy 91.311%\n",
      "Epoch 29, Batch 991, LR 0.714423 Loss 4.176283, Accuracy 91.308%\n",
      "Epoch 29, Batch 992, LR 0.714301 Loss 4.175825, Accuracy 91.309%\n",
      "Epoch 29, Batch 993, LR 0.714180 Loss 4.175807, Accuracy 91.312%\n",
      "Epoch 29, Batch 994, LR 0.714059 Loss 4.175623, Accuracy 91.312%\n",
      "Epoch 29, Batch 995, LR 0.713938 Loss 4.175493, Accuracy 91.313%\n",
      "Epoch 29, Batch 996, LR 0.713817 Loss 4.175357, Accuracy 91.314%\n",
      "Epoch 29, Batch 997, LR 0.713696 Loss 4.175918, Accuracy 91.314%\n",
      "Epoch 29, Batch 998, LR 0.713575 Loss 4.176690, Accuracy 91.309%\n",
      "Epoch 29, Batch 999, LR 0.713454 Loss 4.176355, Accuracy 91.308%\n",
      "Epoch 29, Batch 1000, LR 0.713333 Loss 4.176692, Accuracy 91.309%\n",
      "Epoch 29, Batch 1001, LR 0.713212 Loss 4.176975, Accuracy 91.309%\n",
      "Epoch 29, Batch 1002, LR 0.713092 Loss 4.176975, Accuracy 91.312%\n",
      "Epoch 29, Batch 1003, LR 0.712971 Loss 4.176260, Accuracy 91.314%\n",
      "Epoch 29, Batch 1004, LR 0.712850 Loss 4.176154, Accuracy 91.316%\n",
      "Epoch 29, Batch 1005, LR 0.712729 Loss 4.175980, Accuracy 91.316%\n",
      "Epoch 29, Batch 1006, LR 0.712608 Loss 4.175935, Accuracy 91.316%\n",
      "Epoch 29, Batch 1007, LR 0.712487 Loss 4.175840, Accuracy 91.315%\n",
      "Epoch 29, Batch 1008, LR 0.712366 Loss 4.175995, Accuracy 91.317%\n",
      "Epoch 29, Batch 1009, LR 0.712245 Loss 4.174866, Accuracy 91.322%\n",
      "Epoch 29, Batch 1010, LR 0.712124 Loss 4.174997, Accuracy 91.318%\n",
      "Epoch 29, Batch 1011, LR 0.712003 Loss 4.176047, Accuracy 91.312%\n",
      "Epoch 29, Batch 1012, LR 0.711882 Loss 4.176091, Accuracy 91.312%\n",
      "Epoch 29, Batch 1013, LR 0.711761 Loss 4.175805, Accuracy 91.315%\n",
      "Epoch 29, Batch 1014, LR 0.711640 Loss 4.176116, Accuracy 91.312%\n",
      "Epoch 29, Batch 1015, LR 0.711519 Loss 4.175610, Accuracy 91.317%\n",
      "Epoch 29, Batch 1016, LR 0.711399 Loss 4.176138, Accuracy 91.318%\n",
      "Epoch 29, Batch 1017, LR 0.711278 Loss 4.175826, Accuracy 91.319%\n",
      "Epoch 29, Batch 1018, LR 0.711157 Loss 4.175695, Accuracy 91.319%\n",
      "Epoch 29, Batch 1019, LR 0.711036 Loss 4.175046, Accuracy 91.322%\n",
      "Epoch 29, Batch 1020, LR 0.710915 Loss 4.174981, Accuracy 91.320%\n",
      "Epoch 29, Batch 1021, LR 0.710794 Loss 4.174896, Accuracy 91.320%\n",
      "Epoch 29, Batch 1022, LR 0.710673 Loss 4.174865, Accuracy 91.319%\n",
      "Epoch 29, Batch 1023, LR 0.710553 Loss 4.174703, Accuracy 91.318%\n",
      "Epoch 29, Batch 1024, LR 0.710432 Loss 4.175703, Accuracy 91.310%\n",
      "Epoch 29, Batch 1025, LR 0.710311 Loss 4.175808, Accuracy 91.307%\n",
      "Epoch 29, Batch 1026, LR 0.710190 Loss 4.175207, Accuracy 91.309%\n",
      "Epoch 29, Batch 1027, LR 0.710069 Loss 4.174785, Accuracy 91.312%\n",
      "Epoch 29, Batch 1028, LR 0.709948 Loss 4.174613, Accuracy 91.311%\n",
      "Epoch 29, Batch 1029, LR 0.709828 Loss 4.174375, Accuracy 91.311%\n",
      "Epoch 29, Batch 1030, LR 0.709707 Loss 4.174126, Accuracy 91.307%\n",
      "Epoch 29, Batch 1031, LR 0.709586 Loss 4.173276, Accuracy 91.312%\n",
      "Epoch 29, Batch 1032, LR 0.709465 Loss 4.172907, Accuracy 91.314%\n",
      "Epoch 29, Batch 1033, LR 0.709345 Loss 4.173236, Accuracy 91.311%\n",
      "Epoch 29, Batch 1034, LR 0.709224 Loss 4.173141, Accuracy 91.310%\n",
      "Epoch 29, Batch 1035, LR 0.709103 Loss 4.172941, Accuracy 91.312%\n",
      "Epoch 29, Batch 1036, LR 0.708982 Loss 4.172641, Accuracy 91.312%\n",
      "Epoch 29, Batch 1037, LR 0.708861 Loss 4.172137, Accuracy 91.317%\n",
      "Epoch 29, Batch 1038, LR 0.708741 Loss 4.171798, Accuracy 91.317%\n",
      "Epoch 29, Batch 1039, LR 0.708620 Loss 4.171434, Accuracy 91.320%\n",
      "Epoch 29, Batch 1040, LR 0.708499 Loss 4.171102, Accuracy 91.321%\n",
      "Epoch 29, Batch 1041, LR 0.708379 Loss 4.171096, Accuracy 91.320%\n",
      "Epoch 29, Batch 1042, LR 0.708258 Loss 4.171389, Accuracy 91.319%\n",
      "Epoch 29, Batch 1043, LR 0.708137 Loss 4.170924, Accuracy 91.317%\n",
      "Epoch 29, Batch 1044, LR 0.708016 Loss 4.170888, Accuracy 91.318%\n",
      "Epoch 29, Batch 1045, LR 0.707896 Loss 4.170600, Accuracy 91.322%\n",
      "Epoch 29, Batch 1046, LR 0.707775 Loss 4.169947, Accuracy 91.323%\n",
      "Epoch 29, Batch 1047, LR 0.707654 Loss 4.170238, Accuracy 91.322%\n",
      "Epoch 29, Loss (train set) 4.170238, Accuracy (train set) 91.322%\n",
      "Epoch 29, Accuracy (validation set) 88.576%\n",
      "Epoch 30, Batch 1, LR 0.707534 Loss 3.834876, Accuracy 94.531%\n",
      "Epoch 30, Batch 2, LR 0.707413 Loss 4.253311, Accuracy 91.016%\n",
      "Epoch 30, Batch 3, LR 0.707292 Loss 4.129748, Accuracy 92.708%\n",
      "Epoch 30, Batch 4, LR 0.707172 Loss 4.143426, Accuracy 92.578%\n",
      "Epoch 30, Batch 5, LR 0.707051 Loss 4.100386, Accuracy 92.344%\n",
      "Epoch 30, Batch 6, LR 0.706930 Loss 4.018880, Accuracy 92.318%\n",
      "Epoch 30, Batch 7, LR 0.706810 Loss 3.996560, Accuracy 92.299%\n",
      "Epoch 30, Batch 8, LR 0.706689 Loss 3.986518, Accuracy 92.090%\n",
      "Epoch 30, Batch 9, LR 0.706568 Loss 3.999075, Accuracy 91.753%\n",
      "Epoch 30, Batch 10, LR 0.706448 Loss 4.042008, Accuracy 91.562%\n",
      "Epoch 30, Batch 11, LR 0.706327 Loss 4.024344, Accuracy 91.548%\n",
      "Epoch 30, Batch 12, LR 0.706206 Loss 4.064695, Accuracy 91.406%\n",
      "Epoch 30, Batch 13, LR 0.706086 Loss 4.035484, Accuracy 91.406%\n",
      "Epoch 30, Batch 14, LR 0.705965 Loss 4.083798, Accuracy 91.127%\n",
      "Epoch 30, Batch 15, LR 0.705845 Loss 4.052587, Accuracy 91.198%\n",
      "Epoch 30, Batch 16, LR 0.705724 Loss 4.026407, Accuracy 91.455%\n",
      "Epoch 30, Batch 17, LR 0.705603 Loss 4.029851, Accuracy 91.498%\n",
      "Epoch 30, Batch 18, LR 0.705483 Loss 4.018684, Accuracy 91.536%\n",
      "Epoch 30, Batch 19, LR 0.705362 Loss 4.009446, Accuracy 91.530%\n",
      "Epoch 30, Batch 20, LR 0.705242 Loss 4.001345, Accuracy 91.602%\n",
      "Epoch 30, Batch 21, LR 0.705121 Loss 3.981831, Accuracy 91.815%\n",
      "Epoch 30, Batch 22, LR 0.705001 Loss 3.982141, Accuracy 91.868%\n",
      "Epoch 30, Batch 23, LR 0.704880 Loss 3.949702, Accuracy 92.120%\n",
      "Epoch 30, Batch 24, LR 0.704760 Loss 3.948366, Accuracy 92.090%\n",
      "Epoch 30, Batch 25, LR 0.704639 Loss 3.952284, Accuracy 91.969%\n",
      "Epoch 30, Batch 26, LR 0.704519 Loss 3.964840, Accuracy 91.977%\n",
      "Epoch 30, Batch 27, LR 0.704398 Loss 3.974069, Accuracy 91.869%\n",
      "Epoch 30, Batch 28, LR 0.704277 Loss 3.957936, Accuracy 91.936%\n",
      "Epoch 30, Batch 29, LR 0.704157 Loss 3.979507, Accuracy 91.864%\n",
      "Epoch 30, Batch 30, LR 0.704036 Loss 3.977737, Accuracy 91.771%\n",
      "Epoch 30, Batch 31, LR 0.703916 Loss 3.983866, Accuracy 91.784%\n",
      "Epoch 30, Batch 32, LR 0.703795 Loss 3.990021, Accuracy 91.699%\n",
      "Epoch 30, Batch 33, LR 0.703675 Loss 3.975106, Accuracy 91.738%\n",
      "Epoch 30, Batch 34, LR 0.703555 Loss 3.959143, Accuracy 91.774%\n",
      "Epoch 30, Batch 35, LR 0.703434 Loss 3.953011, Accuracy 91.786%\n",
      "Epoch 30, Batch 36, LR 0.703314 Loss 3.954396, Accuracy 91.775%\n",
      "Epoch 30, Batch 37, LR 0.703193 Loss 3.938908, Accuracy 91.850%\n",
      "Epoch 30, Batch 38, LR 0.703073 Loss 3.933562, Accuracy 91.920%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 39, LR 0.702952 Loss 3.963579, Accuracy 91.767%\n",
      "Epoch 30, Batch 40, LR 0.702832 Loss 3.964709, Accuracy 91.816%\n",
      "Epoch 30, Batch 41, LR 0.702711 Loss 3.983199, Accuracy 91.730%\n",
      "Epoch 30, Batch 42, LR 0.702591 Loss 3.977377, Accuracy 91.778%\n",
      "Epoch 30, Batch 43, LR 0.702470 Loss 3.981345, Accuracy 91.806%\n",
      "Epoch 30, Batch 44, LR 0.702350 Loss 3.977602, Accuracy 91.815%\n",
      "Epoch 30, Batch 45, LR 0.702230 Loss 3.987747, Accuracy 91.788%\n",
      "Epoch 30, Batch 46, LR 0.702109 Loss 3.999582, Accuracy 91.729%\n",
      "Epoch 30, Batch 47, LR 0.701989 Loss 4.009155, Accuracy 91.672%\n",
      "Epoch 30, Batch 48, LR 0.701868 Loss 4.008986, Accuracy 91.667%\n",
      "Epoch 30, Batch 49, LR 0.701748 Loss 4.022582, Accuracy 91.614%\n",
      "Epoch 30, Batch 50, LR 0.701628 Loss 4.027775, Accuracy 91.562%\n",
      "Epoch 30, Batch 51, LR 0.701507 Loss 4.033926, Accuracy 91.544%\n",
      "Epoch 30, Batch 52, LR 0.701387 Loss 4.025582, Accuracy 91.587%\n",
      "Epoch 30, Batch 53, LR 0.701267 Loss 4.031497, Accuracy 91.568%\n",
      "Epoch 30, Batch 54, LR 0.701146 Loss 4.030165, Accuracy 91.536%\n",
      "Epoch 30, Batch 55, LR 0.701026 Loss 4.025336, Accuracy 91.534%\n",
      "Epoch 30, Batch 56, LR 0.700906 Loss 4.009660, Accuracy 91.616%\n",
      "Epoch 30, Batch 57, LR 0.700785 Loss 4.016422, Accuracy 91.584%\n",
      "Epoch 30, Batch 58, LR 0.700665 Loss 4.018484, Accuracy 91.595%\n",
      "Epoch 30, Batch 59, LR 0.700545 Loss 4.023424, Accuracy 91.578%\n",
      "Epoch 30, Batch 60, LR 0.700424 Loss 4.020101, Accuracy 91.589%\n",
      "Epoch 30, Batch 61, LR 0.700304 Loss 4.018594, Accuracy 91.611%\n",
      "Epoch 30, Batch 62, LR 0.700184 Loss 4.026985, Accuracy 91.595%\n",
      "Epoch 30, Batch 63, LR 0.700063 Loss 4.030373, Accuracy 91.580%\n",
      "Epoch 30, Batch 64, LR 0.699943 Loss 4.027834, Accuracy 91.626%\n",
      "Epoch 30, Batch 65, LR 0.699823 Loss 4.023188, Accuracy 91.623%\n",
      "Epoch 30, Batch 66, LR 0.699703 Loss 4.017748, Accuracy 91.655%\n",
      "Epoch 30, Batch 67, LR 0.699582 Loss 4.019696, Accuracy 91.674%\n",
      "Epoch 30, Batch 68, LR 0.699462 Loss 4.031078, Accuracy 91.636%\n",
      "Epoch 30, Batch 69, LR 0.699342 Loss 4.032647, Accuracy 91.667%\n",
      "Epoch 30, Batch 70, LR 0.699221 Loss 4.035010, Accuracy 91.663%\n",
      "Epoch 30, Batch 71, LR 0.699101 Loss 4.047852, Accuracy 91.593%\n",
      "Epoch 30, Batch 72, LR 0.698981 Loss 4.050951, Accuracy 91.602%\n",
      "Epoch 30, Batch 73, LR 0.698861 Loss 4.054187, Accuracy 91.610%\n",
      "Epoch 30, Batch 74, LR 0.698741 Loss 4.061006, Accuracy 91.543%\n",
      "Epoch 30, Batch 75, LR 0.698620 Loss 4.066342, Accuracy 91.552%\n",
      "Epoch 30, Batch 76, LR 0.698500 Loss 4.073454, Accuracy 91.509%\n",
      "Epoch 30, Batch 77, LR 0.698380 Loss 4.070529, Accuracy 91.487%\n",
      "Epoch 30, Batch 78, LR 0.698260 Loss 4.069760, Accuracy 91.446%\n",
      "Epoch 30, Batch 79, LR 0.698140 Loss 4.065786, Accuracy 91.466%\n",
      "Epoch 30, Batch 80, LR 0.698019 Loss 4.070596, Accuracy 91.475%\n",
      "Epoch 30, Batch 81, LR 0.697899 Loss 4.067034, Accuracy 91.483%\n",
      "Epoch 30, Batch 82, LR 0.697779 Loss 4.062597, Accuracy 91.511%\n",
      "Epoch 30, Batch 83, LR 0.697659 Loss 4.057954, Accuracy 91.538%\n",
      "Epoch 30, Batch 84, LR 0.697539 Loss 4.056025, Accuracy 91.546%\n",
      "Epoch 30, Batch 85, LR 0.697418 Loss 4.053778, Accuracy 91.581%\n",
      "Epoch 30, Batch 86, LR 0.697298 Loss 4.059197, Accuracy 91.552%\n",
      "Epoch 30, Batch 87, LR 0.697178 Loss 4.060433, Accuracy 91.568%\n",
      "Epoch 30, Batch 88, LR 0.697058 Loss 4.058845, Accuracy 91.584%\n",
      "Epoch 30, Batch 89, LR 0.696938 Loss 4.063158, Accuracy 91.582%\n",
      "Epoch 30, Batch 90, LR 0.696818 Loss 4.062703, Accuracy 91.562%\n",
      "Epoch 30, Batch 91, LR 0.696698 Loss 4.064133, Accuracy 91.544%\n",
      "Epoch 30, Batch 92, LR 0.696578 Loss 4.066307, Accuracy 91.517%\n",
      "Epoch 30, Batch 93, LR 0.696457 Loss 4.069612, Accuracy 91.482%\n",
      "Epoch 30, Batch 94, LR 0.696337 Loss 4.083148, Accuracy 91.439%\n",
      "Epoch 30, Batch 95, LR 0.696217 Loss 4.084295, Accuracy 91.447%\n",
      "Epoch 30, Batch 96, LR 0.696097 Loss 4.082646, Accuracy 91.463%\n",
      "Epoch 30, Batch 97, LR 0.695977 Loss 4.076371, Accuracy 91.479%\n",
      "Epoch 30, Batch 98, LR 0.695857 Loss 4.076380, Accuracy 91.486%\n",
      "Epoch 30, Batch 99, LR 0.695737 Loss 4.076613, Accuracy 91.477%\n",
      "Epoch 30, Batch 100, LR 0.695617 Loss 4.065854, Accuracy 91.492%\n",
      "Epoch 30, Batch 101, LR 0.695497 Loss 4.065718, Accuracy 91.499%\n",
      "Epoch 30, Batch 102, LR 0.695377 Loss 4.067685, Accuracy 91.475%\n",
      "Epoch 30, Batch 103, LR 0.695257 Loss 4.073531, Accuracy 91.475%\n",
      "Epoch 30, Batch 104, LR 0.695137 Loss 4.073864, Accuracy 91.474%\n",
      "Epoch 30, Batch 105, LR 0.695017 Loss 4.072218, Accuracy 91.510%\n",
      "Epoch 30, Batch 106, LR 0.694897 Loss 4.077028, Accuracy 91.517%\n",
      "Epoch 30, Batch 107, LR 0.694777 Loss 4.077868, Accuracy 91.530%\n",
      "Epoch 30, Batch 108, LR 0.694657 Loss 4.075884, Accuracy 91.565%\n",
      "Epoch 30, Batch 109, LR 0.694537 Loss 4.076420, Accuracy 91.542%\n",
      "Epoch 30, Batch 110, LR 0.694417 Loss 4.075593, Accuracy 91.534%\n",
      "Epoch 30, Batch 111, LR 0.694297 Loss 4.073918, Accuracy 91.547%\n",
      "Epoch 30, Batch 112, LR 0.694177 Loss 4.072271, Accuracy 91.567%\n",
      "Epoch 30, Batch 113, LR 0.694057 Loss 4.076606, Accuracy 91.551%\n",
      "Epoch 30, Batch 114, LR 0.693937 Loss 4.076953, Accuracy 91.530%\n",
      "Epoch 30, Batch 115, LR 0.693817 Loss 4.075529, Accuracy 91.535%\n",
      "Epoch 30, Batch 116, LR 0.693697 Loss 4.077526, Accuracy 91.521%\n",
      "Epoch 30, Batch 117, LR 0.693577 Loss 4.075989, Accuracy 91.506%\n",
      "Epoch 30, Batch 118, LR 0.693457 Loss 4.078749, Accuracy 91.506%\n",
      "Epoch 30, Batch 119, LR 0.693337 Loss 4.083920, Accuracy 91.498%\n",
      "Epoch 30, Batch 120, LR 0.693217 Loss 4.083318, Accuracy 91.523%\n",
      "Epoch 30, Batch 121, LR 0.693097 Loss 4.081635, Accuracy 91.529%\n",
      "Epoch 30, Batch 122, LR 0.692977 Loss 4.082629, Accuracy 91.515%\n",
      "Epoch 30, Batch 123, LR 0.692857 Loss 4.086576, Accuracy 91.514%\n",
      "Epoch 30, Batch 124, LR 0.692737 Loss 4.084510, Accuracy 91.532%\n",
      "Epoch 30, Batch 125, LR 0.692617 Loss 4.088919, Accuracy 91.487%\n",
      "Epoch 30, Batch 126, LR 0.692498 Loss 4.083537, Accuracy 91.512%\n",
      "Epoch 30, Batch 127, LR 0.692378 Loss 4.091358, Accuracy 91.486%\n",
      "Epoch 30, Batch 128, LR 0.692258 Loss 4.093826, Accuracy 91.486%\n",
      "Epoch 30, Batch 129, LR 0.692138 Loss 4.091172, Accuracy 91.509%\n",
      "Epoch 30, Batch 130, LR 0.692018 Loss 4.088872, Accuracy 91.508%\n",
      "Epoch 30, Batch 131, LR 0.691898 Loss 4.088845, Accuracy 91.496%\n",
      "Epoch 30, Batch 132, LR 0.691778 Loss 4.091943, Accuracy 91.489%\n",
      "Epoch 30, Batch 133, LR 0.691658 Loss 4.091212, Accuracy 91.483%\n",
      "Epoch 30, Batch 134, LR 0.691539 Loss 4.088238, Accuracy 91.494%\n",
      "Epoch 30, Batch 135, LR 0.691419 Loss 4.086853, Accuracy 91.493%\n",
      "Epoch 30, Batch 136, LR 0.691299 Loss 4.092366, Accuracy 91.446%\n",
      "Epoch 30, Batch 137, LR 0.691179 Loss 4.095637, Accuracy 91.446%\n",
      "Epoch 30, Batch 138, LR 0.691059 Loss 4.096208, Accuracy 91.423%\n",
      "Epoch 30, Batch 139, LR 0.690939 Loss 4.099300, Accuracy 91.412%\n",
      "Epoch 30, Batch 140, LR 0.690820 Loss 4.100439, Accuracy 91.412%\n",
      "Epoch 30, Batch 141, LR 0.690700 Loss 4.104841, Accuracy 91.390%\n",
      "Epoch 30, Batch 142, LR 0.690580 Loss 4.106952, Accuracy 91.379%\n",
      "Epoch 30, Batch 143, LR 0.690460 Loss 4.108823, Accuracy 91.373%\n",
      "Epoch 30, Batch 144, LR 0.690341 Loss 4.107158, Accuracy 91.390%\n",
      "Epoch 30, Batch 145, LR 0.690221 Loss 4.108522, Accuracy 91.385%\n",
      "Epoch 30, Batch 146, LR 0.690101 Loss 4.108275, Accuracy 91.363%\n",
      "Epoch 30, Batch 147, LR 0.689981 Loss 4.110245, Accuracy 91.358%\n",
      "Epoch 30, Batch 148, LR 0.689861 Loss 4.107629, Accuracy 91.375%\n",
      "Epoch 30, Batch 149, LR 0.689742 Loss 4.108054, Accuracy 91.375%\n",
      "Epoch 30, Batch 150, LR 0.689622 Loss 4.113156, Accuracy 91.359%\n",
      "Epoch 30, Batch 151, LR 0.689502 Loss 4.111622, Accuracy 91.365%\n",
      "Epoch 30, Batch 152, LR 0.689383 Loss 4.114074, Accuracy 91.365%\n",
      "Epoch 30, Batch 153, LR 0.689263 Loss 4.113072, Accuracy 91.381%\n",
      "Epoch 30, Batch 154, LR 0.689143 Loss 4.111314, Accuracy 91.396%\n",
      "Epoch 30, Batch 155, LR 0.689023 Loss 4.111297, Accuracy 91.391%\n",
      "Epoch 30, Batch 156, LR 0.688904 Loss 4.107823, Accuracy 91.396%\n",
      "Epoch 30, Batch 157, LR 0.688784 Loss 4.105964, Accuracy 91.406%\n",
      "Epoch 30, Batch 158, LR 0.688664 Loss 4.106830, Accuracy 91.396%\n",
      "Epoch 30, Batch 159, LR 0.688545 Loss 4.110808, Accuracy 91.392%\n",
      "Epoch 30, Batch 160, LR 0.688425 Loss 4.106418, Accuracy 91.411%\n",
      "Epoch 30, Batch 161, LR 0.688305 Loss 4.105074, Accuracy 91.416%\n",
      "Epoch 30, Batch 162, LR 0.688186 Loss 4.103432, Accuracy 91.421%\n",
      "Epoch 30, Batch 163, LR 0.688066 Loss 4.102203, Accuracy 91.425%\n",
      "Epoch 30, Batch 164, LR 0.687946 Loss 4.100966, Accuracy 91.435%\n",
      "Epoch 30, Batch 165, LR 0.687827 Loss 4.102305, Accuracy 91.439%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 166, LR 0.687707 Loss 4.102263, Accuracy 91.425%\n",
      "Epoch 30, Batch 167, LR 0.687587 Loss 4.102071, Accuracy 91.425%\n",
      "Epoch 30, Batch 168, LR 0.687468 Loss 4.100124, Accuracy 91.439%\n",
      "Epoch 30, Batch 169, LR 0.687348 Loss 4.100381, Accuracy 91.448%\n",
      "Epoch 30, Batch 170, LR 0.687229 Loss 4.100873, Accuracy 91.434%\n",
      "Epoch 30, Batch 171, LR 0.687109 Loss 4.103579, Accuracy 91.420%\n",
      "Epoch 30, Batch 172, LR 0.686989 Loss 4.099676, Accuracy 91.452%\n",
      "Epoch 30, Batch 173, LR 0.686870 Loss 4.098182, Accuracy 91.456%\n",
      "Epoch 30, Batch 174, LR 0.686750 Loss 4.098338, Accuracy 91.451%\n",
      "Epoch 30, Batch 175, LR 0.686631 Loss 4.096187, Accuracy 91.464%\n",
      "Epoch 30, Batch 176, LR 0.686511 Loss 4.092866, Accuracy 91.482%\n",
      "Epoch 30, Batch 177, LR 0.686391 Loss 4.089759, Accuracy 91.490%\n",
      "Epoch 30, Batch 178, LR 0.686272 Loss 4.091781, Accuracy 91.459%\n",
      "Epoch 30, Batch 179, LR 0.686152 Loss 4.090944, Accuracy 91.454%\n",
      "Epoch 30, Batch 180, LR 0.686033 Loss 4.090832, Accuracy 91.463%\n",
      "Epoch 30, Batch 181, LR 0.685913 Loss 4.091762, Accuracy 91.462%\n",
      "Epoch 30, Batch 182, LR 0.685794 Loss 4.091514, Accuracy 91.458%\n",
      "Epoch 30, Batch 183, LR 0.685674 Loss 4.088914, Accuracy 91.479%\n",
      "Epoch 30, Batch 184, LR 0.685555 Loss 4.088438, Accuracy 91.491%\n",
      "Epoch 30, Batch 185, LR 0.685435 Loss 4.090278, Accuracy 91.491%\n",
      "Epoch 30, Batch 186, LR 0.685316 Loss 4.089009, Accuracy 91.490%\n",
      "Epoch 30, Batch 187, LR 0.685196 Loss 4.087071, Accuracy 91.490%\n",
      "Epoch 30, Batch 188, LR 0.685077 Loss 4.084701, Accuracy 91.514%\n",
      "Epoch 30, Batch 189, LR 0.684957 Loss 4.082279, Accuracy 91.522%\n",
      "Epoch 30, Batch 190, LR 0.684838 Loss 4.079855, Accuracy 91.530%\n",
      "Epoch 30, Batch 191, LR 0.684718 Loss 4.082521, Accuracy 91.521%\n",
      "Epoch 30, Batch 192, LR 0.684599 Loss 4.083401, Accuracy 91.508%\n",
      "Epoch 30, Batch 193, LR 0.684479 Loss 4.080746, Accuracy 91.520%\n",
      "Epoch 30, Batch 194, LR 0.684360 Loss 4.081321, Accuracy 91.507%\n",
      "Epoch 30, Batch 195, LR 0.684240 Loss 4.078334, Accuracy 91.522%\n",
      "Epoch 30, Batch 196, LR 0.684121 Loss 4.076220, Accuracy 91.542%\n",
      "Epoch 30, Batch 197, LR 0.684001 Loss 4.073707, Accuracy 91.549%\n",
      "Epoch 30, Batch 198, LR 0.683882 Loss 4.077294, Accuracy 91.517%\n",
      "Epoch 30, Batch 199, LR 0.683763 Loss 4.076460, Accuracy 91.524%\n",
      "Epoch 30, Batch 200, LR 0.683643 Loss 4.076157, Accuracy 91.531%\n",
      "Epoch 30, Batch 201, LR 0.683524 Loss 4.078439, Accuracy 91.500%\n",
      "Epoch 30, Batch 202, LR 0.683404 Loss 4.078900, Accuracy 91.511%\n",
      "Epoch 30, Batch 203, LR 0.683285 Loss 4.074365, Accuracy 91.526%\n",
      "Epoch 30, Batch 204, LR 0.683166 Loss 4.074496, Accuracy 91.525%\n",
      "Epoch 30, Batch 205, LR 0.683046 Loss 4.075700, Accuracy 91.509%\n",
      "Epoch 30, Batch 206, LR 0.682927 Loss 4.079136, Accuracy 91.490%\n",
      "Epoch 30, Batch 207, LR 0.682807 Loss 4.076546, Accuracy 91.512%\n",
      "Epoch 30, Batch 208, LR 0.682688 Loss 4.077226, Accuracy 91.515%\n",
      "Epoch 30, Batch 209, LR 0.682569 Loss 4.074187, Accuracy 91.522%\n",
      "Epoch 30, Batch 210, LR 0.682449 Loss 4.075049, Accuracy 91.510%\n",
      "Epoch 30, Batch 211, LR 0.682330 Loss 4.074684, Accuracy 91.510%\n",
      "Epoch 30, Batch 212, LR 0.682211 Loss 4.077610, Accuracy 91.495%\n",
      "Epoch 30, Batch 213, LR 0.682091 Loss 4.074131, Accuracy 91.502%\n",
      "Epoch 30, Batch 214, LR 0.681972 Loss 4.070533, Accuracy 91.512%\n",
      "Epoch 30, Batch 215, LR 0.681853 Loss 4.067216, Accuracy 91.508%\n",
      "Epoch 30, Batch 216, LR 0.681733 Loss 4.068151, Accuracy 91.515%\n",
      "Epoch 30, Batch 217, LR 0.681614 Loss 4.067309, Accuracy 91.507%\n",
      "Epoch 30, Batch 218, LR 0.681495 Loss 4.064565, Accuracy 91.525%\n",
      "Epoch 30, Batch 219, LR 0.681375 Loss 4.061539, Accuracy 91.538%\n",
      "Epoch 30, Batch 220, LR 0.681256 Loss 4.062382, Accuracy 91.545%\n",
      "Epoch 30, Batch 221, LR 0.681137 Loss 4.062686, Accuracy 91.555%\n",
      "Epoch 30, Batch 222, LR 0.681018 Loss 4.066567, Accuracy 91.543%\n",
      "Epoch 30, Batch 223, LR 0.680898 Loss 4.069523, Accuracy 91.539%\n",
      "Epoch 30, Batch 224, LR 0.680779 Loss 4.070487, Accuracy 91.532%\n",
      "Epoch 30, Batch 225, LR 0.680660 Loss 4.071139, Accuracy 91.535%\n",
      "Epoch 30, Batch 226, LR 0.680541 Loss 4.069563, Accuracy 91.534%\n",
      "Epoch 30, Batch 227, LR 0.680421 Loss 4.067378, Accuracy 91.554%\n",
      "Epoch 30, Batch 228, LR 0.680302 Loss 4.066612, Accuracy 91.550%\n",
      "Epoch 30, Batch 229, LR 0.680183 Loss 4.063632, Accuracy 91.567%\n",
      "Epoch 30, Batch 230, LR 0.680064 Loss 4.065205, Accuracy 91.562%\n",
      "Epoch 30, Batch 231, LR 0.679944 Loss 4.065518, Accuracy 91.572%\n",
      "Epoch 30, Batch 232, LR 0.679825 Loss 4.064492, Accuracy 91.568%\n",
      "Epoch 30, Batch 233, LR 0.679706 Loss 4.064775, Accuracy 91.567%\n",
      "Epoch 30, Batch 234, LR 0.679587 Loss 4.064815, Accuracy 91.570%\n",
      "Epoch 30, Batch 235, LR 0.679468 Loss 4.062038, Accuracy 91.582%\n",
      "Epoch 30, Batch 236, LR 0.679348 Loss 4.063391, Accuracy 91.578%\n",
      "Epoch 30, Batch 237, LR 0.679229 Loss 4.064978, Accuracy 91.564%\n",
      "Epoch 30, Batch 238, LR 0.679110 Loss 4.062663, Accuracy 91.570%\n",
      "Epoch 30, Batch 239, LR 0.678991 Loss 4.064522, Accuracy 91.560%\n",
      "Epoch 30, Batch 240, LR 0.678872 Loss 4.064927, Accuracy 91.572%\n",
      "Epoch 30, Batch 241, LR 0.678753 Loss 4.065734, Accuracy 91.572%\n",
      "Epoch 30, Batch 242, LR 0.678634 Loss 4.065818, Accuracy 91.577%\n",
      "Epoch 30, Batch 243, LR 0.678514 Loss 4.066713, Accuracy 91.567%\n",
      "Epoch 30, Batch 244, LR 0.678395 Loss 4.069321, Accuracy 91.557%\n",
      "Epoch 30, Batch 245, LR 0.678276 Loss 4.068151, Accuracy 91.572%\n",
      "Epoch 30, Batch 246, LR 0.678157 Loss 4.068347, Accuracy 91.571%\n",
      "Epoch 30, Batch 247, LR 0.678038 Loss 4.068719, Accuracy 91.583%\n",
      "Epoch 30, Batch 248, LR 0.677919 Loss 4.067944, Accuracy 91.586%\n",
      "Epoch 30, Batch 249, LR 0.677800 Loss 4.068748, Accuracy 91.588%\n",
      "Epoch 30, Batch 250, LR 0.677681 Loss 4.069940, Accuracy 91.581%\n",
      "Epoch 30, Batch 251, LR 0.677562 Loss 4.068429, Accuracy 91.584%\n",
      "Epoch 30, Batch 252, LR 0.677442 Loss 4.070724, Accuracy 91.561%\n",
      "Epoch 30, Batch 253, LR 0.677323 Loss 4.069526, Accuracy 91.548%\n",
      "Epoch 30, Batch 254, LR 0.677204 Loss 4.070252, Accuracy 91.566%\n",
      "Epoch 30, Batch 255, LR 0.677085 Loss 4.071325, Accuracy 91.559%\n",
      "Epoch 30, Batch 256, LR 0.676966 Loss 4.069573, Accuracy 91.571%\n",
      "Epoch 30, Batch 257, LR 0.676847 Loss 4.071714, Accuracy 91.564%\n",
      "Epoch 30, Batch 258, LR 0.676728 Loss 4.073569, Accuracy 91.549%\n",
      "Epoch 30, Batch 259, LR 0.676609 Loss 4.073761, Accuracy 91.548%\n",
      "Epoch 30, Batch 260, LR 0.676490 Loss 4.071888, Accuracy 91.550%\n",
      "Epoch 30, Batch 261, LR 0.676371 Loss 4.069865, Accuracy 91.562%\n",
      "Epoch 30, Batch 262, LR 0.676252 Loss 4.069771, Accuracy 91.567%\n",
      "Epoch 30, Batch 263, LR 0.676133 Loss 4.069981, Accuracy 91.567%\n",
      "Epoch 30, Batch 264, LR 0.676014 Loss 4.071300, Accuracy 91.554%\n",
      "Epoch 30, Batch 265, LR 0.675895 Loss 4.070351, Accuracy 91.557%\n",
      "Epoch 30, Batch 266, LR 0.675776 Loss 4.071150, Accuracy 91.553%\n",
      "Epoch 30, Batch 267, LR 0.675657 Loss 4.069948, Accuracy 91.561%\n",
      "Epoch 30, Batch 268, LR 0.675538 Loss 4.067708, Accuracy 91.561%\n",
      "Epoch 30, Batch 269, LR 0.675419 Loss 4.066940, Accuracy 91.572%\n",
      "Epoch 30, Batch 270, LR 0.675300 Loss 4.067114, Accuracy 91.577%\n",
      "Epoch 30, Batch 271, LR 0.675181 Loss 4.067812, Accuracy 91.573%\n",
      "Epoch 30, Batch 272, LR 0.675062 Loss 4.067072, Accuracy 91.570%\n",
      "Epoch 30, Batch 273, LR 0.674943 Loss 4.065457, Accuracy 91.578%\n",
      "Epoch 30, Batch 274, LR 0.674824 Loss 4.063729, Accuracy 91.597%\n",
      "Epoch 30, Batch 275, LR 0.674705 Loss 4.063251, Accuracy 91.605%\n",
      "Epoch 30, Batch 276, LR 0.674587 Loss 4.062352, Accuracy 91.599%\n",
      "Epoch 30, Batch 277, LR 0.674468 Loss 4.060725, Accuracy 91.601%\n",
      "Epoch 30, Batch 278, LR 0.674349 Loss 4.060349, Accuracy 91.603%\n",
      "Epoch 30, Batch 279, LR 0.674230 Loss 4.061563, Accuracy 91.591%\n",
      "Epoch 30, Batch 280, LR 0.674111 Loss 4.061639, Accuracy 91.582%\n",
      "Epoch 30, Batch 281, LR 0.673992 Loss 4.060969, Accuracy 91.587%\n",
      "Epoch 30, Batch 282, LR 0.673873 Loss 4.063492, Accuracy 91.575%\n",
      "Epoch 30, Batch 283, LR 0.673754 Loss 4.065800, Accuracy 91.569%\n",
      "Epoch 30, Batch 284, LR 0.673635 Loss 4.067103, Accuracy 91.555%\n",
      "Epoch 30, Batch 285, LR 0.673517 Loss 4.071219, Accuracy 91.532%\n",
      "Epoch 30, Batch 286, LR 0.673398 Loss 4.072260, Accuracy 91.524%\n",
      "Epoch 30, Batch 287, LR 0.673279 Loss 4.071512, Accuracy 91.526%\n",
      "Epoch 30, Batch 288, LR 0.673160 Loss 4.071983, Accuracy 91.523%\n",
      "Epoch 30, Batch 289, LR 0.673041 Loss 4.071765, Accuracy 91.531%\n",
      "Epoch 30, Batch 290, LR 0.672922 Loss 4.073128, Accuracy 91.519%\n",
      "Epoch 30, Batch 291, LR 0.672804 Loss 4.072283, Accuracy 91.527%\n",
      "Epoch 30, Batch 292, LR 0.672685 Loss 4.071084, Accuracy 91.527%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 293, LR 0.672566 Loss 4.070800, Accuracy 91.532%\n",
      "Epoch 30, Batch 294, LR 0.672447 Loss 4.070361, Accuracy 91.539%\n",
      "Epoch 30, Batch 295, LR 0.672328 Loss 4.070006, Accuracy 91.541%\n",
      "Epoch 30, Batch 296, LR 0.672210 Loss 4.070668, Accuracy 91.533%\n",
      "Epoch 30, Batch 297, LR 0.672091 Loss 4.073161, Accuracy 91.522%\n",
      "Epoch 30, Batch 298, LR 0.671972 Loss 4.072940, Accuracy 91.524%\n",
      "Epoch 30, Batch 299, LR 0.671853 Loss 4.074316, Accuracy 91.516%\n",
      "Epoch 30, Batch 300, LR 0.671734 Loss 4.075838, Accuracy 91.510%\n",
      "Epoch 30, Batch 301, LR 0.671616 Loss 4.077616, Accuracy 91.497%\n",
      "Epoch 30, Batch 302, LR 0.671497 Loss 4.078471, Accuracy 91.489%\n",
      "Epoch 30, Batch 303, LR 0.671378 Loss 4.078280, Accuracy 91.484%\n",
      "Epoch 30, Batch 304, LR 0.671260 Loss 4.079822, Accuracy 91.473%\n",
      "Epoch 30, Batch 305, LR 0.671141 Loss 4.082293, Accuracy 91.460%\n",
      "Epoch 30, Batch 306, LR 0.671022 Loss 4.083689, Accuracy 91.447%\n",
      "Epoch 30, Batch 307, LR 0.670903 Loss 4.082213, Accuracy 91.457%\n",
      "Epoch 30, Batch 308, LR 0.670785 Loss 4.081879, Accuracy 91.452%\n",
      "Epoch 30, Batch 309, LR 0.670666 Loss 4.082031, Accuracy 91.449%\n",
      "Epoch 30, Batch 310, LR 0.670547 Loss 4.082195, Accuracy 91.449%\n",
      "Epoch 30, Batch 311, LR 0.670429 Loss 4.081126, Accuracy 91.449%\n",
      "Epoch 30, Batch 312, LR 0.670310 Loss 4.080029, Accuracy 91.449%\n",
      "Epoch 30, Batch 313, LR 0.670191 Loss 4.079447, Accuracy 91.454%\n",
      "Epoch 30, Batch 314, LR 0.670073 Loss 4.078167, Accuracy 91.458%\n",
      "Epoch 30, Batch 315, LR 0.669954 Loss 4.078480, Accuracy 91.463%\n",
      "Epoch 30, Batch 316, LR 0.669835 Loss 4.077977, Accuracy 91.461%\n",
      "Epoch 30, Batch 317, LR 0.669717 Loss 4.077804, Accuracy 91.453%\n",
      "Epoch 30, Batch 318, LR 0.669598 Loss 4.077814, Accuracy 91.458%\n",
      "Epoch 30, Batch 319, LR 0.669479 Loss 4.077269, Accuracy 91.465%\n",
      "Epoch 30, Batch 320, LR 0.669361 Loss 4.075323, Accuracy 91.472%\n",
      "Epoch 30, Batch 321, LR 0.669242 Loss 4.073481, Accuracy 91.489%\n",
      "Epoch 30, Batch 322, LR 0.669123 Loss 4.073506, Accuracy 91.481%\n",
      "Epoch 30, Batch 323, LR 0.669005 Loss 4.073112, Accuracy 91.486%\n",
      "Epoch 30, Batch 324, LR 0.668886 Loss 4.072676, Accuracy 91.488%\n",
      "Epoch 30, Batch 325, LR 0.668768 Loss 4.072873, Accuracy 91.500%\n",
      "Epoch 30, Batch 326, LR 0.668649 Loss 4.075137, Accuracy 91.488%\n",
      "Epoch 30, Batch 327, LR 0.668530 Loss 4.073873, Accuracy 91.499%\n",
      "Epoch 30, Batch 328, LR 0.668412 Loss 4.074258, Accuracy 91.502%\n",
      "Epoch 30, Batch 329, LR 0.668293 Loss 4.071915, Accuracy 91.518%\n",
      "Epoch 30, Batch 330, LR 0.668175 Loss 4.070289, Accuracy 91.529%\n",
      "Epoch 30, Batch 331, LR 0.668056 Loss 4.065667, Accuracy 91.543%\n",
      "Epoch 30, Batch 332, LR 0.667938 Loss 4.065211, Accuracy 91.554%\n",
      "Epoch 30, Batch 333, LR 0.667819 Loss 4.062764, Accuracy 91.561%\n",
      "Epoch 30, Batch 334, LR 0.667701 Loss 4.061302, Accuracy 91.556%\n",
      "Epoch 30, Batch 335, LR 0.667582 Loss 4.061403, Accuracy 91.556%\n",
      "Epoch 30, Batch 336, LR 0.667464 Loss 4.060781, Accuracy 91.550%\n",
      "Epoch 30, Batch 337, LR 0.667345 Loss 4.059432, Accuracy 91.559%\n",
      "Epoch 30, Batch 338, LR 0.667227 Loss 4.057088, Accuracy 91.577%\n",
      "Epoch 30, Batch 339, LR 0.667108 Loss 4.056329, Accuracy 91.577%\n",
      "Epoch 30, Batch 340, LR 0.666990 Loss 4.056079, Accuracy 91.576%\n",
      "Epoch 30, Batch 341, LR 0.666871 Loss 4.056621, Accuracy 91.578%\n",
      "Epoch 30, Batch 342, LR 0.666753 Loss 4.058033, Accuracy 91.575%\n",
      "Epoch 30, Batch 343, LR 0.666634 Loss 4.057745, Accuracy 91.584%\n",
      "Epoch 30, Batch 344, LR 0.666516 Loss 4.057667, Accuracy 91.590%\n",
      "Epoch 30, Batch 345, LR 0.666397 Loss 4.057898, Accuracy 91.594%\n",
      "Epoch 30, Batch 346, LR 0.666279 Loss 4.058043, Accuracy 91.589%\n",
      "Epoch 30, Batch 347, LR 0.666160 Loss 4.057182, Accuracy 91.593%\n",
      "Epoch 30, Batch 348, LR 0.666042 Loss 4.056576, Accuracy 91.595%\n",
      "Epoch 30, Batch 349, LR 0.665923 Loss 4.056480, Accuracy 91.592%\n",
      "Epoch 30, Batch 350, LR 0.665805 Loss 4.055534, Accuracy 91.598%\n",
      "Epoch 30, Batch 351, LR 0.665687 Loss 4.053930, Accuracy 91.607%\n",
      "Epoch 30, Batch 352, LR 0.665568 Loss 4.051884, Accuracy 91.613%\n",
      "Epoch 30, Batch 353, LR 0.665450 Loss 4.052540, Accuracy 91.610%\n",
      "Epoch 30, Batch 354, LR 0.665331 Loss 4.050373, Accuracy 91.618%\n",
      "Epoch 30, Batch 355, LR 0.665213 Loss 4.049740, Accuracy 91.624%\n",
      "Epoch 30, Batch 356, LR 0.665095 Loss 4.048495, Accuracy 91.632%\n",
      "Epoch 30, Batch 357, LR 0.664976 Loss 4.049764, Accuracy 91.625%\n",
      "Epoch 30, Batch 358, LR 0.664858 Loss 4.049574, Accuracy 91.633%\n",
      "Epoch 30, Batch 359, LR 0.664739 Loss 4.048500, Accuracy 91.648%\n",
      "Epoch 30, Batch 360, LR 0.664621 Loss 4.050618, Accuracy 91.643%\n",
      "Epoch 30, Batch 361, LR 0.664503 Loss 4.051180, Accuracy 91.642%\n",
      "Epoch 30, Batch 362, LR 0.664384 Loss 4.050244, Accuracy 91.650%\n",
      "Epoch 30, Batch 363, LR 0.664266 Loss 4.050569, Accuracy 91.647%\n",
      "Epoch 30, Batch 364, LR 0.664148 Loss 4.050885, Accuracy 91.642%\n",
      "Epoch 30, Batch 365, LR 0.664029 Loss 4.050498, Accuracy 91.646%\n",
      "Epoch 30, Batch 366, LR 0.663911 Loss 4.049552, Accuracy 91.650%\n",
      "Epoch 30, Batch 367, LR 0.663793 Loss 4.048529, Accuracy 91.655%\n",
      "Epoch 30, Batch 368, LR 0.663674 Loss 4.050312, Accuracy 91.648%\n",
      "Epoch 30, Batch 369, LR 0.663556 Loss 4.051645, Accuracy 91.637%\n",
      "Epoch 30, Batch 370, LR 0.663438 Loss 4.051853, Accuracy 91.630%\n",
      "Epoch 30, Batch 371, LR 0.663320 Loss 4.052150, Accuracy 91.642%\n",
      "Epoch 30, Batch 372, LR 0.663201 Loss 4.053377, Accuracy 91.646%\n",
      "Epoch 30, Batch 373, LR 0.663083 Loss 4.053035, Accuracy 91.651%\n",
      "Epoch 30, Batch 374, LR 0.662965 Loss 4.052851, Accuracy 91.661%\n",
      "Epoch 30, Batch 375, LR 0.662846 Loss 4.051631, Accuracy 91.667%\n",
      "Epoch 30, Batch 376, LR 0.662728 Loss 4.052329, Accuracy 91.666%\n",
      "Epoch 30, Batch 377, LR 0.662610 Loss 4.051204, Accuracy 91.667%\n",
      "Epoch 30, Batch 378, LR 0.662492 Loss 4.049983, Accuracy 91.681%\n",
      "Epoch 30, Batch 379, LR 0.662374 Loss 4.049705, Accuracy 91.676%\n",
      "Epoch 30, Batch 380, LR 0.662255 Loss 4.048704, Accuracy 91.678%\n",
      "Epoch 30, Batch 381, LR 0.662137 Loss 4.048371, Accuracy 91.681%\n",
      "Epoch 30, Batch 382, LR 0.662019 Loss 4.049503, Accuracy 91.670%\n",
      "Epoch 30, Batch 383, LR 0.661901 Loss 4.050514, Accuracy 91.665%\n",
      "Epoch 30, Batch 384, LR 0.661782 Loss 4.051914, Accuracy 91.654%\n",
      "Epoch 30, Batch 385, LR 0.661664 Loss 4.052697, Accuracy 91.660%\n",
      "Epoch 30, Batch 386, LR 0.661546 Loss 4.050461, Accuracy 91.669%\n",
      "Epoch 30, Batch 387, LR 0.661428 Loss 4.051747, Accuracy 91.667%\n",
      "Epoch 30, Batch 388, LR 0.661310 Loss 4.050989, Accuracy 91.670%\n",
      "Epoch 30, Batch 389, LR 0.661192 Loss 4.053452, Accuracy 91.659%\n",
      "Epoch 30, Batch 390, LR 0.661073 Loss 4.052592, Accuracy 91.653%\n",
      "Epoch 30, Batch 391, LR 0.660955 Loss 4.051295, Accuracy 91.658%\n",
      "Epoch 30, Batch 392, LR 0.660837 Loss 4.052404, Accuracy 91.661%\n",
      "Epoch 30, Batch 393, LR 0.660719 Loss 4.052998, Accuracy 91.659%\n",
      "Epoch 30, Batch 394, LR 0.660601 Loss 4.052857, Accuracy 91.656%\n",
      "Epoch 30, Batch 395, LR 0.660483 Loss 4.053259, Accuracy 91.653%\n",
      "Epoch 30, Batch 396, LR 0.660365 Loss 4.053279, Accuracy 91.657%\n",
      "Epoch 30, Batch 397, LR 0.660246 Loss 4.054927, Accuracy 91.656%\n",
      "Epoch 30, Batch 398, LR 0.660128 Loss 4.055985, Accuracy 91.658%\n",
      "Epoch 30, Batch 399, LR 0.660010 Loss 4.054281, Accuracy 91.667%\n",
      "Epoch 30, Batch 400, LR 0.659892 Loss 4.052577, Accuracy 91.672%\n",
      "Epoch 30, Batch 401, LR 0.659774 Loss 4.050871, Accuracy 91.683%\n",
      "Epoch 30, Batch 402, LR 0.659656 Loss 4.051085, Accuracy 91.678%\n",
      "Epoch 30, Batch 403, LR 0.659538 Loss 4.051866, Accuracy 91.672%\n",
      "Epoch 30, Batch 404, LR 0.659420 Loss 4.051336, Accuracy 91.675%\n",
      "Epoch 30, Batch 405, LR 0.659302 Loss 4.050303, Accuracy 91.682%\n",
      "Epoch 30, Batch 406, LR 0.659184 Loss 4.050040, Accuracy 91.687%\n",
      "Epoch 30, Batch 407, LR 0.659066 Loss 4.048797, Accuracy 91.696%\n",
      "Epoch 30, Batch 408, LR 0.658948 Loss 4.048649, Accuracy 91.693%\n",
      "Epoch 30, Batch 409, LR 0.658830 Loss 4.049728, Accuracy 91.691%\n",
      "Epoch 30, Batch 410, LR 0.658712 Loss 4.048571, Accuracy 91.690%\n",
      "Epoch 30, Batch 411, LR 0.658594 Loss 4.049938, Accuracy 91.684%\n",
      "Epoch 30, Batch 412, LR 0.658476 Loss 4.050092, Accuracy 91.685%\n",
      "Epoch 30, Batch 413, LR 0.658358 Loss 4.050477, Accuracy 91.682%\n",
      "Epoch 30, Batch 414, LR 0.658240 Loss 4.049629, Accuracy 91.689%\n",
      "Epoch 30, Batch 415, LR 0.658122 Loss 4.049414, Accuracy 91.692%\n",
      "Epoch 30, Batch 416, LR 0.658004 Loss 4.048829, Accuracy 91.694%\n",
      "Epoch 30, Batch 417, LR 0.657886 Loss 4.048051, Accuracy 91.699%\n",
      "Epoch 30, Batch 418, LR 0.657768 Loss 4.047629, Accuracy 91.700%\n",
      "Epoch 30, Batch 419, LR 0.657650 Loss 4.048323, Accuracy 91.697%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 420, LR 0.657532 Loss 4.048866, Accuracy 91.695%\n",
      "Epoch 30, Batch 421, LR 0.657414 Loss 4.048423, Accuracy 91.698%\n",
      "Epoch 30, Batch 422, LR 0.657296 Loss 4.046974, Accuracy 91.704%\n",
      "Epoch 30, Batch 423, LR 0.657178 Loss 4.047886, Accuracy 91.704%\n",
      "Epoch 30, Batch 424, LR 0.657060 Loss 4.047770, Accuracy 91.699%\n",
      "Epoch 30, Batch 425, LR 0.656942 Loss 4.046607, Accuracy 91.699%\n",
      "Epoch 30, Batch 426, LR 0.656824 Loss 4.046208, Accuracy 91.700%\n",
      "Epoch 30, Batch 427, LR 0.656706 Loss 4.047149, Accuracy 91.699%\n",
      "Epoch 30, Batch 428, LR 0.656588 Loss 4.047866, Accuracy 91.704%\n",
      "Epoch 30, Batch 429, LR 0.656471 Loss 4.048484, Accuracy 91.707%\n",
      "Epoch 30, Batch 430, LR 0.656353 Loss 4.047732, Accuracy 91.717%\n",
      "Epoch 30, Batch 431, LR 0.656235 Loss 4.046149, Accuracy 91.722%\n",
      "Epoch 30, Batch 432, LR 0.656117 Loss 4.046238, Accuracy 91.717%\n",
      "Epoch 30, Batch 433, LR 0.655999 Loss 4.046577, Accuracy 91.718%\n",
      "Epoch 30, Batch 434, LR 0.655881 Loss 4.045179, Accuracy 91.723%\n",
      "Epoch 30, Batch 435, LR 0.655763 Loss 4.045850, Accuracy 91.730%\n",
      "Epoch 30, Batch 436, LR 0.655646 Loss 4.045087, Accuracy 91.734%\n",
      "Epoch 30, Batch 437, LR 0.655528 Loss 4.045927, Accuracy 91.732%\n",
      "Epoch 30, Batch 438, LR 0.655410 Loss 4.046450, Accuracy 91.733%\n",
      "Epoch 30, Batch 439, LR 0.655292 Loss 4.045729, Accuracy 91.734%\n",
      "Epoch 30, Batch 440, LR 0.655174 Loss 4.048538, Accuracy 91.724%\n",
      "Epoch 30, Batch 441, LR 0.655056 Loss 4.048580, Accuracy 91.723%\n",
      "Epoch 30, Batch 442, LR 0.654939 Loss 4.048076, Accuracy 91.726%\n",
      "Epoch 30, Batch 443, LR 0.654821 Loss 4.047655, Accuracy 91.729%\n",
      "Epoch 30, Batch 444, LR 0.654703 Loss 4.046393, Accuracy 91.732%\n",
      "Epoch 30, Batch 445, LR 0.654585 Loss 4.045080, Accuracy 91.736%\n",
      "Epoch 30, Batch 446, LR 0.654467 Loss 4.044158, Accuracy 91.743%\n",
      "Epoch 30, Batch 447, LR 0.654350 Loss 4.044298, Accuracy 91.744%\n",
      "Epoch 30, Batch 448, LR 0.654232 Loss 4.044140, Accuracy 91.748%\n",
      "Epoch 30, Batch 449, LR 0.654114 Loss 4.043625, Accuracy 91.749%\n",
      "Epoch 30, Batch 450, LR 0.653996 Loss 4.043429, Accuracy 91.757%\n",
      "Epoch 30, Batch 451, LR 0.653879 Loss 4.042783, Accuracy 91.761%\n",
      "Epoch 30, Batch 452, LR 0.653761 Loss 4.042265, Accuracy 91.767%\n",
      "Epoch 30, Batch 453, LR 0.653643 Loss 4.041124, Accuracy 91.774%\n",
      "Epoch 30, Batch 454, LR 0.653525 Loss 4.040989, Accuracy 91.775%\n",
      "Epoch 30, Batch 455, LR 0.653408 Loss 4.040361, Accuracy 91.772%\n",
      "Epoch 30, Batch 456, LR 0.653290 Loss 4.041141, Accuracy 91.769%\n",
      "Epoch 30, Batch 457, LR 0.653172 Loss 4.041625, Accuracy 91.765%\n",
      "Epoch 30, Batch 458, LR 0.653055 Loss 4.041869, Accuracy 91.768%\n",
      "Epoch 30, Batch 459, LR 0.652937 Loss 4.040794, Accuracy 91.774%\n",
      "Epoch 30, Batch 460, LR 0.652819 Loss 4.040400, Accuracy 91.780%\n",
      "Epoch 30, Batch 461, LR 0.652702 Loss 4.040241, Accuracy 91.776%\n",
      "Epoch 30, Batch 462, LR 0.652584 Loss 4.039306, Accuracy 91.780%\n",
      "Epoch 30, Batch 463, LR 0.652466 Loss 4.039953, Accuracy 91.783%\n",
      "Epoch 30, Batch 464, LR 0.652349 Loss 4.040756, Accuracy 91.785%\n",
      "Epoch 30, Batch 465, LR 0.652231 Loss 4.040820, Accuracy 91.788%\n",
      "Epoch 30, Batch 466, LR 0.652113 Loss 4.040662, Accuracy 91.785%\n",
      "Epoch 30, Batch 467, LR 0.651996 Loss 4.040068, Accuracy 91.794%\n",
      "Epoch 30, Batch 468, LR 0.651878 Loss 4.039602, Accuracy 91.794%\n",
      "Epoch 30, Batch 469, LR 0.651760 Loss 4.038351, Accuracy 91.798%\n",
      "Epoch 30, Batch 470, LR 0.651643 Loss 4.037529, Accuracy 91.800%\n",
      "Epoch 30, Batch 471, LR 0.651525 Loss 4.036249, Accuracy 91.806%\n",
      "Epoch 30, Batch 472, LR 0.651408 Loss 4.037298, Accuracy 91.807%\n",
      "Epoch 30, Batch 473, LR 0.651290 Loss 4.036043, Accuracy 91.814%\n",
      "Epoch 30, Batch 474, LR 0.651172 Loss 4.036146, Accuracy 91.813%\n",
      "Epoch 30, Batch 475, LR 0.651055 Loss 4.035132, Accuracy 91.814%\n",
      "Epoch 30, Batch 476, LR 0.650937 Loss 4.034711, Accuracy 91.821%\n",
      "Epoch 30, Batch 477, LR 0.650820 Loss 4.035605, Accuracy 91.816%\n",
      "Epoch 30, Batch 478, LR 0.650702 Loss 4.036602, Accuracy 91.815%\n",
      "Epoch 30, Batch 479, LR 0.650585 Loss 4.037609, Accuracy 91.812%\n",
      "Epoch 30, Batch 480, LR 0.650467 Loss 4.038483, Accuracy 91.810%\n",
      "Epoch 30, Batch 481, LR 0.650350 Loss 4.039386, Accuracy 91.801%\n",
      "Epoch 30, Batch 482, LR 0.650232 Loss 4.040051, Accuracy 91.794%\n",
      "Epoch 30, Batch 483, LR 0.650115 Loss 4.038962, Accuracy 91.788%\n",
      "Epoch 30, Batch 484, LR 0.649997 Loss 4.038608, Accuracy 91.795%\n",
      "Epoch 30, Batch 485, LR 0.649879 Loss 4.039544, Accuracy 91.782%\n",
      "Epoch 30, Batch 486, LR 0.649762 Loss 4.038359, Accuracy 91.787%\n",
      "Epoch 30, Batch 487, LR 0.649645 Loss 4.038259, Accuracy 91.794%\n",
      "Epoch 30, Batch 488, LR 0.649527 Loss 4.038087, Accuracy 91.794%\n",
      "Epoch 30, Batch 489, LR 0.649410 Loss 4.036664, Accuracy 91.802%\n",
      "Epoch 30, Batch 490, LR 0.649292 Loss 4.036326, Accuracy 91.805%\n",
      "Epoch 30, Batch 491, LR 0.649175 Loss 4.035872, Accuracy 91.809%\n",
      "Epoch 30, Batch 492, LR 0.649057 Loss 4.035402, Accuracy 91.811%\n",
      "Epoch 30, Batch 493, LR 0.648940 Loss 4.035845, Accuracy 91.812%\n",
      "Epoch 30, Batch 494, LR 0.648822 Loss 4.036568, Accuracy 91.805%\n",
      "Epoch 30, Batch 495, LR 0.648705 Loss 4.036463, Accuracy 91.809%\n",
      "Epoch 30, Batch 496, LR 0.648587 Loss 4.037323, Accuracy 91.797%\n",
      "Epoch 30, Batch 497, LR 0.648470 Loss 4.037555, Accuracy 91.795%\n",
      "Epoch 30, Batch 498, LR 0.648353 Loss 4.037278, Accuracy 91.794%\n",
      "Epoch 30, Batch 499, LR 0.648235 Loss 4.037451, Accuracy 91.793%\n",
      "Epoch 30, Batch 500, LR 0.648118 Loss 4.038802, Accuracy 91.787%\n",
      "Epoch 30, Batch 501, LR 0.648000 Loss 4.039036, Accuracy 91.790%\n",
      "Epoch 30, Batch 502, LR 0.647883 Loss 4.038598, Accuracy 91.791%\n",
      "Epoch 30, Batch 503, LR 0.647766 Loss 4.037768, Accuracy 91.790%\n",
      "Epoch 30, Batch 504, LR 0.647648 Loss 4.038530, Accuracy 91.791%\n",
      "Epoch 30, Batch 505, LR 0.647531 Loss 4.037004, Accuracy 91.799%\n",
      "Epoch 30, Batch 506, LR 0.647413 Loss 4.037168, Accuracy 91.797%\n",
      "Epoch 30, Batch 507, LR 0.647296 Loss 4.037997, Accuracy 91.788%\n",
      "Epoch 30, Batch 508, LR 0.647179 Loss 4.038856, Accuracy 91.788%\n",
      "Epoch 30, Batch 509, LR 0.647061 Loss 4.038730, Accuracy 91.790%\n",
      "Epoch 30, Batch 510, LR 0.646944 Loss 4.038625, Accuracy 91.791%\n",
      "Epoch 30, Batch 511, LR 0.646827 Loss 4.038575, Accuracy 91.790%\n",
      "Epoch 30, Batch 512, LR 0.646709 Loss 4.037442, Accuracy 91.795%\n",
      "Epoch 30, Batch 513, LR 0.646592 Loss 4.037426, Accuracy 91.793%\n",
      "Epoch 30, Batch 514, LR 0.646475 Loss 4.037217, Accuracy 91.794%\n",
      "Epoch 30, Batch 515, LR 0.646357 Loss 4.037158, Accuracy 91.785%\n",
      "Epoch 30, Batch 516, LR 0.646240 Loss 4.037579, Accuracy 91.782%\n",
      "Epoch 30, Batch 517, LR 0.646123 Loss 4.037070, Accuracy 91.789%\n",
      "Epoch 30, Batch 518, LR 0.646006 Loss 4.036744, Accuracy 91.789%\n",
      "Epoch 30, Batch 519, LR 0.645888 Loss 4.036111, Accuracy 91.793%\n",
      "Epoch 30, Batch 520, LR 0.645771 Loss 4.036861, Accuracy 91.789%\n",
      "Epoch 30, Batch 521, LR 0.645654 Loss 4.037075, Accuracy 91.795%\n",
      "Epoch 30, Batch 522, LR 0.645537 Loss 4.036587, Accuracy 91.795%\n",
      "Epoch 30, Batch 523, LR 0.645419 Loss 4.034591, Accuracy 91.805%\n",
      "Epoch 30, Batch 524, LR 0.645302 Loss 4.034382, Accuracy 91.806%\n",
      "Epoch 30, Batch 525, LR 0.645185 Loss 4.035247, Accuracy 91.799%\n",
      "Epoch 30, Batch 526, LR 0.645068 Loss 4.034087, Accuracy 91.798%\n",
      "Epoch 30, Batch 527, LR 0.644950 Loss 4.034264, Accuracy 91.802%\n",
      "Epoch 30, Batch 528, LR 0.644833 Loss 4.033839, Accuracy 91.806%\n",
      "Epoch 30, Batch 529, LR 0.644716 Loss 4.035056, Accuracy 91.802%\n",
      "Epoch 30, Batch 530, LR 0.644599 Loss 4.036873, Accuracy 91.788%\n",
      "Epoch 30, Batch 531, LR 0.644482 Loss 4.036150, Accuracy 91.792%\n",
      "Epoch 30, Batch 532, LR 0.644364 Loss 4.036972, Accuracy 91.788%\n",
      "Epoch 30, Batch 533, LR 0.644247 Loss 4.038425, Accuracy 91.771%\n",
      "Epoch 30, Batch 534, LR 0.644130 Loss 4.037907, Accuracy 91.773%\n",
      "Epoch 30, Batch 535, LR 0.644013 Loss 4.038641, Accuracy 91.767%\n",
      "Epoch 30, Batch 536, LR 0.643896 Loss 4.039632, Accuracy 91.762%\n",
      "Epoch 30, Batch 537, LR 0.643779 Loss 4.040893, Accuracy 91.755%\n",
      "Epoch 30, Batch 538, LR 0.643661 Loss 4.040951, Accuracy 91.762%\n",
      "Epoch 30, Batch 539, LR 0.643544 Loss 4.041278, Accuracy 91.764%\n",
      "Epoch 30, Batch 540, LR 0.643427 Loss 4.041379, Accuracy 91.762%\n",
      "Epoch 30, Batch 541, LR 0.643310 Loss 4.042187, Accuracy 91.757%\n",
      "Epoch 30, Batch 542, LR 0.643193 Loss 4.042129, Accuracy 91.754%\n",
      "Epoch 30, Batch 543, LR 0.643076 Loss 4.042268, Accuracy 91.754%\n",
      "Epoch 30, Batch 544, LR 0.642959 Loss 4.042395, Accuracy 91.757%\n",
      "Epoch 30, Batch 545, LR 0.642842 Loss 4.042732, Accuracy 91.752%\n",
      "Epoch 30, Batch 546, LR 0.642725 Loss 4.041632, Accuracy 91.754%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 547, LR 0.642607 Loss 4.042549, Accuracy 91.749%\n",
      "Epoch 30, Batch 548, LR 0.642490 Loss 4.041086, Accuracy 91.757%\n",
      "Epoch 30, Batch 549, LR 0.642373 Loss 4.040958, Accuracy 91.758%\n",
      "Epoch 30, Batch 550, LR 0.642256 Loss 4.041259, Accuracy 91.759%\n",
      "Epoch 30, Batch 551, LR 0.642139 Loss 4.040779, Accuracy 91.756%\n",
      "Epoch 30, Batch 552, LR 0.642022 Loss 4.041531, Accuracy 91.754%\n",
      "Epoch 30, Batch 553, LR 0.641905 Loss 4.041729, Accuracy 91.752%\n",
      "Epoch 30, Batch 554, LR 0.641788 Loss 4.042307, Accuracy 91.752%\n",
      "Epoch 30, Batch 555, LR 0.641671 Loss 4.042160, Accuracy 91.754%\n",
      "Epoch 30, Batch 556, LR 0.641554 Loss 4.042513, Accuracy 91.758%\n",
      "Epoch 30, Batch 557, LR 0.641437 Loss 4.041498, Accuracy 91.761%\n",
      "Epoch 30, Batch 558, LR 0.641320 Loss 4.042195, Accuracy 91.760%\n",
      "Epoch 30, Batch 559, LR 0.641203 Loss 4.041716, Accuracy 91.760%\n",
      "Epoch 30, Batch 560, LR 0.641086 Loss 4.040276, Accuracy 91.772%\n",
      "Epoch 30, Batch 561, LR 0.640969 Loss 4.041025, Accuracy 91.768%\n",
      "Epoch 30, Batch 562, LR 0.640852 Loss 4.040898, Accuracy 91.770%\n",
      "Epoch 30, Batch 563, LR 0.640735 Loss 4.039879, Accuracy 91.777%\n",
      "Epoch 30, Batch 564, LR 0.640618 Loss 4.039203, Accuracy 91.773%\n",
      "Epoch 30, Batch 565, LR 0.640501 Loss 4.039778, Accuracy 91.766%\n",
      "Epoch 30, Batch 566, LR 0.640384 Loss 4.040144, Accuracy 91.769%\n",
      "Epoch 30, Batch 567, LR 0.640267 Loss 4.039927, Accuracy 91.771%\n",
      "Epoch 30, Batch 568, LR 0.640150 Loss 4.039654, Accuracy 91.769%\n",
      "Epoch 30, Batch 569, LR 0.640034 Loss 4.037849, Accuracy 91.771%\n",
      "Epoch 30, Batch 570, LR 0.639917 Loss 4.037530, Accuracy 91.772%\n",
      "Epoch 30, Batch 571, LR 0.639800 Loss 4.037665, Accuracy 91.769%\n",
      "Epoch 30, Batch 572, LR 0.639683 Loss 4.037703, Accuracy 91.761%\n",
      "Epoch 30, Batch 573, LR 0.639566 Loss 4.037949, Accuracy 91.761%\n",
      "Epoch 30, Batch 574, LR 0.639449 Loss 4.036733, Accuracy 91.774%\n",
      "Epoch 30, Batch 575, LR 0.639332 Loss 4.036497, Accuracy 91.769%\n",
      "Epoch 30, Batch 576, LR 0.639215 Loss 4.035349, Accuracy 91.771%\n",
      "Epoch 30, Batch 577, LR 0.639098 Loss 4.034645, Accuracy 91.772%\n",
      "Epoch 30, Batch 578, LR 0.638981 Loss 4.035829, Accuracy 91.767%\n",
      "Epoch 30, Batch 579, LR 0.638865 Loss 4.036014, Accuracy 91.767%\n",
      "Epoch 30, Batch 580, LR 0.638748 Loss 4.036371, Accuracy 91.763%\n",
      "Epoch 30, Batch 581, LR 0.638631 Loss 4.035339, Accuracy 91.768%\n",
      "Epoch 30, Batch 582, LR 0.638514 Loss 4.034372, Accuracy 91.774%\n",
      "Epoch 30, Batch 583, LR 0.638397 Loss 4.033984, Accuracy 91.775%\n",
      "Epoch 30, Batch 584, LR 0.638280 Loss 4.033522, Accuracy 91.777%\n",
      "Epoch 30, Batch 585, LR 0.638164 Loss 4.034037, Accuracy 91.776%\n",
      "Epoch 30, Batch 586, LR 0.638047 Loss 4.034140, Accuracy 91.772%\n",
      "Epoch 30, Batch 587, LR 0.637930 Loss 4.032793, Accuracy 91.779%\n",
      "Epoch 30, Batch 588, LR 0.637813 Loss 4.033073, Accuracy 91.776%\n",
      "Epoch 30, Batch 589, LR 0.637696 Loss 4.033149, Accuracy 91.776%\n",
      "Epoch 30, Batch 590, LR 0.637580 Loss 4.032462, Accuracy 91.778%\n",
      "Epoch 30, Batch 591, LR 0.637463 Loss 4.032270, Accuracy 91.780%\n",
      "Epoch 30, Batch 592, LR 0.637346 Loss 4.031072, Accuracy 91.782%\n",
      "Epoch 30, Batch 593, LR 0.637229 Loss 4.030763, Accuracy 91.790%\n",
      "Epoch 30, Batch 594, LR 0.637113 Loss 4.029532, Accuracy 91.792%\n",
      "Epoch 30, Batch 595, LR 0.636996 Loss 4.031588, Accuracy 91.790%\n",
      "Epoch 30, Batch 596, LR 0.636879 Loss 4.031262, Accuracy 91.790%\n",
      "Epoch 30, Batch 597, LR 0.636762 Loss 4.031928, Accuracy 91.787%\n",
      "Epoch 30, Batch 598, LR 0.636646 Loss 4.031837, Accuracy 91.789%\n",
      "Epoch 30, Batch 599, LR 0.636529 Loss 4.031012, Accuracy 91.790%\n",
      "Epoch 30, Batch 600, LR 0.636412 Loss 4.030699, Accuracy 91.796%\n",
      "Epoch 30, Batch 601, LR 0.636296 Loss 4.031019, Accuracy 91.795%\n",
      "Epoch 30, Batch 602, LR 0.636179 Loss 4.030879, Accuracy 91.797%\n",
      "Epoch 30, Batch 603, LR 0.636062 Loss 4.030490, Accuracy 91.805%\n",
      "Epoch 30, Batch 604, LR 0.635946 Loss 4.030154, Accuracy 91.805%\n",
      "Epoch 30, Batch 605, LR 0.635829 Loss 4.031223, Accuracy 91.794%\n",
      "Epoch 30, Batch 606, LR 0.635712 Loss 4.031894, Accuracy 91.796%\n",
      "Epoch 30, Batch 607, LR 0.635596 Loss 4.032007, Accuracy 91.798%\n",
      "Epoch 30, Batch 608, LR 0.635479 Loss 4.031617, Accuracy 91.805%\n",
      "Epoch 30, Batch 609, LR 0.635362 Loss 4.030422, Accuracy 91.808%\n",
      "Epoch 30, Batch 610, LR 0.635246 Loss 4.030623, Accuracy 91.801%\n",
      "Epoch 30, Batch 611, LR 0.635129 Loss 4.030016, Accuracy 91.800%\n",
      "Epoch 30, Batch 612, LR 0.635012 Loss 4.028436, Accuracy 91.806%\n",
      "Epoch 30, Batch 613, LR 0.634896 Loss 4.028032, Accuracy 91.808%\n",
      "Epoch 30, Batch 614, LR 0.634779 Loss 4.028060, Accuracy 91.808%\n",
      "Epoch 30, Batch 615, LR 0.634663 Loss 4.027406, Accuracy 91.813%\n",
      "Epoch 30, Batch 616, LR 0.634546 Loss 4.026255, Accuracy 91.820%\n",
      "Epoch 30, Batch 617, LR 0.634429 Loss 4.026208, Accuracy 91.825%\n",
      "Epoch 30, Batch 618, LR 0.634313 Loss 4.026243, Accuracy 91.822%\n",
      "Epoch 30, Batch 619, LR 0.634196 Loss 4.024434, Accuracy 91.829%\n",
      "Epoch 30, Batch 620, LR 0.634080 Loss 4.025787, Accuracy 91.821%\n",
      "Epoch 30, Batch 621, LR 0.633963 Loss 4.025891, Accuracy 91.818%\n",
      "Epoch 30, Batch 622, LR 0.633847 Loss 4.026002, Accuracy 91.818%\n",
      "Epoch 30, Batch 623, LR 0.633730 Loss 4.026029, Accuracy 91.819%\n",
      "Epoch 30, Batch 624, LR 0.633613 Loss 4.026369, Accuracy 91.821%\n",
      "Epoch 30, Batch 625, LR 0.633497 Loss 4.026050, Accuracy 91.817%\n",
      "Epoch 30, Batch 626, LR 0.633380 Loss 4.025021, Accuracy 91.824%\n",
      "Epoch 30, Batch 627, LR 0.633264 Loss 4.025842, Accuracy 91.820%\n",
      "Epoch 30, Batch 628, LR 0.633147 Loss 4.024804, Accuracy 91.824%\n",
      "Epoch 30, Batch 629, LR 0.633031 Loss 4.025162, Accuracy 91.820%\n",
      "Epoch 30, Batch 630, LR 0.632914 Loss 4.026212, Accuracy 91.819%\n",
      "Epoch 30, Batch 631, LR 0.632798 Loss 4.025923, Accuracy 91.822%\n",
      "Epoch 30, Batch 632, LR 0.632681 Loss 4.025591, Accuracy 91.819%\n",
      "Epoch 30, Batch 633, LR 0.632565 Loss 4.025257, Accuracy 91.815%\n",
      "Epoch 30, Batch 634, LR 0.632448 Loss 4.026325, Accuracy 91.815%\n",
      "Epoch 30, Batch 635, LR 0.632332 Loss 4.025617, Accuracy 91.818%\n",
      "Epoch 30, Batch 636, LR 0.632216 Loss 4.025905, Accuracy 91.815%\n",
      "Epoch 30, Batch 637, LR 0.632099 Loss 4.025096, Accuracy 91.821%\n",
      "Epoch 30, Batch 638, LR 0.631983 Loss 4.026300, Accuracy 91.815%\n",
      "Epoch 30, Batch 639, LR 0.631866 Loss 4.026085, Accuracy 91.813%\n",
      "Epoch 30, Batch 640, LR 0.631750 Loss 4.026530, Accuracy 91.810%\n",
      "Epoch 30, Batch 641, LR 0.631633 Loss 4.026614, Accuracy 91.811%\n",
      "Epoch 30, Batch 642, LR 0.631517 Loss 4.026325, Accuracy 91.814%\n",
      "Epoch 30, Batch 643, LR 0.631401 Loss 4.027214, Accuracy 91.811%\n",
      "Epoch 30, Batch 644, LR 0.631284 Loss 4.027481, Accuracy 91.808%\n",
      "Epoch 30, Batch 645, LR 0.631168 Loss 4.027578, Accuracy 91.806%\n",
      "Epoch 30, Batch 646, LR 0.631051 Loss 4.026763, Accuracy 91.810%\n",
      "Epoch 30, Batch 647, LR 0.630935 Loss 4.025565, Accuracy 91.813%\n",
      "Epoch 30, Batch 648, LR 0.630819 Loss 4.025844, Accuracy 91.814%\n",
      "Epoch 30, Batch 649, LR 0.630702 Loss 4.025166, Accuracy 91.818%\n",
      "Epoch 30, Batch 650, LR 0.630586 Loss 4.024518, Accuracy 91.817%\n",
      "Epoch 30, Batch 651, LR 0.630470 Loss 4.024346, Accuracy 91.815%\n",
      "Epoch 30, Batch 652, LR 0.630353 Loss 4.023450, Accuracy 91.816%\n",
      "Epoch 30, Batch 653, LR 0.630237 Loss 4.024556, Accuracy 91.811%\n",
      "Epoch 30, Batch 654, LR 0.630121 Loss 4.023882, Accuracy 91.815%\n",
      "Epoch 30, Batch 655, LR 0.630004 Loss 4.024283, Accuracy 91.812%\n",
      "Epoch 30, Batch 656, LR 0.629888 Loss 4.024140, Accuracy 91.811%\n",
      "Epoch 30, Batch 657, LR 0.629772 Loss 4.024047, Accuracy 91.811%\n",
      "Epoch 30, Batch 658, LR 0.629655 Loss 4.024158, Accuracy 91.805%\n",
      "Epoch 30, Batch 659, LR 0.629539 Loss 4.025111, Accuracy 91.802%\n",
      "Epoch 30, Batch 660, LR 0.629423 Loss 4.024567, Accuracy 91.804%\n",
      "Epoch 30, Batch 661, LR 0.629307 Loss 4.024262, Accuracy 91.806%\n",
      "Epoch 30, Batch 662, LR 0.629190 Loss 4.025361, Accuracy 91.802%\n",
      "Epoch 30, Batch 663, LR 0.629074 Loss 4.025276, Accuracy 91.803%\n",
      "Epoch 30, Batch 664, LR 0.628958 Loss 4.024990, Accuracy 91.807%\n",
      "Epoch 30, Batch 665, LR 0.628842 Loss 4.024798, Accuracy 91.807%\n",
      "Epoch 30, Batch 666, LR 0.628725 Loss 4.024608, Accuracy 91.803%\n",
      "Epoch 30, Batch 667, LR 0.628609 Loss 4.025510, Accuracy 91.796%\n",
      "Epoch 30, Batch 668, LR 0.628493 Loss 4.025627, Accuracy 91.793%\n",
      "Epoch 30, Batch 669, LR 0.628377 Loss 4.025247, Accuracy 91.795%\n",
      "Epoch 30, Batch 670, LR 0.628260 Loss 4.024475, Accuracy 91.797%\n",
      "Epoch 30, Batch 671, LR 0.628144 Loss 4.024305, Accuracy 91.797%\n",
      "Epoch 30, Batch 672, LR 0.628028 Loss 4.023593, Accuracy 91.799%\n",
      "Epoch 30, Batch 673, LR 0.627912 Loss 4.022538, Accuracy 91.803%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 674, LR 0.627796 Loss 4.023336, Accuracy 91.798%\n",
      "Epoch 30, Batch 675, LR 0.627679 Loss 4.023821, Accuracy 91.795%\n",
      "Epoch 30, Batch 676, LR 0.627563 Loss 4.023667, Accuracy 91.792%\n",
      "Epoch 30, Batch 677, LR 0.627447 Loss 4.024052, Accuracy 91.789%\n",
      "Epoch 30, Batch 678, LR 0.627331 Loss 4.024666, Accuracy 91.791%\n",
      "Epoch 30, Batch 679, LR 0.627215 Loss 4.024508, Accuracy 91.794%\n",
      "Epoch 30, Batch 680, LR 0.627099 Loss 4.025247, Accuracy 91.791%\n",
      "Epoch 30, Batch 681, LR 0.626983 Loss 4.024736, Accuracy 91.792%\n",
      "Epoch 30, Batch 682, LR 0.626866 Loss 4.025488, Accuracy 91.791%\n",
      "Epoch 30, Batch 683, LR 0.626750 Loss 4.026144, Accuracy 91.792%\n",
      "Epoch 30, Batch 684, LR 0.626634 Loss 4.026551, Accuracy 91.793%\n",
      "Epoch 30, Batch 685, LR 0.626518 Loss 4.026969, Accuracy 91.788%\n",
      "Epoch 30, Batch 686, LR 0.626402 Loss 4.027238, Accuracy 91.785%\n",
      "Epoch 30, Batch 687, LR 0.626286 Loss 4.027047, Accuracy 91.788%\n",
      "Epoch 30, Batch 688, LR 0.626170 Loss 4.026568, Accuracy 91.787%\n",
      "Epoch 30, Batch 689, LR 0.626054 Loss 4.026549, Accuracy 91.791%\n",
      "Epoch 30, Batch 690, LR 0.625938 Loss 4.025741, Accuracy 91.791%\n",
      "Epoch 30, Batch 691, LR 0.625822 Loss 4.026262, Accuracy 91.791%\n",
      "Epoch 30, Batch 692, LR 0.625706 Loss 4.026919, Accuracy 91.791%\n",
      "Epoch 30, Batch 693, LR 0.625589 Loss 4.026268, Accuracy 91.792%\n",
      "Epoch 30, Batch 694, LR 0.625473 Loss 4.026542, Accuracy 91.788%\n",
      "Epoch 30, Batch 695, LR 0.625357 Loss 4.026637, Accuracy 91.787%\n",
      "Epoch 30, Batch 696, LR 0.625241 Loss 4.027088, Accuracy 91.781%\n",
      "Epoch 30, Batch 697, LR 0.625125 Loss 4.026924, Accuracy 91.781%\n",
      "Epoch 30, Batch 698, LR 0.625009 Loss 4.027417, Accuracy 91.778%\n",
      "Epoch 30, Batch 699, LR 0.624893 Loss 4.026758, Accuracy 91.781%\n",
      "Epoch 30, Batch 700, LR 0.624777 Loss 4.027071, Accuracy 91.780%\n",
      "Epoch 30, Batch 701, LR 0.624661 Loss 4.026654, Accuracy 91.778%\n",
      "Epoch 30, Batch 702, LR 0.624545 Loss 4.026207, Accuracy 91.778%\n",
      "Epoch 30, Batch 703, LR 0.624429 Loss 4.026943, Accuracy 91.773%\n",
      "Epoch 30, Batch 704, LR 0.624313 Loss 4.027653, Accuracy 91.770%\n",
      "Epoch 30, Batch 705, LR 0.624198 Loss 4.028389, Accuracy 91.773%\n",
      "Epoch 30, Batch 706, LR 0.624082 Loss 4.028410, Accuracy 91.776%\n",
      "Epoch 30, Batch 707, LR 0.623966 Loss 4.029111, Accuracy 91.772%\n",
      "Epoch 30, Batch 708, LR 0.623850 Loss 4.029939, Accuracy 91.773%\n",
      "Epoch 30, Batch 709, LR 0.623734 Loss 4.030029, Accuracy 91.769%\n",
      "Epoch 30, Batch 710, LR 0.623618 Loss 4.030508, Accuracy 91.765%\n",
      "Epoch 30, Batch 711, LR 0.623502 Loss 4.030727, Accuracy 91.761%\n",
      "Epoch 30, Batch 712, LR 0.623386 Loss 4.030553, Accuracy 91.762%\n",
      "Epoch 30, Batch 713, LR 0.623270 Loss 4.031896, Accuracy 91.755%\n",
      "Epoch 30, Batch 714, LR 0.623154 Loss 4.031905, Accuracy 91.754%\n",
      "Epoch 30, Batch 715, LR 0.623038 Loss 4.032088, Accuracy 91.754%\n",
      "Epoch 30, Batch 716, LR 0.622922 Loss 4.031524, Accuracy 91.758%\n",
      "Epoch 30, Batch 717, LR 0.622807 Loss 4.031893, Accuracy 91.753%\n",
      "Epoch 30, Batch 718, LR 0.622691 Loss 4.031810, Accuracy 91.756%\n",
      "Epoch 30, Batch 719, LR 0.622575 Loss 4.031894, Accuracy 91.757%\n",
      "Epoch 30, Batch 720, LR 0.622459 Loss 4.031581, Accuracy 91.759%\n",
      "Epoch 30, Batch 721, LR 0.622343 Loss 4.031979, Accuracy 91.761%\n",
      "Epoch 30, Batch 722, LR 0.622227 Loss 4.032193, Accuracy 91.764%\n",
      "Epoch 30, Batch 723, LR 0.622111 Loss 4.031769, Accuracy 91.765%\n",
      "Epoch 30, Batch 724, LR 0.621996 Loss 4.031776, Accuracy 91.767%\n",
      "Epoch 30, Batch 725, LR 0.621880 Loss 4.032232, Accuracy 91.764%\n",
      "Epoch 30, Batch 726, LR 0.621764 Loss 4.032411, Accuracy 91.765%\n",
      "Epoch 30, Batch 727, LR 0.621648 Loss 4.031579, Accuracy 91.766%\n",
      "Epoch 30, Batch 728, LR 0.621532 Loss 4.031501, Accuracy 91.768%\n",
      "Epoch 30, Batch 729, LR 0.621417 Loss 4.031547, Accuracy 91.773%\n",
      "Epoch 30, Batch 730, LR 0.621301 Loss 4.032069, Accuracy 91.769%\n",
      "Epoch 30, Batch 731, LR 0.621185 Loss 4.032429, Accuracy 91.769%\n",
      "Epoch 30, Batch 732, LR 0.621069 Loss 4.032443, Accuracy 91.768%\n",
      "Epoch 30, Batch 733, LR 0.620954 Loss 4.033416, Accuracy 91.764%\n",
      "Epoch 30, Batch 734, LR 0.620838 Loss 4.033731, Accuracy 91.762%\n",
      "Epoch 30, Batch 735, LR 0.620722 Loss 4.033312, Accuracy 91.766%\n",
      "Epoch 30, Batch 736, LR 0.620606 Loss 4.032671, Accuracy 91.770%\n",
      "Epoch 30, Batch 737, LR 0.620491 Loss 4.032581, Accuracy 91.773%\n",
      "Epoch 30, Batch 738, LR 0.620375 Loss 4.033047, Accuracy 91.771%\n",
      "Epoch 30, Batch 739, LR 0.620259 Loss 4.032725, Accuracy 91.771%\n",
      "Epoch 30, Batch 740, LR 0.620143 Loss 4.032862, Accuracy 91.773%\n",
      "Epoch 30, Batch 741, LR 0.620028 Loss 4.033007, Accuracy 91.773%\n",
      "Epoch 30, Batch 742, LR 0.619912 Loss 4.032288, Accuracy 91.778%\n",
      "Epoch 30, Batch 743, LR 0.619796 Loss 4.033234, Accuracy 91.774%\n",
      "Epoch 30, Batch 744, LR 0.619681 Loss 4.033012, Accuracy 91.776%\n",
      "Epoch 30, Batch 745, LR 0.619565 Loss 4.032936, Accuracy 91.780%\n",
      "Epoch 30, Batch 746, LR 0.619449 Loss 4.033228, Accuracy 91.779%\n",
      "Epoch 30, Batch 747, LR 0.619334 Loss 4.032708, Accuracy 91.782%\n",
      "Epoch 30, Batch 748, LR 0.619218 Loss 4.032880, Accuracy 91.784%\n",
      "Epoch 30, Batch 749, LR 0.619102 Loss 4.034148, Accuracy 91.781%\n",
      "Epoch 30, Batch 750, LR 0.618987 Loss 4.033606, Accuracy 91.783%\n",
      "Epoch 30, Batch 751, LR 0.618871 Loss 4.032571, Accuracy 91.788%\n",
      "Epoch 30, Batch 752, LR 0.618756 Loss 4.031722, Accuracy 91.792%\n",
      "Epoch 30, Batch 753, LR 0.618640 Loss 4.031367, Accuracy 91.795%\n",
      "Epoch 30, Batch 754, LR 0.618524 Loss 4.030770, Accuracy 91.798%\n",
      "Epoch 30, Batch 755, LR 0.618409 Loss 4.030289, Accuracy 91.799%\n",
      "Epoch 30, Batch 756, LR 0.618293 Loss 4.029987, Accuracy 91.802%\n",
      "Epoch 30, Batch 757, LR 0.618178 Loss 4.030001, Accuracy 91.800%\n",
      "Epoch 30, Batch 758, LR 0.618062 Loss 4.029746, Accuracy 91.802%\n",
      "Epoch 30, Batch 759, LR 0.617946 Loss 4.029007, Accuracy 91.804%\n",
      "Epoch 30, Batch 760, LR 0.617831 Loss 4.028916, Accuracy 91.806%\n",
      "Epoch 30, Batch 761, LR 0.617715 Loss 4.029832, Accuracy 91.808%\n",
      "Epoch 30, Batch 762, LR 0.617600 Loss 4.029140, Accuracy 91.810%\n",
      "Epoch 30, Batch 763, LR 0.617484 Loss 4.029489, Accuracy 91.810%\n",
      "Epoch 30, Batch 764, LR 0.617369 Loss 4.029241, Accuracy 91.812%\n",
      "Epoch 30, Batch 765, LR 0.617253 Loss 4.029624, Accuracy 91.810%\n",
      "Epoch 30, Batch 766, LR 0.617138 Loss 4.029824, Accuracy 91.807%\n",
      "Epoch 30, Batch 767, LR 0.617022 Loss 4.029751, Accuracy 91.807%\n",
      "Epoch 30, Batch 768, LR 0.616907 Loss 4.029569, Accuracy 91.805%\n",
      "Epoch 30, Batch 769, LR 0.616791 Loss 4.030203, Accuracy 91.801%\n",
      "Epoch 30, Batch 770, LR 0.616676 Loss 4.030875, Accuracy 91.799%\n",
      "Epoch 30, Batch 771, LR 0.616560 Loss 4.031637, Accuracy 91.790%\n",
      "Epoch 30, Batch 772, LR 0.616445 Loss 4.031082, Accuracy 91.792%\n",
      "Epoch 30, Batch 773, LR 0.616329 Loss 4.031561, Accuracy 91.792%\n",
      "Epoch 30, Batch 774, LR 0.616214 Loss 4.031334, Accuracy 91.792%\n",
      "Epoch 30, Batch 775, LR 0.616098 Loss 4.031142, Accuracy 91.790%\n",
      "Epoch 30, Batch 776, LR 0.615983 Loss 4.030965, Accuracy 91.793%\n",
      "Epoch 30, Batch 777, LR 0.615867 Loss 4.031714, Accuracy 91.791%\n",
      "Epoch 30, Batch 778, LR 0.615752 Loss 4.031859, Accuracy 91.789%\n",
      "Epoch 30, Batch 779, LR 0.615636 Loss 4.031212, Accuracy 91.790%\n",
      "Epoch 30, Batch 780, LR 0.615521 Loss 4.031115, Accuracy 91.789%\n",
      "Epoch 30, Batch 781, LR 0.615406 Loss 4.030636, Accuracy 91.796%\n",
      "Epoch 30, Batch 782, LR 0.615290 Loss 4.030195, Accuracy 91.798%\n",
      "Epoch 30, Batch 783, LR 0.615175 Loss 4.029941, Accuracy 91.801%\n",
      "Epoch 30, Batch 784, LR 0.615059 Loss 4.030506, Accuracy 91.801%\n",
      "Epoch 30, Batch 785, LR 0.614944 Loss 4.030519, Accuracy 91.801%\n",
      "Epoch 30, Batch 786, LR 0.614829 Loss 4.030073, Accuracy 91.800%\n",
      "Epoch 30, Batch 787, LR 0.614713 Loss 4.030241, Accuracy 91.800%\n",
      "Epoch 30, Batch 788, LR 0.614598 Loss 4.029912, Accuracy 91.796%\n",
      "Epoch 30, Batch 789, LR 0.614483 Loss 4.030500, Accuracy 91.790%\n",
      "Epoch 30, Batch 790, LR 0.614367 Loss 4.030368, Accuracy 91.789%\n",
      "Epoch 30, Batch 791, LR 0.614252 Loss 4.030105, Accuracy 91.789%\n",
      "Epoch 30, Batch 792, LR 0.614137 Loss 4.030442, Accuracy 91.788%\n",
      "Epoch 30, Batch 793, LR 0.614021 Loss 4.030514, Accuracy 91.789%\n",
      "Epoch 30, Batch 794, LR 0.613906 Loss 4.030690, Accuracy 91.787%\n",
      "Epoch 30, Batch 795, LR 0.613791 Loss 4.031057, Accuracy 91.787%\n",
      "Epoch 30, Batch 796, LR 0.613675 Loss 4.030600, Accuracy 91.790%\n",
      "Epoch 30, Batch 797, LR 0.613560 Loss 4.030565, Accuracy 91.790%\n",
      "Epoch 30, Batch 798, LR 0.613445 Loss 4.030844, Accuracy 91.788%\n",
      "Epoch 30, Batch 799, LR 0.613330 Loss 4.031266, Accuracy 91.785%\n",
      "Epoch 30, Batch 800, LR 0.613214 Loss 4.031501, Accuracy 91.784%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 801, LR 0.613099 Loss 4.032019, Accuracy 91.779%\n",
      "Epoch 30, Batch 802, LR 0.612984 Loss 4.032029, Accuracy 91.777%\n",
      "Epoch 30, Batch 803, LR 0.612868 Loss 4.032260, Accuracy 91.778%\n",
      "Epoch 30, Batch 804, LR 0.612753 Loss 4.031765, Accuracy 91.782%\n",
      "Epoch 30, Batch 805, LR 0.612638 Loss 4.030711, Accuracy 91.787%\n",
      "Epoch 30, Batch 806, LR 0.612523 Loss 4.031539, Accuracy 91.783%\n",
      "Epoch 30, Batch 807, LR 0.612408 Loss 4.031451, Accuracy 91.780%\n",
      "Epoch 30, Batch 808, LR 0.612292 Loss 4.031289, Accuracy 91.782%\n",
      "Epoch 30, Batch 809, LR 0.612177 Loss 4.030669, Accuracy 91.785%\n",
      "Epoch 30, Batch 810, LR 0.612062 Loss 4.031251, Accuracy 91.781%\n",
      "Epoch 30, Batch 811, LR 0.611947 Loss 4.031356, Accuracy 91.779%\n",
      "Epoch 30, Batch 812, LR 0.611832 Loss 4.031464, Accuracy 91.778%\n",
      "Epoch 30, Batch 813, LR 0.611716 Loss 4.030379, Accuracy 91.780%\n",
      "Epoch 30, Batch 814, LR 0.611601 Loss 4.029799, Accuracy 91.780%\n",
      "Epoch 30, Batch 815, LR 0.611486 Loss 4.029708, Accuracy 91.779%\n",
      "Epoch 30, Batch 816, LR 0.611371 Loss 4.029495, Accuracy 91.783%\n",
      "Epoch 30, Batch 817, LR 0.611256 Loss 4.030149, Accuracy 91.782%\n",
      "Epoch 30, Batch 818, LR 0.611141 Loss 4.030119, Accuracy 91.784%\n",
      "Epoch 30, Batch 819, LR 0.611025 Loss 4.030800, Accuracy 91.781%\n",
      "Epoch 30, Batch 820, LR 0.610910 Loss 4.030372, Accuracy 91.780%\n",
      "Epoch 30, Batch 821, LR 0.610795 Loss 4.031605, Accuracy 91.773%\n",
      "Epoch 30, Batch 822, LR 0.610680 Loss 4.032442, Accuracy 91.772%\n",
      "Epoch 30, Batch 823, LR 0.610565 Loss 4.033056, Accuracy 91.768%\n",
      "Epoch 30, Batch 824, LR 0.610450 Loss 4.033254, Accuracy 91.766%\n",
      "Epoch 30, Batch 825, LR 0.610335 Loss 4.033116, Accuracy 91.768%\n",
      "Epoch 30, Batch 826, LR 0.610220 Loss 4.032754, Accuracy 91.769%\n",
      "Epoch 30, Batch 827, LR 0.610105 Loss 4.031420, Accuracy 91.775%\n",
      "Epoch 30, Batch 828, LR 0.609990 Loss 4.031599, Accuracy 91.775%\n",
      "Epoch 30, Batch 829, LR 0.609875 Loss 4.030880, Accuracy 91.777%\n",
      "Epoch 30, Batch 830, LR 0.609760 Loss 4.031942, Accuracy 91.773%\n",
      "Epoch 30, Batch 831, LR 0.609644 Loss 4.032121, Accuracy 91.771%\n",
      "Epoch 30, Batch 832, LR 0.609529 Loss 4.032197, Accuracy 91.772%\n",
      "Epoch 30, Batch 833, LR 0.609414 Loss 4.031815, Accuracy 91.775%\n",
      "Epoch 30, Batch 834, LR 0.609299 Loss 4.032327, Accuracy 91.770%\n",
      "Epoch 30, Batch 835, LR 0.609184 Loss 4.032465, Accuracy 91.768%\n",
      "Epoch 30, Batch 836, LR 0.609069 Loss 4.031674, Accuracy 91.770%\n",
      "Epoch 30, Batch 837, LR 0.608954 Loss 4.032272, Accuracy 91.770%\n",
      "Epoch 30, Batch 838, LR 0.608839 Loss 4.031893, Accuracy 91.772%\n",
      "Epoch 30, Batch 839, LR 0.608724 Loss 4.031656, Accuracy 91.772%\n",
      "Epoch 30, Batch 840, LR 0.608609 Loss 4.031621, Accuracy 91.772%\n",
      "Epoch 30, Batch 841, LR 0.608494 Loss 4.031907, Accuracy 91.770%\n",
      "Epoch 30, Batch 842, LR 0.608379 Loss 4.031540, Accuracy 91.774%\n",
      "Epoch 30, Batch 843, LR 0.608265 Loss 4.031911, Accuracy 91.771%\n",
      "Epoch 30, Batch 844, LR 0.608150 Loss 4.031561, Accuracy 91.773%\n",
      "Epoch 30, Batch 845, LR 0.608035 Loss 4.031923, Accuracy 91.776%\n",
      "Epoch 30, Batch 846, LR 0.607920 Loss 4.031308, Accuracy 91.774%\n",
      "Epoch 30, Batch 847, LR 0.607805 Loss 4.030552, Accuracy 91.776%\n",
      "Epoch 30, Batch 848, LR 0.607690 Loss 4.031099, Accuracy 91.773%\n",
      "Epoch 30, Batch 849, LR 0.607575 Loss 4.030264, Accuracy 91.774%\n",
      "Epoch 30, Batch 850, LR 0.607460 Loss 4.029746, Accuracy 91.778%\n",
      "Epoch 30, Batch 851, LR 0.607345 Loss 4.029986, Accuracy 91.773%\n",
      "Epoch 30, Batch 852, LR 0.607230 Loss 4.030715, Accuracy 91.770%\n",
      "Epoch 30, Batch 853, LR 0.607115 Loss 4.030596, Accuracy 91.772%\n",
      "Epoch 30, Batch 854, LR 0.607000 Loss 4.031030, Accuracy 91.768%\n",
      "Epoch 30, Batch 855, LR 0.606886 Loss 4.030946, Accuracy 91.766%\n",
      "Epoch 30, Batch 856, LR 0.606771 Loss 4.030915, Accuracy 91.769%\n",
      "Epoch 30, Batch 857, LR 0.606656 Loss 4.030155, Accuracy 91.770%\n",
      "Epoch 30, Batch 858, LR 0.606541 Loss 4.030110, Accuracy 91.774%\n",
      "Epoch 30, Batch 859, LR 0.606426 Loss 4.030062, Accuracy 91.775%\n",
      "Epoch 30, Batch 860, LR 0.606311 Loss 4.029887, Accuracy 91.777%\n",
      "Epoch 30, Batch 861, LR 0.606197 Loss 4.029972, Accuracy 91.774%\n",
      "Epoch 30, Batch 862, LR 0.606082 Loss 4.029900, Accuracy 91.772%\n",
      "Epoch 30, Batch 863, LR 0.605967 Loss 4.030539, Accuracy 91.770%\n",
      "Epoch 30, Batch 864, LR 0.605852 Loss 4.030703, Accuracy 91.769%\n",
      "Epoch 30, Batch 865, LR 0.605737 Loss 4.030734, Accuracy 91.770%\n",
      "Epoch 30, Batch 866, LR 0.605623 Loss 4.030887, Accuracy 91.768%\n",
      "Epoch 30, Batch 867, LR 0.605508 Loss 4.031256, Accuracy 91.764%\n",
      "Epoch 30, Batch 868, LR 0.605393 Loss 4.031397, Accuracy 91.761%\n",
      "Epoch 30, Batch 869, LR 0.605278 Loss 4.031360, Accuracy 91.761%\n",
      "Epoch 30, Batch 870, LR 0.605163 Loss 4.031022, Accuracy 91.765%\n",
      "Epoch 30, Batch 871, LR 0.605049 Loss 4.030992, Accuracy 91.766%\n",
      "Epoch 30, Batch 872, LR 0.604934 Loss 4.031411, Accuracy 91.768%\n",
      "Epoch 30, Batch 873, LR 0.604819 Loss 4.030790, Accuracy 91.768%\n",
      "Epoch 30, Batch 874, LR 0.604705 Loss 4.030071, Accuracy 91.771%\n",
      "Epoch 30, Batch 875, LR 0.604590 Loss 4.030313, Accuracy 91.770%\n",
      "Epoch 30, Batch 876, LR 0.604475 Loss 4.030849, Accuracy 91.767%\n",
      "Epoch 30, Batch 877, LR 0.604360 Loss 4.031516, Accuracy 91.765%\n",
      "Epoch 30, Batch 878, LR 0.604246 Loss 4.032341, Accuracy 91.758%\n",
      "Epoch 30, Batch 879, LR 0.604131 Loss 4.032353, Accuracy 91.758%\n",
      "Epoch 30, Batch 880, LR 0.604016 Loss 4.032442, Accuracy 91.754%\n",
      "Epoch 30, Batch 881, LR 0.603902 Loss 4.032624, Accuracy 91.755%\n",
      "Epoch 30, Batch 882, LR 0.603787 Loss 4.032457, Accuracy 91.758%\n",
      "Epoch 30, Batch 883, LR 0.603672 Loss 4.032351, Accuracy 91.758%\n",
      "Epoch 30, Batch 884, LR 0.603558 Loss 4.032118, Accuracy 91.757%\n",
      "Epoch 30, Batch 885, LR 0.603443 Loss 4.031794, Accuracy 91.759%\n",
      "Epoch 30, Batch 886, LR 0.603328 Loss 4.033066, Accuracy 91.752%\n",
      "Epoch 30, Batch 887, LR 0.603214 Loss 4.033262, Accuracy 91.753%\n",
      "Epoch 30, Batch 888, LR 0.603099 Loss 4.032748, Accuracy 91.752%\n",
      "Epoch 30, Batch 889, LR 0.602985 Loss 4.032435, Accuracy 91.753%\n",
      "Epoch 30, Batch 890, LR 0.602870 Loss 4.032262, Accuracy 91.753%\n",
      "Epoch 30, Batch 891, LR 0.602755 Loss 4.033071, Accuracy 91.751%\n",
      "Epoch 30, Batch 892, LR 0.602641 Loss 4.032325, Accuracy 91.754%\n",
      "Epoch 30, Batch 893, LR 0.602526 Loss 4.032552, Accuracy 91.754%\n",
      "Epoch 30, Batch 894, LR 0.602412 Loss 4.032340, Accuracy 91.754%\n",
      "Epoch 30, Batch 895, LR 0.602297 Loss 4.031857, Accuracy 91.755%\n",
      "Epoch 30, Batch 896, LR 0.602182 Loss 4.031943, Accuracy 91.755%\n",
      "Epoch 30, Batch 897, LR 0.602068 Loss 4.032758, Accuracy 91.751%\n",
      "Epoch 30, Batch 898, LR 0.601953 Loss 4.032340, Accuracy 91.752%\n",
      "Epoch 30, Batch 899, LR 0.601839 Loss 4.032549, Accuracy 91.753%\n",
      "Epoch 30, Batch 900, LR 0.601724 Loss 4.032759, Accuracy 91.751%\n",
      "Epoch 30, Batch 901, LR 0.601610 Loss 4.032917, Accuracy 91.751%\n",
      "Epoch 30, Batch 902, LR 0.601495 Loss 4.032883, Accuracy 91.752%\n",
      "Epoch 30, Batch 903, LR 0.601381 Loss 4.033876, Accuracy 91.748%\n",
      "Epoch 30, Batch 904, LR 0.601266 Loss 4.034128, Accuracy 91.748%\n",
      "Epoch 30, Batch 905, LR 0.601152 Loss 4.034706, Accuracy 91.744%\n",
      "Epoch 30, Batch 906, LR 0.601037 Loss 4.034807, Accuracy 91.743%\n",
      "Epoch 30, Batch 907, LR 0.600923 Loss 4.034343, Accuracy 91.746%\n",
      "Epoch 30, Batch 908, LR 0.600808 Loss 4.033960, Accuracy 91.746%\n",
      "Epoch 30, Batch 909, LR 0.600694 Loss 4.033680, Accuracy 91.749%\n",
      "Epoch 30, Batch 910, LR 0.600579 Loss 4.034095, Accuracy 91.750%\n",
      "Epoch 30, Batch 911, LR 0.600465 Loss 4.033826, Accuracy 91.749%\n",
      "Epoch 30, Batch 912, LR 0.600350 Loss 4.033749, Accuracy 91.748%\n",
      "Epoch 30, Batch 913, LR 0.600236 Loss 4.033756, Accuracy 91.747%\n",
      "Epoch 30, Batch 914, LR 0.600122 Loss 4.033723, Accuracy 91.749%\n",
      "Epoch 30, Batch 915, LR 0.600007 Loss 4.034209, Accuracy 91.749%\n",
      "Epoch 30, Batch 916, LR 0.599893 Loss 4.034330, Accuracy 91.750%\n",
      "Epoch 30, Batch 917, LR 0.599778 Loss 4.034529, Accuracy 91.749%\n",
      "Epoch 30, Batch 918, LR 0.599664 Loss 4.033734, Accuracy 91.753%\n",
      "Epoch 30, Batch 919, LR 0.599550 Loss 4.033367, Accuracy 91.754%\n",
      "Epoch 30, Batch 920, LR 0.599435 Loss 4.033570, Accuracy 91.755%\n",
      "Epoch 30, Batch 921, LR 0.599321 Loss 4.033840, Accuracy 91.751%\n",
      "Epoch 30, Batch 922, LR 0.599206 Loss 4.034155, Accuracy 91.752%\n",
      "Epoch 30, Batch 923, LR 0.599092 Loss 4.033471, Accuracy 91.755%\n",
      "Epoch 30, Batch 924, LR 0.598978 Loss 4.033659, Accuracy 91.753%\n",
      "Epoch 30, Batch 925, LR 0.598863 Loss 4.033221, Accuracy 91.756%\n",
      "Epoch 30, Batch 926, LR 0.598749 Loss 4.033555, Accuracy 91.756%\n",
      "Epoch 30, Batch 927, LR 0.598635 Loss 4.033980, Accuracy 91.756%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 928, LR 0.598520 Loss 4.034564, Accuracy 91.754%\n",
      "Epoch 30, Batch 929, LR 0.598406 Loss 4.035142, Accuracy 91.752%\n",
      "Epoch 30, Batch 930, LR 0.598292 Loss 4.034837, Accuracy 91.754%\n",
      "Epoch 30, Batch 931, LR 0.598177 Loss 4.035317, Accuracy 91.749%\n",
      "Epoch 30, Batch 932, LR 0.598063 Loss 4.034732, Accuracy 91.753%\n",
      "Epoch 30, Batch 933, LR 0.597949 Loss 4.034627, Accuracy 91.754%\n",
      "Epoch 30, Batch 934, LR 0.597835 Loss 4.034781, Accuracy 91.754%\n",
      "Epoch 30, Batch 935, LR 0.597720 Loss 4.035082, Accuracy 91.753%\n",
      "Epoch 30, Batch 936, LR 0.597606 Loss 4.034883, Accuracy 91.757%\n",
      "Epoch 30, Batch 937, LR 0.597492 Loss 4.035580, Accuracy 91.754%\n",
      "Epoch 30, Batch 938, LR 0.597377 Loss 4.035419, Accuracy 91.754%\n",
      "Epoch 30, Batch 939, LR 0.597263 Loss 4.035759, Accuracy 91.752%\n",
      "Epoch 30, Batch 940, LR 0.597149 Loss 4.036798, Accuracy 91.746%\n",
      "Epoch 30, Batch 941, LR 0.597035 Loss 4.036568, Accuracy 91.750%\n",
      "Epoch 30, Batch 942, LR 0.596921 Loss 4.037206, Accuracy 91.749%\n",
      "Epoch 30, Batch 943, LR 0.596806 Loss 4.037082, Accuracy 91.748%\n",
      "Epoch 30, Batch 944, LR 0.596692 Loss 4.036651, Accuracy 91.751%\n",
      "Epoch 30, Batch 945, LR 0.596578 Loss 4.037317, Accuracy 91.751%\n",
      "Epoch 30, Batch 946, LR 0.596464 Loss 4.036939, Accuracy 91.752%\n",
      "Epoch 30, Batch 947, LR 0.596350 Loss 4.036860, Accuracy 91.752%\n",
      "Epoch 30, Batch 948, LR 0.596235 Loss 4.036101, Accuracy 91.756%\n",
      "Epoch 30, Batch 949, LR 0.596121 Loss 4.036261, Accuracy 91.754%\n",
      "Epoch 30, Batch 950, LR 0.596007 Loss 4.036286, Accuracy 91.755%\n",
      "Epoch 30, Batch 951, LR 0.595893 Loss 4.036106, Accuracy 91.756%\n",
      "Epoch 30, Batch 952, LR 0.595779 Loss 4.036688, Accuracy 91.753%\n",
      "Epoch 30, Batch 953, LR 0.595665 Loss 4.036284, Accuracy 91.753%\n",
      "Epoch 30, Batch 954, LR 0.595551 Loss 4.036222, Accuracy 91.753%\n",
      "Epoch 30, Batch 955, LR 0.595436 Loss 4.036436, Accuracy 91.755%\n",
      "Epoch 30, Batch 956, LR 0.595322 Loss 4.036373, Accuracy 91.751%\n",
      "Epoch 30, Batch 957, LR 0.595208 Loss 4.035868, Accuracy 91.755%\n",
      "Epoch 30, Batch 958, LR 0.595094 Loss 4.036130, Accuracy 91.753%\n",
      "Epoch 30, Batch 959, LR 0.594980 Loss 4.036837, Accuracy 91.747%\n",
      "Epoch 30, Batch 960, LR 0.594866 Loss 4.036383, Accuracy 91.749%\n",
      "Epoch 30, Batch 961, LR 0.594752 Loss 4.036023, Accuracy 91.749%\n",
      "Epoch 30, Batch 962, LR 0.594638 Loss 4.035773, Accuracy 91.750%\n",
      "Epoch 30, Batch 963, LR 0.594524 Loss 4.036175, Accuracy 91.749%\n",
      "Epoch 30, Batch 964, LR 0.594410 Loss 4.036254, Accuracy 91.747%\n",
      "Epoch 30, Batch 965, LR 0.594296 Loss 4.036059, Accuracy 91.751%\n",
      "Epoch 30, Batch 966, LR 0.594182 Loss 4.036008, Accuracy 91.751%\n",
      "Epoch 30, Batch 967, LR 0.594068 Loss 4.036001, Accuracy 91.751%\n",
      "Epoch 30, Batch 968, LR 0.593953 Loss 4.035715, Accuracy 91.753%\n",
      "Epoch 30, Batch 969, LR 0.593839 Loss 4.036163, Accuracy 91.748%\n",
      "Epoch 30, Batch 970, LR 0.593725 Loss 4.035981, Accuracy 91.749%\n",
      "Epoch 30, Batch 971, LR 0.593611 Loss 4.035484, Accuracy 91.751%\n",
      "Epoch 30, Batch 972, LR 0.593497 Loss 4.035536, Accuracy 91.750%\n",
      "Epoch 30, Batch 973, LR 0.593383 Loss 4.035683, Accuracy 91.752%\n",
      "Epoch 30, Batch 974, LR 0.593270 Loss 4.035625, Accuracy 91.757%\n",
      "Epoch 30, Batch 975, LR 0.593156 Loss 4.035758, Accuracy 91.757%\n",
      "Epoch 30, Batch 976, LR 0.593042 Loss 4.036451, Accuracy 91.754%\n",
      "Epoch 30, Batch 977, LR 0.592928 Loss 4.036373, Accuracy 91.756%\n",
      "Epoch 30, Batch 978, LR 0.592814 Loss 4.036629, Accuracy 91.751%\n",
      "Epoch 30, Batch 979, LR 0.592700 Loss 4.036451, Accuracy 91.753%\n",
      "Epoch 30, Batch 980, LR 0.592586 Loss 4.036282, Accuracy 91.751%\n",
      "Epoch 30, Batch 981, LR 0.592472 Loss 4.036769, Accuracy 91.749%\n",
      "Epoch 30, Batch 982, LR 0.592358 Loss 4.037205, Accuracy 91.748%\n",
      "Epoch 30, Batch 983, LR 0.592244 Loss 4.036714, Accuracy 91.749%\n",
      "Epoch 30, Batch 984, LR 0.592130 Loss 4.036759, Accuracy 91.748%\n",
      "Epoch 30, Batch 985, LR 0.592016 Loss 4.036643, Accuracy 91.752%\n",
      "Epoch 30, Batch 986, LR 0.591902 Loss 4.036384, Accuracy 91.751%\n",
      "Epoch 30, Batch 987, LR 0.591788 Loss 4.036623, Accuracy 91.751%\n",
      "Epoch 30, Batch 988, LR 0.591675 Loss 4.036836, Accuracy 91.753%\n",
      "Epoch 30, Batch 989, LR 0.591561 Loss 4.037601, Accuracy 91.752%\n",
      "Epoch 30, Batch 990, LR 0.591447 Loss 4.037063, Accuracy 91.760%\n",
      "Epoch 30, Batch 991, LR 0.591333 Loss 4.036933, Accuracy 91.760%\n",
      "Epoch 30, Batch 992, LR 0.591219 Loss 4.037216, Accuracy 91.760%\n",
      "Epoch 30, Batch 993, LR 0.591105 Loss 4.037336, Accuracy 91.761%\n",
      "Epoch 30, Batch 994, LR 0.590992 Loss 4.037219, Accuracy 91.763%\n",
      "Epoch 30, Batch 995, LR 0.590878 Loss 4.036867, Accuracy 91.764%\n",
      "Epoch 30, Batch 996, LR 0.590764 Loss 4.037403, Accuracy 91.759%\n",
      "Epoch 30, Batch 997, LR 0.590650 Loss 4.037085, Accuracy 91.761%\n",
      "Epoch 30, Batch 998, LR 0.590536 Loss 4.036926, Accuracy 91.762%\n",
      "Epoch 30, Batch 999, LR 0.590422 Loss 4.037629, Accuracy 91.758%\n",
      "Epoch 30, Batch 1000, LR 0.590309 Loss 4.037392, Accuracy 91.758%\n",
      "Epoch 30, Batch 1001, LR 0.590195 Loss 4.037039, Accuracy 91.759%\n",
      "Epoch 30, Batch 1002, LR 0.590081 Loss 4.037758, Accuracy 91.757%\n",
      "Epoch 30, Batch 1003, LR 0.589967 Loss 4.038181, Accuracy 91.757%\n",
      "Epoch 30, Batch 1004, LR 0.589854 Loss 4.037791, Accuracy 91.758%\n",
      "Epoch 30, Batch 1005, LR 0.589740 Loss 4.037857, Accuracy 91.754%\n",
      "Epoch 30, Batch 1006, LR 0.589626 Loss 4.037836, Accuracy 91.751%\n",
      "Epoch 30, Batch 1007, LR 0.589512 Loss 4.038241, Accuracy 91.750%\n",
      "Epoch 30, Batch 1008, LR 0.589399 Loss 4.038646, Accuracy 91.749%\n",
      "Epoch 30, Batch 1009, LR 0.589285 Loss 4.038599, Accuracy 91.749%\n",
      "Epoch 30, Batch 1010, LR 0.589171 Loss 4.038249, Accuracy 91.750%\n",
      "Epoch 30, Batch 1011, LR 0.589058 Loss 4.038501, Accuracy 91.747%\n",
      "Epoch 30, Batch 1012, LR 0.588944 Loss 4.038084, Accuracy 91.752%\n",
      "Epoch 30, Batch 1013, LR 0.588830 Loss 4.038419, Accuracy 91.753%\n",
      "Epoch 30, Batch 1014, LR 0.588717 Loss 4.038436, Accuracy 91.752%\n",
      "Epoch 30, Batch 1015, LR 0.588603 Loss 4.038160, Accuracy 91.750%\n",
      "Epoch 30, Batch 1016, LR 0.588489 Loss 4.038780, Accuracy 91.749%\n",
      "Epoch 30, Batch 1017, LR 0.588376 Loss 4.038116, Accuracy 91.753%\n",
      "Epoch 30, Batch 1018, LR 0.588262 Loss 4.038170, Accuracy 91.752%\n",
      "Epoch 30, Batch 1019, LR 0.588148 Loss 4.038022, Accuracy 91.750%\n",
      "Epoch 30, Batch 1020, LR 0.588035 Loss 4.037776, Accuracy 91.751%\n",
      "Epoch 30, Batch 1021, LR 0.587921 Loss 4.038120, Accuracy 91.745%\n",
      "Epoch 30, Batch 1022, LR 0.587807 Loss 4.037811, Accuracy 91.745%\n",
      "Epoch 30, Batch 1023, LR 0.587694 Loss 4.037888, Accuracy 91.742%\n",
      "Epoch 30, Batch 1024, LR 0.587580 Loss 4.037512, Accuracy 91.742%\n",
      "Epoch 30, Batch 1025, LR 0.587467 Loss 4.037052, Accuracy 91.742%\n",
      "Epoch 30, Batch 1026, LR 0.587353 Loss 4.037511, Accuracy 91.741%\n",
      "Epoch 30, Batch 1027, LR 0.587239 Loss 4.037015, Accuracy 91.742%\n",
      "Epoch 30, Batch 1028, LR 0.587126 Loss 4.036893, Accuracy 91.745%\n",
      "Epoch 30, Batch 1029, LR 0.587012 Loss 4.036349, Accuracy 91.747%\n",
      "Epoch 30, Batch 1030, LR 0.586899 Loss 4.036034, Accuracy 91.747%\n",
      "Epoch 30, Batch 1031, LR 0.586785 Loss 4.035828, Accuracy 91.749%\n",
      "Epoch 30, Batch 1032, LR 0.586672 Loss 4.036130, Accuracy 91.748%\n",
      "Epoch 30, Batch 1033, LR 0.586558 Loss 4.035715, Accuracy 91.750%\n",
      "Epoch 30, Batch 1034, LR 0.586445 Loss 4.035674, Accuracy 91.749%\n",
      "Epoch 30, Batch 1035, LR 0.586331 Loss 4.035390, Accuracy 91.750%\n",
      "Epoch 30, Batch 1036, LR 0.586218 Loss 4.035976, Accuracy 91.749%\n",
      "Epoch 30, Batch 1037, LR 0.586104 Loss 4.036097, Accuracy 91.747%\n",
      "Epoch 30, Batch 1038, LR 0.585991 Loss 4.035887, Accuracy 91.747%\n",
      "Epoch 30, Batch 1039, LR 0.585877 Loss 4.035873, Accuracy 91.748%\n",
      "Epoch 30, Batch 1040, LR 0.585764 Loss 4.035354, Accuracy 91.749%\n",
      "Epoch 30, Batch 1041, LR 0.585650 Loss 4.035050, Accuracy 91.748%\n",
      "Epoch 30, Batch 1042, LR 0.585537 Loss 4.034821, Accuracy 91.750%\n",
      "Epoch 30, Batch 1043, LR 0.585423 Loss 4.034710, Accuracy 91.750%\n",
      "Epoch 30, Batch 1044, LR 0.585310 Loss 4.035093, Accuracy 91.748%\n",
      "Epoch 30, Batch 1045, LR 0.585196 Loss 4.034929, Accuracy 91.751%\n",
      "Epoch 30, Batch 1046, LR 0.585083 Loss 4.034644, Accuracy 91.752%\n",
      "Epoch 30, Batch 1047, LR 0.584969 Loss 4.034948, Accuracy 91.750%\n",
      "Epoch 30, Loss (train set) 4.034948, Accuracy (train set) 91.750%\n",
      "Epoch 31, Batch 1, LR 0.584856 Loss 4.155333, Accuracy 94.531%\n",
      "Epoch 31, Batch 2, LR 0.584743 Loss 4.183072, Accuracy 92.578%\n",
      "Epoch 31, Batch 3, LR 0.584629 Loss 4.116666, Accuracy 91.927%\n",
      "Epoch 31, Batch 4, LR 0.584516 Loss 4.126163, Accuracy 91.211%\n",
      "Epoch 31, Batch 5, LR 0.584402 Loss 4.144686, Accuracy 90.938%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 6, LR 0.584289 Loss 4.205199, Accuracy 90.495%\n",
      "Epoch 31, Batch 7, LR 0.584176 Loss 4.237386, Accuracy 89.955%\n",
      "Epoch 31, Batch 8, LR 0.584062 Loss 4.209870, Accuracy 89.746%\n",
      "Epoch 31, Batch 9, LR 0.583949 Loss 4.179955, Accuracy 90.365%\n",
      "Epoch 31, Batch 10, LR 0.583836 Loss 4.148549, Accuracy 90.547%\n",
      "Epoch 31, Batch 11, LR 0.583722 Loss 4.149972, Accuracy 90.554%\n",
      "Epoch 31, Batch 12, LR 0.583609 Loss 4.124659, Accuracy 90.755%\n",
      "Epoch 31, Batch 13, LR 0.583496 Loss 4.118274, Accuracy 91.046%\n",
      "Epoch 31, Batch 14, LR 0.583382 Loss 4.114882, Accuracy 91.016%\n",
      "Epoch 31, Batch 15, LR 0.583269 Loss 4.082807, Accuracy 91.250%\n",
      "Epoch 31, Batch 16, LR 0.583156 Loss 4.056072, Accuracy 91.309%\n",
      "Epoch 31, Batch 17, LR 0.583042 Loss 4.059627, Accuracy 91.360%\n",
      "Epoch 31, Batch 18, LR 0.582929 Loss 4.063567, Accuracy 91.276%\n",
      "Epoch 31, Batch 19, LR 0.582816 Loss 4.081453, Accuracy 91.118%\n",
      "Epoch 31, Batch 20, LR 0.582703 Loss 4.072654, Accuracy 91.094%\n",
      "Epoch 31, Batch 21, LR 0.582589 Loss 4.040885, Accuracy 91.071%\n",
      "Epoch 31, Batch 22, LR 0.582476 Loss 4.029297, Accuracy 91.087%\n",
      "Epoch 31, Batch 23, LR 0.582363 Loss 4.025041, Accuracy 91.067%\n",
      "Epoch 31, Batch 24, LR 0.582250 Loss 4.023431, Accuracy 91.048%\n",
      "Epoch 31, Batch 25, LR 0.582136 Loss 4.030229, Accuracy 91.031%\n",
      "Epoch 31, Batch 26, LR 0.582023 Loss 4.032970, Accuracy 90.986%\n",
      "Epoch 31, Batch 27, LR 0.581910 Loss 4.028641, Accuracy 91.001%\n",
      "Epoch 31, Batch 28, LR 0.581797 Loss 4.030137, Accuracy 91.127%\n",
      "Epoch 31, Batch 29, LR 0.581683 Loss 4.008385, Accuracy 91.218%\n",
      "Epoch 31, Batch 30, LR 0.581570 Loss 4.012797, Accuracy 91.172%\n",
      "Epoch 31, Batch 31, LR 0.581457 Loss 3.976878, Accuracy 91.331%\n",
      "Epoch 31, Batch 32, LR 0.581344 Loss 3.958980, Accuracy 91.333%\n",
      "Epoch 31, Batch 33, LR 0.581231 Loss 3.970320, Accuracy 91.264%\n",
      "Epoch 31, Batch 34, LR 0.581118 Loss 3.953914, Accuracy 91.314%\n",
      "Epoch 31, Batch 35, LR 0.581004 Loss 3.970248, Accuracy 91.295%\n",
      "Epoch 31, Batch 36, LR 0.580891 Loss 3.977713, Accuracy 91.341%\n",
      "Epoch 31, Batch 37, LR 0.580778 Loss 3.971346, Accuracy 91.322%\n",
      "Epoch 31, Batch 38, LR 0.580665 Loss 3.989543, Accuracy 91.242%\n",
      "Epoch 31, Batch 39, LR 0.580552 Loss 3.999156, Accuracy 91.246%\n",
      "Epoch 31, Batch 40, LR 0.580439 Loss 4.004882, Accuracy 91.230%\n",
      "Epoch 31, Batch 41, LR 0.580326 Loss 4.016323, Accuracy 91.159%\n",
      "Epoch 31, Batch 42, LR 0.580213 Loss 4.000411, Accuracy 91.257%\n",
      "Epoch 31, Batch 43, LR 0.580099 Loss 4.014790, Accuracy 91.188%\n",
      "Epoch 31, Batch 44, LR 0.579986 Loss 4.022690, Accuracy 91.229%\n",
      "Epoch 31, Batch 45, LR 0.579873 Loss 4.012101, Accuracy 91.285%\n",
      "Epoch 31, Batch 46, LR 0.579760 Loss 3.997147, Accuracy 91.321%\n",
      "Epoch 31, Batch 47, LR 0.579647 Loss 3.995019, Accuracy 91.323%\n",
      "Epoch 31, Batch 48, LR 0.579534 Loss 4.004588, Accuracy 91.309%\n",
      "Epoch 31, Batch 49, LR 0.579421 Loss 4.012643, Accuracy 91.327%\n",
      "Epoch 31, Batch 50, LR 0.579308 Loss 4.018511, Accuracy 91.297%\n",
      "Epoch 31, Batch 51, LR 0.579195 Loss 4.003985, Accuracy 91.391%\n",
      "Epoch 31, Batch 52, LR 0.579082 Loss 4.000641, Accuracy 91.406%\n",
      "Epoch 31, Batch 53, LR 0.578969 Loss 3.991602, Accuracy 91.421%\n",
      "Epoch 31, Batch 54, LR 0.578856 Loss 3.998347, Accuracy 91.450%\n",
      "Epoch 31, Batch 55, LR 0.578743 Loss 3.988291, Accuracy 91.491%\n",
      "Epoch 31, Batch 56, LR 0.578630 Loss 3.987959, Accuracy 91.490%\n",
      "Epoch 31, Batch 57, LR 0.578517 Loss 3.990565, Accuracy 91.461%\n",
      "Epoch 31, Batch 58, LR 0.578404 Loss 3.995040, Accuracy 91.447%\n",
      "Epoch 31, Batch 59, LR 0.578291 Loss 3.998044, Accuracy 91.446%\n",
      "Epoch 31, Batch 60, LR 0.578178 Loss 3.993556, Accuracy 91.497%\n",
      "Epoch 31, Batch 61, LR 0.578065 Loss 3.986953, Accuracy 91.509%\n",
      "Epoch 31, Batch 62, LR 0.577952 Loss 3.983881, Accuracy 91.520%\n",
      "Epoch 31, Batch 63, LR 0.577839 Loss 3.982773, Accuracy 91.592%\n",
      "Epoch 31, Batch 64, LR 0.577726 Loss 3.979407, Accuracy 91.602%\n",
      "Epoch 31, Batch 65, LR 0.577613 Loss 3.974987, Accuracy 91.587%\n",
      "Epoch 31, Batch 66, LR 0.577500 Loss 3.967600, Accuracy 91.619%\n",
      "Epoch 31, Batch 67, LR 0.577387 Loss 3.962496, Accuracy 91.651%\n",
      "Epoch 31, Batch 68, LR 0.577275 Loss 3.944242, Accuracy 91.739%\n",
      "Epoch 31, Batch 69, LR 0.577162 Loss 3.945483, Accuracy 91.757%\n",
      "Epoch 31, Batch 70, LR 0.577049 Loss 3.948934, Accuracy 91.775%\n",
      "Epoch 31, Batch 71, LR 0.576936 Loss 3.943429, Accuracy 91.802%\n",
      "Epoch 31, Batch 72, LR 0.576823 Loss 3.940940, Accuracy 91.786%\n",
      "Epoch 31, Batch 73, LR 0.576710 Loss 3.945422, Accuracy 91.738%\n",
      "Epoch 31, Batch 74, LR 0.576597 Loss 3.953843, Accuracy 91.681%\n",
      "Epoch 31, Batch 75, LR 0.576484 Loss 3.953892, Accuracy 91.708%\n",
      "Epoch 31, Batch 76, LR 0.576372 Loss 3.947929, Accuracy 91.694%\n",
      "Epoch 31, Batch 77, LR 0.576259 Loss 3.943804, Accuracy 91.721%\n",
      "Epoch 31, Batch 78, LR 0.576146 Loss 3.939708, Accuracy 91.727%\n",
      "Epoch 31, Batch 79, LR 0.576033 Loss 3.941791, Accuracy 91.713%\n",
      "Epoch 31, Batch 80, LR 0.575920 Loss 3.939579, Accuracy 91.748%\n",
      "Epoch 31, Batch 81, LR 0.575808 Loss 3.939395, Accuracy 91.763%\n",
      "Epoch 31, Batch 82, LR 0.575695 Loss 3.943701, Accuracy 91.759%\n",
      "Epoch 31, Batch 83, LR 0.575582 Loss 3.939415, Accuracy 91.773%\n",
      "Epoch 31, Batch 84, LR 0.575469 Loss 3.944797, Accuracy 91.732%\n",
      "Epoch 31, Batch 85, LR 0.575356 Loss 3.939509, Accuracy 91.774%\n",
      "Epoch 31, Batch 86, LR 0.575244 Loss 3.941348, Accuracy 91.761%\n",
      "Epoch 31, Batch 87, LR 0.575131 Loss 3.946329, Accuracy 91.810%\n",
      "Epoch 31, Batch 88, LR 0.575018 Loss 3.949130, Accuracy 91.779%\n",
      "Epoch 31, Batch 89, LR 0.574905 Loss 3.945258, Accuracy 91.828%\n",
      "Epoch 31, Batch 90, LR 0.574793 Loss 3.938087, Accuracy 91.858%\n",
      "Epoch 31, Batch 91, LR 0.574680 Loss 3.939629, Accuracy 91.827%\n",
      "Epoch 31, Batch 92, LR 0.574567 Loss 3.934701, Accuracy 91.831%\n",
      "Epoch 31, Batch 93, LR 0.574454 Loss 3.928437, Accuracy 91.877%\n",
      "Epoch 31, Batch 94, LR 0.574342 Loss 3.927089, Accuracy 91.872%\n",
      "Epoch 31, Batch 95, LR 0.574229 Loss 3.928648, Accuracy 91.842%\n",
      "Epoch 31, Batch 96, LR 0.574116 Loss 3.928518, Accuracy 91.846%\n",
      "Epoch 31, Batch 97, LR 0.574004 Loss 3.936576, Accuracy 91.833%\n",
      "Epoch 31, Batch 98, LR 0.573891 Loss 3.936250, Accuracy 91.821%\n",
      "Epoch 31, Batch 99, LR 0.573778 Loss 3.938731, Accuracy 91.832%\n",
      "Epoch 31, Batch 100, LR 0.573666 Loss 3.932582, Accuracy 91.828%\n",
      "Epoch 31, Batch 101, LR 0.573553 Loss 3.932263, Accuracy 91.839%\n",
      "Epoch 31, Batch 102, LR 0.573440 Loss 3.926763, Accuracy 91.850%\n",
      "Epoch 31, Batch 103, LR 0.573328 Loss 3.925032, Accuracy 91.861%\n",
      "Epoch 31, Batch 104, LR 0.573215 Loss 3.925304, Accuracy 91.834%\n",
      "Epoch 31, Batch 105, LR 0.573103 Loss 3.922658, Accuracy 91.845%\n",
      "Epoch 31, Batch 106, LR 0.572990 Loss 3.923469, Accuracy 91.812%\n",
      "Epoch 31, Batch 107, LR 0.572877 Loss 3.919699, Accuracy 91.822%\n",
      "Epoch 31, Batch 108, LR 0.572765 Loss 3.919886, Accuracy 91.855%\n",
      "Epoch 31, Batch 109, LR 0.572652 Loss 3.919305, Accuracy 91.872%\n",
      "Epoch 31, Batch 110, LR 0.572540 Loss 3.916816, Accuracy 91.896%\n",
      "Epoch 31, Batch 111, LR 0.572427 Loss 3.914964, Accuracy 91.913%\n",
      "Epoch 31, Batch 112, LR 0.572314 Loss 3.920870, Accuracy 91.860%\n",
      "Epoch 31, Batch 113, LR 0.572202 Loss 3.921513, Accuracy 91.869%\n",
      "Epoch 31, Batch 114, LR 0.572089 Loss 3.925318, Accuracy 91.852%\n",
      "Epoch 31, Batch 115, LR 0.571977 Loss 3.925730, Accuracy 91.861%\n",
      "Epoch 31, Batch 116, LR 0.571864 Loss 3.925050, Accuracy 91.884%\n",
      "Epoch 31, Batch 117, LR 0.571752 Loss 3.918386, Accuracy 91.914%\n",
      "Epoch 31, Batch 118, LR 0.571639 Loss 3.920354, Accuracy 91.896%\n",
      "Epoch 31, Batch 119, LR 0.571527 Loss 3.918332, Accuracy 91.899%\n",
      "Epoch 31, Batch 120, LR 0.571414 Loss 3.917497, Accuracy 91.901%\n",
      "Epoch 31, Batch 121, LR 0.571302 Loss 3.921682, Accuracy 91.903%\n",
      "Epoch 31, Batch 122, LR 0.571189 Loss 3.920239, Accuracy 91.887%\n",
      "Epoch 31, Batch 123, LR 0.571077 Loss 3.916858, Accuracy 91.889%\n",
      "Epoch 31, Batch 124, LR 0.570964 Loss 3.922452, Accuracy 91.828%\n",
      "Epoch 31, Batch 125, LR 0.570852 Loss 3.922338, Accuracy 91.838%\n",
      "Epoch 31, Batch 126, LR 0.570739 Loss 3.918863, Accuracy 91.865%\n",
      "Epoch 31, Batch 127, LR 0.570627 Loss 3.920017, Accuracy 91.861%\n",
      "Epoch 31, Batch 128, LR 0.570515 Loss 3.921389, Accuracy 91.870%\n",
      "Epoch 31, Batch 129, LR 0.570402 Loss 3.919325, Accuracy 91.860%\n",
      "Epoch 31, Batch 130, LR 0.570290 Loss 3.920935, Accuracy 91.845%\n",
      "Epoch 31, Batch 131, LR 0.570177 Loss 3.919585, Accuracy 91.854%\n",
      "Epoch 31, Batch 132, LR 0.570065 Loss 3.917236, Accuracy 91.892%\n",
      "Epoch 31, Batch 133, LR 0.569952 Loss 3.911510, Accuracy 91.917%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 134, LR 0.569840 Loss 3.909999, Accuracy 91.919%\n",
      "Epoch 31, Batch 135, LR 0.569728 Loss 3.906479, Accuracy 91.927%\n",
      "Epoch 31, Batch 136, LR 0.569615 Loss 3.904114, Accuracy 91.940%\n",
      "Epoch 31, Batch 137, LR 0.569503 Loss 3.908596, Accuracy 91.914%\n",
      "Epoch 31, Batch 138, LR 0.569391 Loss 3.907201, Accuracy 91.916%\n",
      "Epoch 31, Batch 139, LR 0.569278 Loss 3.910302, Accuracy 91.890%\n",
      "Epoch 31, Batch 140, LR 0.569166 Loss 3.912382, Accuracy 91.886%\n",
      "Epoch 31, Batch 141, LR 0.569054 Loss 3.911177, Accuracy 91.883%\n",
      "Epoch 31, Batch 142, LR 0.568941 Loss 3.911247, Accuracy 91.868%\n",
      "Epoch 31, Batch 143, LR 0.568829 Loss 3.910510, Accuracy 91.854%\n",
      "Epoch 31, Batch 144, LR 0.568717 Loss 3.913326, Accuracy 91.862%\n",
      "Epoch 31, Batch 145, LR 0.568604 Loss 3.913283, Accuracy 91.843%\n",
      "Epoch 31, Batch 146, LR 0.568492 Loss 3.913589, Accuracy 91.834%\n",
      "Epoch 31, Batch 147, LR 0.568380 Loss 3.913404, Accuracy 91.842%\n",
      "Epoch 31, Batch 148, LR 0.568267 Loss 3.912258, Accuracy 91.860%\n",
      "Epoch 31, Batch 149, LR 0.568155 Loss 3.912891, Accuracy 91.883%\n",
      "Epoch 31, Batch 150, LR 0.568043 Loss 3.911547, Accuracy 91.917%\n",
      "Epoch 31, Batch 151, LR 0.567931 Loss 3.915461, Accuracy 91.893%\n",
      "Epoch 31, Batch 152, LR 0.567818 Loss 3.914829, Accuracy 91.900%\n",
      "Epoch 31, Batch 153, LR 0.567706 Loss 3.908521, Accuracy 91.927%\n",
      "Epoch 31, Batch 154, LR 0.567594 Loss 3.907183, Accuracy 91.954%\n",
      "Epoch 31, Batch 155, LR 0.567482 Loss 3.900899, Accuracy 91.981%\n",
      "Epoch 31, Batch 156, LR 0.567369 Loss 3.898957, Accuracy 91.977%\n",
      "Epoch 31, Batch 157, LR 0.567257 Loss 3.896134, Accuracy 91.988%\n",
      "Epoch 31, Batch 158, LR 0.567145 Loss 3.893775, Accuracy 91.995%\n",
      "Epoch 31, Batch 159, LR 0.567033 Loss 3.895203, Accuracy 91.966%\n",
      "Epoch 31, Batch 160, LR 0.566921 Loss 3.892826, Accuracy 91.992%\n",
      "Epoch 31, Batch 161, LR 0.566809 Loss 3.889561, Accuracy 92.008%\n",
      "Epoch 31, Batch 162, LR 0.566696 Loss 3.890842, Accuracy 92.019%\n",
      "Epoch 31, Batch 163, LR 0.566584 Loss 3.894679, Accuracy 91.991%\n",
      "Epoch 31, Batch 164, LR 0.566472 Loss 3.896655, Accuracy 91.964%\n",
      "Epoch 31, Batch 165, LR 0.566360 Loss 3.899172, Accuracy 91.970%\n",
      "Epoch 31, Batch 166, LR 0.566248 Loss 3.900880, Accuracy 91.976%\n",
      "Epoch 31, Batch 167, LR 0.566136 Loss 3.900863, Accuracy 91.968%\n",
      "Epoch 31, Batch 168, LR 0.566023 Loss 3.900928, Accuracy 91.988%\n",
      "Epoch 31, Batch 169, LR 0.565911 Loss 3.899740, Accuracy 92.003%\n",
      "Epoch 31, Batch 170, LR 0.565799 Loss 3.905013, Accuracy 91.981%\n",
      "Epoch 31, Batch 171, LR 0.565687 Loss 3.908186, Accuracy 91.973%\n",
      "Epoch 31, Batch 172, LR 0.565575 Loss 3.912974, Accuracy 91.960%\n",
      "Epoch 31, Batch 173, LR 0.565463 Loss 3.912816, Accuracy 91.975%\n",
      "Epoch 31, Batch 174, LR 0.565351 Loss 3.915098, Accuracy 91.963%\n",
      "Epoch 31, Batch 175, LR 0.565239 Loss 3.913491, Accuracy 91.973%\n",
      "Epoch 31, Batch 176, LR 0.565127 Loss 3.911863, Accuracy 91.988%\n",
      "Epoch 31, Batch 177, LR 0.565015 Loss 3.914541, Accuracy 91.984%\n",
      "Epoch 31, Batch 178, LR 0.564903 Loss 3.914513, Accuracy 91.986%\n",
      "Epoch 31, Batch 179, LR 0.564791 Loss 3.915421, Accuracy 91.982%\n",
      "Epoch 31, Batch 180, LR 0.564679 Loss 3.918392, Accuracy 91.966%\n",
      "Epoch 31, Batch 181, LR 0.564567 Loss 3.919941, Accuracy 91.967%\n",
      "Epoch 31, Batch 182, LR 0.564455 Loss 3.921747, Accuracy 91.943%\n",
      "Epoch 31, Batch 183, LR 0.564343 Loss 3.917531, Accuracy 91.961%\n",
      "Epoch 31, Batch 184, LR 0.564231 Loss 3.918276, Accuracy 91.954%\n",
      "Epoch 31, Batch 185, LR 0.564119 Loss 3.918373, Accuracy 91.968%\n",
      "Epoch 31, Batch 186, LR 0.564007 Loss 3.915709, Accuracy 91.990%\n",
      "Epoch 31, Batch 187, LR 0.563895 Loss 3.914281, Accuracy 91.995%\n",
      "Epoch 31, Batch 188, LR 0.563783 Loss 3.915247, Accuracy 91.996%\n",
      "Epoch 31, Batch 189, LR 0.563671 Loss 3.918279, Accuracy 91.977%\n",
      "Epoch 31, Batch 190, LR 0.563559 Loss 3.915374, Accuracy 92.002%\n",
      "Epoch 31, Batch 191, LR 0.563447 Loss 3.914801, Accuracy 92.008%\n",
      "Epoch 31, Batch 192, LR 0.563335 Loss 3.912688, Accuracy 92.021%\n",
      "Epoch 31, Batch 193, LR 0.563223 Loss 3.909732, Accuracy 92.034%\n",
      "Epoch 31, Batch 194, LR 0.563111 Loss 3.910488, Accuracy 92.030%\n",
      "Epoch 31, Batch 195, LR 0.562999 Loss 3.909579, Accuracy 92.039%\n",
      "Epoch 31, Batch 196, LR 0.562887 Loss 3.908779, Accuracy 92.056%\n",
      "Epoch 31, Batch 197, LR 0.562775 Loss 3.908568, Accuracy 92.057%\n",
      "Epoch 31, Batch 198, LR 0.562663 Loss 3.905284, Accuracy 92.061%\n",
      "Epoch 31, Batch 199, LR 0.562552 Loss 3.904438, Accuracy 92.058%\n",
      "Epoch 31, Batch 200, LR 0.562440 Loss 3.906823, Accuracy 92.043%\n",
      "Epoch 31, Batch 201, LR 0.562328 Loss 3.908374, Accuracy 92.036%\n",
      "Epoch 31, Batch 202, LR 0.562216 Loss 3.910554, Accuracy 92.033%\n",
      "Epoch 31, Batch 203, LR 0.562104 Loss 3.910798, Accuracy 92.045%\n",
      "Epoch 31, Batch 204, LR 0.561992 Loss 3.907752, Accuracy 92.057%\n",
      "Epoch 31, Batch 205, LR 0.561880 Loss 3.910707, Accuracy 92.062%\n",
      "Epoch 31, Batch 206, LR 0.561769 Loss 3.908753, Accuracy 92.074%\n",
      "Epoch 31, Batch 207, LR 0.561657 Loss 3.907227, Accuracy 92.074%\n",
      "Epoch 31, Batch 208, LR 0.561545 Loss 3.906024, Accuracy 92.075%\n",
      "Epoch 31, Batch 209, LR 0.561433 Loss 3.908691, Accuracy 92.057%\n",
      "Epoch 31, Batch 210, LR 0.561321 Loss 3.908606, Accuracy 92.054%\n",
      "Epoch 31, Batch 211, LR 0.561210 Loss 3.913317, Accuracy 92.032%\n",
      "Epoch 31, Batch 212, LR 0.561098 Loss 3.910437, Accuracy 92.044%\n",
      "Epoch 31, Batch 213, LR 0.560986 Loss 3.912054, Accuracy 92.048%\n",
      "Epoch 31, Batch 214, LR 0.560874 Loss 3.911053, Accuracy 92.063%\n",
      "Epoch 31, Batch 215, LR 0.560763 Loss 3.912517, Accuracy 92.060%\n",
      "Epoch 31, Batch 216, LR 0.560651 Loss 3.908959, Accuracy 92.079%\n",
      "Epoch 31, Batch 217, LR 0.560539 Loss 3.906031, Accuracy 92.083%\n",
      "Epoch 31, Batch 218, LR 0.560427 Loss 3.908373, Accuracy 92.062%\n",
      "Epoch 31, Batch 219, LR 0.560316 Loss 3.909015, Accuracy 92.056%\n",
      "Epoch 31, Batch 220, LR 0.560204 Loss 3.911542, Accuracy 92.035%\n",
      "Epoch 31, Batch 221, LR 0.560092 Loss 3.912300, Accuracy 92.028%\n",
      "Epoch 31, Batch 222, LR 0.559980 Loss 3.913752, Accuracy 92.033%\n",
      "Epoch 31, Batch 223, LR 0.559869 Loss 3.916980, Accuracy 92.023%\n",
      "Epoch 31, Batch 224, LR 0.559757 Loss 3.920734, Accuracy 92.010%\n",
      "Epoch 31, Batch 225, LR 0.559645 Loss 3.921787, Accuracy 92.003%\n",
      "Epoch 31, Batch 226, LR 0.559534 Loss 3.924413, Accuracy 91.984%\n",
      "Epoch 31, Batch 227, LR 0.559422 Loss 3.925098, Accuracy 91.974%\n",
      "Epoch 31, Batch 228, LR 0.559310 Loss 3.925139, Accuracy 91.978%\n",
      "Epoch 31, Batch 229, LR 0.559199 Loss 3.925110, Accuracy 91.986%\n",
      "Epoch 31, Batch 230, LR 0.559087 Loss 3.923807, Accuracy 91.990%\n",
      "Epoch 31, Batch 231, LR 0.558976 Loss 3.922949, Accuracy 91.995%\n",
      "Epoch 31, Batch 232, LR 0.558864 Loss 3.921581, Accuracy 92.006%\n",
      "Epoch 31, Batch 233, LR 0.558752 Loss 3.921586, Accuracy 92.013%\n",
      "Epoch 31, Batch 234, LR 0.558641 Loss 3.921370, Accuracy 92.014%\n",
      "Epoch 31, Batch 235, LR 0.558529 Loss 3.922542, Accuracy 92.005%\n",
      "Epoch 31, Batch 236, LR 0.558418 Loss 3.922099, Accuracy 92.012%\n",
      "Epoch 31, Batch 237, LR 0.558306 Loss 3.921406, Accuracy 92.023%\n",
      "Epoch 31, Batch 238, LR 0.558194 Loss 3.922355, Accuracy 92.023%\n",
      "Epoch 31, Batch 239, LR 0.558083 Loss 3.923987, Accuracy 92.021%\n",
      "Epoch 31, Batch 240, LR 0.557971 Loss 3.928048, Accuracy 91.995%\n",
      "Epoch 31, Batch 241, LR 0.557860 Loss 3.926931, Accuracy 92.003%\n",
      "Epoch 31, Batch 242, LR 0.557748 Loss 3.926000, Accuracy 92.010%\n",
      "Epoch 31, Batch 243, LR 0.557637 Loss 3.928805, Accuracy 92.007%\n",
      "Epoch 31, Batch 244, LR 0.557525 Loss 3.929825, Accuracy 91.995%\n",
      "Epoch 31, Batch 245, LR 0.557414 Loss 3.931374, Accuracy 91.996%\n",
      "Epoch 31, Batch 246, LR 0.557302 Loss 3.931495, Accuracy 91.997%\n",
      "Epoch 31, Batch 247, LR 0.557191 Loss 3.931313, Accuracy 91.995%\n",
      "Epoch 31, Batch 248, LR 0.557079 Loss 3.931348, Accuracy 91.998%\n",
      "Epoch 31, Batch 249, LR 0.556968 Loss 3.932288, Accuracy 91.999%\n",
      "Epoch 31, Batch 250, LR 0.556856 Loss 3.935282, Accuracy 91.984%\n",
      "Epoch 31, Batch 251, LR 0.556745 Loss 3.936935, Accuracy 91.979%\n",
      "Epoch 31, Batch 252, LR 0.556633 Loss 3.940413, Accuracy 91.964%\n",
      "Epoch 31, Batch 253, LR 0.556522 Loss 3.938452, Accuracy 91.968%\n",
      "Epoch 31, Batch 254, LR 0.556410 Loss 3.938559, Accuracy 91.978%\n",
      "Epoch 31, Batch 255, LR 0.556299 Loss 3.935779, Accuracy 91.994%\n",
      "Epoch 31, Batch 256, LR 0.556187 Loss 3.933969, Accuracy 92.010%\n",
      "Epoch 31, Batch 257, LR 0.556076 Loss 3.936041, Accuracy 92.002%\n",
      "Epoch 31, Batch 258, LR 0.555965 Loss 3.935039, Accuracy 92.000%\n",
      "Epoch 31, Batch 259, LR 0.555853 Loss 3.933757, Accuracy 91.997%\n",
      "Epoch 31, Batch 260, LR 0.555742 Loss 3.932922, Accuracy 92.007%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 261, LR 0.555630 Loss 3.932221, Accuracy 92.011%\n",
      "Epoch 31, Batch 262, LR 0.555519 Loss 3.932012, Accuracy 92.018%\n",
      "Epoch 31, Batch 263, LR 0.555408 Loss 3.932526, Accuracy 92.006%\n",
      "Epoch 31, Batch 264, LR 0.555296 Loss 3.933445, Accuracy 91.995%\n",
      "Epoch 31, Batch 265, LR 0.555185 Loss 3.933424, Accuracy 91.990%\n",
      "Epoch 31, Batch 266, LR 0.555074 Loss 3.935632, Accuracy 91.988%\n",
      "Epoch 31, Batch 267, LR 0.554962 Loss 3.934023, Accuracy 92.003%\n",
      "Epoch 31, Batch 268, LR 0.554851 Loss 3.931600, Accuracy 92.016%\n",
      "Epoch 31, Batch 269, LR 0.554740 Loss 3.930358, Accuracy 92.016%\n",
      "Epoch 31, Batch 270, LR 0.554628 Loss 3.931361, Accuracy 92.008%\n",
      "Epoch 31, Batch 271, LR 0.554517 Loss 3.931800, Accuracy 92.015%\n",
      "Epoch 31, Batch 272, LR 0.554406 Loss 3.932415, Accuracy 92.018%\n",
      "Epoch 31, Batch 273, LR 0.554294 Loss 3.931939, Accuracy 92.019%\n",
      "Epoch 31, Batch 274, LR 0.554183 Loss 3.932098, Accuracy 92.022%\n",
      "Epoch 31, Batch 275, LR 0.554072 Loss 3.931506, Accuracy 92.023%\n",
      "Epoch 31, Batch 276, LR 0.553961 Loss 3.932719, Accuracy 92.023%\n",
      "Epoch 31, Batch 277, LR 0.553849 Loss 3.931372, Accuracy 92.018%\n",
      "Epoch 31, Batch 278, LR 0.553738 Loss 3.930765, Accuracy 92.019%\n",
      "Epoch 31, Batch 279, LR 0.553627 Loss 3.930498, Accuracy 92.022%\n",
      "Epoch 31, Batch 280, LR 0.553516 Loss 3.931920, Accuracy 92.006%\n",
      "Epoch 31, Batch 281, LR 0.553404 Loss 3.931471, Accuracy 92.012%\n",
      "Epoch 31, Batch 282, LR 0.553293 Loss 3.932059, Accuracy 92.007%\n",
      "Epoch 31, Batch 283, LR 0.553182 Loss 3.934440, Accuracy 91.989%\n",
      "Epoch 31, Batch 284, LR 0.553071 Loss 3.936382, Accuracy 91.978%\n",
      "Epoch 31, Batch 285, LR 0.552960 Loss 3.939514, Accuracy 91.965%\n",
      "Epoch 31, Batch 286, LR 0.552848 Loss 3.937531, Accuracy 91.977%\n",
      "Epoch 31, Batch 287, LR 0.552737 Loss 3.938080, Accuracy 91.967%\n",
      "Epoch 31, Batch 288, LR 0.552626 Loss 3.938568, Accuracy 91.970%\n",
      "Epoch 31, Batch 289, LR 0.552515 Loss 3.936751, Accuracy 91.974%\n",
      "Epoch 31, Batch 290, LR 0.552404 Loss 3.935724, Accuracy 91.983%\n",
      "Epoch 31, Batch 291, LR 0.552293 Loss 3.935576, Accuracy 91.981%\n",
      "Epoch 31, Batch 292, LR 0.552181 Loss 3.935025, Accuracy 91.987%\n",
      "Epoch 31, Batch 293, LR 0.552070 Loss 3.935270, Accuracy 91.980%\n",
      "Epoch 31, Batch 294, LR 0.551959 Loss 3.936548, Accuracy 91.975%\n",
      "Epoch 31, Batch 295, LR 0.551848 Loss 3.939380, Accuracy 91.962%\n",
      "Epoch 31, Batch 296, LR 0.551737 Loss 3.937149, Accuracy 91.971%\n",
      "Epoch 31, Batch 297, LR 0.551626 Loss 3.935713, Accuracy 91.972%\n",
      "Epoch 31, Batch 298, LR 0.551515 Loss 3.934377, Accuracy 91.975%\n",
      "Epoch 31, Batch 299, LR 0.551404 Loss 3.934268, Accuracy 91.971%\n",
      "Epoch 31, Batch 300, LR 0.551293 Loss 3.936663, Accuracy 91.956%\n",
      "Epoch 31, Batch 301, LR 0.551182 Loss 3.935156, Accuracy 91.967%\n",
      "Epoch 31, Batch 302, LR 0.551070 Loss 3.934928, Accuracy 91.975%\n",
      "Epoch 31, Batch 303, LR 0.550959 Loss 3.935792, Accuracy 91.976%\n",
      "Epoch 31, Batch 304, LR 0.550848 Loss 3.936453, Accuracy 91.972%\n",
      "Epoch 31, Batch 305, LR 0.550737 Loss 3.937839, Accuracy 91.965%\n",
      "Epoch 31, Batch 306, LR 0.550626 Loss 3.937695, Accuracy 91.965%\n",
      "Epoch 31, Batch 307, LR 0.550515 Loss 3.937508, Accuracy 91.969%\n",
      "Epoch 31, Batch 308, LR 0.550404 Loss 3.937558, Accuracy 91.972%\n",
      "Epoch 31, Batch 309, LR 0.550293 Loss 3.939609, Accuracy 91.965%\n",
      "Epoch 31, Batch 310, LR 0.550182 Loss 3.940183, Accuracy 91.953%\n",
      "Epoch 31, Batch 311, LR 0.550071 Loss 3.940538, Accuracy 91.946%\n",
      "Epoch 31, Batch 312, LR 0.549960 Loss 3.940174, Accuracy 91.940%\n",
      "Epoch 31, Batch 313, LR 0.549849 Loss 3.938020, Accuracy 91.953%\n",
      "Epoch 31, Batch 314, LR 0.549738 Loss 3.937784, Accuracy 91.949%\n",
      "Epoch 31, Batch 315, LR 0.549627 Loss 3.939386, Accuracy 91.949%\n",
      "Epoch 31, Batch 316, LR 0.549516 Loss 3.938145, Accuracy 91.958%\n",
      "Epoch 31, Batch 317, LR 0.549406 Loss 3.936460, Accuracy 91.968%\n",
      "Epoch 31, Batch 318, LR 0.549295 Loss 3.935374, Accuracy 91.981%\n",
      "Epoch 31, Batch 319, LR 0.549184 Loss 3.935930, Accuracy 91.977%\n",
      "Epoch 31, Batch 320, LR 0.549073 Loss 3.936766, Accuracy 91.980%\n",
      "Epoch 31, Batch 321, LR 0.548962 Loss 3.938447, Accuracy 91.978%\n",
      "Epoch 31, Batch 322, LR 0.548851 Loss 3.936848, Accuracy 91.986%\n",
      "Epoch 31, Batch 323, LR 0.548740 Loss 3.937483, Accuracy 91.989%\n",
      "Epoch 31, Batch 324, LR 0.548629 Loss 3.935105, Accuracy 92.004%\n",
      "Epoch 31, Batch 325, LR 0.548518 Loss 3.935211, Accuracy 92.005%\n",
      "Epoch 31, Batch 326, LR 0.548407 Loss 3.936471, Accuracy 92.008%\n",
      "Epoch 31, Batch 327, LR 0.548297 Loss 3.937704, Accuracy 91.999%\n",
      "Epoch 31, Batch 328, LR 0.548186 Loss 3.937267, Accuracy 92.004%\n",
      "Epoch 31, Batch 329, LR 0.548075 Loss 3.937011, Accuracy 92.009%\n",
      "Epoch 31, Batch 330, LR 0.547964 Loss 3.935642, Accuracy 92.017%\n",
      "Epoch 31, Batch 331, LR 0.547853 Loss 3.934735, Accuracy 92.022%\n",
      "Epoch 31, Batch 332, LR 0.547742 Loss 3.935168, Accuracy 92.025%\n",
      "Epoch 31, Batch 333, LR 0.547632 Loss 3.934898, Accuracy 92.033%\n",
      "Epoch 31, Batch 334, LR 0.547521 Loss 3.935474, Accuracy 92.035%\n",
      "Epoch 31, Batch 335, LR 0.547410 Loss 3.933868, Accuracy 92.043%\n",
      "Epoch 31, Batch 336, LR 0.547299 Loss 3.931830, Accuracy 92.050%\n",
      "Epoch 31, Batch 337, LR 0.547188 Loss 3.932137, Accuracy 92.058%\n",
      "Epoch 31, Batch 338, LR 0.547078 Loss 3.932815, Accuracy 92.060%\n",
      "Epoch 31, Batch 339, LR 0.546967 Loss 3.931268, Accuracy 92.065%\n",
      "Epoch 31, Batch 340, LR 0.546856 Loss 3.931222, Accuracy 92.063%\n",
      "Epoch 31, Batch 341, LR 0.546745 Loss 3.931317, Accuracy 92.059%\n",
      "Epoch 31, Batch 342, LR 0.546635 Loss 3.933607, Accuracy 92.055%\n",
      "Epoch 31, Batch 343, LR 0.546524 Loss 3.933723, Accuracy 92.046%\n",
      "Epoch 31, Batch 344, LR 0.546413 Loss 3.933375, Accuracy 92.047%\n",
      "Epoch 31, Batch 345, LR 0.546302 Loss 3.932107, Accuracy 92.043%\n",
      "Epoch 31, Batch 346, LR 0.546192 Loss 3.933011, Accuracy 92.041%\n",
      "Epoch 31, Batch 347, LR 0.546081 Loss 3.933267, Accuracy 92.039%\n",
      "Epoch 31, Batch 348, LR 0.545970 Loss 3.932338, Accuracy 92.053%\n",
      "Epoch 31, Batch 349, LR 0.545860 Loss 3.931665, Accuracy 92.064%\n",
      "Epoch 31, Batch 350, LR 0.545749 Loss 3.931435, Accuracy 92.067%\n",
      "Epoch 31, Batch 351, LR 0.545638 Loss 3.930725, Accuracy 92.070%\n",
      "Epoch 31, Batch 352, LR 0.545528 Loss 3.932545, Accuracy 92.065%\n",
      "Epoch 31, Batch 353, LR 0.545417 Loss 3.932558, Accuracy 92.068%\n",
      "Epoch 31, Batch 354, LR 0.545306 Loss 3.933198, Accuracy 92.066%\n",
      "Epoch 31, Batch 355, LR 0.545196 Loss 3.932394, Accuracy 92.071%\n",
      "Epoch 31, Batch 356, LR 0.545085 Loss 3.932585, Accuracy 92.071%\n",
      "Epoch 31, Batch 357, LR 0.544975 Loss 3.931494, Accuracy 92.076%\n",
      "Epoch 31, Batch 358, LR 0.544864 Loss 3.931761, Accuracy 92.074%\n",
      "Epoch 31, Batch 359, LR 0.544753 Loss 3.931428, Accuracy 92.079%\n",
      "Epoch 31, Batch 360, LR 0.544643 Loss 3.930962, Accuracy 92.077%\n",
      "Epoch 31, Batch 361, LR 0.544532 Loss 3.931810, Accuracy 92.060%\n",
      "Epoch 31, Batch 362, LR 0.544422 Loss 3.930899, Accuracy 92.060%\n",
      "Epoch 31, Batch 363, LR 0.544311 Loss 3.930445, Accuracy 92.058%\n",
      "Epoch 31, Batch 364, LR 0.544200 Loss 3.930559, Accuracy 92.059%\n",
      "Epoch 31, Batch 365, LR 0.544090 Loss 3.929912, Accuracy 92.068%\n",
      "Epoch 31, Batch 366, LR 0.543979 Loss 3.931278, Accuracy 92.070%\n",
      "Epoch 31, Batch 367, LR 0.543869 Loss 3.929993, Accuracy 92.077%\n",
      "Epoch 31, Batch 368, LR 0.543758 Loss 3.929914, Accuracy 92.077%\n",
      "Epoch 31, Batch 369, LR 0.543648 Loss 3.929415, Accuracy 92.073%\n",
      "Epoch 31, Batch 370, LR 0.543537 Loss 3.930546, Accuracy 92.063%\n",
      "Epoch 31, Batch 371, LR 0.543427 Loss 3.931462, Accuracy 92.072%\n",
      "Epoch 31, Batch 372, LR 0.543316 Loss 3.931954, Accuracy 92.061%\n",
      "Epoch 31, Batch 373, LR 0.543206 Loss 3.932615, Accuracy 92.056%\n",
      "Epoch 31, Batch 374, LR 0.543095 Loss 3.932969, Accuracy 92.043%\n",
      "Epoch 31, Batch 375, LR 0.542985 Loss 3.933592, Accuracy 92.037%\n",
      "Epoch 31, Batch 376, LR 0.542874 Loss 3.931450, Accuracy 92.040%\n",
      "Epoch 31, Batch 377, LR 0.542764 Loss 3.932399, Accuracy 92.040%\n",
      "Epoch 31, Batch 378, LR 0.542653 Loss 3.932104, Accuracy 92.045%\n",
      "Epoch 31, Batch 379, LR 0.542543 Loss 3.933651, Accuracy 92.035%\n",
      "Epoch 31, Batch 380, LR 0.542433 Loss 3.932387, Accuracy 92.035%\n",
      "Epoch 31, Batch 381, LR 0.542322 Loss 3.933269, Accuracy 92.032%\n",
      "Epoch 31, Batch 382, LR 0.542212 Loss 3.933226, Accuracy 92.024%\n",
      "Epoch 31, Batch 383, LR 0.542101 Loss 3.934270, Accuracy 92.018%\n",
      "Epoch 31, Batch 384, LR 0.541991 Loss 3.934142, Accuracy 92.021%\n",
      "Epoch 31, Batch 385, LR 0.541881 Loss 3.935418, Accuracy 92.017%\n",
      "Epoch 31, Batch 386, LR 0.541770 Loss 3.934671, Accuracy 92.020%\n",
      "Epoch 31, Batch 387, LR 0.541660 Loss 3.934307, Accuracy 92.010%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 388, LR 0.541549 Loss 3.935803, Accuracy 92.002%\n",
      "Epoch 31, Batch 389, LR 0.541439 Loss 3.935614, Accuracy 92.003%\n",
      "Epoch 31, Batch 390, LR 0.541329 Loss 3.935880, Accuracy 91.999%\n",
      "Epoch 31, Batch 391, LR 0.541218 Loss 3.935244, Accuracy 92.004%\n",
      "Epoch 31, Batch 392, LR 0.541108 Loss 3.933846, Accuracy 92.010%\n",
      "Epoch 31, Batch 393, LR 0.540998 Loss 3.933720, Accuracy 92.007%\n",
      "Epoch 31, Batch 394, LR 0.540887 Loss 3.934474, Accuracy 92.005%\n",
      "Epoch 31, Batch 395, LR 0.540777 Loss 3.934154, Accuracy 92.006%\n",
      "Epoch 31, Batch 396, LR 0.540667 Loss 3.934181, Accuracy 92.000%\n",
      "Epoch 31, Batch 397, LR 0.540557 Loss 3.933332, Accuracy 92.006%\n",
      "Epoch 31, Batch 398, LR 0.540446 Loss 3.933563, Accuracy 92.003%\n",
      "Epoch 31, Batch 399, LR 0.540336 Loss 3.933127, Accuracy 92.005%\n",
      "Epoch 31, Batch 400, LR 0.540226 Loss 3.932270, Accuracy 92.008%\n",
      "Epoch 31, Batch 401, LR 0.540115 Loss 3.931621, Accuracy 92.014%\n",
      "Epoch 31, Batch 402, LR 0.540005 Loss 3.930034, Accuracy 92.022%\n",
      "Epoch 31, Batch 403, LR 0.539895 Loss 3.931268, Accuracy 92.027%\n",
      "Epoch 31, Batch 404, LR 0.539785 Loss 3.930624, Accuracy 92.023%\n",
      "Epoch 31, Batch 405, LR 0.539674 Loss 3.931596, Accuracy 92.014%\n",
      "Epoch 31, Batch 406, LR 0.539564 Loss 3.931559, Accuracy 92.010%\n",
      "Epoch 31, Batch 407, LR 0.539454 Loss 3.931746, Accuracy 92.005%\n",
      "Epoch 31, Batch 408, LR 0.539344 Loss 3.931547, Accuracy 91.996%\n",
      "Epoch 31, Batch 409, LR 0.539234 Loss 3.929848, Accuracy 91.998%\n",
      "Epoch 31, Batch 410, LR 0.539123 Loss 3.928819, Accuracy 92.003%\n",
      "Epoch 31, Batch 411, LR 0.539013 Loss 3.928050, Accuracy 92.016%\n",
      "Epoch 31, Batch 412, LR 0.538903 Loss 3.927103, Accuracy 92.017%\n",
      "Epoch 31, Batch 413, LR 0.538793 Loss 3.927327, Accuracy 92.002%\n",
      "Epoch 31, Batch 414, LR 0.538683 Loss 3.928987, Accuracy 91.991%\n",
      "Epoch 31, Batch 415, LR 0.538573 Loss 3.929735, Accuracy 91.990%\n",
      "Epoch 31, Batch 416, LR 0.538463 Loss 3.929853, Accuracy 92.002%\n",
      "Epoch 31, Batch 417, LR 0.538352 Loss 3.930031, Accuracy 91.993%\n",
      "Epoch 31, Batch 418, LR 0.538242 Loss 3.929456, Accuracy 91.991%\n",
      "Epoch 31, Batch 419, LR 0.538132 Loss 3.930725, Accuracy 91.984%\n",
      "Epoch 31, Batch 420, LR 0.538022 Loss 3.930119, Accuracy 91.988%\n",
      "Epoch 31, Batch 421, LR 0.537912 Loss 3.930548, Accuracy 91.983%\n",
      "Epoch 31, Batch 422, LR 0.537802 Loss 3.930896, Accuracy 91.984%\n",
      "Epoch 31, Batch 423, LR 0.537692 Loss 3.929935, Accuracy 91.990%\n",
      "Epoch 31, Batch 424, LR 0.537582 Loss 3.929923, Accuracy 91.989%\n",
      "Epoch 31, Batch 425, LR 0.537472 Loss 3.929297, Accuracy 91.993%\n",
      "Epoch 31, Batch 426, LR 0.537362 Loss 3.929231, Accuracy 91.989%\n",
      "Epoch 31, Batch 427, LR 0.537252 Loss 3.929862, Accuracy 91.986%\n",
      "Epoch 31, Batch 428, LR 0.537142 Loss 3.931209, Accuracy 91.979%\n",
      "Epoch 31, Batch 429, LR 0.537031 Loss 3.931406, Accuracy 91.973%\n",
      "Epoch 31, Batch 430, LR 0.536921 Loss 3.932084, Accuracy 91.971%\n",
      "Epoch 31, Batch 431, LR 0.536811 Loss 3.932162, Accuracy 91.975%\n",
      "Epoch 31, Batch 432, LR 0.536701 Loss 3.933163, Accuracy 91.976%\n",
      "Epoch 31, Batch 433, LR 0.536591 Loss 3.932926, Accuracy 91.980%\n",
      "Epoch 31, Batch 434, LR 0.536481 Loss 3.932658, Accuracy 91.977%\n",
      "Epoch 31, Batch 435, LR 0.536371 Loss 3.931562, Accuracy 91.981%\n",
      "Epoch 31, Batch 436, LR 0.536262 Loss 3.931448, Accuracy 91.978%\n",
      "Epoch 31, Batch 437, LR 0.536152 Loss 3.932116, Accuracy 91.971%\n",
      "Epoch 31, Batch 438, LR 0.536042 Loss 3.930562, Accuracy 91.977%\n",
      "Epoch 31, Batch 439, LR 0.535932 Loss 3.931064, Accuracy 91.969%\n",
      "Epoch 31, Batch 440, LR 0.535822 Loss 3.931742, Accuracy 91.964%\n",
      "Epoch 31, Batch 441, LR 0.535712 Loss 3.932234, Accuracy 91.959%\n",
      "Epoch 31, Batch 442, LR 0.535602 Loss 3.931216, Accuracy 91.961%\n",
      "Epoch 31, Batch 443, LR 0.535492 Loss 3.931463, Accuracy 91.964%\n",
      "Epoch 31, Batch 444, LR 0.535382 Loss 3.931737, Accuracy 91.961%\n",
      "Epoch 31, Batch 445, LR 0.535272 Loss 3.932351, Accuracy 91.958%\n",
      "Epoch 31, Batch 446, LR 0.535162 Loss 3.933469, Accuracy 91.951%\n",
      "Epoch 31, Batch 447, LR 0.535052 Loss 3.932175, Accuracy 91.957%\n",
      "Epoch 31, Batch 448, LR 0.534942 Loss 3.933218, Accuracy 91.950%\n",
      "Epoch 31, Batch 449, LR 0.534833 Loss 3.932917, Accuracy 91.951%\n",
      "Epoch 31, Batch 450, LR 0.534723 Loss 3.932510, Accuracy 91.950%\n",
      "Epoch 31, Batch 451, LR 0.534613 Loss 3.933019, Accuracy 91.947%\n",
      "Epoch 31, Batch 452, LR 0.534503 Loss 3.935386, Accuracy 91.937%\n",
      "Epoch 31, Batch 453, LR 0.534393 Loss 3.934074, Accuracy 91.943%\n",
      "Epoch 31, Batch 454, LR 0.534283 Loss 3.934997, Accuracy 91.938%\n",
      "Epoch 31, Batch 455, LR 0.534174 Loss 3.935153, Accuracy 91.939%\n",
      "Epoch 31, Batch 456, LR 0.534064 Loss 3.936222, Accuracy 91.939%\n",
      "Epoch 31, Batch 457, LR 0.533954 Loss 3.936633, Accuracy 91.933%\n",
      "Epoch 31, Batch 458, LR 0.533844 Loss 3.935599, Accuracy 91.938%\n",
      "Epoch 31, Batch 459, LR 0.533734 Loss 3.934025, Accuracy 91.942%\n",
      "Epoch 31, Batch 460, LR 0.533625 Loss 3.933283, Accuracy 91.943%\n",
      "Epoch 31, Batch 461, LR 0.533515 Loss 3.932936, Accuracy 91.950%\n",
      "Epoch 31, Batch 462, LR 0.533405 Loss 3.932135, Accuracy 91.959%\n",
      "Epoch 31, Batch 463, LR 0.533295 Loss 3.931421, Accuracy 91.970%\n",
      "Epoch 31, Batch 464, LR 0.533186 Loss 3.931247, Accuracy 91.972%\n",
      "Epoch 31, Batch 465, LR 0.533076 Loss 3.930497, Accuracy 91.976%\n",
      "Epoch 31, Batch 466, LR 0.532966 Loss 3.930123, Accuracy 91.973%\n",
      "Epoch 31, Batch 467, LR 0.532856 Loss 3.929989, Accuracy 91.968%\n",
      "Epoch 31, Batch 468, LR 0.532747 Loss 3.931230, Accuracy 91.954%\n",
      "Epoch 31, Batch 469, LR 0.532637 Loss 3.933192, Accuracy 91.943%\n",
      "Epoch 31, Batch 470, LR 0.532527 Loss 3.933615, Accuracy 91.940%\n",
      "Epoch 31, Batch 471, LR 0.532418 Loss 3.932681, Accuracy 91.945%\n",
      "Epoch 31, Batch 472, LR 0.532308 Loss 3.933419, Accuracy 91.938%\n",
      "Epoch 31, Batch 473, LR 0.532198 Loss 3.934161, Accuracy 91.931%\n",
      "Epoch 31, Batch 474, LR 0.532089 Loss 3.933159, Accuracy 91.939%\n",
      "Epoch 31, Batch 475, LR 0.531979 Loss 3.931799, Accuracy 91.942%\n",
      "Epoch 31, Batch 476, LR 0.531869 Loss 3.931890, Accuracy 91.941%\n",
      "Epoch 31, Batch 477, LR 0.531760 Loss 3.931312, Accuracy 91.947%\n",
      "Epoch 31, Batch 478, LR 0.531650 Loss 3.931367, Accuracy 91.949%\n",
      "Epoch 31, Batch 479, LR 0.531540 Loss 3.933097, Accuracy 91.940%\n",
      "Epoch 31, Batch 480, LR 0.531431 Loss 3.932477, Accuracy 91.942%\n",
      "Epoch 31, Batch 481, LR 0.531321 Loss 3.932981, Accuracy 91.941%\n",
      "Epoch 31, Batch 482, LR 0.531212 Loss 3.933180, Accuracy 91.938%\n",
      "Epoch 31, Batch 483, LR 0.531102 Loss 3.932735, Accuracy 91.940%\n",
      "Epoch 31, Batch 484, LR 0.530992 Loss 3.933305, Accuracy 91.939%\n",
      "Epoch 31, Batch 485, LR 0.530883 Loss 3.933586, Accuracy 91.933%\n",
      "Epoch 31, Batch 486, LR 0.530773 Loss 3.932689, Accuracy 91.932%\n",
      "Epoch 31, Batch 487, LR 0.530664 Loss 3.933169, Accuracy 91.932%\n",
      "Epoch 31, Batch 488, LR 0.530554 Loss 3.933835, Accuracy 91.935%\n",
      "Epoch 31, Batch 489, LR 0.530445 Loss 3.933842, Accuracy 91.932%\n",
      "Epoch 31, Batch 490, LR 0.530335 Loss 3.932975, Accuracy 91.936%\n",
      "Epoch 31, Batch 491, LR 0.530226 Loss 3.931832, Accuracy 91.946%\n",
      "Epoch 31, Batch 492, LR 0.530116 Loss 3.931691, Accuracy 91.946%\n",
      "Epoch 31, Batch 493, LR 0.530007 Loss 3.931461, Accuracy 91.948%\n",
      "Epoch 31, Batch 494, LR 0.529897 Loss 3.931247, Accuracy 91.955%\n",
      "Epoch 31, Batch 495, LR 0.529788 Loss 3.930976, Accuracy 91.962%\n",
      "Epoch 31, Batch 496, LR 0.529678 Loss 3.931599, Accuracy 91.961%\n",
      "Epoch 31, Batch 497, LR 0.529569 Loss 3.932255, Accuracy 91.963%\n",
      "Epoch 31, Batch 498, LR 0.529459 Loss 3.934078, Accuracy 91.957%\n",
      "Epoch 31, Batch 499, LR 0.529350 Loss 3.934378, Accuracy 91.959%\n",
      "Epoch 31, Batch 500, LR 0.529240 Loss 3.935458, Accuracy 91.959%\n",
      "Epoch 31, Batch 501, LR 0.529131 Loss 3.934982, Accuracy 91.961%\n",
      "Epoch 31, Batch 502, LR 0.529021 Loss 3.935644, Accuracy 91.963%\n",
      "Epoch 31, Batch 503, LR 0.528912 Loss 3.935964, Accuracy 91.967%\n",
      "Epoch 31, Batch 504, LR 0.528803 Loss 3.934682, Accuracy 91.975%\n",
      "Epoch 31, Batch 505, LR 0.528693 Loss 3.934897, Accuracy 91.971%\n",
      "Epoch 31, Batch 506, LR 0.528584 Loss 3.935406, Accuracy 91.971%\n",
      "Epoch 31, Batch 507, LR 0.528474 Loss 3.934702, Accuracy 91.973%\n",
      "Epoch 31, Batch 508, LR 0.528365 Loss 3.934228, Accuracy 91.975%\n",
      "Epoch 31, Batch 509, LR 0.528256 Loss 3.933300, Accuracy 91.980%\n",
      "Epoch 31, Batch 510, LR 0.528146 Loss 3.932860, Accuracy 91.982%\n",
      "Epoch 31, Batch 511, LR 0.528037 Loss 3.932593, Accuracy 91.983%\n",
      "Epoch 31, Batch 512, LR 0.527928 Loss 3.933890, Accuracy 91.983%\n",
      "Epoch 31, Batch 513, LR 0.527818 Loss 3.933327, Accuracy 91.980%\n",
      "Epoch 31, Batch 514, LR 0.527709 Loss 3.933498, Accuracy 91.979%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 515, LR 0.527600 Loss 3.932730, Accuracy 91.978%\n",
      "Epoch 31, Batch 516, LR 0.527490 Loss 3.932856, Accuracy 91.986%\n",
      "Epoch 31, Batch 517, LR 0.527381 Loss 3.932944, Accuracy 91.987%\n",
      "Epoch 31, Batch 518, LR 0.527272 Loss 3.933659, Accuracy 91.990%\n",
      "Epoch 31, Batch 519, LR 0.527162 Loss 3.932874, Accuracy 91.986%\n",
      "Epoch 31, Batch 520, LR 0.527053 Loss 3.932999, Accuracy 91.986%\n",
      "Epoch 31, Batch 521, LR 0.526944 Loss 3.932252, Accuracy 91.993%\n",
      "Epoch 31, Batch 522, LR 0.526835 Loss 3.931721, Accuracy 91.991%\n",
      "Epoch 31, Batch 523, LR 0.526725 Loss 3.931147, Accuracy 91.993%\n",
      "Epoch 31, Batch 524, LR 0.526616 Loss 3.929976, Accuracy 92.000%\n",
      "Epoch 31, Batch 525, LR 0.526507 Loss 3.928723, Accuracy 92.006%\n",
      "Epoch 31, Batch 526, LR 0.526398 Loss 3.928067, Accuracy 92.011%\n",
      "Epoch 31, Batch 527, LR 0.526288 Loss 3.929082, Accuracy 92.007%\n",
      "Epoch 31, Batch 528, LR 0.526179 Loss 3.927840, Accuracy 92.013%\n",
      "Epoch 31, Batch 529, LR 0.526070 Loss 3.927439, Accuracy 92.016%\n",
      "Epoch 31, Batch 530, LR 0.525961 Loss 3.927714, Accuracy 92.011%\n",
      "Epoch 31, Batch 531, LR 0.525852 Loss 3.927708, Accuracy 92.011%\n",
      "Epoch 31, Batch 532, LR 0.525742 Loss 3.927938, Accuracy 92.011%\n",
      "Epoch 31, Batch 533, LR 0.525633 Loss 3.929706, Accuracy 92.003%\n",
      "Epoch 31, Batch 534, LR 0.525524 Loss 3.929745, Accuracy 92.003%\n",
      "Epoch 31, Batch 535, LR 0.525415 Loss 3.930677, Accuracy 92.002%\n",
      "Epoch 31, Batch 536, LR 0.525306 Loss 3.931790, Accuracy 91.995%\n",
      "Epoch 31, Batch 537, LR 0.525197 Loss 3.930546, Accuracy 91.998%\n",
      "Epoch 31, Batch 538, LR 0.525088 Loss 3.930361, Accuracy 91.999%\n",
      "Epoch 31, Batch 539, LR 0.524978 Loss 3.929411, Accuracy 91.999%\n",
      "Epoch 31, Batch 540, LR 0.524869 Loss 3.930729, Accuracy 91.994%\n",
      "Epoch 31, Batch 541, LR 0.524760 Loss 3.931010, Accuracy 91.994%\n",
      "Epoch 31, Batch 542, LR 0.524651 Loss 3.931659, Accuracy 91.996%\n",
      "Epoch 31, Batch 543, LR 0.524542 Loss 3.931653, Accuracy 91.989%\n",
      "Epoch 31, Batch 544, LR 0.524433 Loss 3.932346, Accuracy 91.988%\n",
      "Epoch 31, Batch 545, LR 0.524324 Loss 3.932047, Accuracy 91.990%\n",
      "Epoch 31, Batch 546, LR 0.524215 Loss 3.933558, Accuracy 91.990%\n",
      "Epoch 31, Batch 547, LR 0.524106 Loss 3.933500, Accuracy 91.993%\n",
      "Epoch 31, Batch 548, LR 0.523997 Loss 3.932853, Accuracy 91.992%\n",
      "Epoch 31, Batch 549, LR 0.523888 Loss 3.933695, Accuracy 91.988%\n",
      "Epoch 31, Batch 550, LR 0.523779 Loss 3.934574, Accuracy 91.986%\n",
      "Epoch 31, Batch 551, LR 0.523670 Loss 3.934648, Accuracy 91.993%\n",
      "Epoch 31, Batch 552, LR 0.523561 Loss 3.934939, Accuracy 91.995%\n",
      "Epoch 31, Batch 553, LR 0.523452 Loss 3.935372, Accuracy 91.994%\n",
      "Epoch 31, Batch 554, LR 0.523343 Loss 3.934979, Accuracy 91.993%\n",
      "Epoch 31, Batch 555, LR 0.523234 Loss 3.935679, Accuracy 91.986%\n",
      "Epoch 31, Batch 556, LR 0.523125 Loss 3.935362, Accuracy 91.985%\n",
      "Epoch 31, Batch 557, LR 0.523016 Loss 3.936984, Accuracy 91.987%\n",
      "Epoch 31, Batch 558, LR 0.522907 Loss 3.936041, Accuracy 91.991%\n",
      "Epoch 31, Batch 559, LR 0.522798 Loss 3.936897, Accuracy 91.992%\n",
      "Epoch 31, Batch 560, LR 0.522689 Loss 3.936713, Accuracy 91.988%\n",
      "Epoch 31, Batch 561, LR 0.522580 Loss 3.937542, Accuracy 91.980%\n",
      "Epoch 31, Batch 562, LR 0.522471 Loss 3.937287, Accuracy 91.982%\n",
      "Epoch 31, Batch 563, LR 0.522362 Loss 3.937247, Accuracy 91.984%\n",
      "Epoch 31, Batch 564, LR 0.522253 Loss 3.936983, Accuracy 91.984%\n",
      "Epoch 31, Batch 565, LR 0.522144 Loss 3.935958, Accuracy 91.988%\n",
      "Epoch 31, Batch 566, LR 0.522035 Loss 3.934742, Accuracy 91.993%\n",
      "Epoch 31, Batch 567, LR 0.521926 Loss 3.934267, Accuracy 91.999%\n",
      "Epoch 31, Batch 568, LR 0.521817 Loss 3.934032, Accuracy 91.995%\n",
      "Epoch 31, Batch 569, LR 0.521709 Loss 3.934432, Accuracy 91.994%\n",
      "Epoch 31, Batch 570, LR 0.521600 Loss 3.932917, Accuracy 92.002%\n",
      "Epoch 31, Batch 571, LR 0.521491 Loss 3.933079, Accuracy 92.006%\n",
      "Epoch 31, Batch 572, LR 0.521382 Loss 3.933309, Accuracy 92.004%\n",
      "Epoch 31, Batch 573, LR 0.521273 Loss 3.934301, Accuracy 91.998%\n",
      "Epoch 31, Batch 574, LR 0.521164 Loss 3.934465, Accuracy 91.998%\n",
      "Epoch 31, Batch 575, LR 0.521056 Loss 3.934334, Accuracy 92.005%\n",
      "Epoch 31, Batch 576, LR 0.520947 Loss 3.933360, Accuracy 92.010%\n",
      "Epoch 31, Batch 577, LR 0.520838 Loss 3.934122, Accuracy 92.010%\n",
      "Epoch 31, Batch 578, LR 0.520729 Loss 3.934648, Accuracy 92.009%\n",
      "Epoch 31, Batch 579, LR 0.520620 Loss 3.934294, Accuracy 92.007%\n",
      "Epoch 31, Batch 580, LR 0.520512 Loss 3.933761, Accuracy 92.011%\n",
      "Epoch 31, Batch 581, LR 0.520403 Loss 3.934129, Accuracy 92.011%\n",
      "Epoch 31, Batch 582, LR 0.520294 Loss 3.933555, Accuracy 92.012%\n",
      "Epoch 31, Batch 583, LR 0.520185 Loss 3.933754, Accuracy 92.012%\n",
      "Epoch 31, Batch 584, LR 0.520077 Loss 3.934388, Accuracy 92.011%\n",
      "Epoch 31, Batch 585, LR 0.519968 Loss 3.934648, Accuracy 92.010%\n",
      "Epoch 31, Batch 586, LR 0.519859 Loss 3.933895, Accuracy 92.013%\n",
      "Epoch 31, Batch 587, LR 0.519750 Loss 3.934122, Accuracy 92.012%\n",
      "Epoch 31, Batch 588, LR 0.519642 Loss 3.933673, Accuracy 92.013%\n",
      "Epoch 31, Batch 589, LR 0.519533 Loss 3.933829, Accuracy 92.010%\n",
      "Epoch 31, Batch 590, LR 0.519424 Loss 3.934341, Accuracy 92.014%\n",
      "Epoch 31, Batch 591, LR 0.519316 Loss 3.932874, Accuracy 92.020%\n",
      "Epoch 31, Batch 592, LR 0.519207 Loss 3.933183, Accuracy 92.019%\n",
      "Epoch 31, Batch 593, LR 0.519098 Loss 3.933965, Accuracy 92.014%\n",
      "Epoch 31, Batch 594, LR 0.518990 Loss 3.934095, Accuracy 92.019%\n",
      "Epoch 31, Batch 595, LR 0.518881 Loss 3.934311, Accuracy 92.021%\n",
      "Epoch 31, Batch 596, LR 0.518772 Loss 3.935666, Accuracy 92.013%\n",
      "Epoch 31, Batch 597, LR 0.518664 Loss 3.935872, Accuracy 92.015%\n",
      "Epoch 31, Batch 598, LR 0.518555 Loss 3.935470, Accuracy 92.012%\n",
      "Epoch 31, Batch 599, LR 0.518446 Loss 3.935317, Accuracy 92.010%\n",
      "Epoch 31, Batch 600, LR 0.518338 Loss 3.935404, Accuracy 92.010%\n",
      "Epoch 31, Batch 601, LR 0.518229 Loss 3.935162, Accuracy 92.011%\n",
      "Epoch 31, Batch 602, LR 0.518121 Loss 3.935428, Accuracy 92.010%\n",
      "Epoch 31, Batch 603, LR 0.518012 Loss 3.934131, Accuracy 92.018%\n",
      "Epoch 31, Batch 604, LR 0.517903 Loss 3.933773, Accuracy 92.018%\n",
      "Epoch 31, Batch 605, LR 0.517795 Loss 3.933294, Accuracy 92.024%\n",
      "Epoch 31, Batch 606, LR 0.517686 Loss 3.932692, Accuracy 92.024%\n",
      "Epoch 31, Batch 607, LR 0.517578 Loss 3.932857, Accuracy 92.019%\n",
      "Epoch 31, Batch 608, LR 0.517469 Loss 3.932853, Accuracy 92.020%\n",
      "Epoch 31, Batch 609, LR 0.517361 Loss 3.934277, Accuracy 92.016%\n",
      "Epoch 31, Batch 610, LR 0.517252 Loss 3.935144, Accuracy 92.009%\n",
      "Epoch 31, Batch 611, LR 0.517144 Loss 3.936984, Accuracy 91.998%\n",
      "Epoch 31, Batch 612, LR 0.517035 Loss 3.937848, Accuracy 91.992%\n",
      "Epoch 31, Batch 613, LR 0.516927 Loss 3.937179, Accuracy 91.994%\n",
      "Epoch 31, Batch 614, LR 0.516818 Loss 3.937157, Accuracy 91.997%\n",
      "Epoch 31, Batch 615, LR 0.516710 Loss 3.937425, Accuracy 91.992%\n",
      "Epoch 31, Batch 616, LR 0.516601 Loss 3.936886, Accuracy 91.993%\n",
      "Epoch 31, Batch 617, LR 0.516493 Loss 3.936255, Accuracy 91.995%\n",
      "Epoch 31, Batch 618, LR 0.516384 Loss 3.937087, Accuracy 91.993%\n",
      "Epoch 31, Batch 619, LR 0.516276 Loss 3.937895, Accuracy 91.984%\n",
      "Epoch 31, Batch 620, LR 0.516167 Loss 3.939433, Accuracy 91.975%\n",
      "Epoch 31, Batch 621, LR 0.516059 Loss 3.939887, Accuracy 91.972%\n",
      "Epoch 31, Batch 622, LR 0.515950 Loss 3.940075, Accuracy 91.974%\n",
      "Epoch 31, Batch 623, LR 0.515842 Loss 3.940676, Accuracy 91.973%\n",
      "Epoch 31, Batch 624, LR 0.515734 Loss 3.939451, Accuracy 91.980%\n",
      "Epoch 31, Batch 625, LR 0.515625 Loss 3.940198, Accuracy 91.971%\n",
      "Epoch 31, Batch 626, LR 0.515517 Loss 3.940103, Accuracy 91.977%\n",
      "Epoch 31, Batch 627, LR 0.515408 Loss 3.939021, Accuracy 91.983%\n",
      "Epoch 31, Batch 628, LR 0.515300 Loss 3.939144, Accuracy 91.976%\n",
      "Epoch 31, Batch 629, LR 0.515192 Loss 3.938681, Accuracy 91.976%\n",
      "Epoch 31, Batch 630, LR 0.515083 Loss 3.939888, Accuracy 91.970%\n",
      "Epoch 31, Batch 631, LR 0.514975 Loss 3.938916, Accuracy 91.975%\n",
      "Epoch 31, Batch 632, LR 0.514867 Loss 3.938704, Accuracy 91.977%\n",
      "Epoch 31, Batch 633, LR 0.514758 Loss 3.937774, Accuracy 91.981%\n",
      "Epoch 31, Batch 634, LR 0.514650 Loss 3.937895, Accuracy 91.985%\n",
      "Epoch 31, Batch 635, LR 0.514542 Loss 3.938723, Accuracy 91.983%\n",
      "Epoch 31, Batch 636, LR 0.514433 Loss 3.938540, Accuracy 91.981%\n",
      "Epoch 31, Batch 637, LR 0.514325 Loss 3.937813, Accuracy 91.985%\n",
      "Epoch 31, Batch 638, LR 0.514217 Loss 3.938032, Accuracy 91.987%\n",
      "Epoch 31, Batch 639, LR 0.514108 Loss 3.937628, Accuracy 91.989%\n",
      "Epoch 31, Batch 640, LR 0.514000 Loss 3.936601, Accuracy 91.990%\n",
      "Epoch 31, Batch 641, LR 0.513892 Loss 3.937298, Accuracy 91.984%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 642, LR 0.513784 Loss 3.937692, Accuracy 91.981%\n",
      "Epoch 31, Batch 643, LR 0.513675 Loss 3.937682, Accuracy 91.977%\n",
      "Epoch 31, Batch 644, LR 0.513567 Loss 3.937242, Accuracy 91.978%\n",
      "Epoch 31, Batch 645, LR 0.513459 Loss 3.936932, Accuracy 91.978%\n",
      "Epoch 31, Batch 646, LR 0.513351 Loss 3.937429, Accuracy 91.978%\n",
      "Epoch 31, Batch 647, LR 0.513242 Loss 3.937111, Accuracy 91.979%\n",
      "Epoch 31, Batch 648, LR 0.513134 Loss 3.937568, Accuracy 91.978%\n",
      "Epoch 31, Batch 649, LR 0.513026 Loss 3.937066, Accuracy 91.978%\n",
      "Epoch 31, Batch 650, LR 0.512918 Loss 3.937367, Accuracy 91.978%\n",
      "Epoch 31, Batch 651, LR 0.512810 Loss 3.936745, Accuracy 91.982%\n",
      "Epoch 31, Batch 652, LR 0.512702 Loss 3.936970, Accuracy 91.981%\n",
      "Epoch 31, Batch 653, LR 0.512593 Loss 3.936100, Accuracy 91.987%\n",
      "Epoch 31, Batch 654, LR 0.512485 Loss 3.936394, Accuracy 91.986%\n",
      "Epoch 31, Batch 655, LR 0.512377 Loss 3.936697, Accuracy 91.988%\n",
      "Epoch 31, Batch 656, LR 0.512269 Loss 3.937185, Accuracy 91.991%\n",
      "Epoch 31, Batch 657, LR 0.512161 Loss 3.936587, Accuracy 91.996%\n",
      "Epoch 31, Batch 658, LR 0.512053 Loss 3.935403, Accuracy 92.001%\n",
      "Epoch 31, Batch 659, LR 0.511945 Loss 3.935185, Accuracy 92.003%\n",
      "Epoch 31, Batch 660, LR 0.511836 Loss 3.934980, Accuracy 92.004%\n",
      "Epoch 31, Batch 661, LR 0.511728 Loss 3.934729, Accuracy 92.004%\n",
      "Epoch 31, Batch 662, LR 0.511620 Loss 3.934461, Accuracy 92.005%\n",
      "Epoch 31, Batch 663, LR 0.511512 Loss 3.934316, Accuracy 92.005%\n",
      "Epoch 31, Batch 664, LR 0.511404 Loss 3.934413, Accuracy 92.003%\n",
      "Epoch 31, Batch 665, LR 0.511296 Loss 3.934218, Accuracy 92.005%\n",
      "Epoch 31, Batch 666, LR 0.511188 Loss 3.935368, Accuracy 92.001%\n",
      "Epoch 31, Batch 667, LR 0.511080 Loss 3.936738, Accuracy 91.998%\n",
      "Epoch 31, Batch 668, LR 0.510972 Loss 3.936863, Accuracy 91.998%\n",
      "Epoch 31, Batch 669, LR 0.510864 Loss 3.936183, Accuracy 92.002%\n",
      "Epoch 31, Batch 670, LR 0.510756 Loss 3.935729, Accuracy 92.006%\n",
      "Epoch 31, Batch 671, LR 0.510648 Loss 3.934727, Accuracy 92.013%\n",
      "Epoch 31, Batch 672, LR 0.510540 Loss 3.935290, Accuracy 92.007%\n",
      "Epoch 31, Batch 673, LR 0.510432 Loss 3.934618, Accuracy 92.012%\n",
      "Epoch 31, Batch 674, LR 0.510324 Loss 3.934454, Accuracy 92.016%\n",
      "Epoch 31, Batch 675, LR 0.510216 Loss 3.933912, Accuracy 92.016%\n",
      "Epoch 31, Batch 676, LR 0.510108 Loss 3.933768, Accuracy 92.015%\n",
      "Epoch 31, Batch 677, LR 0.510000 Loss 3.932548, Accuracy 92.017%\n",
      "Epoch 31, Batch 678, LR 0.509892 Loss 3.932620, Accuracy 92.019%\n",
      "Epoch 31, Batch 679, LR 0.509784 Loss 3.933139, Accuracy 92.011%\n",
      "Epoch 31, Batch 680, LR 0.509676 Loss 3.932698, Accuracy 92.013%\n",
      "Epoch 31, Batch 681, LR 0.509568 Loss 3.932056, Accuracy 92.013%\n",
      "Epoch 31, Batch 682, LR 0.509460 Loss 3.932209, Accuracy 92.013%\n",
      "Epoch 31, Batch 683, LR 0.509352 Loss 3.931943, Accuracy 92.017%\n",
      "Epoch 31, Batch 684, LR 0.509244 Loss 3.932219, Accuracy 92.017%\n",
      "Epoch 31, Batch 685, LR 0.509137 Loss 3.932242, Accuracy 92.015%\n",
      "Epoch 31, Batch 686, LR 0.509029 Loss 3.931286, Accuracy 92.014%\n",
      "Epoch 31, Batch 687, LR 0.508921 Loss 3.931553, Accuracy 92.010%\n",
      "Epoch 31, Batch 688, LR 0.508813 Loss 3.931067, Accuracy 92.013%\n",
      "Epoch 31, Batch 689, LR 0.508705 Loss 3.930677, Accuracy 92.019%\n",
      "Epoch 31, Batch 690, LR 0.508597 Loss 3.930403, Accuracy 92.015%\n",
      "Epoch 31, Batch 691, LR 0.508489 Loss 3.930705, Accuracy 92.011%\n",
      "Epoch 31, Batch 692, LR 0.508382 Loss 3.931225, Accuracy 92.010%\n",
      "Epoch 31, Batch 693, LR 0.508274 Loss 3.931148, Accuracy 92.009%\n",
      "Epoch 31, Batch 694, LR 0.508166 Loss 3.932102, Accuracy 92.006%\n",
      "Epoch 31, Batch 695, LR 0.508058 Loss 3.932374, Accuracy 92.005%\n",
      "Epoch 31, Batch 696, LR 0.507950 Loss 3.931661, Accuracy 92.011%\n",
      "Epoch 31, Batch 697, LR 0.507843 Loss 3.931429, Accuracy 92.016%\n",
      "Epoch 31, Batch 698, LR 0.507735 Loss 3.931162, Accuracy 92.022%\n",
      "Epoch 31, Batch 699, LR 0.507627 Loss 3.930613, Accuracy 92.025%\n",
      "Epoch 31, Batch 700, LR 0.507519 Loss 3.930780, Accuracy 92.027%\n",
      "Epoch 31, Batch 701, LR 0.507411 Loss 3.930157, Accuracy 92.034%\n",
      "Epoch 31, Batch 702, LR 0.507304 Loss 3.930208, Accuracy 92.034%\n",
      "Epoch 31, Batch 703, LR 0.507196 Loss 3.929595, Accuracy 92.034%\n",
      "Epoch 31, Batch 704, LR 0.507088 Loss 3.929058, Accuracy 92.039%\n",
      "Epoch 31, Batch 705, LR 0.506980 Loss 3.929875, Accuracy 92.035%\n",
      "Epoch 31, Batch 706, LR 0.506873 Loss 3.929781, Accuracy 92.036%\n",
      "Epoch 31, Batch 707, LR 0.506765 Loss 3.929192, Accuracy 92.042%\n",
      "Epoch 31, Batch 708, LR 0.506657 Loss 3.928944, Accuracy 92.045%\n",
      "Epoch 31, Batch 709, LR 0.506550 Loss 3.929450, Accuracy 92.046%\n",
      "Epoch 31, Batch 710, LR 0.506442 Loss 3.930529, Accuracy 92.044%\n",
      "Epoch 31, Batch 711, LR 0.506334 Loss 3.931056, Accuracy 92.038%\n",
      "Epoch 31, Batch 712, LR 0.506227 Loss 3.930692, Accuracy 92.038%\n",
      "Epoch 31, Batch 713, LR 0.506119 Loss 3.930511, Accuracy 92.036%\n",
      "Epoch 31, Batch 714, LR 0.506011 Loss 3.932014, Accuracy 92.030%\n",
      "Epoch 31, Batch 715, LR 0.505904 Loss 3.932088, Accuracy 92.028%\n",
      "Epoch 31, Batch 716, LR 0.505796 Loss 3.932427, Accuracy 92.027%\n",
      "Epoch 31, Batch 717, LR 0.505688 Loss 3.931802, Accuracy 92.031%\n",
      "Epoch 31, Batch 718, LR 0.505581 Loss 3.930910, Accuracy 92.036%\n",
      "Epoch 31, Batch 719, LR 0.505473 Loss 3.931040, Accuracy 92.036%\n",
      "Epoch 31, Batch 720, LR 0.505366 Loss 3.930482, Accuracy 92.037%\n",
      "Epoch 31, Batch 721, LR 0.505258 Loss 3.930095, Accuracy 92.038%\n",
      "Epoch 31, Batch 722, LR 0.505151 Loss 3.930392, Accuracy 92.039%\n",
      "Epoch 31, Batch 723, LR 0.505043 Loss 3.930266, Accuracy 92.041%\n",
      "Epoch 31, Batch 724, LR 0.504935 Loss 3.929713, Accuracy 92.043%\n",
      "Epoch 31, Batch 725, LR 0.504828 Loss 3.929445, Accuracy 92.045%\n",
      "Epoch 31, Batch 726, LR 0.504720 Loss 3.930173, Accuracy 92.041%\n",
      "Epoch 31, Batch 727, LR 0.504613 Loss 3.930556, Accuracy 92.042%\n",
      "Epoch 31, Batch 728, LR 0.504505 Loss 3.930061, Accuracy 92.044%\n",
      "Epoch 31, Batch 729, LR 0.504398 Loss 3.930282, Accuracy 92.039%\n",
      "Epoch 31, Batch 730, LR 0.504290 Loss 3.931091, Accuracy 92.036%\n",
      "Epoch 31, Batch 731, LR 0.504183 Loss 3.930008, Accuracy 92.039%\n",
      "Epoch 31, Batch 732, LR 0.504075 Loss 3.929216, Accuracy 92.046%\n",
      "Epoch 31, Batch 733, LR 0.503968 Loss 3.929967, Accuracy 92.041%\n",
      "Epoch 31, Batch 734, LR 0.503860 Loss 3.929569, Accuracy 92.046%\n",
      "Epoch 31, Batch 735, LR 0.503753 Loss 3.930057, Accuracy 92.040%\n",
      "Epoch 31, Batch 736, LR 0.503645 Loss 3.930415, Accuracy 92.041%\n",
      "Epoch 31, Batch 737, LR 0.503538 Loss 3.930512, Accuracy 92.042%\n",
      "Epoch 31, Batch 738, LR 0.503430 Loss 3.930689, Accuracy 92.040%\n",
      "Epoch 31, Batch 739, LR 0.503323 Loss 3.930181, Accuracy 92.042%\n",
      "Epoch 31, Batch 740, LR 0.503216 Loss 3.930972, Accuracy 92.040%\n",
      "Epoch 31, Batch 741, LR 0.503108 Loss 3.930907, Accuracy 92.038%\n",
      "Epoch 31, Batch 742, LR 0.503001 Loss 3.930890, Accuracy 92.034%\n",
      "Epoch 31, Batch 743, LR 0.502893 Loss 3.930915, Accuracy 92.032%\n",
      "Epoch 31, Batch 744, LR 0.502786 Loss 3.931040, Accuracy 92.033%\n",
      "Epoch 31, Batch 745, LR 0.502679 Loss 3.931281, Accuracy 92.029%\n",
      "Epoch 31, Batch 746, LR 0.502571 Loss 3.931578, Accuracy 92.026%\n",
      "Epoch 31, Batch 747, LR 0.502464 Loss 3.931824, Accuracy 92.029%\n",
      "Epoch 31, Batch 748, LR 0.502356 Loss 3.931395, Accuracy 92.030%\n",
      "Epoch 31, Batch 749, LR 0.502249 Loss 3.930974, Accuracy 92.031%\n",
      "Epoch 31, Batch 750, LR 0.502142 Loss 3.931252, Accuracy 92.030%\n",
      "Epoch 31, Batch 751, LR 0.502034 Loss 3.932065, Accuracy 92.027%\n",
      "Epoch 31, Batch 752, LR 0.501927 Loss 3.932517, Accuracy 92.023%\n",
      "Epoch 31, Batch 753, LR 0.501820 Loss 3.931385, Accuracy 92.027%\n",
      "Epoch 31, Batch 754, LR 0.501713 Loss 3.930940, Accuracy 92.027%\n",
      "Epoch 31, Batch 755, LR 0.501605 Loss 3.930137, Accuracy 92.031%\n",
      "Epoch 31, Batch 756, LR 0.501498 Loss 3.930140, Accuracy 92.030%\n",
      "Epoch 31, Batch 757, LR 0.501391 Loss 3.930464, Accuracy 92.033%\n",
      "Epoch 31, Batch 758, LR 0.501283 Loss 3.929815, Accuracy 92.035%\n",
      "Epoch 31, Batch 759, LR 0.501176 Loss 3.930536, Accuracy 92.031%\n",
      "Epoch 31, Batch 760, LR 0.501069 Loss 3.929811, Accuracy 92.031%\n",
      "Epoch 31, Batch 761, LR 0.500962 Loss 3.929934, Accuracy 92.032%\n",
      "Epoch 31, Batch 762, LR 0.500854 Loss 3.929133, Accuracy 92.039%\n",
      "Epoch 31, Batch 763, LR 0.500747 Loss 3.928657, Accuracy 92.041%\n",
      "Epoch 31, Batch 764, LR 0.500640 Loss 3.929209, Accuracy 92.035%\n",
      "Epoch 31, Batch 765, LR 0.500533 Loss 3.928902, Accuracy 92.035%\n",
      "Epoch 31, Batch 766, LR 0.500426 Loss 3.928238, Accuracy 92.040%\n",
      "Epoch 31, Batch 767, LR 0.500318 Loss 3.928064, Accuracy 92.041%\n",
      "Epoch 31, Batch 768, LR 0.500211 Loss 3.927733, Accuracy 92.041%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 769, LR 0.500104 Loss 3.927314, Accuracy 92.040%\n",
      "Epoch 31, Batch 770, LR 0.499997 Loss 3.927702, Accuracy 92.037%\n",
      "Epoch 31, Batch 771, LR 0.499890 Loss 3.927716, Accuracy 92.040%\n",
      "Epoch 31, Batch 772, LR 0.499783 Loss 3.927675, Accuracy 92.044%\n",
      "Epoch 31, Batch 773, LR 0.499675 Loss 3.927788, Accuracy 92.040%\n",
      "Epoch 31, Batch 774, LR 0.499568 Loss 3.927849, Accuracy 92.040%\n",
      "Epoch 31, Batch 775, LR 0.499461 Loss 3.928119, Accuracy 92.043%\n",
      "Epoch 31, Batch 776, LR 0.499354 Loss 3.927365, Accuracy 92.048%\n",
      "Epoch 31, Batch 777, LR 0.499247 Loss 3.927546, Accuracy 92.047%\n",
      "Epoch 31, Batch 778, LR 0.499140 Loss 3.927164, Accuracy 92.046%\n",
      "Epoch 31, Batch 779, LR 0.499033 Loss 3.926462, Accuracy 92.046%\n",
      "Epoch 31, Batch 780, LR 0.498926 Loss 3.926090, Accuracy 92.049%\n",
      "Epoch 31, Batch 781, LR 0.498819 Loss 3.925873, Accuracy 92.051%\n",
      "Epoch 31, Batch 782, LR 0.498712 Loss 3.925551, Accuracy 92.054%\n",
      "Epoch 31, Batch 783, LR 0.498604 Loss 3.925444, Accuracy 92.053%\n",
      "Epoch 31, Batch 784, LR 0.498497 Loss 3.925697, Accuracy 92.047%\n",
      "Epoch 31, Batch 785, LR 0.498390 Loss 3.924642, Accuracy 92.053%\n",
      "Epoch 31, Batch 786, LR 0.498283 Loss 3.924722, Accuracy 92.054%\n",
      "Epoch 31, Batch 787, LR 0.498176 Loss 3.925296, Accuracy 92.053%\n",
      "Epoch 31, Batch 788, LR 0.498069 Loss 3.925689, Accuracy 92.051%\n",
      "Epoch 31, Batch 789, LR 0.497962 Loss 3.925817, Accuracy 92.053%\n",
      "Epoch 31, Batch 790, LR 0.497855 Loss 3.925675, Accuracy 92.058%\n",
      "Epoch 31, Batch 791, LR 0.497748 Loss 3.925049, Accuracy 92.057%\n",
      "Epoch 31, Batch 792, LR 0.497641 Loss 3.925109, Accuracy 92.052%\n",
      "Epoch 31, Batch 793, LR 0.497534 Loss 3.925509, Accuracy 92.051%\n",
      "Epoch 31, Batch 794, LR 0.497427 Loss 3.925183, Accuracy 92.052%\n",
      "Epoch 31, Batch 795, LR 0.497321 Loss 3.925240, Accuracy 92.052%\n",
      "Epoch 31, Batch 796, LR 0.497214 Loss 3.925404, Accuracy 92.048%\n",
      "Epoch 31, Batch 797, LR 0.497107 Loss 3.925589, Accuracy 92.047%\n",
      "Epoch 31, Batch 798, LR 0.497000 Loss 3.925408, Accuracy 92.048%\n",
      "Epoch 31, Batch 799, LR 0.496893 Loss 3.925494, Accuracy 92.046%\n",
      "Epoch 31, Batch 800, LR 0.496786 Loss 3.925550, Accuracy 92.047%\n",
      "Epoch 31, Batch 801, LR 0.496679 Loss 3.925640, Accuracy 92.049%\n",
      "Epoch 31, Batch 802, LR 0.496572 Loss 3.925603, Accuracy 92.048%\n",
      "Epoch 31, Batch 803, LR 0.496465 Loss 3.926123, Accuracy 92.048%\n",
      "Epoch 31, Batch 804, LR 0.496358 Loss 3.925762, Accuracy 92.050%\n",
      "Epoch 31, Batch 805, LR 0.496251 Loss 3.925624, Accuracy 92.049%\n",
      "Epoch 31, Batch 806, LR 0.496145 Loss 3.926504, Accuracy 92.048%\n",
      "Epoch 31, Batch 807, LR 0.496038 Loss 3.926356, Accuracy 92.049%\n",
      "Epoch 31, Batch 808, LR 0.495931 Loss 3.926519, Accuracy 92.050%\n",
      "Epoch 31, Batch 809, LR 0.495824 Loss 3.926250, Accuracy 92.051%\n",
      "Epoch 31, Batch 810, LR 0.495717 Loss 3.926617, Accuracy 92.051%\n",
      "Epoch 31, Batch 811, LR 0.495610 Loss 3.926862, Accuracy 92.047%\n",
      "Epoch 31, Batch 812, LR 0.495504 Loss 3.928348, Accuracy 92.041%\n",
      "Epoch 31, Batch 813, LR 0.495397 Loss 3.928088, Accuracy 92.047%\n",
      "Epoch 31, Batch 814, LR 0.495290 Loss 3.927943, Accuracy 92.050%\n",
      "Epoch 31, Batch 815, LR 0.495183 Loss 3.928094, Accuracy 92.052%\n",
      "Epoch 31, Batch 816, LR 0.495077 Loss 3.927900, Accuracy 92.053%\n",
      "Epoch 31, Batch 817, LR 0.494970 Loss 3.927829, Accuracy 92.054%\n",
      "Epoch 31, Batch 818, LR 0.494863 Loss 3.928172, Accuracy 92.054%\n",
      "Epoch 31, Batch 819, LR 0.494756 Loss 3.928109, Accuracy 92.053%\n",
      "Epoch 31, Batch 820, LR 0.494650 Loss 3.928179, Accuracy 92.053%\n",
      "Epoch 31, Batch 821, LR 0.494543 Loss 3.927829, Accuracy 92.053%\n",
      "Epoch 31, Batch 822, LR 0.494436 Loss 3.927821, Accuracy 92.053%\n",
      "Epoch 31, Batch 823, LR 0.494329 Loss 3.927211, Accuracy 92.056%\n",
      "Epoch 31, Batch 824, LR 0.494223 Loss 3.926532, Accuracy 92.057%\n",
      "Epoch 31, Batch 825, LR 0.494116 Loss 3.925968, Accuracy 92.057%\n",
      "Epoch 31, Batch 826, LR 0.494009 Loss 3.925860, Accuracy 92.056%\n",
      "Epoch 31, Batch 827, LR 0.493903 Loss 3.925550, Accuracy 92.063%\n",
      "Epoch 31, Batch 828, LR 0.493796 Loss 3.925692, Accuracy 92.062%\n",
      "Epoch 31, Batch 829, LR 0.493689 Loss 3.925255, Accuracy 92.070%\n",
      "Epoch 31, Batch 830, LR 0.493583 Loss 3.925491, Accuracy 92.066%\n",
      "Epoch 31, Batch 831, LR 0.493476 Loss 3.924476, Accuracy 92.067%\n",
      "Epoch 31, Batch 832, LR 0.493369 Loss 3.923976, Accuracy 92.069%\n",
      "Epoch 31, Batch 833, LR 0.493263 Loss 3.923884, Accuracy 92.067%\n",
      "Epoch 31, Batch 834, LR 0.493156 Loss 3.924064, Accuracy 92.063%\n",
      "Epoch 31, Batch 835, LR 0.493050 Loss 3.923540, Accuracy 92.065%\n",
      "Epoch 31, Batch 836, LR 0.492943 Loss 3.923362, Accuracy 92.061%\n",
      "Epoch 31, Batch 837, LR 0.492836 Loss 3.923039, Accuracy 92.064%\n",
      "Epoch 31, Batch 838, LR 0.492730 Loss 3.922147, Accuracy 92.069%\n",
      "Epoch 31, Batch 839, LR 0.492623 Loss 3.922393, Accuracy 92.068%\n",
      "Epoch 31, Batch 840, LR 0.492517 Loss 3.922106, Accuracy 92.071%\n",
      "Epoch 31, Batch 841, LR 0.492410 Loss 3.922212, Accuracy 92.073%\n",
      "Epoch 31, Batch 842, LR 0.492304 Loss 3.922443, Accuracy 92.074%\n",
      "Epoch 31, Batch 843, LR 0.492197 Loss 3.922188, Accuracy 92.074%\n",
      "Epoch 31, Batch 844, LR 0.492091 Loss 3.923502, Accuracy 92.066%\n",
      "Epoch 31, Batch 845, LR 0.491984 Loss 3.923536, Accuracy 92.061%\n",
      "Epoch 31, Batch 846, LR 0.491878 Loss 3.923145, Accuracy 92.067%\n",
      "Epoch 31, Batch 847, LR 0.491771 Loss 3.923459, Accuracy 92.067%\n",
      "Epoch 31, Batch 848, LR 0.491665 Loss 3.924067, Accuracy 92.066%\n",
      "Epoch 31, Batch 849, LR 0.491558 Loss 3.924371, Accuracy 92.065%\n",
      "Epoch 31, Batch 850, LR 0.491452 Loss 3.924101, Accuracy 92.066%\n",
      "Epoch 31, Batch 851, LR 0.491345 Loss 3.924019, Accuracy 92.066%\n",
      "Epoch 31, Batch 852, LR 0.491239 Loss 3.924351, Accuracy 92.066%\n",
      "Epoch 31, Batch 853, LR 0.491132 Loss 3.924495, Accuracy 92.066%\n",
      "Epoch 31, Batch 854, LR 0.491026 Loss 3.924500, Accuracy 92.066%\n",
      "Epoch 31, Batch 855, LR 0.490919 Loss 3.924898, Accuracy 92.068%\n",
      "Epoch 31, Batch 856, LR 0.490813 Loss 3.924720, Accuracy 92.069%\n",
      "Epoch 31, Batch 857, LR 0.490707 Loss 3.924024, Accuracy 92.073%\n",
      "Epoch 31, Batch 858, LR 0.490600 Loss 3.923707, Accuracy 92.075%\n",
      "Epoch 31, Batch 859, LR 0.490494 Loss 3.923766, Accuracy 92.075%\n",
      "Epoch 31, Batch 860, LR 0.490387 Loss 3.923581, Accuracy 92.072%\n",
      "Epoch 31, Batch 861, LR 0.490281 Loss 3.923701, Accuracy 92.071%\n",
      "Epoch 31, Batch 862, LR 0.490175 Loss 3.923596, Accuracy 92.073%\n",
      "Epoch 31, Batch 863, LR 0.490068 Loss 3.923977, Accuracy 92.073%\n",
      "Epoch 31, Batch 864, LR 0.489962 Loss 3.924379, Accuracy 92.074%\n",
      "Epoch 31, Batch 865, LR 0.489856 Loss 3.924133, Accuracy 92.075%\n",
      "Epoch 31, Batch 866, LR 0.489749 Loss 3.923958, Accuracy 92.076%\n",
      "Epoch 31, Batch 867, LR 0.489643 Loss 3.924050, Accuracy 92.077%\n",
      "Epoch 31, Batch 868, LR 0.489537 Loss 3.924672, Accuracy 92.077%\n",
      "Epoch 31, Batch 869, LR 0.489430 Loss 3.924862, Accuracy 92.076%\n",
      "Epoch 31, Batch 870, LR 0.489324 Loss 3.924817, Accuracy 92.076%\n",
      "Epoch 31, Batch 871, LR 0.489218 Loss 3.925349, Accuracy 92.070%\n",
      "Epoch 31, Batch 872, LR 0.489111 Loss 3.925517, Accuracy 92.067%\n",
      "Epoch 31, Batch 873, LR 0.489005 Loss 3.925698, Accuracy 92.064%\n",
      "Epoch 31, Batch 874, LR 0.488899 Loss 3.925858, Accuracy 92.065%\n",
      "Epoch 31, Batch 875, LR 0.488793 Loss 3.926468, Accuracy 92.063%\n",
      "Epoch 31, Batch 876, LR 0.488686 Loss 3.925754, Accuracy 92.067%\n",
      "Epoch 31, Batch 877, LR 0.488580 Loss 3.925401, Accuracy 92.069%\n",
      "Epoch 31, Batch 878, LR 0.488474 Loss 3.925067, Accuracy 92.071%\n",
      "Epoch 31, Batch 879, LR 0.488368 Loss 3.925414, Accuracy 92.068%\n",
      "Epoch 31, Batch 880, LR 0.488261 Loss 3.925348, Accuracy 92.069%\n",
      "Epoch 31, Batch 881, LR 0.488155 Loss 3.925218, Accuracy 92.069%\n",
      "Epoch 31, Batch 882, LR 0.488049 Loss 3.925017, Accuracy 92.070%\n",
      "Epoch 31, Batch 883, LR 0.487943 Loss 3.924937, Accuracy 92.070%\n",
      "Epoch 31, Batch 884, LR 0.487837 Loss 3.924875, Accuracy 92.069%\n",
      "Epoch 31, Batch 885, LR 0.487731 Loss 3.926381, Accuracy 92.060%\n",
      "Epoch 31, Batch 886, LR 0.487624 Loss 3.927481, Accuracy 92.058%\n",
      "Epoch 31, Batch 887, LR 0.487518 Loss 3.927123, Accuracy 92.060%\n",
      "Epoch 31, Batch 888, LR 0.487412 Loss 3.927774, Accuracy 92.061%\n",
      "Epoch 31, Batch 889, LR 0.487306 Loss 3.927711, Accuracy 92.059%\n",
      "Epoch 31, Batch 890, LR 0.487200 Loss 3.927239, Accuracy 92.061%\n",
      "Epoch 31, Batch 891, LR 0.487094 Loss 3.927715, Accuracy 92.055%\n",
      "Epoch 31, Batch 892, LR 0.486988 Loss 3.927359, Accuracy 92.057%\n",
      "Epoch 31, Batch 893, LR 0.486882 Loss 3.927383, Accuracy 92.056%\n",
      "Epoch 31, Batch 894, LR 0.486775 Loss 3.928097, Accuracy 92.053%\n",
      "Epoch 31, Batch 895, LR 0.486669 Loss 3.928434, Accuracy 92.051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 896, LR 0.486563 Loss 3.928460, Accuracy 92.049%\n",
      "Epoch 31, Batch 897, LR 0.486457 Loss 3.927772, Accuracy 92.052%\n",
      "Epoch 31, Batch 898, LR 0.486351 Loss 3.928446, Accuracy 92.048%\n",
      "Epoch 31, Batch 899, LR 0.486245 Loss 3.928104, Accuracy 92.048%\n",
      "Epoch 31, Batch 900, LR 0.486139 Loss 3.928332, Accuracy 92.047%\n",
      "Epoch 31, Batch 901, LR 0.486033 Loss 3.928305, Accuracy 92.046%\n",
      "Epoch 31, Batch 902, LR 0.485927 Loss 3.928151, Accuracy 92.046%\n",
      "Epoch 31, Batch 903, LR 0.485821 Loss 3.928198, Accuracy 92.046%\n",
      "Epoch 31, Batch 904, LR 0.485715 Loss 3.927236, Accuracy 92.050%\n",
      "Epoch 31, Batch 905, LR 0.485609 Loss 3.927602, Accuracy 92.049%\n",
      "Epoch 31, Batch 906, LR 0.485503 Loss 3.928113, Accuracy 92.046%\n",
      "Epoch 31, Batch 907, LR 0.485397 Loss 3.928186, Accuracy 92.044%\n",
      "Epoch 31, Batch 908, LR 0.485291 Loss 3.928323, Accuracy 92.046%\n",
      "Epoch 31, Batch 909, LR 0.485185 Loss 3.928727, Accuracy 92.042%\n",
      "Epoch 31, Batch 910, LR 0.485079 Loss 3.928302, Accuracy 92.044%\n",
      "Epoch 31, Batch 911, LR 0.484973 Loss 3.927820, Accuracy 92.048%\n",
      "Epoch 31, Batch 912, LR 0.484867 Loss 3.927812, Accuracy 92.048%\n",
      "Epoch 31, Batch 913, LR 0.484761 Loss 3.928475, Accuracy 92.045%\n",
      "Epoch 31, Batch 914, LR 0.484656 Loss 3.928088, Accuracy 92.047%\n",
      "Epoch 31, Batch 915, LR 0.484550 Loss 3.928825, Accuracy 92.041%\n",
      "Epoch 31, Batch 916, LR 0.484444 Loss 3.928570, Accuracy 92.043%\n",
      "Epoch 31, Batch 917, LR 0.484338 Loss 3.928668, Accuracy 92.039%\n",
      "Epoch 31, Batch 918, LR 0.484232 Loss 3.928157, Accuracy 92.039%\n",
      "Epoch 31, Batch 919, LR 0.484126 Loss 3.928125, Accuracy 92.041%\n",
      "Epoch 31, Batch 920, LR 0.484020 Loss 3.927344, Accuracy 92.047%\n",
      "Epoch 31, Batch 921, LR 0.483914 Loss 3.927351, Accuracy 92.046%\n",
      "Epoch 31, Batch 922, LR 0.483809 Loss 3.927694, Accuracy 92.046%\n",
      "Epoch 31, Batch 923, LR 0.483703 Loss 3.927740, Accuracy 92.046%\n",
      "Epoch 31, Batch 924, LR 0.483597 Loss 3.926903, Accuracy 92.049%\n",
      "Epoch 31, Batch 925, LR 0.483491 Loss 3.926710, Accuracy 92.051%\n",
      "Epoch 31, Batch 926, LR 0.483385 Loss 3.926876, Accuracy 92.052%\n",
      "Epoch 31, Batch 927, LR 0.483279 Loss 3.927729, Accuracy 92.043%\n",
      "Epoch 31, Batch 928, LR 0.483174 Loss 3.928329, Accuracy 92.041%\n",
      "Epoch 31, Batch 929, LR 0.483068 Loss 3.927550, Accuracy 92.045%\n",
      "Epoch 31, Batch 930, LR 0.482962 Loss 3.927266, Accuracy 92.046%\n",
      "Epoch 31, Batch 931, LR 0.482856 Loss 3.926848, Accuracy 92.047%\n",
      "Epoch 31, Batch 932, LR 0.482751 Loss 3.926733, Accuracy 92.046%\n",
      "Epoch 31, Batch 933, LR 0.482645 Loss 3.926273, Accuracy 92.045%\n",
      "Epoch 31, Batch 934, LR 0.482539 Loss 3.926240, Accuracy 92.042%\n",
      "Epoch 31, Batch 935, LR 0.482433 Loss 3.926182, Accuracy 92.041%\n",
      "Epoch 31, Batch 936, LR 0.482328 Loss 3.925780, Accuracy 92.042%\n",
      "Epoch 31, Batch 937, LR 0.482222 Loss 3.925446, Accuracy 92.041%\n",
      "Epoch 31, Batch 938, LR 0.482116 Loss 3.925475, Accuracy 92.042%\n",
      "Epoch 31, Batch 939, LR 0.482011 Loss 3.925568, Accuracy 92.045%\n",
      "Epoch 31, Batch 940, LR 0.481905 Loss 3.924987, Accuracy 92.047%\n",
      "Epoch 31, Batch 941, LR 0.481799 Loss 3.925324, Accuracy 92.045%\n",
      "Epoch 31, Batch 942, LR 0.481694 Loss 3.925601, Accuracy 92.045%\n",
      "Epoch 31, Batch 943, LR 0.481588 Loss 3.925576, Accuracy 92.047%\n",
      "Epoch 31, Batch 944, LR 0.481482 Loss 3.925273, Accuracy 92.050%\n",
      "Epoch 31, Batch 945, LR 0.481377 Loss 3.925240, Accuracy 92.051%\n",
      "Epoch 31, Batch 946, LR 0.481271 Loss 3.925413, Accuracy 92.049%\n",
      "Epoch 31, Batch 947, LR 0.481165 Loss 3.925737, Accuracy 92.046%\n",
      "Epoch 31, Batch 948, LR 0.481060 Loss 3.925665, Accuracy 92.043%\n",
      "Epoch 31, Batch 949, LR 0.480954 Loss 3.925570, Accuracy 92.046%\n",
      "Epoch 31, Batch 950, LR 0.480849 Loss 3.926350, Accuracy 92.041%\n",
      "Epoch 31, Batch 951, LR 0.480743 Loss 3.926096, Accuracy 92.047%\n",
      "Epoch 31, Batch 952, LR 0.480637 Loss 3.925776, Accuracy 92.046%\n",
      "Epoch 31, Batch 953, LR 0.480532 Loss 3.925350, Accuracy 92.049%\n",
      "Epoch 31, Batch 954, LR 0.480426 Loss 3.925255, Accuracy 92.054%\n",
      "Epoch 31, Batch 955, LR 0.480321 Loss 3.924994, Accuracy 92.056%\n",
      "Epoch 31, Batch 956, LR 0.480215 Loss 3.924502, Accuracy 92.058%\n",
      "Epoch 31, Batch 957, LR 0.480110 Loss 3.924718, Accuracy 92.055%\n",
      "Epoch 31, Batch 958, LR 0.480004 Loss 3.925565, Accuracy 92.051%\n",
      "Epoch 31, Batch 959, LR 0.479899 Loss 3.925585, Accuracy 92.052%\n",
      "Epoch 31, Batch 960, LR 0.479793 Loss 3.925975, Accuracy 92.050%\n",
      "Epoch 31, Batch 961, LR 0.479688 Loss 3.925628, Accuracy 92.055%\n",
      "Epoch 31, Batch 962, LR 0.479582 Loss 3.925307, Accuracy 92.059%\n",
      "Epoch 31, Batch 963, LR 0.479477 Loss 3.925380, Accuracy 92.059%\n",
      "Epoch 31, Batch 964, LR 0.479371 Loss 3.925297, Accuracy 92.062%\n",
      "Epoch 31, Batch 965, LR 0.479266 Loss 3.924099, Accuracy 92.066%\n",
      "Epoch 31, Batch 966, LR 0.479160 Loss 3.924409, Accuracy 92.065%\n",
      "Epoch 31, Batch 967, LR 0.479055 Loss 3.924762, Accuracy 92.061%\n",
      "Epoch 31, Batch 968, LR 0.478949 Loss 3.924922, Accuracy 92.060%\n",
      "Epoch 31, Batch 969, LR 0.478844 Loss 3.924666, Accuracy 92.061%\n",
      "Epoch 31, Batch 970, LR 0.478738 Loss 3.924979, Accuracy 92.061%\n",
      "Epoch 31, Batch 971, LR 0.478633 Loss 3.925202, Accuracy 92.060%\n",
      "Epoch 31, Batch 972, LR 0.478528 Loss 3.925130, Accuracy 92.058%\n",
      "Epoch 31, Batch 973, LR 0.478422 Loss 3.925000, Accuracy 92.057%\n",
      "Epoch 31, Batch 974, LR 0.478317 Loss 3.923931, Accuracy 92.060%\n",
      "Epoch 31, Batch 975, LR 0.478211 Loss 3.923989, Accuracy 92.062%\n",
      "Epoch 31, Batch 976, LR 0.478106 Loss 3.924379, Accuracy 92.062%\n",
      "Epoch 31, Batch 977, LR 0.478001 Loss 3.924368, Accuracy 92.062%\n",
      "Epoch 31, Batch 978, LR 0.477895 Loss 3.924713, Accuracy 92.058%\n",
      "Epoch 31, Batch 979, LR 0.477790 Loss 3.924907, Accuracy 92.057%\n",
      "Epoch 31, Batch 980, LR 0.477685 Loss 3.925122, Accuracy 92.054%\n",
      "Epoch 31, Batch 981, LR 0.477579 Loss 3.924899, Accuracy 92.054%\n",
      "Epoch 31, Batch 982, LR 0.477474 Loss 3.925034, Accuracy 92.053%\n",
      "Epoch 31, Batch 983, LR 0.477369 Loss 3.924507, Accuracy 92.055%\n",
      "Epoch 31, Batch 984, LR 0.477264 Loss 3.924140, Accuracy 92.058%\n",
      "Epoch 31, Batch 985, LR 0.477158 Loss 3.924429, Accuracy 92.054%\n",
      "Epoch 31, Batch 986, LR 0.477053 Loss 3.925005, Accuracy 92.053%\n",
      "Epoch 31, Batch 987, LR 0.476948 Loss 3.925199, Accuracy 92.054%\n",
      "Epoch 31, Batch 988, LR 0.476842 Loss 3.924554, Accuracy 92.057%\n",
      "Epoch 31, Batch 989, LR 0.476737 Loss 3.924496, Accuracy 92.057%\n",
      "Epoch 31, Batch 990, LR 0.476632 Loss 3.924589, Accuracy 92.057%\n",
      "Epoch 31, Batch 991, LR 0.476527 Loss 3.924853, Accuracy 92.055%\n",
      "Epoch 31, Batch 992, LR 0.476421 Loss 3.925224, Accuracy 92.055%\n",
      "Epoch 31, Batch 993, LR 0.476316 Loss 3.926001, Accuracy 92.052%\n",
      "Epoch 31, Batch 994, LR 0.476211 Loss 3.925923, Accuracy 92.051%\n",
      "Epoch 31, Batch 995, LR 0.476106 Loss 3.926252, Accuracy 92.045%\n",
      "Epoch 31, Batch 996, LR 0.476001 Loss 3.926905, Accuracy 92.043%\n",
      "Epoch 31, Batch 997, LR 0.475895 Loss 3.926933, Accuracy 92.046%\n",
      "Epoch 31, Batch 998, LR 0.475790 Loss 3.926805, Accuracy 92.044%\n",
      "Epoch 31, Batch 999, LR 0.475685 Loss 3.926985, Accuracy 92.044%\n",
      "Epoch 31, Batch 1000, LR 0.475580 Loss 3.926734, Accuracy 92.044%\n",
      "Epoch 31, Batch 1001, LR 0.475475 Loss 3.926838, Accuracy 92.043%\n",
      "Epoch 31, Batch 1002, LR 0.475370 Loss 3.927581, Accuracy 92.039%\n",
      "Epoch 31, Batch 1003, LR 0.475265 Loss 3.927438, Accuracy 92.040%\n",
      "Epoch 31, Batch 1004, LR 0.475159 Loss 3.927185, Accuracy 92.039%\n",
      "Epoch 31, Batch 1005, LR 0.475054 Loss 3.927079, Accuracy 92.037%\n",
      "Epoch 31, Batch 1006, LR 0.474949 Loss 3.927383, Accuracy 92.035%\n",
      "Epoch 31, Batch 1007, LR 0.474844 Loss 3.927419, Accuracy 92.032%\n",
      "Epoch 31, Batch 1008, LR 0.474739 Loss 3.926616, Accuracy 92.036%\n",
      "Epoch 31, Batch 1009, LR 0.474634 Loss 3.926780, Accuracy 92.037%\n",
      "Epoch 31, Batch 1010, LR 0.474529 Loss 3.927098, Accuracy 92.035%\n",
      "Epoch 31, Batch 1011, LR 0.474424 Loss 3.927351, Accuracy 92.034%\n",
      "Epoch 31, Batch 1012, LR 0.474319 Loss 3.927806, Accuracy 92.029%\n",
      "Epoch 31, Batch 1013, LR 0.474214 Loss 3.927730, Accuracy 92.031%\n",
      "Epoch 31, Batch 1014, LR 0.474109 Loss 3.926756, Accuracy 92.036%\n",
      "Epoch 31, Batch 1015, LR 0.474004 Loss 3.927087, Accuracy 92.034%\n",
      "Epoch 31, Batch 1016, LR 0.473899 Loss 3.927781, Accuracy 92.030%\n",
      "Epoch 31, Batch 1017, LR 0.473794 Loss 3.927416, Accuracy 92.031%\n",
      "Epoch 31, Batch 1018, LR 0.473689 Loss 3.928057, Accuracy 92.029%\n",
      "Epoch 31, Batch 1019, LR 0.473584 Loss 3.928202, Accuracy 92.033%\n",
      "Epoch 31, Batch 1020, LR 0.473479 Loss 3.928340, Accuracy 92.032%\n",
      "Epoch 31, Batch 1021, LR 0.473374 Loss 3.928261, Accuracy 92.031%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 1022, LR 0.473269 Loss 3.928438, Accuracy 92.028%\n",
      "Epoch 31, Batch 1023, LR 0.473164 Loss 3.928010, Accuracy 92.029%\n",
      "Epoch 31, Batch 1024, LR 0.473059 Loss 3.927724, Accuracy 92.029%\n",
      "Epoch 31, Batch 1025, LR 0.472954 Loss 3.927418, Accuracy 92.028%\n",
      "Epoch 31, Batch 1026, LR 0.472849 Loss 3.927530, Accuracy 92.028%\n",
      "Epoch 31, Batch 1027, LR 0.472744 Loss 3.927481, Accuracy 92.029%\n",
      "Epoch 31, Batch 1028, LR 0.472639 Loss 3.927357, Accuracy 92.029%\n",
      "Epoch 31, Batch 1029, LR 0.472534 Loss 3.927717, Accuracy 92.027%\n",
      "Epoch 31, Batch 1030, LR 0.472430 Loss 3.928143, Accuracy 92.026%\n",
      "Epoch 31, Batch 1031, LR 0.472325 Loss 3.927966, Accuracy 92.025%\n",
      "Epoch 31, Batch 1032, LR 0.472220 Loss 3.928043, Accuracy 92.026%\n",
      "Epoch 31, Batch 1033, LR 0.472115 Loss 3.928378, Accuracy 92.023%\n",
      "Epoch 31, Batch 1034, LR 0.472010 Loss 3.927882, Accuracy 92.026%\n",
      "Epoch 31, Batch 1035, LR 0.471905 Loss 3.927729, Accuracy 92.025%\n",
      "Epoch 31, Batch 1036, LR 0.471800 Loss 3.927592, Accuracy 92.026%\n",
      "Epoch 31, Batch 1037, LR 0.471696 Loss 3.927033, Accuracy 92.029%\n",
      "Epoch 31, Batch 1038, LR 0.471591 Loss 3.927832, Accuracy 92.025%\n",
      "Epoch 31, Batch 1039, LR 0.471486 Loss 3.927802, Accuracy 92.027%\n",
      "Epoch 31, Batch 1040, LR 0.471381 Loss 3.927816, Accuracy 92.024%\n",
      "Epoch 31, Batch 1041, LR 0.471276 Loss 3.927360, Accuracy 92.025%\n",
      "Epoch 31, Batch 1042, LR 0.471172 Loss 3.927190, Accuracy 92.026%\n",
      "Epoch 31, Batch 1043, LR 0.471067 Loss 3.926803, Accuracy 92.026%\n",
      "Epoch 31, Batch 1044, LR 0.470962 Loss 3.927228, Accuracy 92.026%\n",
      "Epoch 31, Batch 1045, LR 0.470857 Loss 3.926877, Accuracy 92.028%\n",
      "Epoch 31, Batch 1046, LR 0.470753 Loss 3.926836, Accuracy 92.027%\n",
      "Epoch 31, Batch 1047, LR 0.470648 Loss 3.926841, Accuracy 92.027%\n",
      "Epoch 31, Loss (train set) 3.926841, Accuracy (train set) 92.027%\n",
      "Epoch 32, Batch 1, LR 0.470543 Loss 3.537362, Accuracy 93.750%\n",
      "Epoch 32, Batch 2, LR 0.470438 Loss 3.594533, Accuracy 94.141%\n",
      "Epoch 32, Batch 3, LR 0.470334 Loss 3.595149, Accuracy 93.750%\n",
      "Epoch 32, Batch 4, LR 0.470229 Loss 3.682192, Accuracy 92.773%\n",
      "Epoch 32, Batch 5, LR 0.470124 Loss 3.587314, Accuracy 93.125%\n",
      "Epoch 32, Batch 6, LR 0.470020 Loss 3.529722, Accuracy 93.229%\n",
      "Epoch 32, Batch 7, LR 0.469915 Loss 3.438557, Accuracy 93.750%\n",
      "Epoch 32, Batch 8, LR 0.469810 Loss 3.550316, Accuracy 92.871%\n",
      "Epoch 32, Batch 9, LR 0.469706 Loss 3.638717, Accuracy 92.448%\n",
      "Epoch 32, Batch 10, LR 0.469601 Loss 3.623578, Accuracy 92.188%\n",
      "Epoch 32, Batch 11, LR 0.469496 Loss 3.620994, Accuracy 92.330%\n",
      "Epoch 32, Batch 12, LR 0.469392 Loss 3.592881, Accuracy 92.383%\n",
      "Epoch 32, Batch 13, LR 0.469287 Loss 3.606610, Accuracy 92.608%\n",
      "Epoch 32, Batch 14, LR 0.469183 Loss 3.631210, Accuracy 92.578%\n",
      "Epoch 32, Batch 15, LR 0.469078 Loss 3.625854, Accuracy 92.812%\n",
      "Epoch 32, Batch 16, LR 0.468973 Loss 3.641697, Accuracy 92.920%\n",
      "Epoch 32, Batch 17, LR 0.468869 Loss 3.650150, Accuracy 92.785%\n",
      "Epoch 32, Batch 18, LR 0.468764 Loss 3.699606, Accuracy 92.491%\n",
      "Epoch 32, Batch 19, LR 0.468660 Loss 3.723593, Accuracy 92.434%\n",
      "Epoch 32, Batch 20, LR 0.468555 Loss 3.735278, Accuracy 92.461%\n",
      "Epoch 32, Batch 21, LR 0.468451 Loss 3.728703, Accuracy 92.560%\n",
      "Epoch 32, Batch 22, LR 0.468346 Loss 3.733340, Accuracy 92.614%\n",
      "Epoch 32, Batch 23, LR 0.468242 Loss 3.759354, Accuracy 92.357%\n",
      "Epoch 32, Batch 24, LR 0.468137 Loss 3.763649, Accuracy 92.383%\n",
      "Epoch 32, Batch 25, LR 0.468032 Loss 3.753312, Accuracy 92.500%\n",
      "Epoch 32, Batch 26, LR 0.467928 Loss 3.744027, Accuracy 92.548%\n",
      "Epoch 32, Batch 27, LR 0.467823 Loss 3.776008, Accuracy 92.332%\n",
      "Epoch 32, Batch 28, LR 0.467719 Loss 3.809981, Accuracy 92.160%\n",
      "Epoch 32, Batch 29, LR 0.467615 Loss 3.796066, Accuracy 92.268%\n",
      "Epoch 32, Batch 30, LR 0.467510 Loss 3.799924, Accuracy 92.318%\n",
      "Epoch 32, Batch 31, LR 0.467406 Loss 3.808538, Accuracy 92.364%\n",
      "Epoch 32, Batch 32, LR 0.467301 Loss 3.822221, Accuracy 92.285%\n",
      "Epoch 32, Batch 33, LR 0.467197 Loss 3.813136, Accuracy 92.353%\n",
      "Epoch 32, Batch 34, LR 0.467092 Loss 3.822483, Accuracy 92.302%\n",
      "Epoch 32, Batch 35, LR 0.466988 Loss 3.809424, Accuracy 92.433%\n",
      "Epoch 32, Batch 36, LR 0.466883 Loss 3.813988, Accuracy 92.448%\n",
      "Epoch 32, Batch 37, LR 0.466779 Loss 3.817005, Accuracy 92.420%\n",
      "Epoch 32, Batch 38, LR 0.466675 Loss 3.812830, Accuracy 92.393%\n",
      "Epoch 32, Batch 39, LR 0.466570 Loss 3.805594, Accuracy 92.388%\n",
      "Epoch 32, Batch 40, LR 0.466466 Loss 3.815520, Accuracy 92.305%\n",
      "Epoch 32, Batch 41, LR 0.466362 Loss 3.816564, Accuracy 92.226%\n",
      "Epoch 32, Batch 42, LR 0.466257 Loss 3.811255, Accuracy 92.262%\n",
      "Epoch 32, Batch 43, LR 0.466153 Loss 3.823469, Accuracy 92.242%\n",
      "Epoch 32, Batch 44, LR 0.466049 Loss 3.819484, Accuracy 92.276%\n",
      "Epoch 32, Batch 45, LR 0.465944 Loss 3.810182, Accuracy 92.326%\n",
      "Epoch 32, Batch 46, LR 0.465840 Loss 3.803409, Accuracy 92.374%\n",
      "Epoch 32, Batch 47, LR 0.465736 Loss 3.802753, Accuracy 92.387%\n",
      "Epoch 32, Batch 48, LR 0.465631 Loss 3.810199, Accuracy 92.399%\n",
      "Epoch 32, Batch 49, LR 0.465527 Loss 3.806028, Accuracy 92.443%\n",
      "Epoch 32, Batch 50, LR 0.465423 Loss 3.799628, Accuracy 92.484%\n",
      "Epoch 32, Batch 51, LR 0.465318 Loss 3.796115, Accuracy 92.540%\n",
      "Epoch 32, Batch 52, LR 0.465214 Loss 3.800360, Accuracy 92.518%\n",
      "Epoch 32, Batch 53, LR 0.465110 Loss 3.801879, Accuracy 92.556%\n",
      "Epoch 32, Batch 54, LR 0.465006 Loss 3.800392, Accuracy 92.506%\n",
      "Epoch 32, Batch 55, LR 0.464901 Loss 3.788094, Accuracy 92.514%\n",
      "Epoch 32, Batch 56, LR 0.464797 Loss 3.788658, Accuracy 92.508%\n",
      "Epoch 32, Batch 57, LR 0.464693 Loss 3.792958, Accuracy 92.475%\n",
      "Epoch 32, Batch 58, LR 0.464589 Loss 3.797785, Accuracy 92.457%\n",
      "Epoch 32, Batch 59, LR 0.464485 Loss 3.799408, Accuracy 92.426%\n",
      "Epoch 32, Batch 60, LR 0.464380 Loss 3.801578, Accuracy 92.409%\n",
      "Epoch 32, Batch 61, LR 0.464276 Loss 3.788440, Accuracy 92.469%\n",
      "Epoch 32, Batch 62, LR 0.464172 Loss 3.803318, Accuracy 92.440%\n",
      "Epoch 32, Batch 63, LR 0.464068 Loss 3.805789, Accuracy 92.423%\n",
      "Epoch 32, Batch 64, LR 0.463964 Loss 3.812371, Accuracy 92.395%\n",
      "Epoch 32, Batch 65, LR 0.463859 Loss 3.805505, Accuracy 92.452%\n",
      "Epoch 32, Batch 66, LR 0.463755 Loss 3.796875, Accuracy 92.483%\n",
      "Epoch 32, Batch 67, LR 0.463651 Loss 3.797404, Accuracy 92.514%\n",
      "Epoch 32, Batch 68, LR 0.463547 Loss 3.797052, Accuracy 92.544%\n",
      "Epoch 32, Batch 69, LR 0.463443 Loss 3.803435, Accuracy 92.527%\n",
      "Epoch 32, Batch 70, LR 0.463339 Loss 3.795902, Accuracy 92.600%\n",
      "Epoch 32, Batch 71, LR 0.463235 Loss 3.800880, Accuracy 92.584%\n",
      "Epoch 32, Batch 72, LR 0.463131 Loss 3.791270, Accuracy 92.611%\n",
      "Epoch 32, Batch 73, LR 0.463027 Loss 3.784407, Accuracy 92.658%\n",
      "Epoch 32, Batch 74, LR 0.462923 Loss 3.783902, Accuracy 92.641%\n",
      "Epoch 32, Batch 75, LR 0.462818 Loss 3.784057, Accuracy 92.635%\n",
      "Epoch 32, Batch 76, LR 0.462714 Loss 3.776624, Accuracy 92.671%\n",
      "Epoch 32, Batch 77, LR 0.462610 Loss 3.769610, Accuracy 92.695%\n",
      "Epoch 32, Batch 78, LR 0.462506 Loss 3.771784, Accuracy 92.698%\n",
      "Epoch 32, Batch 79, LR 0.462402 Loss 3.766782, Accuracy 92.751%\n",
      "Epoch 32, Batch 80, LR 0.462298 Loss 3.760943, Accuracy 92.793%\n",
      "Epoch 32, Batch 81, LR 0.462194 Loss 3.765075, Accuracy 92.766%\n",
      "Epoch 32, Batch 82, LR 0.462090 Loss 3.766170, Accuracy 92.788%\n",
      "Epoch 32, Batch 83, LR 0.461986 Loss 3.762499, Accuracy 92.762%\n",
      "Epoch 32, Batch 84, LR 0.461882 Loss 3.765900, Accuracy 92.773%\n",
      "Epoch 32, Batch 85, LR 0.461778 Loss 3.767734, Accuracy 92.776%\n",
      "Epoch 32, Batch 86, LR 0.461674 Loss 3.777338, Accuracy 92.723%\n",
      "Epoch 32, Batch 87, LR 0.461570 Loss 3.777807, Accuracy 92.726%\n",
      "Epoch 32, Batch 88, LR 0.461467 Loss 3.778086, Accuracy 92.676%\n",
      "Epoch 32, Batch 89, LR 0.461363 Loss 3.776588, Accuracy 92.688%\n",
      "Epoch 32, Batch 90, LR 0.461259 Loss 3.784428, Accuracy 92.665%\n",
      "Epoch 32, Batch 91, LR 0.461155 Loss 3.785410, Accuracy 92.651%\n",
      "Epoch 32, Batch 92, LR 0.461051 Loss 3.790494, Accuracy 92.621%\n",
      "Epoch 32, Batch 93, LR 0.460947 Loss 3.784467, Accuracy 92.616%\n",
      "Epoch 32, Batch 94, LR 0.460843 Loss 3.782693, Accuracy 92.578%\n",
      "Epoch 32, Batch 95, LR 0.460739 Loss 3.783564, Accuracy 92.582%\n",
      "Epoch 32, Batch 96, LR 0.460635 Loss 3.783605, Accuracy 92.586%\n",
      "Epoch 32, Batch 97, LR 0.460531 Loss 3.790643, Accuracy 92.534%\n",
      "Epoch 32, Batch 98, LR 0.460428 Loss 3.795031, Accuracy 92.578%\n",
      "Epoch 32, Batch 99, LR 0.460324 Loss 3.799426, Accuracy 92.551%\n",
      "Epoch 32, Batch 100, LR 0.460220 Loss 3.796091, Accuracy 92.570%\n",
      "Epoch 32, Batch 101, LR 0.460116 Loss 3.790751, Accuracy 92.605%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 102, LR 0.460012 Loss 3.791493, Accuracy 92.624%\n",
      "Epoch 32, Batch 103, LR 0.459909 Loss 3.797011, Accuracy 92.582%\n",
      "Epoch 32, Batch 104, LR 0.459805 Loss 3.797504, Accuracy 92.578%\n",
      "Epoch 32, Batch 105, LR 0.459701 Loss 3.792308, Accuracy 92.612%\n",
      "Epoch 32, Batch 106, LR 0.459597 Loss 3.788883, Accuracy 92.637%\n",
      "Epoch 32, Batch 107, LR 0.459493 Loss 3.790216, Accuracy 92.633%\n",
      "Epoch 32, Batch 108, LR 0.459390 Loss 3.791678, Accuracy 92.636%\n",
      "Epoch 32, Batch 109, LR 0.459286 Loss 3.793769, Accuracy 92.603%\n",
      "Epoch 32, Batch 110, LR 0.459182 Loss 3.795253, Accuracy 92.592%\n",
      "Epoch 32, Batch 111, LR 0.459078 Loss 3.787183, Accuracy 92.617%\n",
      "Epoch 32, Batch 112, LR 0.458975 Loss 3.788372, Accuracy 92.634%\n",
      "Epoch 32, Batch 113, LR 0.458871 Loss 3.788905, Accuracy 92.644%\n",
      "Epoch 32, Batch 114, LR 0.458767 Loss 3.789888, Accuracy 92.640%\n",
      "Epoch 32, Batch 115, LR 0.458664 Loss 3.784804, Accuracy 92.629%\n",
      "Epoch 32, Batch 116, LR 0.458560 Loss 3.786333, Accuracy 92.612%\n",
      "Epoch 32, Batch 117, LR 0.458456 Loss 3.787529, Accuracy 92.635%\n",
      "Epoch 32, Batch 118, LR 0.458353 Loss 3.797588, Accuracy 92.585%\n",
      "Epoch 32, Batch 119, LR 0.458249 Loss 3.799834, Accuracy 92.549%\n",
      "Epoch 32, Batch 120, LR 0.458145 Loss 3.801391, Accuracy 92.533%\n",
      "Epoch 32, Batch 121, LR 0.458042 Loss 3.804436, Accuracy 92.530%\n",
      "Epoch 32, Batch 122, LR 0.457938 Loss 3.802276, Accuracy 92.546%\n",
      "Epoch 32, Batch 123, LR 0.457834 Loss 3.804049, Accuracy 92.524%\n",
      "Epoch 32, Batch 124, LR 0.457731 Loss 3.807828, Accuracy 92.515%\n",
      "Epoch 32, Batch 125, LR 0.457627 Loss 3.807493, Accuracy 92.537%\n",
      "Epoch 32, Batch 126, LR 0.457524 Loss 3.811749, Accuracy 92.510%\n",
      "Epoch 32, Batch 127, LR 0.457420 Loss 3.811838, Accuracy 92.507%\n",
      "Epoch 32, Batch 128, LR 0.457316 Loss 3.820648, Accuracy 92.462%\n",
      "Epoch 32, Batch 129, LR 0.457213 Loss 3.822009, Accuracy 92.442%\n",
      "Epoch 32, Batch 130, LR 0.457109 Loss 3.827336, Accuracy 92.422%\n",
      "Epoch 32, Batch 131, LR 0.457006 Loss 3.821968, Accuracy 92.444%\n",
      "Epoch 32, Batch 132, LR 0.456902 Loss 3.824436, Accuracy 92.430%\n",
      "Epoch 32, Batch 133, LR 0.456799 Loss 3.823932, Accuracy 92.428%\n",
      "Epoch 32, Batch 134, LR 0.456695 Loss 3.829650, Accuracy 92.397%\n",
      "Epoch 32, Batch 135, LR 0.456592 Loss 3.827388, Accuracy 92.419%\n",
      "Epoch 32, Batch 136, LR 0.456488 Loss 3.830406, Accuracy 92.412%\n",
      "Epoch 32, Batch 137, LR 0.456385 Loss 3.830379, Accuracy 92.433%\n",
      "Epoch 32, Batch 138, LR 0.456281 Loss 3.828071, Accuracy 92.431%\n",
      "Epoch 32, Batch 139, LR 0.456178 Loss 3.827555, Accuracy 92.440%\n",
      "Epoch 32, Batch 140, LR 0.456074 Loss 3.826189, Accuracy 92.439%\n",
      "Epoch 32, Batch 141, LR 0.455971 Loss 3.826259, Accuracy 92.437%\n",
      "Epoch 32, Batch 142, LR 0.455867 Loss 3.830014, Accuracy 92.419%\n",
      "Epoch 32, Batch 143, LR 0.455764 Loss 3.836352, Accuracy 92.395%\n",
      "Epoch 32, Batch 144, LR 0.455660 Loss 3.842317, Accuracy 92.345%\n",
      "Epoch 32, Batch 145, LR 0.455557 Loss 3.839677, Accuracy 92.355%\n",
      "Epoch 32, Batch 146, LR 0.455454 Loss 3.836989, Accuracy 92.353%\n",
      "Epoch 32, Batch 147, LR 0.455350 Loss 3.839685, Accuracy 92.326%\n",
      "Epoch 32, Batch 148, LR 0.455247 Loss 3.841359, Accuracy 92.314%\n",
      "Epoch 32, Batch 149, LR 0.455143 Loss 3.843742, Accuracy 92.324%\n",
      "Epoch 32, Batch 150, LR 0.455040 Loss 3.842160, Accuracy 92.344%\n",
      "Epoch 32, Batch 151, LR 0.454937 Loss 3.845718, Accuracy 92.332%\n",
      "Epoch 32, Batch 152, LR 0.454833 Loss 3.845837, Accuracy 92.352%\n",
      "Epoch 32, Batch 153, LR 0.454730 Loss 3.843862, Accuracy 92.361%\n",
      "Epoch 32, Batch 154, LR 0.454627 Loss 3.841674, Accuracy 92.365%\n",
      "Epoch 32, Batch 155, LR 0.454523 Loss 3.844527, Accuracy 92.349%\n",
      "Epoch 32, Batch 156, LR 0.454420 Loss 3.846145, Accuracy 92.348%\n",
      "Epoch 32, Batch 157, LR 0.454317 Loss 3.847701, Accuracy 92.342%\n",
      "Epoch 32, Batch 158, LR 0.454213 Loss 3.847929, Accuracy 92.346%\n",
      "Epoch 32, Batch 159, LR 0.454110 Loss 3.847439, Accuracy 92.350%\n",
      "Epoch 32, Batch 160, LR 0.454007 Loss 3.843561, Accuracy 92.378%\n",
      "Epoch 32, Batch 161, LR 0.453903 Loss 3.842818, Accuracy 92.391%\n",
      "Epoch 32, Batch 162, LR 0.453800 Loss 3.842009, Accuracy 92.390%\n",
      "Epoch 32, Batch 163, LR 0.453697 Loss 3.839165, Accuracy 92.394%\n",
      "Epoch 32, Batch 164, LR 0.453594 Loss 3.838327, Accuracy 92.397%\n",
      "Epoch 32, Batch 165, LR 0.453490 Loss 3.837281, Accuracy 92.405%\n",
      "Epoch 32, Batch 166, LR 0.453387 Loss 3.838840, Accuracy 92.409%\n",
      "Epoch 32, Batch 167, LR 0.453284 Loss 3.836795, Accuracy 92.407%\n",
      "Epoch 32, Batch 168, LR 0.453181 Loss 3.839526, Accuracy 92.406%\n",
      "Epoch 32, Batch 169, LR 0.453078 Loss 3.835966, Accuracy 92.414%\n",
      "Epoch 32, Batch 170, LR 0.452974 Loss 3.831487, Accuracy 92.436%\n",
      "Epoch 32, Batch 171, LR 0.452871 Loss 3.834585, Accuracy 92.421%\n",
      "Epoch 32, Batch 172, LR 0.452768 Loss 3.835943, Accuracy 92.415%\n",
      "Epoch 32, Batch 173, LR 0.452665 Loss 3.837803, Accuracy 92.409%\n",
      "Epoch 32, Batch 174, LR 0.452562 Loss 3.838305, Accuracy 92.403%\n",
      "Epoch 32, Batch 175, LR 0.452458 Loss 3.833789, Accuracy 92.429%\n",
      "Epoch 32, Batch 176, LR 0.452355 Loss 3.834834, Accuracy 92.401%\n",
      "Epoch 32, Batch 177, LR 0.452252 Loss 3.839043, Accuracy 92.382%\n",
      "Epoch 32, Batch 178, LR 0.452149 Loss 3.841115, Accuracy 92.372%\n",
      "Epoch 32, Batch 179, LR 0.452046 Loss 3.841397, Accuracy 92.371%\n",
      "Epoch 32, Batch 180, LR 0.451943 Loss 3.840340, Accuracy 92.357%\n",
      "Epoch 32, Batch 181, LR 0.451840 Loss 3.841634, Accuracy 92.352%\n",
      "Epoch 32, Batch 182, LR 0.451737 Loss 3.838888, Accuracy 92.368%\n",
      "Epoch 32, Batch 183, LR 0.451634 Loss 3.839049, Accuracy 92.384%\n",
      "Epoch 32, Batch 184, LR 0.451531 Loss 3.837578, Accuracy 92.391%\n",
      "Epoch 32, Batch 185, LR 0.451427 Loss 3.834971, Accuracy 92.416%\n",
      "Epoch 32, Batch 186, LR 0.451324 Loss 3.835753, Accuracy 92.419%\n",
      "Epoch 32, Batch 187, LR 0.451221 Loss 3.838204, Accuracy 92.396%\n",
      "Epoch 32, Batch 188, LR 0.451118 Loss 3.841390, Accuracy 92.370%\n",
      "Epoch 32, Batch 189, LR 0.451015 Loss 3.839803, Accuracy 92.374%\n",
      "Epoch 32, Batch 190, LR 0.450912 Loss 3.841030, Accuracy 92.381%\n",
      "Epoch 32, Batch 191, LR 0.450809 Loss 3.835271, Accuracy 92.392%\n",
      "Epoch 32, Batch 192, LR 0.450706 Loss 3.834708, Accuracy 92.387%\n",
      "Epoch 32, Batch 193, LR 0.450603 Loss 3.838087, Accuracy 92.362%\n",
      "Epoch 32, Batch 194, LR 0.450500 Loss 3.836420, Accuracy 92.369%\n",
      "Epoch 32, Batch 195, LR 0.450397 Loss 3.835528, Accuracy 92.368%\n",
      "Epoch 32, Batch 196, LR 0.450294 Loss 3.834465, Accuracy 92.355%\n",
      "Epoch 32, Batch 197, LR 0.450192 Loss 3.835316, Accuracy 92.358%\n",
      "Epoch 32, Batch 198, LR 0.450089 Loss 3.834252, Accuracy 92.373%\n",
      "Epoch 32, Batch 199, LR 0.449986 Loss 3.831837, Accuracy 92.384%\n",
      "Epoch 32, Batch 200, LR 0.449883 Loss 3.834038, Accuracy 92.375%\n",
      "Epoch 32, Batch 201, LR 0.449780 Loss 3.834734, Accuracy 92.374%\n",
      "Epoch 32, Batch 202, LR 0.449677 Loss 3.833368, Accuracy 92.381%\n",
      "Epoch 32, Batch 203, LR 0.449574 Loss 3.831226, Accuracy 92.403%\n",
      "Epoch 32, Batch 204, LR 0.449471 Loss 3.830298, Accuracy 92.406%\n",
      "Epoch 32, Batch 205, LR 0.449368 Loss 3.827748, Accuracy 92.409%\n",
      "Epoch 32, Batch 206, LR 0.449265 Loss 3.826727, Accuracy 92.423%\n",
      "Epoch 32, Batch 207, LR 0.449163 Loss 3.828386, Accuracy 92.399%\n",
      "Epoch 32, Batch 208, LR 0.449060 Loss 3.828512, Accuracy 92.398%\n",
      "Epoch 32, Batch 209, LR 0.448957 Loss 3.829639, Accuracy 92.408%\n",
      "Epoch 32, Batch 210, LR 0.448854 Loss 3.828368, Accuracy 92.422%\n",
      "Epoch 32, Batch 211, LR 0.448751 Loss 3.827047, Accuracy 92.428%\n",
      "Epoch 32, Batch 212, LR 0.448648 Loss 3.825472, Accuracy 92.438%\n",
      "Epoch 32, Batch 213, LR 0.448546 Loss 3.824313, Accuracy 92.441%\n",
      "Epoch 32, Batch 214, LR 0.448443 Loss 3.824869, Accuracy 92.428%\n",
      "Epoch 32, Batch 215, LR 0.448340 Loss 3.823109, Accuracy 92.424%\n",
      "Epoch 32, Batch 216, LR 0.448237 Loss 3.820192, Accuracy 92.426%\n",
      "Epoch 32, Batch 217, LR 0.448135 Loss 3.818514, Accuracy 92.443%\n",
      "Epoch 32, Batch 218, LR 0.448032 Loss 3.819581, Accuracy 92.449%\n",
      "Epoch 32, Batch 219, LR 0.447929 Loss 3.821358, Accuracy 92.444%\n",
      "Epoch 32, Batch 220, LR 0.447826 Loss 3.821783, Accuracy 92.436%\n",
      "Epoch 32, Batch 221, LR 0.447724 Loss 3.822580, Accuracy 92.428%\n",
      "Epoch 32, Batch 222, LR 0.447621 Loss 3.826388, Accuracy 92.423%\n",
      "Epoch 32, Batch 223, LR 0.447518 Loss 3.827948, Accuracy 92.422%\n",
      "Epoch 32, Batch 224, LR 0.447415 Loss 3.827519, Accuracy 92.428%\n",
      "Epoch 32, Batch 225, LR 0.447313 Loss 3.827966, Accuracy 92.431%\n",
      "Epoch 32, Batch 226, LR 0.447210 Loss 3.828177, Accuracy 92.423%\n",
      "Epoch 32, Batch 227, LR 0.447107 Loss 3.825393, Accuracy 92.435%\n",
      "Epoch 32, Batch 228, LR 0.447005 Loss 3.826449, Accuracy 92.427%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 229, LR 0.446902 Loss 3.826826, Accuracy 92.409%\n",
      "Epoch 32, Batch 230, LR 0.446799 Loss 3.829095, Accuracy 92.408%\n",
      "Epoch 32, Batch 231, LR 0.446697 Loss 3.830778, Accuracy 92.401%\n",
      "Epoch 32, Batch 232, LR 0.446594 Loss 3.831901, Accuracy 92.410%\n",
      "Epoch 32, Batch 233, LR 0.446492 Loss 3.834917, Accuracy 92.399%\n",
      "Epoch 32, Batch 234, LR 0.446389 Loss 3.834030, Accuracy 92.408%\n",
      "Epoch 32, Batch 235, LR 0.446286 Loss 3.832959, Accuracy 92.414%\n",
      "Epoch 32, Batch 236, LR 0.446184 Loss 3.834347, Accuracy 92.413%\n",
      "Epoch 32, Batch 237, LR 0.446081 Loss 3.838949, Accuracy 92.375%\n",
      "Epoch 32, Batch 238, LR 0.445979 Loss 3.842467, Accuracy 92.348%\n",
      "Epoch 32, Batch 239, LR 0.445876 Loss 3.845873, Accuracy 92.335%\n",
      "Epoch 32, Batch 240, LR 0.445774 Loss 3.845842, Accuracy 92.340%\n",
      "Epoch 32, Batch 241, LR 0.445671 Loss 3.843664, Accuracy 92.340%\n",
      "Epoch 32, Batch 242, LR 0.445568 Loss 3.843106, Accuracy 92.339%\n",
      "Epoch 32, Batch 243, LR 0.445466 Loss 3.844141, Accuracy 92.332%\n",
      "Epoch 32, Batch 244, LR 0.445363 Loss 3.842110, Accuracy 92.338%\n",
      "Epoch 32, Batch 245, LR 0.445261 Loss 3.839825, Accuracy 92.337%\n",
      "Epoch 32, Batch 246, LR 0.445158 Loss 3.842365, Accuracy 92.327%\n",
      "Epoch 32, Batch 247, LR 0.445056 Loss 3.843615, Accuracy 92.311%\n",
      "Epoch 32, Batch 248, LR 0.444953 Loss 3.841580, Accuracy 92.320%\n",
      "Epoch 32, Batch 249, LR 0.444851 Loss 3.841187, Accuracy 92.319%\n",
      "Epoch 32, Batch 250, LR 0.444748 Loss 3.841434, Accuracy 92.316%\n",
      "Epoch 32, Batch 251, LR 0.444646 Loss 3.839314, Accuracy 92.324%\n",
      "Epoch 32, Batch 252, LR 0.444544 Loss 3.840429, Accuracy 92.321%\n",
      "Epoch 32, Batch 253, LR 0.444441 Loss 3.839818, Accuracy 92.333%\n",
      "Epoch 32, Batch 254, LR 0.444339 Loss 3.840856, Accuracy 92.335%\n",
      "Epoch 32, Batch 255, LR 0.444236 Loss 3.842231, Accuracy 92.328%\n",
      "Epoch 32, Batch 256, LR 0.444134 Loss 3.840749, Accuracy 92.328%\n",
      "Epoch 32, Batch 257, LR 0.444032 Loss 3.842669, Accuracy 92.315%\n",
      "Epoch 32, Batch 258, LR 0.443929 Loss 3.841927, Accuracy 92.315%\n",
      "Epoch 32, Batch 259, LR 0.443827 Loss 3.841064, Accuracy 92.323%\n",
      "Epoch 32, Batch 260, LR 0.443724 Loss 3.843245, Accuracy 92.308%\n",
      "Epoch 32, Batch 261, LR 0.443622 Loss 3.842400, Accuracy 92.316%\n",
      "Epoch 32, Batch 262, LR 0.443520 Loss 3.842037, Accuracy 92.319%\n",
      "Epoch 32, Batch 263, LR 0.443417 Loss 3.840606, Accuracy 92.324%\n",
      "Epoch 32, Batch 264, LR 0.443315 Loss 3.840987, Accuracy 92.324%\n",
      "Epoch 32, Batch 265, LR 0.443213 Loss 3.842837, Accuracy 92.320%\n",
      "Epoch 32, Batch 266, LR 0.443110 Loss 3.843434, Accuracy 92.320%\n",
      "Epoch 32, Batch 267, LR 0.443008 Loss 3.844443, Accuracy 92.319%\n",
      "Epoch 32, Batch 268, LR 0.442906 Loss 3.841877, Accuracy 92.336%\n",
      "Epoch 32, Batch 269, LR 0.442804 Loss 3.841293, Accuracy 92.330%\n",
      "Epoch 32, Batch 270, LR 0.442701 Loss 3.840253, Accuracy 92.338%\n",
      "Epoch 32, Batch 271, LR 0.442599 Loss 3.839046, Accuracy 92.346%\n",
      "Epoch 32, Batch 272, LR 0.442497 Loss 3.836553, Accuracy 92.360%\n",
      "Epoch 32, Batch 273, LR 0.442395 Loss 3.839170, Accuracy 92.359%\n",
      "Epoch 32, Batch 274, LR 0.442292 Loss 3.840744, Accuracy 92.347%\n",
      "Epoch 32, Batch 275, LR 0.442190 Loss 3.839634, Accuracy 92.341%\n",
      "Epoch 32, Batch 276, LR 0.442088 Loss 3.841557, Accuracy 92.332%\n",
      "Epoch 32, Batch 277, LR 0.441986 Loss 3.840655, Accuracy 92.340%\n",
      "Epoch 32, Batch 278, LR 0.441883 Loss 3.840236, Accuracy 92.339%\n",
      "Epoch 32, Batch 279, LR 0.441781 Loss 3.841713, Accuracy 92.336%\n",
      "Epoch 32, Batch 280, LR 0.441679 Loss 3.842648, Accuracy 92.338%\n",
      "Epoch 32, Batch 281, LR 0.441577 Loss 3.843191, Accuracy 92.338%\n",
      "Epoch 32, Batch 282, LR 0.441475 Loss 3.843164, Accuracy 92.329%\n",
      "Epoch 32, Batch 283, LR 0.441373 Loss 3.842774, Accuracy 92.331%\n",
      "Epoch 32, Batch 284, LR 0.441270 Loss 3.840876, Accuracy 92.342%\n",
      "Epoch 32, Batch 285, LR 0.441168 Loss 3.841424, Accuracy 92.336%\n",
      "Epoch 32, Batch 286, LR 0.441066 Loss 3.840494, Accuracy 92.327%\n",
      "Epoch 32, Batch 287, LR 0.440964 Loss 3.840259, Accuracy 92.326%\n",
      "Epoch 32, Batch 288, LR 0.440862 Loss 3.839954, Accuracy 92.331%\n",
      "Epoch 32, Batch 289, LR 0.440760 Loss 3.839512, Accuracy 92.336%\n",
      "Epoch 32, Batch 290, LR 0.440658 Loss 3.839874, Accuracy 92.328%\n",
      "Epoch 32, Batch 291, LR 0.440556 Loss 3.841859, Accuracy 92.314%\n",
      "Epoch 32, Batch 292, LR 0.440454 Loss 3.840731, Accuracy 92.319%\n",
      "Epoch 32, Batch 293, LR 0.440352 Loss 3.841634, Accuracy 92.318%\n",
      "Epoch 32, Batch 294, LR 0.440249 Loss 3.840336, Accuracy 92.326%\n",
      "Epoch 32, Batch 295, LR 0.440147 Loss 3.840459, Accuracy 92.315%\n",
      "Epoch 32, Batch 296, LR 0.440045 Loss 3.837727, Accuracy 92.325%\n",
      "Epoch 32, Batch 297, LR 0.439943 Loss 3.836512, Accuracy 92.327%\n",
      "Epoch 32, Batch 298, LR 0.439841 Loss 3.836131, Accuracy 92.326%\n",
      "Epoch 32, Batch 299, LR 0.439739 Loss 3.836270, Accuracy 92.323%\n",
      "Epoch 32, Batch 300, LR 0.439637 Loss 3.837069, Accuracy 92.326%\n",
      "Epoch 32, Batch 301, LR 0.439535 Loss 3.835278, Accuracy 92.333%\n",
      "Epoch 32, Batch 302, LR 0.439433 Loss 3.835753, Accuracy 92.335%\n",
      "Epoch 32, Batch 303, LR 0.439331 Loss 3.837812, Accuracy 92.316%\n",
      "Epoch 32, Batch 304, LR 0.439229 Loss 3.838401, Accuracy 92.313%\n",
      "Epoch 32, Batch 305, LR 0.439128 Loss 3.838759, Accuracy 92.313%\n",
      "Epoch 32, Batch 306, LR 0.439026 Loss 3.837660, Accuracy 92.325%\n",
      "Epoch 32, Batch 307, LR 0.438924 Loss 3.837552, Accuracy 92.338%\n",
      "Epoch 32, Batch 308, LR 0.438822 Loss 3.836406, Accuracy 92.345%\n",
      "Epoch 32, Batch 309, LR 0.438720 Loss 3.838518, Accuracy 92.334%\n",
      "Epoch 32, Batch 310, LR 0.438618 Loss 3.837807, Accuracy 92.334%\n",
      "Epoch 32, Batch 311, LR 0.438516 Loss 3.837237, Accuracy 92.341%\n",
      "Epoch 32, Batch 312, LR 0.438414 Loss 3.836941, Accuracy 92.348%\n",
      "Epoch 32, Batch 313, LR 0.438312 Loss 3.836940, Accuracy 92.347%\n",
      "Epoch 32, Batch 314, LR 0.438210 Loss 3.837004, Accuracy 92.347%\n",
      "Epoch 32, Batch 315, LR 0.438109 Loss 3.837146, Accuracy 92.341%\n",
      "Epoch 32, Batch 316, LR 0.438007 Loss 3.838989, Accuracy 92.331%\n",
      "Epoch 32, Batch 317, LR 0.437905 Loss 3.838139, Accuracy 92.328%\n",
      "Epoch 32, Batch 318, LR 0.437803 Loss 3.840532, Accuracy 92.328%\n",
      "Epoch 32, Batch 319, LR 0.437701 Loss 3.841209, Accuracy 92.325%\n",
      "Epoch 32, Batch 320, LR 0.437599 Loss 3.841157, Accuracy 92.327%\n",
      "Epoch 32, Batch 321, LR 0.437498 Loss 3.840646, Accuracy 92.326%\n",
      "Epoch 32, Batch 322, LR 0.437396 Loss 3.839049, Accuracy 92.328%\n",
      "Epoch 32, Batch 323, LR 0.437294 Loss 3.840711, Accuracy 92.323%\n",
      "Epoch 32, Batch 324, LR 0.437192 Loss 3.839880, Accuracy 92.308%\n",
      "Epoch 32, Batch 325, LR 0.437091 Loss 3.842600, Accuracy 92.298%\n",
      "Epoch 32, Batch 326, LR 0.436989 Loss 3.841483, Accuracy 92.298%\n",
      "Epoch 32, Batch 327, LR 0.436887 Loss 3.842047, Accuracy 92.293%\n",
      "Epoch 32, Batch 328, LR 0.436785 Loss 3.842612, Accuracy 92.285%\n",
      "Epoch 32, Batch 329, LR 0.436684 Loss 3.841720, Accuracy 92.294%\n",
      "Epoch 32, Batch 330, LR 0.436582 Loss 3.842362, Accuracy 92.287%\n",
      "Epoch 32, Batch 331, LR 0.436480 Loss 3.842228, Accuracy 92.287%\n",
      "Epoch 32, Batch 332, LR 0.436378 Loss 3.841600, Accuracy 92.291%\n",
      "Epoch 32, Batch 333, LR 0.436277 Loss 3.840335, Accuracy 92.302%\n",
      "Epoch 32, Batch 334, LR 0.436175 Loss 3.839294, Accuracy 92.309%\n",
      "Epoch 32, Batch 335, LR 0.436073 Loss 3.840704, Accuracy 92.295%\n",
      "Epoch 32, Batch 336, LR 0.435972 Loss 3.840743, Accuracy 92.294%\n",
      "Epoch 32, Batch 337, LR 0.435870 Loss 3.841271, Accuracy 92.292%\n",
      "Epoch 32, Batch 338, LR 0.435768 Loss 3.840409, Accuracy 92.289%\n",
      "Epoch 32, Batch 339, LR 0.435667 Loss 3.841284, Accuracy 92.280%\n",
      "Epoch 32, Batch 340, LR 0.435565 Loss 3.840462, Accuracy 92.277%\n",
      "Epoch 32, Batch 341, LR 0.435464 Loss 3.842109, Accuracy 92.265%\n",
      "Epoch 32, Batch 342, LR 0.435362 Loss 3.842814, Accuracy 92.274%\n",
      "Epoch 32, Batch 343, LR 0.435260 Loss 3.843699, Accuracy 92.272%\n",
      "Epoch 32, Batch 344, LR 0.435159 Loss 3.842573, Accuracy 92.278%\n",
      "Epoch 32, Batch 345, LR 0.435057 Loss 3.843535, Accuracy 92.274%\n",
      "Epoch 32, Batch 346, LR 0.434956 Loss 3.844004, Accuracy 92.271%\n",
      "Epoch 32, Batch 347, LR 0.434854 Loss 3.845611, Accuracy 92.271%\n",
      "Epoch 32, Batch 348, LR 0.434753 Loss 3.846037, Accuracy 92.275%\n",
      "Epoch 32, Batch 349, LR 0.434651 Loss 3.846665, Accuracy 92.273%\n",
      "Epoch 32, Batch 350, LR 0.434550 Loss 3.847266, Accuracy 92.266%\n",
      "Epoch 32, Batch 351, LR 0.434448 Loss 3.845775, Accuracy 92.272%\n",
      "Epoch 32, Batch 352, LR 0.434347 Loss 3.845680, Accuracy 92.270%\n",
      "Epoch 32, Batch 353, LR 0.434245 Loss 3.845805, Accuracy 92.265%\n",
      "Epoch 32, Batch 354, LR 0.434144 Loss 3.844123, Accuracy 92.269%\n",
      "Epoch 32, Batch 355, LR 0.434042 Loss 3.845585, Accuracy 92.262%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 356, LR 0.433941 Loss 3.845063, Accuracy 92.258%\n",
      "Epoch 32, Batch 357, LR 0.433839 Loss 3.844794, Accuracy 92.260%\n",
      "Epoch 32, Batch 358, LR 0.433738 Loss 3.845022, Accuracy 92.262%\n",
      "Epoch 32, Batch 359, LR 0.433636 Loss 3.843603, Accuracy 92.272%\n",
      "Epoch 32, Batch 360, LR 0.433535 Loss 3.844199, Accuracy 92.261%\n",
      "Epoch 32, Batch 361, LR 0.433433 Loss 3.844943, Accuracy 92.263%\n",
      "Epoch 32, Batch 362, LR 0.433332 Loss 3.844683, Accuracy 92.254%\n",
      "Epoch 32, Batch 363, LR 0.433231 Loss 3.845764, Accuracy 92.252%\n",
      "Epoch 32, Batch 364, LR 0.433129 Loss 3.848140, Accuracy 92.245%\n",
      "Epoch 32, Batch 365, LR 0.433028 Loss 3.849353, Accuracy 92.232%\n",
      "Epoch 32, Batch 366, LR 0.432926 Loss 3.848994, Accuracy 92.239%\n",
      "Epoch 32, Batch 367, LR 0.432825 Loss 3.847435, Accuracy 92.251%\n",
      "Epoch 32, Batch 368, LR 0.432724 Loss 3.844298, Accuracy 92.266%\n",
      "Epoch 32, Batch 369, LR 0.432622 Loss 3.845848, Accuracy 92.262%\n",
      "Epoch 32, Batch 370, LR 0.432521 Loss 3.845436, Accuracy 92.259%\n",
      "Epoch 32, Batch 371, LR 0.432420 Loss 3.847088, Accuracy 92.253%\n",
      "Epoch 32, Batch 372, LR 0.432318 Loss 3.847109, Accuracy 92.253%\n",
      "Epoch 32, Batch 373, LR 0.432217 Loss 3.845750, Accuracy 92.263%\n",
      "Epoch 32, Batch 374, LR 0.432116 Loss 3.845570, Accuracy 92.263%\n",
      "Epoch 32, Batch 375, LR 0.432014 Loss 3.845180, Accuracy 92.265%\n",
      "Epoch 32, Batch 376, LR 0.431913 Loss 3.846148, Accuracy 92.262%\n",
      "Epoch 32, Batch 377, LR 0.431812 Loss 3.846663, Accuracy 92.260%\n",
      "Epoch 32, Batch 378, LR 0.431711 Loss 3.845214, Accuracy 92.270%\n",
      "Epoch 32, Batch 379, LR 0.431609 Loss 3.842364, Accuracy 92.280%\n",
      "Epoch 32, Batch 380, LR 0.431508 Loss 3.843377, Accuracy 92.282%\n",
      "Epoch 32, Batch 381, LR 0.431407 Loss 3.842935, Accuracy 92.284%\n",
      "Epoch 32, Batch 382, LR 0.431306 Loss 3.844435, Accuracy 92.277%\n",
      "Epoch 32, Batch 383, LR 0.431204 Loss 3.844564, Accuracy 92.287%\n",
      "Epoch 32, Batch 384, LR 0.431103 Loss 3.844036, Accuracy 92.293%\n",
      "Epoch 32, Batch 385, LR 0.431002 Loss 3.843584, Accuracy 92.293%\n",
      "Epoch 32, Batch 386, LR 0.430901 Loss 3.843821, Accuracy 92.299%\n",
      "Epoch 32, Batch 387, LR 0.430800 Loss 3.843957, Accuracy 92.303%\n",
      "Epoch 32, Batch 388, LR 0.430698 Loss 3.842909, Accuracy 92.306%\n",
      "Epoch 32, Batch 389, LR 0.430597 Loss 3.843676, Accuracy 92.298%\n",
      "Epoch 32, Batch 390, LR 0.430496 Loss 3.844141, Accuracy 92.306%\n",
      "Epoch 32, Batch 391, LR 0.430395 Loss 3.844712, Accuracy 92.303%\n",
      "Epoch 32, Batch 392, LR 0.430294 Loss 3.845797, Accuracy 92.293%\n",
      "Epoch 32, Batch 393, LR 0.430193 Loss 3.844015, Accuracy 92.299%\n",
      "Epoch 32, Batch 394, LR 0.430092 Loss 3.844716, Accuracy 92.295%\n",
      "Epoch 32, Batch 395, LR 0.429990 Loss 3.844489, Accuracy 92.298%\n",
      "Epoch 32, Batch 396, LR 0.429889 Loss 3.844814, Accuracy 92.292%\n",
      "Epoch 32, Batch 397, LR 0.429788 Loss 3.843827, Accuracy 92.298%\n",
      "Epoch 32, Batch 398, LR 0.429687 Loss 3.844207, Accuracy 92.288%\n",
      "Epoch 32, Batch 399, LR 0.429586 Loss 3.843294, Accuracy 92.289%\n",
      "Epoch 32, Batch 400, LR 0.429485 Loss 3.842137, Accuracy 92.293%\n",
      "Epoch 32, Batch 401, LR 0.429384 Loss 3.841906, Accuracy 92.295%\n",
      "Epoch 32, Batch 402, LR 0.429283 Loss 3.842276, Accuracy 92.294%\n",
      "Epoch 32, Batch 403, LR 0.429182 Loss 3.840793, Accuracy 92.300%\n",
      "Epoch 32, Batch 404, LR 0.429081 Loss 3.840730, Accuracy 92.300%\n",
      "Epoch 32, Batch 405, LR 0.428980 Loss 3.842593, Accuracy 92.297%\n",
      "Epoch 32, Batch 406, LR 0.428879 Loss 3.842826, Accuracy 92.293%\n",
      "Epoch 32, Batch 407, LR 0.428778 Loss 3.842496, Accuracy 92.291%\n",
      "Epoch 32, Batch 408, LR 0.428677 Loss 3.844772, Accuracy 92.285%\n",
      "Epoch 32, Batch 409, LR 0.428576 Loss 3.843186, Accuracy 92.294%\n",
      "Epoch 32, Batch 410, LR 0.428475 Loss 3.844032, Accuracy 92.294%\n",
      "Epoch 32, Batch 411, LR 0.428374 Loss 3.844754, Accuracy 92.286%\n",
      "Epoch 32, Batch 412, LR 0.428273 Loss 3.844811, Accuracy 92.284%\n",
      "Epoch 32, Batch 413, LR 0.428172 Loss 3.844216, Accuracy 92.280%\n",
      "Epoch 32, Batch 414, LR 0.428071 Loss 3.844622, Accuracy 92.269%\n",
      "Epoch 32, Batch 415, LR 0.427970 Loss 3.843767, Accuracy 92.267%\n",
      "Epoch 32, Batch 416, LR 0.427869 Loss 3.843595, Accuracy 92.270%\n",
      "Epoch 32, Batch 417, LR 0.427769 Loss 3.844784, Accuracy 92.261%\n",
      "Epoch 32, Batch 418, LR 0.427668 Loss 3.843832, Accuracy 92.266%\n",
      "Epoch 32, Batch 419, LR 0.427567 Loss 3.842748, Accuracy 92.271%\n",
      "Epoch 32, Batch 420, LR 0.427466 Loss 3.842613, Accuracy 92.271%\n",
      "Epoch 32, Batch 421, LR 0.427365 Loss 3.841463, Accuracy 92.278%\n",
      "Epoch 32, Batch 422, LR 0.427264 Loss 3.842760, Accuracy 92.263%\n",
      "Epoch 32, Batch 423, LR 0.427163 Loss 3.842272, Accuracy 92.269%\n",
      "Epoch 32, Batch 424, LR 0.427063 Loss 3.842146, Accuracy 92.272%\n",
      "Epoch 32, Batch 425, LR 0.426962 Loss 3.841511, Accuracy 92.276%\n",
      "Epoch 32, Batch 426, LR 0.426861 Loss 3.840357, Accuracy 92.274%\n",
      "Epoch 32, Batch 427, LR 0.426760 Loss 3.840518, Accuracy 92.272%\n",
      "Epoch 32, Batch 428, LR 0.426659 Loss 3.839015, Accuracy 92.271%\n",
      "Epoch 32, Batch 429, LR 0.426559 Loss 3.838192, Accuracy 92.275%\n",
      "Epoch 32, Batch 430, LR 0.426458 Loss 3.838572, Accuracy 92.277%\n",
      "Epoch 32, Batch 431, LR 0.426357 Loss 3.838480, Accuracy 92.282%\n",
      "Epoch 32, Batch 432, LR 0.426256 Loss 3.839318, Accuracy 92.278%\n",
      "Epoch 32, Batch 433, LR 0.426155 Loss 3.840311, Accuracy 92.272%\n",
      "Epoch 32, Batch 434, LR 0.426055 Loss 3.842590, Accuracy 92.265%\n",
      "Epoch 32, Batch 435, LR 0.425954 Loss 3.842980, Accuracy 92.254%\n",
      "Epoch 32, Batch 436, LR 0.425853 Loss 3.843361, Accuracy 92.252%\n",
      "Epoch 32, Batch 437, LR 0.425753 Loss 3.843004, Accuracy 92.259%\n",
      "Epoch 32, Batch 438, LR 0.425652 Loss 3.841478, Accuracy 92.270%\n",
      "Epoch 32, Batch 439, LR 0.425551 Loss 3.840675, Accuracy 92.269%\n",
      "Epoch 32, Batch 440, LR 0.425451 Loss 3.842604, Accuracy 92.260%\n",
      "Epoch 32, Batch 441, LR 0.425350 Loss 3.842475, Accuracy 92.257%\n",
      "Epoch 32, Batch 442, LR 0.425249 Loss 3.842316, Accuracy 92.260%\n",
      "Epoch 32, Batch 443, LR 0.425149 Loss 3.844065, Accuracy 92.249%\n",
      "Epoch 32, Batch 444, LR 0.425048 Loss 3.843817, Accuracy 92.249%\n",
      "Epoch 32, Batch 445, LR 0.424947 Loss 3.842898, Accuracy 92.252%\n",
      "Epoch 32, Batch 446, LR 0.424847 Loss 3.842750, Accuracy 92.249%\n",
      "Epoch 32, Batch 447, LR 0.424746 Loss 3.843202, Accuracy 92.245%\n",
      "Epoch 32, Batch 448, LR 0.424645 Loss 3.841718, Accuracy 92.250%\n",
      "Epoch 32, Batch 449, LR 0.424545 Loss 3.840504, Accuracy 92.252%\n",
      "Epoch 32, Batch 450, LR 0.424444 Loss 3.840117, Accuracy 92.252%\n",
      "Epoch 32, Batch 451, LR 0.424344 Loss 3.840212, Accuracy 92.246%\n",
      "Epoch 32, Batch 452, LR 0.424243 Loss 3.840602, Accuracy 92.245%\n",
      "Epoch 32, Batch 453, LR 0.424143 Loss 3.839085, Accuracy 92.250%\n",
      "Epoch 32, Batch 454, LR 0.424042 Loss 3.838537, Accuracy 92.251%\n",
      "Epoch 32, Batch 455, LR 0.423941 Loss 3.836667, Accuracy 92.260%\n",
      "Epoch 32, Batch 456, LR 0.423841 Loss 3.838219, Accuracy 92.253%\n",
      "Epoch 32, Batch 457, LR 0.423740 Loss 3.838126, Accuracy 92.251%\n",
      "Epoch 32, Batch 458, LR 0.423640 Loss 3.839322, Accuracy 92.245%\n",
      "Epoch 32, Batch 459, LR 0.423539 Loss 3.839402, Accuracy 92.242%\n",
      "Epoch 32, Batch 460, LR 0.423439 Loss 3.839821, Accuracy 92.238%\n",
      "Epoch 32, Batch 461, LR 0.423338 Loss 3.840347, Accuracy 92.238%\n",
      "Epoch 32, Batch 462, LR 0.423238 Loss 3.839916, Accuracy 92.238%\n",
      "Epoch 32, Batch 463, LR 0.423137 Loss 3.839154, Accuracy 92.248%\n",
      "Epoch 32, Batch 464, LR 0.423037 Loss 3.839169, Accuracy 92.251%\n",
      "Epoch 32, Batch 465, LR 0.422937 Loss 3.838422, Accuracy 92.255%\n",
      "Epoch 32, Batch 466, LR 0.422836 Loss 3.837185, Accuracy 92.260%\n",
      "Epoch 32, Batch 467, LR 0.422736 Loss 3.837137, Accuracy 92.261%\n",
      "Epoch 32, Batch 468, LR 0.422635 Loss 3.837858, Accuracy 92.258%\n",
      "Epoch 32, Batch 469, LR 0.422535 Loss 3.836773, Accuracy 92.259%\n",
      "Epoch 32, Batch 470, LR 0.422434 Loss 3.837258, Accuracy 92.259%\n",
      "Epoch 32, Batch 471, LR 0.422334 Loss 3.836737, Accuracy 92.262%\n",
      "Epoch 32, Batch 472, LR 0.422234 Loss 3.837228, Accuracy 92.264%\n",
      "Epoch 32, Batch 473, LR 0.422133 Loss 3.838185, Accuracy 92.255%\n",
      "Epoch 32, Batch 474, LR 0.422033 Loss 3.837371, Accuracy 92.260%\n",
      "Epoch 32, Batch 475, LR 0.421933 Loss 3.837363, Accuracy 92.260%\n",
      "Epoch 32, Batch 476, LR 0.421832 Loss 3.839518, Accuracy 92.252%\n",
      "Epoch 32, Batch 477, LR 0.421732 Loss 3.838131, Accuracy 92.255%\n",
      "Epoch 32, Batch 478, LR 0.421632 Loss 3.838707, Accuracy 92.250%\n",
      "Epoch 32, Batch 479, LR 0.421531 Loss 3.839363, Accuracy 92.249%\n",
      "Epoch 32, Batch 480, LR 0.421431 Loss 3.839430, Accuracy 92.251%\n",
      "Epoch 32, Batch 481, LR 0.421331 Loss 3.840468, Accuracy 92.241%\n",
      "Epoch 32, Batch 482, LR 0.421230 Loss 3.838724, Accuracy 92.249%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 483, LR 0.421130 Loss 3.839302, Accuracy 92.247%\n",
      "Epoch 32, Batch 484, LR 0.421030 Loss 3.837542, Accuracy 92.257%\n",
      "Epoch 32, Batch 485, LR 0.420930 Loss 3.836952, Accuracy 92.258%\n",
      "Epoch 32, Batch 486, LR 0.420829 Loss 3.837388, Accuracy 92.261%\n",
      "Epoch 32, Batch 487, LR 0.420729 Loss 3.837256, Accuracy 92.263%\n",
      "Epoch 32, Batch 488, LR 0.420629 Loss 3.836484, Accuracy 92.268%\n",
      "Epoch 32, Batch 489, LR 0.420529 Loss 3.837520, Accuracy 92.267%\n",
      "Epoch 32, Batch 490, LR 0.420429 Loss 3.836403, Accuracy 92.267%\n",
      "Epoch 32, Batch 491, LR 0.420328 Loss 3.837111, Accuracy 92.265%\n",
      "Epoch 32, Batch 492, LR 0.420228 Loss 3.836862, Accuracy 92.267%\n",
      "Epoch 32, Batch 493, LR 0.420128 Loss 3.836400, Accuracy 92.271%\n",
      "Epoch 32, Batch 494, LR 0.420028 Loss 3.835146, Accuracy 92.278%\n",
      "Epoch 32, Batch 495, LR 0.419928 Loss 3.834907, Accuracy 92.271%\n",
      "Epoch 32, Batch 496, LR 0.419828 Loss 3.835565, Accuracy 92.265%\n",
      "Epoch 32, Batch 497, LR 0.419727 Loss 3.835729, Accuracy 92.265%\n",
      "Epoch 32, Batch 498, LR 0.419627 Loss 3.834707, Accuracy 92.269%\n",
      "Epoch 32, Batch 499, LR 0.419527 Loss 3.835660, Accuracy 92.264%\n",
      "Epoch 32, Batch 500, LR 0.419427 Loss 3.835392, Accuracy 92.269%\n",
      "Epoch 32, Batch 501, LR 0.419327 Loss 3.835211, Accuracy 92.275%\n",
      "Epoch 32, Batch 502, LR 0.419227 Loss 3.834684, Accuracy 92.267%\n",
      "Epoch 32, Batch 503, LR 0.419127 Loss 3.835719, Accuracy 92.259%\n",
      "Epoch 32, Batch 504, LR 0.419027 Loss 3.835165, Accuracy 92.262%\n",
      "Epoch 32, Batch 505, LR 0.418927 Loss 3.834832, Accuracy 92.266%\n",
      "Epoch 32, Batch 506, LR 0.418827 Loss 3.835683, Accuracy 92.260%\n",
      "Epoch 32, Batch 507, LR 0.418727 Loss 3.835841, Accuracy 92.257%\n",
      "Epoch 32, Batch 508, LR 0.418626 Loss 3.835377, Accuracy 92.261%\n",
      "Epoch 32, Batch 509, LR 0.418526 Loss 3.834720, Accuracy 92.266%\n",
      "Epoch 32, Batch 510, LR 0.418426 Loss 3.833900, Accuracy 92.272%\n",
      "Epoch 32, Batch 511, LR 0.418326 Loss 3.834047, Accuracy 92.270%\n",
      "Epoch 32, Batch 512, LR 0.418226 Loss 3.834910, Accuracy 92.262%\n",
      "Epoch 32, Batch 513, LR 0.418126 Loss 3.833826, Accuracy 92.274%\n",
      "Epoch 32, Batch 514, LR 0.418026 Loss 3.832700, Accuracy 92.273%\n",
      "Epoch 32, Batch 515, LR 0.417927 Loss 3.833008, Accuracy 92.265%\n",
      "Epoch 32, Batch 516, LR 0.417827 Loss 3.832787, Accuracy 92.266%\n",
      "Epoch 32, Batch 517, LR 0.417727 Loss 3.833168, Accuracy 92.265%\n",
      "Epoch 32, Batch 518, LR 0.417627 Loss 3.832911, Accuracy 92.269%\n",
      "Epoch 32, Batch 519, LR 0.417527 Loss 3.833607, Accuracy 92.270%\n",
      "Epoch 32, Batch 520, LR 0.417427 Loss 3.833128, Accuracy 92.267%\n",
      "Epoch 32, Batch 521, LR 0.417327 Loss 3.833955, Accuracy 92.262%\n",
      "Epoch 32, Batch 522, LR 0.417227 Loss 3.833748, Accuracy 92.262%\n",
      "Epoch 32, Batch 523, LR 0.417127 Loss 3.833534, Accuracy 92.265%\n",
      "Epoch 32, Batch 524, LR 0.417027 Loss 3.832378, Accuracy 92.267%\n",
      "Epoch 32, Batch 525, LR 0.416927 Loss 3.832187, Accuracy 92.268%\n",
      "Epoch 32, Batch 526, LR 0.416828 Loss 3.832100, Accuracy 92.268%\n",
      "Epoch 32, Batch 527, LR 0.416728 Loss 3.830683, Accuracy 92.269%\n",
      "Epoch 32, Batch 528, LR 0.416628 Loss 3.831032, Accuracy 92.269%\n",
      "Epoch 32, Batch 529, LR 0.416528 Loss 3.831353, Accuracy 92.266%\n",
      "Epoch 32, Batch 530, LR 0.416428 Loss 3.831372, Accuracy 92.273%\n",
      "Epoch 32, Batch 531, LR 0.416328 Loss 3.831919, Accuracy 92.270%\n",
      "Epoch 32, Batch 532, LR 0.416229 Loss 3.831663, Accuracy 92.271%\n",
      "Epoch 32, Batch 533, LR 0.416129 Loss 3.830756, Accuracy 92.273%\n",
      "Epoch 32, Batch 534, LR 0.416029 Loss 3.831514, Accuracy 92.272%\n",
      "Epoch 32, Batch 535, LR 0.415929 Loss 3.832433, Accuracy 92.274%\n",
      "Epoch 32, Batch 536, LR 0.415829 Loss 3.832406, Accuracy 92.271%\n",
      "Epoch 32, Batch 537, LR 0.415730 Loss 3.832796, Accuracy 92.272%\n",
      "Epoch 32, Batch 538, LR 0.415630 Loss 3.831879, Accuracy 92.275%\n",
      "Epoch 32, Batch 539, LR 0.415530 Loss 3.832401, Accuracy 92.277%\n",
      "Epoch 32, Batch 540, LR 0.415430 Loss 3.831796, Accuracy 92.280%\n",
      "Epoch 32, Batch 541, LR 0.415331 Loss 3.829711, Accuracy 92.284%\n",
      "Epoch 32, Batch 542, LR 0.415231 Loss 3.829892, Accuracy 92.286%\n",
      "Epoch 32, Batch 543, LR 0.415131 Loss 3.829965, Accuracy 92.287%\n",
      "Epoch 32, Batch 544, LR 0.415032 Loss 3.829040, Accuracy 92.289%\n",
      "Epoch 32, Batch 545, LR 0.414932 Loss 3.828575, Accuracy 92.294%\n",
      "Epoch 32, Batch 546, LR 0.414832 Loss 3.829488, Accuracy 92.282%\n",
      "Epoch 32, Batch 547, LR 0.414733 Loss 3.828879, Accuracy 92.282%\n",
      "Epoch 32, Batch 548, LR 0.414633 Loss 3.827997, Accuracy 92.284%\n",
      "Epoch 32, Batch 549, LR 0.414533 Loss 3.828930, Accuracy 92.277%\n",
      "Epoch 32, Batch 550, LR 0.414434 Loss 3.826511, Accuracy 92.288%\n",
      "Epoch 32, Batch 551, LR 0.414334 Loss 3.825228, Accuracy 92.295%\n",
      "Epoch 32, Batch 552, LR 0.414234 Loss 3.825779, Accuracy 92.288%\n",
      "Epoch 32, Batch 553, LR 0.414135 Loss 3.825286, Accuracy 92.291%\n",
      "Epoch 32, Batch 554, LR 0.414035 Loss 3.826147, Accuracy 92.286%\n",
      "Epoch 32, Batch 555, LR 0.413936 Loss 3.825353, Accuracy 92.290%\n",
      "Epoch 32, Batch 556, LR 0.413836 Loss 3.823782, Accuracy 92.299%\n",
      "Epoch 32, Batch 557, LR 0.413737 Loss 3.824149, Accuracy 92.300%\n",
      "Epoch 32, Batch 558, LR 0.413637 Loss 3.824426, Accuracy 92.295%\n",
      "Epoch 32, Batch 559, LR 0.413537 Loss 3.824845, Accuracy 92.294%\n",
      "Epoch 32, Batch 560, LR 0.413438 Loss 3.824131, Accuracy 92.295%\n",
      "Epoch 32, Batch 561, LR 0.413338 Loss 3.823389, Accuracy 92.303%\n",
      "Epoch 32, Batch 562, LR 0.413239 Loss 3.822650, Accuracy 92.303%\n",
      "Epoch 32, Batch 563, LR 0.413139 Loss 3.822453, Accuracy 92.307%\n",
      "Epoch 32, Batch 564, LR 0.413040 Loss 3.821857, Accuracy 92.308%\n",
      "Epoch 32, Batch 565, LR 0.412940 Loss 3.821585, Accuracy 92.309%\n",
      "Epoch 32, Batch 566, LR 0.412841 Loss 3.821229, Accuracy 92.310%\n",
      "Epoch 32, Batch 567, LR 0.412741 Loss 3.821046, Accuracy 92.313%\n",
      "Epoch 32, Batch 568, LR 0.412642 Loss 3.821415, Accuracy 92.310%\n",
      "Epoch 32, Batch 569, LR 0.412543 Loss 3.822043, Accuracy 92.310%\n",
      "Epoch 32, Batch 570, LR 0.412443 Loss 3.821317, Accuracy 92.311%\n",
      "Epoch 32, Batch 571, LR 0.412344 Loss 3.819943, Accuracy 92.315%\n",
      "Epoch 32, Batch 572, LR 0.412244 Loss 3.821627, Accuracy 92.305%\n",
      "Epoch 32, Batch 573, LR 0.412145 Loss 3.821055, Accuracy 92.302%\n",
      "Epoch 32, Batch 574, LR 0.412045 Loss 3.820774, Accuracy 92.307%\n",
      "Epoch 32, Batch 575, LR 0.411946 Loss 3.820195, Accuracy 92.314%\n",
      "Epoch 32, Batch 576, LR 0.411847 Loss 3.819076, Accuracy 92.322%\n",
      "Epoch 32, Batch 577, LR 0.411747 Loss 3.819556, Accuracy 92.320%\n",
      "Epoch 32, Batch 578, LR 0.411648 Loss 3.820278, Accuracy 92.319%\n",
      "Epoch 32, Batch 579, LR 0.411549 Loss 3.820172, Accuracy 92.325%\n",
      "Epoch 32, Batch 580, LR 0.411449 Loss 3.820729, Accuracy 92.320%\n",
      "Epoch 32, Batch 581, LR 0.411350 Loss 3.821381, Accuracy 92.319%\n",
      "Epoch 32, Batch 582, LR 0.411251 Loss 3.820710, Accuracy 92.324%\n",
      "Epoch 32, Batch 583, LR 0.411151 Loss 3.820781, Accuracy 92.326%\n",
      "Epoch 32, Batch 584, LR 0.411052 Loss 3.821737, Accuracy 92.313%\n",
      "Epoch 32, Batch 585, LR 0.410953 Loss 3.821681, Accuracy 92.309%\n",
      "Epoch 32, Batch 586, LR 0.410853 Loss 3.821677, Accuracy 92.311%\n",
      "Epoch 32, Batch 587, LR 0.410754 Loss 3.822534, Accuracy 92.309%\n",
      "Epoch 32, Batch 588, LR 0.410655 Loss 3.822518, Accuracy 92.308%\n",
      "Epoch 32, Batch 589, LR 0.410556 Loss 3.822501, Accuracy 92.308%\n",
      "Epoch 32, Batch 590, LR 0.410456 Loss 3.822722, Accuracy 92.311%\n",
      "Epoch 32, Batch 591, LR 0.410357 Loss 3.822608, Accuracy 92.306%\n",
      "Epoch 32, Batch 592, LR 0.410258 Loss 3.822437, Accuracy 92.309%\n",
      "Epoch 32, Batch 593, LR 0.410159 Loss 3.821694, Accuracy 92.311%\n",
      "Epoch 32, Batch 594, LR 0.410059 Loss 3.822229, Accuracy 92.309%\n",
      "Epoch 32, Batch 595, LR 0.409960 Loss 3.822425, Accuracy 92.308%\n",
      "Epoch 32, Batch 596, LR 0.409861 Loss 3.823547, Accuracy 92.307%\n",
      "Epoch 32, Batch 597, LR 0.409762 Loss 3.823488, Accuracy 92.309%\n",
      "Epoch 32, Batch 598, LR 0.409663 Loss 3.822425, Accuracy 92.313%\n",
      "Epoch 32, Batch 599, LR 0.409564 Loss 3.822765, Accuracy 92.304%\n",
      "Epoch 32, Batch 600, LR 0.409464 Loss 3.822955, Accuracy 92.298%\n",
      "Epoch 32, Batch 601, LR 0.409365 Loss 3.823430, Accuracy 92.295%\n",
      "Epoch 32, Batch 602, LR 0.409266 Loss 3.823152, Accuracy 92.302%\n",
      "Epoch 32, Batch 603, LR 0.409167 Loss 3.823630, Accuracy 92.302%\n",
      "Epoch 32, Batch 604, LR 0.409068 Loss 3.823754, Accuracy 92.300%\n",
      "Epoch 32, Batch 605, LR 0.408969 Loss 3.823726, Accuracy 92.299%\n",
      "Epoch 32, Batch 606, LR 0.408870 Loss 3.823515, Accuracy 92.301%\n",
      "Epoch 32, Batch 607, LR 0.408771 Loss 3.823356, Accuracy 92.305%\n",
      "Epoch 32, Batch 608, LR 0.408672 Loss 3.824088, Accuracy 92.302%\n",
      "Epoch 32, Batch 609, LR 0.408572 Loss 3.824269, Accuracy 92.302%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 610, LR 0.408473 Loss 3.823959, Accuracy 92.303%\n",
      "Epoch 32, Batch 611, LR 0.408374 Loss 3.824454, Accuracy 92.300%\n",
      "Epoch 32, Batch 612, LR 0.408275 Loss 3.824529, Accuracy 92.299%\n",
      "Epoch 32, Batch 613, LR 0.408176 Loss 3.824934, Accuracy 92.296%\n",
      "Epoch 32, Batch 614, LR 0.408077 Loss 3.823523, Accuracy 92.296%\n",
      "Epoch 32, Batch 615, LR 0.407978 Loss 3.822979, Accuracy 92.301%\n",
      "Epoch 32, Batch 616, LR 0.407879 Loss 3.823101, Accuracy 92.295%\n",
      "Epoch 32, Batch 617, LR 0.407780 Loss 3.822803, Accuracy 92.298%\n",
      "Epoch 32, Batch 618, LR 0.407681 Loss 3.823123, Accuracy 92.304%\n",
      "Epoch 32, Batch 619, LR 0.407582 Loss 3.823471, Accuracy 92.304%\n",
      "Epoch 32, Batch 620, LR 0.407483 Loss 3.823511, Accuracy 92.306%\n",
      "Epoch 32, Batch 621, LR 0.407384 Loss 3.823718, Accuracy 92.308%\n",
      "Epoch 32, Batch 622, LR 0.407285 Loss 3.823683, Accuracy 92.304%\n",
      "Epoch 32, Batch 623, LR 0.407187 Loss 3.823624, Accuracy 92.302%\n",
      "Epoch 32, Batch 624, LR 0.407088 Loss 3.824241, Accuracy 92.299%\n",
      "Epoch 32, Batch 625, LR 0.406989 Loss 3.824292, Accuracy 92.299%\n",
      "Epoch 32, Batch 626, LR 0.406890 Loss 3.825095, Accuracy 92.295%\n",
      "Epoch 32, Batch 627, LR 0.406791 Loss 3.825129, Accuracy 92.291%\n",
      "Epoch 32, Batch 628, LR 0.406692 Loss 3.824268, Accuracy 92.291%\n",
      "Epoch 32, Batch 629, LR 0.406593 Loss 3.824495, Accuracy 92.294%\n",
      "Epoch 32, Batch 630, LR 0.406494 Loss 3.824598, Accuracy 92.295%\n",
      "Epoch 32, Batch 631, LR 0.406395 Loss 3.824412, Accuracy 92.299%\n",
      "Epoch 32, Batch 632, LR 0.406297 Loss 3.825037, Accuracy 92.296%\n",
      "Epoch 32, Batch 633, LR 0.406198 Loss 3.824410, Accuracy 92.301%\n",
      "Epoch 32, Batch 634, LR 0.406099 Loss 3.823858, Accuracy 92.308%\n",
      "Epoch 32, Batch 635, LR 0.406000 Loss 3.824531, Accuracy 92.309%\n",
      "Epoch 32, Batch 636, LR 0.405901 Loss 3.823719, Accuracy 92.308%\n",
      "Epoch 32, Batch 637, LR 0.405803 Loss 3.823825, Accuracy 92.304%\n",
      "Epoch 32, Batch 638, LR 0.405704 Loss 3.823714, Accuracy 92.309%\n",
      "Epoch 32, Batch 639, LR 0.405605 Loss 3.824384, Accuracy 92.306%\n",
      "Epoch 32, Batch 640, LR 0.405506 Loss 3.824830, Accuracy 92.303%\n",
      "Epoch 32, Batch 641, LR 0.405407 Loss 3.823636, Accuracy 92.307%\n",
      "Epoch 32, Batch 642, LR 0.405309 Loss 3.823493, Accuracy 92.310%\n",
      "Epoch 32, Batch 643, LR 0.405210 Loss 3.824142, Accuracy 92.309%\n",
      "Epoch 32, Batch 644, LR 0.405111 Loss 3.823668, Accuracy 92.314%\n",
      "Epoch 32, Batch 645, LR 0.405013 Loss 3.824671, Accuracy 92.307%\n",
      "Epoch 32, Batch 646, LR 0.404914 Loss 3.825046, Accuracy 92.302%\n",
      "Epoch 32, Batch 647, LR 0.404815 Loss 3.824438, Accuracy 92.305%\n",
      "Epoch 32, Batch 648, LR 0.404716 Loss 3.824386, Accuracy 92.306%\n",
      "Epoch 32, Batch 649, LR 0.404618 Loss 3.824963, Accuracy 92.302%\n",
      "Epoch 32, Batch 650, LR 0.404519 Loss 3.826141, Accuracy 92.293%\n",
      "Epoch 32, Batch 651, LR 0.404420 Loss 3.825898, Accuracy 92.294%\n",
      "Epoch 32, Batch 652, LR 0.404322 Loss 3.826906, Accuracy 92.285%\n",
      "Epoch 32, Batch 653, LR 0.404223 Loss 3.826653, Accuracy 92.286%\n",
      "Epoch 32, Batch 654, LR 0.404125 Loss 3.826797, Accuracy 92.287%\n",
      "Epoch 32, Batch 655, LR 0.404026 Loss 3.827008, Accuracy 92.286%\n",
      "Epoch 32, Batch 656, LR 0.403927 Loss 3.826258, Accuracy 92.289%\n",
      "Epoch 32, Batch 657, LR 0.403829 Loss 3.826226, Accuracy 92.289%\n",
      "Epoch 32, Batch 658, LR 0.403730 Loss 3.825895, Accuracy 92.288%\n",
      "Epoch 32, Batch 659, LR 0.403632 Loss 3.825733, Accuracy 92.289%\n",
      "Epoch 32, Batch 660, LR 0.403533 Loss 3.825545, Accuracy 92.289%\n",
      "Epoch 32, Batch 661, LR 0.403434 Loss 3.826611, Accuracy 92.288%\n",
      "Epoch 32, Batch 662, LR 0.403336 Loss 3.826921, Accuracy 92.284%\n",
      "Epoch 32, Batch 663, LR 0.403237 Loss 3.827029, Accuracy 92.284%\n",
      "Epoch 32, Batch 664, LR 0.403139 Loss 3.827061, Accuracy 92.285%\n",
      "Epoch 32, Batch 665, LR 0.403040 Loss 3.827349, Accuracy 92.283%\n",
      "Epoch 32, Batch 666, LR 0.402942 Loss 3.827107, Accuracy 92.280%\n",
      "Epoch 32, Batch 667, LR 0.402843 Loss 3.828200, Accuracy 92.278%\n",
      "Epoch 32, Batch 668, LR 0.402745 Loss 3.828707, Accuracy 92.279%\n",
      "Epoch 32, Batch 669, LR 0.402646 Loss 3.827920, Accuracy 92.286%\n",
      "Epoch 32, Batch 670, LR 0.402548 Loss 3.826991, Accuracy 92.290%\n",
      "Epoch 32, Batch 671, LR 0.402449 Loss 3.827288, Accuracy 92.292%\n",
      "Epoch 32, Batch 672, LR 0.402351 Loss 3.826910, Accuracy 92.291%\n",
      "Epoch 32, Batch 673, LR 0.402252 Loss 3.827062, Accuracy 92.292%\n",
      "Epoch 32, Batch 674, LR 0.402154 Loss 3.826830, Accuracy 92.293%\n",
      "Epoch 32, Batch 675, LR 0.402056 Loss 3.827655, Accuracy 92.288%\n",
      "Epoch 32, Batch 676, LR 0.401957 Loss 3.828099, Accuracy 92.283%\n",
      "Epoch 32, Batch 677, LR 0.401859 Loss 3.828345, Accuracy 92.282%\n",
      "Epoch 32, Batch 678, LR 0.401760 Loss 3.828060, Accuracy 92.280%\n",
      "Epoch 32, Batch 679, LR 0.401662 Loss 3.827466, Accuracy 92.278%\n",
      "Epoch 32, Batch 680, LR 0.401564 Loss 3.827064, Accuracy 92.277%\n",
      "Epoch 32, Batch 681, LR 0.401465 Loss 3.828075, Accuracy 92.271%\n",
      "Epoch 32, Batch 682, LR 0.401367 Loss 3.829055, Accuracy 92.264%\n",
      "Epoch 32, Batch 683, LR 0.401269 Loss 3.828353, Accuracy 92.272%\n",
      "Epoch 32, Batch 684, LR 0.401170 Loss 3.828933, Accuracy 92.267%\n",
      "Epoch 32, Batch 685, LR 0.401072 Loss 3.830850, Accuracy 92.258%\n",
      "Epoch 32, Batch 686, LR 0.400974 Loss 3.829617, Accuracy 92.262%\n",
      "Epoch 32, Batch 687, LR 0.400875 Loss 3.829375, Accuracy 92.266%\n",
      "Epoch 32, Batch 688, LR 0.400777 Loss 3.828496, Accuracy 92.270%\n",
      "Epoch 32, Batch 689, LR 0.400679 Loss 3.829095, Accuracy 92.270%\n",
      "Epoch 32, Batch 690, LR 0.400580 Loss 3.829272, Accuracy 92.274%\n",
      "Epoch 32, Batch 691, LR 0.400482 Loss 3.830007, Accuracy 92.272%\n",
      "Epoch 32, Batch 692, LR 0.400384 Loss 3.830413, Accuracy 92.272%\n",
      "Epoch 32, Batch 693, LR 0.400286 Loss 3.829248, Accuracy 92.274%\n",
      "Epoch 32, Batch 694, LR 0.400187 Loss 3.829419, Accuracy 92.278%\n",
      "Epoch 32, Batch 695, LR 0.400089 Loss 3.829999, Accuracy 92.271%\n",
      "Epoch 32, Batch 696, LR 0.399991 Loss 3.829459, Accuracy 92.272%\n",
      "Epoch 32, Batch 697, LR 0.399893 Loss 3.829098, Accuracy 92.275%\n",
      "Epoch 32, Batch 698, LR 0.399795 Loss 3.829929, Accuracy 92.271%\n",
      "Epoch 32, Batch 699, LR 0.399696 Loss 3.830622, Accuracy 92.266%\n",
      "Epoch 32, Batch 700, LR 0.399598 Loss 3.830595, Accuracy 92.266%\n",
      "Epoch 32, Batch 701, LR 0.399500 Loss 3.830370, Accuracy 92.264%\n",
      "Epoch 32, Batch 702, LR 0.399402 Loss 3.830365, Accuracy 92.267%\n",
      "Epoch 32, Batch 703, LR 0.399304 Loss 3.830685, Accuracy 92.264%\n",
      "Epoch 32, Batch 704, LR 0.399206 Loss 3.831383, Accuracy 92.261%\n",
      "Epoch 32, Batch 705, LR 0.399107 Loss 3.831737, Accuracy 92.260%\n",
      "Epoch 32, Batch 706, LR 0.399009 Loss 3.831723, Accuracy 92.256%\n",
      "Epoch 32, Batch 707, LR 0.398911 Loss 3.832027, Accuracy 92.257%\n",
      "Epoch 32, Batch 708, LR 0.398813 Loss 3.831890, Accuracy 92.255%\n",
      "Epoch 32, Batch 709, LR 0.398715 Loss 3.831322, Accuracy 92.260%\n",
      "Epoch 32, Batch 710, LR 0.398617 Loss 3.831648, Accuracy 92.261%\n",
      "Epoch 32, Batch 711, LR 0.398519 Loss 3.831655, Accuracy 92.262%\n",
      "Epoch 32, Batch 712, LR 0.398421 Loss 3.831982, Accuracy 92.260%\n",
      "Epoch 32, Batch 713, LR 0.398323 Loss 3.831438, Accuracy 92.265%\n",
      "Epoch 32, Batch 714, LR 0.398225 Loss 3.831998, Accuracy 92.265%\n",
      "Epoch 32, Batch 715, LR 0.398127 Loss 3.831114, Accuracy 92.269%\n",
      "Epoch 32, Batch 716, LR 0.398029 Loss 3.830631, Accuracy 92.267%\n",
      "Epoch 32, Batch 717, LR 0.397931 Loss 3.830499, Accuracy 92.269%\n",
      "Epoch 32, Batch 718, LR 0.397833 Loss 3.830263, Accuracy 92.267%\n",
      "Epoch 32, Batch 719, LR 0.397735 Loss 3.829512, Accuracy 92.271%\n",
      "Epoch 32, Batch 720, LR 0.397637 Loss 3.829790, Accuracy 92.271%\n",
      "Epoch 32, Batch 721, LR 0.397539 Loss 3.829724, Accuracy 92.270%\n",
      "Epoch 32, Batch 722, LR 0.397441 Loss 3.829536, Accuracy 92.272%\n",
      "Epoch 32, Batch 723, LR 0.397343 Loss 3.828858, Accuracy 92.276%\n",
      "Epoch 32, Batch 724, LR 0.397245 Loss 3.829543, Accuracy 92.273%\n",
      "Epoch 32, Batch 725, LR 0.397147 Loss 3.829630, Accuracy 92.275%\n",
      "Epoch 32, Batch 726, LR 0.397049 Loss 3.829336, Accuracy 92.277%\n",
      "Epoch 32, Batch 727, LR 0.396951 Loss 3.829786, Accuracy 92.276%\n",
      "Epoch 32, Batch 728, LR 0.396853 Loss 3.830249, Accuracy 92.277%\n",
      "Epoch 32, Batch 729, LR 0.396755 Loss 3.830422, Accuracy 92.278%\n",
      "Epoch 32, Batch 730, LR 0.396657 Loss 3.830385, Accuracy 92.280%\n",
      "Epoch 32, Batch 731, LR 0.396559 Loss 3.830942, Accuracy 92.279%\n",
      "Epoch 32, Batch 732, LR 0.396462 Loss 3.830566, Accuracy 92.279%\n",
      "Epoch 32, Batch 733, LR 0.396364 Loss 3.830445, Accuracy 92.283%\n",
      "Epoch 32, Batch 734, LR 0.396266 Loss 3.830791, Accuracy 92.284%\n",
      "Epoch 32, Batch 735, LR 0.396168 Loss 3.831799, Accuracy 92.281%\n",
      "Epoch 32, Batch 736, LR 0.396070 Loss 3.832138, Accuracy 92.280%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 737, LR 0.395972 Loss 3.832187, Accuracy 92.280%\n",
      "Epoch 32, Batch 738, LR 0.395875 Loss 3.832574, Accuracy 92.277%\n",
      "Epoch 32, Batch 739, LR 0.395777 Loss 3.833298, Accuracy 92.274%\n",
      "Epoch 32, Batch 740, LR 0.395679 Loss 3.832408, Accuracy 92.277%\n",
      "Epoch 32, Batch 741, LR 0.395581 Loss 3.833147, Accuracy 92.271%\n",
      "Epoch 32, Batch 742, LR 0.395483 Loss 3.833008, Accuracy 92.275%\n",
      "Epoch 32, Batch 743, LR 0.395386 Loss 3.832674, Accuracy 92.275%\n",
      "Epoch 32, Batch 744, LR 0.395288 Loss 3.832583, Accuracy 92.274%\n",
      "Epoch 32, Batch 745, LR 0.395190 Loss 3.833047, Accuracy 92.272%\n",
      "Epoch 32, Batch 746, LR 0.395093 Loss 3.833471, Accuracy 92.269%\n",
      "Epoch 32, Batch 747, LR 0.394995 Loss 3.833474, Accuracy 92.271%\n",
      "Epoch 32, Batch 748, LR 0.394897 Loss 3.833579, Accuracy 92.267%\n",
      "Epoch 32, Batch 749, LR 0.394799 Loss 3.832997, Accuracy 92.267%\n",
      "Epoch 32, Batch 750, LR 0.394702 Loss 3.833110, Accuracy 92.269%\n",
      "Epoch 32, Batch 751, LR 0.394604 Loss 3.833286, Accuracy 92.268%\n",
      "Epoch 32, Batch 752, LR 0.394506 Loss 3.832821, Accuracy 92.270%\n",
      "Epoch 32, Batch 753, LR 0.394409 Loss 3.833108, Accuracy 92.266%\n",
      "Epoch 32, Batch 754, LR 0.394311 Loss 3.833403, Accuracy 92.264%\n",
      "Epoch 32, Batch 755, LR 0.394213 Loss 3.832642, Accuracy 92.270%\n",
      "Epoch 32, Batch 756, LR 0.394116 Loss 3.831511, Accuracy 92.275%\n",
      "Epoch 32, Batch 757, LR 0.394018 Loss 3.831409, Accuracy 92.277%\n",
      "Epoch 32, Batch 758, LR 0.393921 Loss 3.831376, Accuracy 92.274%\n",
      "Epoch 32, Batch 759, LR 0.393823 Loss 3.831868, Accuracy 92.270%\n",
      "Epoch 32, Batch 760, LR 0.393725 Loss 3.830883, Accuracy 92.276%\n",
      "Epoch 32, Batch 761, LR 0.393628 Loss 3.831025, Accuracy 92.273%\n",
      "Epoch 32, Batch 762, LR 0.393530 Loss 3.830520, Accuracy 92.275%\n",
      "Epoch 32, Batch 763, LR 0.393433 Loss 3.831353, Accuracy 92.270%\n",
      "Epoch 32, Batch 764, LR 0.393335 Loss 3.832421, Accuracy 92.267%\n",
      "Epoch 32, Batch 765, LR 0.393238 Loss 3.832978, Accuracy 92.264%\n",
      "Epoch 32, Batch 766, LR 0.393140 Loss 3.833196, Accuracy 92.259%\n",
      "Epoch 32, Batch 767, LR 0.393042 Loss 3.832642, Accuracy 92.265%\n",
      "Epoch 32, Batch 768, LR 0.392945 Loss 3.832598, Accuracy 92.267%\n",
      "Epoch 32, Batch 769, LR 0.392847 Loss 3.832757, Accuracy 92.267%\n",
      "Epoch 32, Batch 770, LR 0.392750 Loss 3.832981, Accuracy 92.263%\n",
      "Epoch 32, Batch 771, LR 0.392652 Loss 3.833513, Accuracy 92.257%\n",
      "Epoch 32, Batch 772, LR 0.392555 Loss 3.833722, Accuracy 92.258%\n",
      "Epoch 32, Batch 773, LR 0.392458 Loss 3.833574, Accuracy 92.260%\n",
      "Epoch 32, Batch 774, LR 0.392360 Loss 3.834000, Accuracy 92.258%\n",
      "Epoch 32, Batch 775, LR 0.392263 Loss 3.833999, Accuracy 92.258%\n",
      "Epoch 32, Batch 776, LR 0.392165 Loss 3.834078, Accuracy 92.257%\n",
      "Epoch 32, Batch 777, LR 0.392068 Loss 3.833639, Accuracy 92.256%\n",
      "Epoch 32, Batch 778, LR 0.391970 Loss 3.833471, Accuracy 92.257%\n",
      "Epoch 32, Batch 779, LR 0.391873 Loss 3.832921, Accuracy 92.262%\n",
      "Epoch 32, Batch 780, LR 0.391776 Loss 3.832818, Accuracy 92.263%\n",
      "Epoch 32, Batch 781, LR 0.391678 Loss 3.833041, Accuracy 92.262%\n",
      "Epoch 32, Batch 782, LR 0.391581 Loss 3.833101, Accuracy 92.262%\n",
      "Epoch 32, Batch 783, LR 0.391483 Loss 3.832903, Accuracy 92.264%\n",
      "Epoch 32, Batch 784, LR 0.391386 Loss 3.832427, Accuracy 92.266%\n",
      "Epoch 32, Batch 785, LR 0.391289 Loss 3.831747, Accuracy 92.266%\n",
      "Epoch 32, Batch 786, LR 0.391191 Loss 3.832023, Accuracy 92.265%\n",
      "Epoch 32, Batch 787, LR 0.391094 Loss 3.832359, Accuracy 92.265%\n",
      "Epoch 32, Batch 788, LR 0.390997 Loss 3.832213, Accuracy 92.266%\n",
      "Epoch 32, Batch 789, LR 0.390899 Loss 3.832936, Accuracy 92.265%\n",
      "Epoch 32, Batch 790, LR 0.390802 Loss 3.832766, Accuracy 92.266%\n",
      "Epoch 32, Batch 791, LR 0.390705 Loss 3.833600, Accuracy 92.259%\n",
      "Epoch 32, Batch 792, LR 0.390608 Loss 3.833575, Accuracy 92.260%\n",
      "Epoch 32, Batch 793, LR 0.390510 Loss 3.833258, Accuracy 92.261%\n",
      "Epoch 32, Batch 794, LR 0.390413 Loss 3.833243, Accuracy 92.261%\n",
      "Epoch 32, Batch 795, LR 0.390316 Loss 3.833451, Accuracy 92.259%\n",
      "Epoch 32, Batch 796, LR 0.390219 Loss 3.832538, Accuracy 92.261%\n",
      "Epoch 32, Batch 797, LR 0.390121 Loss 3.832297, Accuracy 92.266%\n",
      "Epoch 32, Batch 798, LR 0.390024 Loss 3.832330, Accuracy 92.269%\n",
      "Epoch 32, Batch 799, LR 0.389927 Loss 3.831637, Accuracy 92.273%\n",
      "Epoch 32, Batch 800, LR 0.389830 Loss 3.831907, Accuracy 92.273%\n",
      "Epoch 32, Batch 801, LR 0.389733 Loss 3.832215, Accuracy 92.271%\n",
      "Epoch 32, Batch 802, LR 0.389635 Loss 3.832082, Accuracy 92.274%\n",
      "Epoch 32, Batch 803, LR 0.389538 Loss 3.831751, Accuracy 92.273%\n",
      "Epoch 32, Batch 804, LR 0.389441 Loss 3.830879, Accuracy 92.277%\n",
      "Epoch 32, Batch 805, LR 0.389344 Loss 3.831807, Accuracy 92.275%\n",
      "Epoch 32, Batch 806, LR 0.389247 Loss 3.832036, Accuracy 92.278%\n",
      "Epoch 32, Batch 807, LR 0.389150 Loss 3.831670, Accuracy 92.280%\n",
      "Epoch 32, Batch 808, LR 0.389052 Loss 3.832555, Accuracy 92.278%\n",
      "Epoch 32, Batch 809, LR 0.388955 Loss 3.832437, Accuracy 92.282%\n",
      "Epoch 32, Batch 810, LR 0.388858 Loss 3.832657, Accuracy 92.280%\n",
      "Epoch 32, Batch 811, LR 0.388761 Loss 3.832966, Accuracy 92.280%\n",
      "Epoch 32, Batch 812, LR 0.388664 Loss 3.833249, Accuracy 92.281%\n",
      "Epoch 32, Batch 813, LR 0.388567 Loss 3.833157, Accuracy 92.280%\n",
      "Epoch 32, Batch 814, LR 0.388470 Loss 3.833408, Accuracy 92.280%\n",
      "Epoch 32, Batch 815, LR 0.388373 Loss 3.833425, Accuracy 92.278%\n",
      "Epoch 32, Batch 816, LR 0.388276 Loss 3.832971, Accuracy 92.276%\n",
      "Epoch 32, Batch 817, LR 0.388179 Loss 3.832788, Accuracy 92.275%\n",
      "Epoch 32, Batch 818, LR 0.388082 Loss 3.832869, Accuracy 92.271%\n",
      "Epoch 32, Batch 819, LR 0.387985 Loss 3.834250, Accuracy 92.270%\n",
      "Epoch 32, Batch 820, LR 0.387888 Loss 3.833732, Accuracy 92.273%\n",
      "Epoch 32, Batch 821, LR 0.387791 Loss 3.833680, Accuracy 92.272%\n",
      "Epoch 32, Batch 822, LR 0.387694 Loss 3.833526, Accuracy 92.273%\n",
      "Epoch 32, Batch 823, LR 0.387597 Loss 3.833496, Accuracy 92.273%\n",
      "Epoch 32, Batch 824, LR 0.387500 Loss 3.833845, Accuracy 92.272%\n",
      "Epoch 32, Batch 825, LR 0.387403 Loss 3.833175, Accuracy 92.277%\n",
      "Epoch 32, Batch 826, LR 0.387306 Loss 3.832787, Accuracy 92.275%\n",
      "Epoch 32, Batch 827, LR 0.387209 Loss 3.832900, Accuracy 92.273%\n",
      "Epoch 32, Batch 828, LR 0.387112 Loss 3.832909, Accuracy 92.278%\n",
      "Epoch 32, Batch 829, LR 0.387015 Loss 3.832786, Accuracy 92.279%\n",
      "Epoch 32, Batch 830, LR 0.386918 Loss 3.832402, Accuracy 92.280%\n",
      "Epoch 32, Batch 831, LR 0.386821 Loss 3.832695, Accuracy 92.279%\n",
      "Epoch 32, Batch 832, LR 0.386725 Loss 3.832989, Accuracy 92.277%\n",
      "Epoch 32, Batch 833, LR 0.386628 Loss 3.833367, Accuracy 92.278%\n",
      "Epoch 32, Batch 834, LR 0.386531 Loss 3.833938, Accuracy 92.277%\n",
      "Epoch 32, Batch 835, LR 0.386434 Loss 3.834189, Accuracy 92.277%\n",
      "Epoch 32, Batch 836, LR 0.386337 Loss 3.834502, Accuracy 92.276%\n",
      "Epoch 32, Batch 837, LR 0.386240 Loss 3.834224, Accuracy 92.282%\n",
      "Epoch 32, Batch 838, LR 0.386144 Loss 3.834385, Accuracy 92.284%\n",
      "Epoch 32, Batch 839, LR 0.386047 Loss 3.834764, Accuracy 92.283%\n",
      "Epoch 32, Batch 840, LR 0.385950 Loss 3.835149, Accuracy 92.283%\n",
      "Epoch 32, Batch 841, LR 0.385853 Loss 3.835763, Accuracy 92.279%\n",
      "Epoch 32, Batch 842, LR 0.385756 Loss 3.835982, Accuracy 92.278%\n",
      "Epoch 32, Batch 843, LR 0.385660 Loss 3.836357, Accuracy 92.277%\n",
      "Epoch 32, Batch 844, LR 0.385563 Loss 3.836085, Accuracy 92.280%\n",
      "Epoch 32, Batch 845, LR 0.385466 Loss 3.835681, Accuracy 92.280%\n",
      "Epoch 32, Batch 846, LR 0.385369 Loss 3.837079, Accuracy 92.273%\n",
      "Epoch 32, Batch 847, LR 0.385273 Loss 3.837093, Accuracy 92.272%\n",
      "Epoch 32, Batch 848, LR 0.385176 Loss 3.837169, Accuracy 92.272%\n",
      "Epoch 32, Batch 849, LR 0.385079 Loss 3.837036, Accuracy 92.275%\n",
      "Epoch 32, Batch 850, LR 0.384982 Loss 3.836583, Accuracy 92.278%\n",
      "Epoch 32, Batch 851, LR 0.384886 Loss 3.836245, Accuracy 92.273%\n",
      "Epoch 32, Batch 852, LR 0.384789 Loss 3.836398, Accuracy 92.269%\n",
      "Epoch 32, Batch 853, LR 0.384692 Loss 3.836250, Accuracy 92.269%\n",
      "Epoch 32, Batch 854, LR 0.384596 Loss 3.837214, Accuracy 92.265%\n",
      "Epoch 32, Batch 855, LR 0.384499 Loss 3.837301, Accuracy 92.261%\n",
      "Epoch 32, Batch 856, LR 0.384402 Loss 3.838004, Accuracy 92.261%\n",
      "Epoch 32, Batch 857, LR 0.384306 Loss 3.837954, Accuracy 92.260%\n",
      "Epoch 32, Batch 858, LR 0.384209 Loss 3.837616, Accuracy 92.259%\n",
      "Epoch 32, Batch 859, LR 0.384113 Loss 3.838119, Accuracy 92.255%\n",
      "Epoch 32, Batch 860, LR 0.384016 Loss 3.837227, Accuracy 92.260%\n",
      "Epoch 32, Batch 861, LR 0.383919 Loss 3.836902, Accuracy 92.262%\n",
      "Epoch 32, Batch 862, LR 0.383823 Loss 3.836273, Accuracy 92.262%\n",
      "Epoch 32, Batch 863, LR 0.383726 Loss 3.835991, Accuracy 92.267%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 864, LR 0.383630 Loss 3.836774, Accuracy 92.266%\n",
      "Epoch 32, Batch 865, LR 0.383533 Loss 3.836823, Accuracy 92.269%\n",
      "Epoch 32, Batch 866, LR 0.383437 Loss 3.835928, Accuracy 92.274%\n",
      "Epoch 32, Batch 867, LR 0.383340 Loss 3.835964, Accuracy 92.277%\n",
      "Epoch 32, Batch 868, LR 0.383243 Loss 3.835778, Accuracy 92.277%\n",
      "Epoch 32, Batch 869, LR 0.383147 Loss 3.836343, Accuracy 92.277%\n",
      "Epoch 32, Batch 870, LR 0.383050 Loss 3.836983, Accuracy 92.273%\n",
      "Epoch 32, Batch 871, LR 0.382954 Loss 3.836421, Accuracy 92.275%\n",
      "Epoch 32, Batch 872, LR 0.382858 Loss 3.836706, Accuracy 92.274%\n",
      "Epoch 32, Batch 873, LR 0.382761 Loss 3.837142, Accuracy 92.273%\n",
      "Epoch 32, Batch 874, LR 0.382665 Loss 3.836916, Accuracy 92.277%\n",
      "Epoch 32, Batch 875, LR 0.382568 Loss 3.837127, Accuracy 92.273%\n",
      "Epoch 32, Batch 876, LR 0.382472 Loss 3.836802, Accuracy 92.278%\n",
      "Epoch 32, Batch 877, LR 0.382375 Loss 3.837060, Accuracy 92.278%\n",
      "Epoch 32, Batch 878, LR 0.382279 Loss 3.836409, Accuracy 92.280%\n",
      "Epoch 32, Batch 879, LR 0.382182 Loss 3.836229, Accuracy 92.282%\n",
      "Epoch 32, Batch 880, LR 0.382086 Loss 3.837385, Accuracy 92.275%\n",
      "Epoch 32, Batch 881, LR 0.381990 Loss 3.837415, Accuracy 92.281%\n",
      "Epoch 32, Batch 882, LR 0.381893 Loss 3.838081, Accuracy 92.279%\n",
      "Epoch 32, Batch 883, LR 0.381797 Loss 3.838047, Accuracy 92.279%\n",
      "Epoch 32, Batch 884, LR 0.381700 Loss 3.838265, Accuracy 92.280%\n",
      "Epoch 32, Batch 885, LR 0.381604 Loss 3.838364, Accuracy 92.279%\n",
      "Epoch 32, Batch 886, LR 0.381508 Loss 3.837674, Accuracy 92.284%\n",
      "Epoch 32, Batch 887, LR 0.381411 Loss 3.838316, Accuracy 92.278%\n",
      "Epoch 32, Batch 888, LR 0.381315 Loss 3.837844, Accuracy 92.283%\n",
      "Epoch 32, Batch 889, LR 0.381219 Loss 3.838298, Accuracy 92.281%\n",
      "Epoch 32, Batch 890, LR 0.381123 Loss 3.837697, Accuracy 92.284%\n",
      "Epoch 32, Batch 891, LR 0.381026 Loss 3.838042, Accuracy 92.281%\n",
      "Epoch 32, Batch 892, LR 0.380930 Loss 3.838083, Accuracy 92.282%\n",
      "Epoch 32, Batch 893, LR 0.380834 Loss 3.837480, Accuracy 92.286%\n",
      "Epoch 32, Batch 894, LR 0.380737 Loss 3.836992, Accuracy 92.288%\n",
      "Epoch 32, Batch 895, LR 0.380641 Loss 3.836621, Accuracy 92.289%\n",
      "Epoch 32, Batch 896, LR 0.380545 Loss 3.836771, Accuracy 92.286%\n",
      "Epoch 32, Batch 897, LR 0.380449 Loss 3.836475, Accuracy 92.288%\n",
      "Epoch 32, Batch 898, LR 0.380352 Loss 3.836150, Accuracy 92.292%\n",
      "Epoch 32, Batch 899, LR 0.380256 Loss 3.836169, Accuracy 92.291%\n",
      "Epoch 32, Batch 900, LR 0.380160 Loss 3.836072, Accuracy 92.295%\n",
      "Epoch 32, Batch 901, LR 0.380064 Loss 3.836095, Accuracy 92.292%\n",
      "Epoch 32, Batch 902, LR 0.379968 Loss 3.835858, Accuracy 92.297%\n",
      "Epoch 32, Batch 903, LR 0.379871 Loss 3.835924, Accuracy 92.298%\n",
      "Epoch 32, Batch 904, LR 0.379775 Loss 3.835720, Accuracy 92.298%\n",
      "Epoch 32, Batch 905, LR 0.379679 Loss 3.836670, Accuracy 92.294%\n",
      "Epoch 32, Batch 906, LR 0.379583 Loss 3.835856, Accuracy 92.296%\n",
      "Epoch 32, Batch 907, LR 0.379487 Loss 3.835956, Accuracy 92.298%\n",
      "Epoch 32, Batch 908, LR 0.379391 Loss 3.835767, Accuracy 92.303%\n",
      "Epoch 32, Batch 909, LR 0.379295 Loss 3.836488, Accuracy 92.302%\n",
      "Epoch 32, Batch 910, LR 0.379199 Loss 3.836019, Accuracy 92.302%\n",
      "Epoch 32, Batch 911, LR 0.379102 Loss 3.835792, Accuracy 92.305%\n",
      "Epoch 32, Batch 912, LR 0.379006 Loss 3.836365, Accuracy 92.302%\n",
      "Epoch 32, Batch 913, LR 0.378910 Loss 3.836074, Accuracy 92.302%\n",
      "Epoch 32, Batch 914, LR 0.378814 Loss 3.836296, Accuracy 92.300%\n",
      "Epoch 32, Batch 915, LR 0.378718 Loss 3.835658, Accuracy 92.304%\n",
      "Epoch 32, Batch 916, LR 0.378622 Loss 3.836199, Accuracy 92.303%\n",
      "Epoch 32, Batch 917, LR 0.378526 Loss 3.836069, Accuracy 92.303%\n",
      "Epoch 32, Batch 918, LR 0.378430 Loss 3.835862, Accuracy 92.302%\n",
      "Epoch 32, Batch 919, LR 0.378334 Loss 3.835520, Accuracy 92.301%\n",
      "Epoch 32, Batch 920, LR 0.378238 Loss 3.836081, Accuracy 92.297%\n",
      "Epoch 32, Batch 921, LR 0.378142 Loss 3.835895, Accuracy 92.299%\n",
      "Epoch 32, Batch 922, LR 0.378046 Loss 3.836084, Accuracy 92.299%\n",
      "Epoch 32, Batch 923, LR 0.377950 Loss 3.835717, Accuracy 92.301%\n",
      "Epoch 32, Batch 924, LR 0.377854 Loss 3.835920, Accuracy 92.298%\n",
      "Epoch 32, Batch 925, LR 0.377758 Loss 3.835994, Accuracy 92.297%\n",
      "Epoch 32, Batch 926, LR 0.377662 Loss 3.835931, Accuracy 92.302%\n",
      "Epoch 32, Batch 927, LR 0.377566 Loss 3.835840, Accuracy 92.306%\n",
      "Epoch 32, Batch 928, LR 0.377470 Loss 3.835493, Accuracy 92.306%\n",
      "Epoch 32, Batch 929, LR 0.377374 Loss 3.835169, Accuracy 92.307%\n",
      "Epoch 32, Batch 930, LR 0.377279 Loss 3.835205, Accuracy 92.308%\n",
      "Epoch 32, Batch 931, LR 0.377183 Loss 3.835022, Accuracy 92.308%\n",
      "Epoch 32, Batch 932, LR 0.377087 Loss 3.835117, Accuracy 92.307%\n",
      "Epoch 32, Batch 933, LR 0.376991 Loss 3.835222, Accuracy 92.308%\n",
      "Epoch 32, Batch 934, LR 0.376895 Loss 3.835051, Accuracy 92.309%\n",
      "Epoch 32, Batch 935, LR 0.376799 Loss 3.835269, Accuracy 92.308%\n",
      "Epoch 32, Batch 936, LR 0.376703 Loss 3.835050, Accuracy 92.311%\n",
      "Epoch 32, Batch 937, LR 0.376607 Loss 3.834482, Accuracy 92.313%\n",
      "Epoch 32, Batch 938, LR 0.376512 Loss 3.834873, Accuracy 92.311%\n",
      "Epoch 32, Batch 939, LR 0.376416 Loss 3.834713, Accuracy 92.313%\n",
      "Epoch 32, Batch 940, LR 0.376320 Loss 3.835350, Accuracy 92.307%\n",
      "Epoch 32, Batch 941, LR 0.376224 Loss 3.835020, Accuracy 92.310%\n",
      "Epoch 32, Batch 942, LR 0.376128 Loss 3.835802, Accuracy 92.304%\n",
      "Epoch 32, Batch 943, LR 0.376033 Loss 3.835604, Accuracy 92.303%\n",
      "Epoch 32, Batch 944, LR 0.375937 Loss 3.835151, Accuracy 92.306%\n",
      "Epoch 32, Batch 945, LR 0.375841 Loss 3.834686, Accuracy 92.307%\n",
      "Epoch 32, Batch 946, LR 0.375745 Loss 3.834325, Accuracy 92.311%\n",
      "Epoch 32, Batch 947, LR 0.375650 Loss 3.833811, Accuracy 92.314%\n",
      "Epoch 32, Batch 948, LR 0.375554 Loss 3.833275, Accuracy 92.316%\n",
      "Epoch 32, Batch 949, LR 0.375458 Loss 3.833254, Accuracy 92.318%\n",
      "Epoch 32, Batch 950, LR 0.375363 Loss 3.833090, Accuracy 92.318%\n",
      "Epoch 32, Batch 951, LR 0.375267 Loss 3.833284, Accuracy 92.314%\n",
      "Epoch 32, Batch 952, LR 0.375171 Loss 3.833242, Accuracy 92.311%\n",
      "Epoch 32, Batch 953, LR 0.375075 Loss 3.832681, Accuracy 92.310%\n",
      "Epoch 32, Batch 954, LR 0.374980 Loss 3.833219, Accuracy 92.309%\n",
      "Epoch 32, Batch 955, LR 0.374884 Loss 3.832862, Accuracy 92.309%\n",
      "Epoch 32, Batch 956, LR 0.374789 Loss 3.833720, Accuracy 92.303%\n",
      "Epoch 32, Batch 957, LR 0.374693 Loss 3.833522, Accuracy 92.305%\n",
      "Epoch 32, Batch 958, LR 0.374597 Loss 3.832666, Accuracy 92.310%\n",
      "Epoch 32, Batch 959, LR 0.374502 Loss 3.832541, Accuracy 92.306%\n",
      "Epoch 32, Batch 960, LR 0.374406 Loss 3.832579, Accuracy 92.308%\n",
      "Epoch 32, Batch 961, LR 0.374310 Loss 3.832161, Accuracy 92.309%\n",
      "Epoch 32, Batch 962, LR 0.374215 Loss 3.832962, Accuracy 92.304%\n",
      "Epoch 32, Batch 963, LR 0.374119 Loss 3.832629, Accuracy 92.305%\n",
      "Epoch 32, Batch 964, LR 0.374024 Loss 3.832476, Accuracy 92.304%\n",
      "Epoch 32, Batch 965, LR 0.373928 Loss 3.831966, Accuracy 92.306%\n",
      "Epoch 32, Batch 966, LR 0.373833 Loss 3.832501, Accuracy 92.303%\n",
      "Epoch 32, Batch 967, LR 0.373737 Loss 3.832139, Accuracy 92.305%\n",
      "Epoch 32, Batch 968, LR 0.373642 Loss 3.832100, Accuracy 92.307%\n",
      "Epoch 32, Batch 969, LR 0.373546 Loss 3.831630, Accuracy 92.307%\n",
      "Epoch 32, Batch 970, LR 0.373451 Loss 3.831388, Accuracy 92.308%\n",
      "Epoch 32, Batch 971, LR 0.373355 Loss 3.831749, Accuracy 92.303%\n",
      "Epoch 32, Batch 972, LR 0.373260 Loss 3.831585, Accuracy 92.303%\n",
      "Epoch 32, Batch 973, LR 0.373164 Loss 3.831042, Accuracy 92.305%\n",
      "Epoch 32, Batch 974, LR 0.373069 Loss 3.831099, Accuracy 92.305%\n",
      "Epoch 32, Batch 975, LR 0.372973 Loss 3.831210, Accuracy 92.304%\n",
      "Epoch 32, Batch 976, LR 0.372878 Loss 3.831432, Accuracy 92.302%\n",
      "Epoch 32, Batch 977, LR 0.372782 Loss 3.832040, Accuracy 92.299%\n",
      "Epoch 32, Batch 978, LR 0.372687 Loss 3.831931, Accuracy 92.301%\n",
      "Epoch 32, Batch 979, LR 0.372592 Loss 3.831216, Accuracy 92.302%\n",
      "Epoch 32, Batch 980, LR 0.372496 Loss 3.831344, Accuracy 92.300%\n",
      "Epoch 32, Batch 981, LR 0.372401 Loss 3.831478, Accuracy 92.301%\n",
      "Epoch 32, Batch 982, LR 0.372305 Loss 3.831564, Accuracy 92.304%\n",
      "Epoch 32, Batch 983, LR 0.372210 Loss 3.832003, Accuracy 92.301%\n",
      "Epoch 32, Batch 984, LR 0.372115 Loss 3.831379, Accuracy 92.305%\n",
      "Epoch 32, Batch 985, LR 0.372019 Loss 3.831297, Accuracy 92.310%\n",
      "Epoch 32, Batch 986, LR 0.371924 Loss 3.831309, Accuracy 92.311%\n",
      "Epoch 32, Batch 987, LR 0.371829 Loss 3.831716, Accuracy 92.310%\n",
      "Epoch 32, Batch 988, LR 0.371733 Loss 3.832076, Accuracy 92.310%\n",
      "Epoch 32, Batch 989, LR 0.371638 Loss 3.832060, Accuracy 92.314%\n",
      "Epoch 32, Batch 990, LR 0.371543 Loss 3.832049, Accuracy 92.315%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 991, LR 0.371447 Loss 3.832064, Accuracy 92.316%\n",
      "Epoch 32, Batch 992, LR 0.371352 Loss 3.832070, Accuracy 92.317%\n",
      "Epoch 32, Batch 993, LR 0.371257 Loss 3.831332, Accuracy 92.319%\n",
      "Epoch 32, Batch 994, LR 0.371162 Loss 3.830464, Accuracy 92.320%\n",
      "Epoch 32, Batch 995, LR 0.371066 Loss 3.830120, Accuracy 92.318%\n",
      "Epoch 32, Batch 996, LR 0.370971 Loss 3.830005, Accuracy 92.315%\n",
      "Epoch 32, Batch 997, LR 0.370876 Loss 3.830653, Accuracy 92.314%\n",
      "Epoch 32, Batch 998, LR 0.370781 Loss 3.830966, Accuracy 92.314%\n",
      "Epoch 32, Batch 999, LR 0.370685 Loss 3.831002, Accuracy 92.314%\n",
      "Epoch 32, Batch 1000, LR 0.370590 Loss 3.830908, Accuracy 92.317%\n",
      "Epoch 32, Batch 1001, LR 0.370495 Loss 3.830703, Accuracy 92.318%\n",
      "Epoch 32, Batch 1002, LR 0.370400 Loss 3.829915, Accuracy 92.322%\n",
      "Epoch 32, Batch 1003, LR 0.370305 Loss 3.829909, Accuracy 92.321%\n",
      "Epoch 32, Batch 1004, LR 0.370209 Loss 3.830105, Accuracy 92.321%\n",
      "Epoch 32, Batch 1005, LR 0.370114 Loss 3.830032, Accuracy 92.319%\n",
      "Epoch 32, Batch 1006, LR 0.370019 Loss 3.830257, Accuracy 92.314%\n",
      "Epoch 32, Batch 1007, LR 0.369924 Loss 3.829764, Accuracy 92.317%\n",
      "Epoch 32, Batch 1008, LR 0.369829 Loss 3.829831, Accuracy 92.317%\n",
      "Epoch 32, Batch 1009, LR 0.369734 Loss 3.829557, Accuracy 92.320%\n",
      "Epoch 32, Batch 1010, LR 0.369639 Loss 3.829006, Accuracy 92.321%\n",
      "Epoch 32, Batch 1011, LR 0.369544 Loss 3.828902, Accuracy 92.322%\n",
      "Epoch 32, Batch 1012, LR 0.369449 Loss 3.828907, Accuracy 92.323%\n",
      "Epoch 32, Batch 1013, LR 0.369354 Loss 3.828963, Accuracy 92.325%\n",
      "Epoch 32, Batch 1014, LR 0.369258 Loss 3.828841, Accuracy 92.325%\n",
      "Epoch 32, Batch 1015, LR 0.369163 Loss 3.828976, Accuracy 92.325%\n",
      "Epoch 32, Batch 1016, LR 0.369068 Loss 3.828944, Accuracy 92.324%\n",
      "Epoch 32, Batch 1017, LR 0.368973 Loss 3.828590, Accuracy 92.324%\n",
      "Epoch 32, Batch 1018, LR 0.368878 Loss 3.829140, Accuracy 92.321%\n",
      "Epoch 32, Batch 1019, LR 0.368783 Loss 3.829151, Accuracy 92.322%\n",
      "Epoch 32, Batch 1020, LR 0.368688 Loss 3.829261, Accuracy 92.322%\n",
      "Epoch 32, Batch 1021, LR 0.368593 Loss 3.828986, Accuracy 92.323%\n",
      "Epoch 32, Batch 1022, LR 0.368498 Loss 3.829135, Accuracy 92.324%\n",
      "Epoch 32, Batch 1023, LR 0.368403 Loss 3.829071, Accuracy 92.324%\n",
      "Epoch 32, Batch 1024, LR 0.368308 Loss 3.829249, Accuracy 92.324%\n",
      "Epoch 32, Batch 1025, LR 0.368213 Loss 3.829209, Accuracy 92.323%\n",
      "Epoch 32, Batch 1026, LR 0.368119 Loss 3.829687, Accuracy 92.321%\n",
      "Epoch 32, Batch 1027, LR 0.368024 Loss 3.830697, Accuracy 92.316%\n",
      "Epoch 32, Batch 1028, LR 0.367929 Loss 3.831528, Accuracy 92.313%\n",
      "Epoch 32, Batch 1029, LR 0.367834 Loss 3.831609, Accuracy 92.315%\n",
      "Epoch 32, Batch 1030, LR 0.367739 Loss 3.831301, Accuracy 92.313%\n",
      "Epoch 32, Batch 1031, LR 0.367644 Loss 3.831559, Accuracy 92.314%\n",
      "Epoch 32, Batch 1032, LR 0.367549 Loss 3.831373, Accuracy 92.313%\n",
      "Epoch 32, Batch 1033, LR 0.367454 Loss 3.830865, Accuracy 92.314%\n",
      "Epoch 32, Batch 1034, LR 0.367359 Loss 3.830799, Accuracy 92.313%\n",
      "Epoch 32, Batch 1035, LR 0.367265 Loss 3.830925, Accuracy 92.311%\n",
      "Epoch 32, Batch 1036, LR 0.367170 Loss 3.830926, Accuracy 92.309%\n",
      "Epoch 32, Batch 1037, LR 0.367075 Loss 3.830887, Accuracy 92.307%\n",
      "Epoch 32, Batch 1038, LR 0.366980 Loss 3.830628, Accuracy 92.303%\n",
      "Epoch 32, Batch 1039, LR 0.366885 Loss 3.830503, Accuracy 92.305%\n",
      "Epoch 32, Batch 1040, LR 0.366790 Loss 3.830193, Accuracy 92.308%\n",
      "Epoch 32, Batch 1041, LR 0.366696 Loss 3.830102, Accuracy 92.309%\n",
      "Epoch 32, Batch 1042, LR 0.366601 Loss 3.830362, Accuracy 92.305%\n",
      "Epoch 32, Batch 1043, LR 0.366506 Loss 3.831222, Accuracy 92.301%\n",
      "Epoch 32, Batch 1044, LR 0.366411 Loss 3.830863, Accuracy 92.300%\n",
      "Epoch 32, Batch 1045, LR 0.366317 Loss 3.830505, Accuracy 92.301%\n",
      "Epoch 32, Batch 1046, LR 0.366222 Loss 3.830473, Accuracy 92.302%\n",
      "Epoch 32, Batch 1047, LR 0.366127 Loss 3.831117, Accuracy 92.296%\n",
      "Epoch 32, Loss (train set) 3.831117, Accuracy (train set) 92.296%\n",
      "Epoch 33, Batch 1, LR 0.366032 Loss 3.764116, Accuracy 92.969%\n",
      "Epoch 33, Batch 2, LR 0.365938 Loss 3.930431, Accuracy 93.359%\n",
      "Epoch 33, Batch 3, LR 0.365843 Loss 4.171525, Accuracy 91.406%\n",
      "Epoch 33, Batch 4, LR 0.365748 Loss 4.094363, Accuracy 91.211%\n",
      "Epoch 33, Batch 5, LR 0.365654 Loss 4.001470, Accuracy 91.562%\n",
      "Epoch 33, Batch 6, LR 0.365559 Loss 3.957830, Accuracy 91.667%\n",
      "Epoch 33, Batch 7, LR 0.365464 Loss 3.871902, Accuracy 92.188%\n",
      "Epoch 33, Batch 8, LR 0.365370 Loss 3.892884, Accuracy 92.090%\n",
      "Epoch 33, Batch 9, LR 0.365275 Loss 3.944408, Accuracy 91.840%\n",
      "Epoch 33, Batch 10, LR 0.365181 Loss 3.949901, Accuracy 91.719%\n",
      "Epoch 33, Batch 11, LR 0.365086 Loss 3.949675, Accuracy 91.832%\n",
      "Epoch 33, Batch 12, LR 0.364991 Loss 3.846904, Accuracy 92.253%\n",
      "Epoch 33, Batch 13, LR 0.364897 Loss 3.845218, Accuracy 92.248%\n",
      "Epoch 33, Batch 14, LR 0.364802 Loss 3.833487, Accuracy 92.132%\n",
      "Epoch 33, Batch 15, LR 0.364708 Loss 3.841076, Accuracy 92.135%\n",
      "Epoch 33, Batch 16, LR 0.364613 Loss 3.882971, Accuracy 92.041%\n",
      "Epoch 33, Batch 17, LR 0.364518 Loss 3.857116, Accuracy 92.142%\n",
      "Epoch 33, Batch 18, LR 0.364424 Loss 3.837803, Accuracy 92.057%\n",
      "Epoch 33, Batch 19, LR 0.364329 Loss 3.807999, Accuracy 92.229%\n",
      "Epoch 33, Batch 20, LR 0.364235 Loss 3.820597, Accuracy 92.305%\n",
      "Epoch 33, Batch 21, LR 0.364140 Loss 3.863508, Accuracy 92.150%\n",
      "Epoch 33, Batch 22, LR 0.364046 Loss 3.863547, Accuracy 92.045%\n",
      "Epoch 33, Batch 23, LR 0.363951 Loss 3.851008, Accuracy 92.086%\n",
      "Epoch 33, Batch 24, LR 0.363857 Loss 3.829805, Accuracy 92.155%\n",
      "Epoch 33, Batch 25, LR 0.363762 Loss 3.830634, Accuracy 92.031%\n",
      "Epoch 33, Batch 26, LR 0.363668 Loss 3.823755, Accuracy 92.007%\n",
      "Epoch 33, Batch 27, LR 0.363573 Loss 3.809149, Accuracy 92.159%\n",
      "Epoch 33, Batch 28, LR 0.363479 Loss 3.800962, Accuracy 92.299%\n",
      "Epoch 33, Batch 29, LR 0.363385 Loss 3.790374, Accuracy 92.295%\n",
      "Epoch 33, Batch 30, LR 0.363290 Loss 3.772985, Accuracy 92.266%\n",
      "Epoch 33, Batch 31, LR 0.363196 Loss 3.760177, Accuracy 92.389%\n",
      "Epoch 33, Batch 32, LR 0.363101 Loss 3.756712, Accuracy 92.505%\n",
      "Epoch 33, Batch 33, LR 0.363007 Loss 3.745484, Accuracy 92.519%\n",
      "Epoch 33, Batch 34, LR 0.362913 Loss 3.754101, Accuracy 92.509%\n",
      "Epoch 33, Batch 35, LR 0.362818 Loss 3.746003, Accuracy 92.522%\n",
      "Epoch 33, Batch 36, LR 0.362724 Loss 3.748826, Accuracy 92.383%\n",
      "Epoch 33, Batch 37, LR 0.362630 Loss 3.749606, Accuracy 92.314%\n",
      "Epoch 33, Batch 38, LR 0.362535 Loss 3.753913, Accuracy 92.270%\n",
      "Epoch 33, Batch 39, LR 0.362441 Loss 3.763767, Accuracy 92.228%\n",
      "Epoch 33, Batch 40, LR 0.362347 Loss 3.754963, Accuracy 92.285%\n",
      "Epoch 33, Batch 41, LR 0.362252 Loss 3.756886, Accuracy 92.321%\n",
      "Epoch 33, Batch 42, LR 0.362158 Loss 3.739234, Accuracy 92.429%\n",
      "Epoch 33, Batch 43, LR 0.362064 Loss 3.747783, Accuracy 92.442%\n",
      "Epoch 33, Batch 44, LR 0.361969 Loss 3.737191, Accuracy 92.472%\n",
      "Epoch 33, Batch 45, LR 0.361875 Loss 3.719621, Accuracy 92.483%\n",
      "Epoch 33, Batch 46, LR 0.361781 Loss 3.715980, Accuracy 92.476%\n",
      "Epoch 33, Batch 47, LR 0.361687 Loss 3.718291, Accuracy 92.453%\n",
      "Epoch 33, Batch 48, LR 0.361592 Loss 3.721390, Accuracy 92.432%\n",
      "Epoch 33, Batch 49, LR 0.361498 Loss 3.727100, Accuracy 92.411%\n",
      "Epoch 33, Batch 50, LR 0.361404 Loss 3.729807, Accuracy 92.406%\n",
      "Epoch 33, Batch 51, LR 0.361310 Loss 3.742872, Accuracy 92.279%\n",
      "Epoch 33, Batch 52, LR 0.361216 Loss 3.743883, Accuracy 92.263%\n",
      "Epoch 33, Batch 53, LR 0.361121 Loss 3.750657, Accuracy 92.291%\n",
      "Epoch 33, Batch 54, LR 0.361027 Loss 3.755465, Accuracy 92.289%\n",
      "Epoch 33, Batch 55, LR 0.360933 Loss 3.750628, Accuracy 92.287%\n",
      "Epoch 33, Batch 56, LR 0.360839 Loss 3.756868, Accuracy 92.257%\n",
      "Epoch 33, Batch 57, LR 0.360745 Loss 3.759491, Accuracy 92.270%\n",
      "Epoch 33, Batch 58, LR 0.360651 Loss 3.753000, Accuracy 92.322%\n",
      "Epoch 33, Batch 59, LR 0.360556 Loss 3.747750, Accuracy 92.346%\n",
      "Epoch 33, Batch 60, LR 0.360462 Loss 3.746474, Accuracy 92.357%\n",
      "Epoch 33, Batch 61, LR 0.360368 Loss 3.734882, Accuracy 92.405%\n",
      "Epoch 33, Batch 62, LR 0.360274 Loss 3.742777, Accuracy 92.364%\n",
      "Epoch 33, Batch 63, LR 0.360180 Loss 3.740342, Accuracy 92.374%\n",
      "Epoch 33, Batch 64, LR 0.360086 Loss 3.729533, Accuracy 92.456%\n",
      "Epoch 33, Batch 65, LR 0.359992 Loss 3.727646, Accuracy 92.500%\n",
      "Epoch 33, Batch 66, LR 0.359898 Loss 3.725203, Accuracy 92.531%\n",
      "Epoch 33, Batch 67, LR 0.359804 Loss 3.735865, Accuracy 92.514%\n",
      "Epoch 33, Batch 68, LR 0.359710 Loss 3.730608, Accuracy 92.578%\n",
      "Epoch 33, Batch 69, LR 0.359616 Loss 3.734972, Accuracy 92.493%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 70, LR 0.359522 Loss 3.731970, Accuracy 92.511%\n",
      "Epoch 33, Batch 71, LR 0.359428 Loss 3.730529, Accuracy 92.507%\n",
      "Epoch 33, Batch 72, LR 0.359334 Loss 3.728419, Accuracy 92.546%\n",
      "Epoch 33, Batch 73, LR 0.359240 Loss 3.726684, Accuracy 92.551%\n",
      "Epoch 33, Batch 74, LR 0.359146 Loss 3.727386, Accuracy 92.568%\n",
      "Epoch 33, Batch 75, LR 0.359052 Loss 3.734858, Accuracy 92.542%\n",
      "Epoch 33, Batch 76, LR 0.358958 Loss 3.731044, Accuracy 92.578%\n",
      "Epoch 33, Batch 77, LR 0.358864 Loss 3.732177, Accuracy 92.583%\n",
      "Epoch 33, Batch 78, LR 0.358770 Loss 3.724062, Accuracy 92.598%\n",
      "Epoch 33, Batch 79, LR 0.358676 Loss 3.724183, Accuracy 92.583%\n",
      "Epoch 33, Batch 80, LR 0.358582 Loss 3.705971, Accuracy 92.637%\n",
      "Epoch 33, Batch 81, LR 0.358488 Loss 3.704788, Accuracy 92.641%\n",
      "Epoch 33, Batch 82, LR 0.358394 Loss 3.714306, Accuracy 92.569%\n",
      "Epoch 33, Batch 83, LR 0.358301 Loss 3.716730, Accuracy 92.564%\n",
      "Epoch 33, Batch 84, LR 0.358207 Loss 3.712481, Accuracy 92.578%\n",
      "Epoch 33, Batch 85, LR 0.358113 Loss 3.712671, Accuracy 92.601%\n",
      "Epoch 33, Batch 86, LR 0.358019 Loss 3.717439, Accuracy 92.587%\n",
      "Epoch 33, Batch 87, LR 0.357925 Loss 3.717416, Accuracy 92.619%\n",
      "Epoch 33, Batch 88, LR 0.357831 Loss 3.708994, Accuracy 92.676%\n",
      "Epoch 33, Batch 89, LR 0.357738 Loss 3.707230, Accuracy 92.644%\n",
      "Epoch 33, Batch 90, LR 0.357644 Loss 3.710479, Accuracy 92.622%\n",
      "Epoch 33, Batch 91, LR 0.357550 Loss 3.714452, Accuracy 92.617%\n",
      "Epoch 33, Batch 92, LR 0.357456 Loss 3.708198, Accuracy 92.629%\n",
      "Epoch 33, Batch 93, LR 0.357362 Loss 3.716231, Accuracy 92.641%\n",
      "Epoch 33, Batch 94, LR 0.357269 Loss 3.715225, Accuracy 92.661%\n",
      "Epoch 33, Batch 95, LR 0.357175 Loss 3.722088, Accuracy 92.648%\n",
      "Epoch 33, Batch 96, LR 0.357081 Loss 3.722229, Accuracy 92.635%\n",
      "Epoch 33, Batch 97, LR 0.356987 Loss 3.713889, Accuracy 92.671%\n",
      "Epoch 33, Batch 98, LR 0.356894 Loss 3.719092, Accuracy 92.650%\n",
      "Epoch 33, Batch 99, LR 0.356800 Loss 3.721897, Accuracy 92.629%\n",
      "Epoch 33, Batch 100, LR 0.356706 Loss 3.717163, Accuracy 92.664%\n",
      "Epoch 33, Batch 101, LR 0.356613 Loss 3.721882, Accuracy 92.621%\n",
      "Epoch 33, Batch 102, LR 0.356519 Loss 3.719819, Accuracy 92.601%\n",
      "Epoch 33, Batch 103, LR 0.356425 Loss 3.714170, Accuracy 92.650%\n",
      "Epoch 33, Batch 104, LR 0.356331 Loss 3.716636, Accuracy 92.653%\n",
      "Epoch 33, Batch 105, LR 0.356238 Loss 3.716264, Accuracy 92.634%\n",
      "Epoch 33, Batch 106, LR 0.356144 Loss 3.715577, Accuracy 92.608%\n",
      "Epoch 33, Batch 107, LR 0.356051 Loss 3.714919, Accuracy 92.618%\n",
      "Epoch 33, Batch 108, LR 0.355957 Loss 3.710319, Accuracy 92.643%\n",
      "Epoch 33, Batch 109, LR 0.355863 Loss 3.711786, Accuracy 92.632%\n",
      "Epoch 33, Batch 110, LR 0.355770 Loss 3.717662, Accuracy 92.635%\n",
      "Epoch 33, Batch 111, LR 0.355676 Loss 3.716131, Accuracy 92.652%\n",
      "Epoch 33, Batch 112, LR 0.355583 Loss 3.712179, Accuracy 92.676%\n",
      "Epoch 33, Batch 113, LR 0.355489 Loss 3.713103, Accuracy 92.678%\n",
      "Epoch 33, Batch 114, LR 0.355395 Loss 3.717042, Accuracy 92.660%\n",
      "Epoch 33, Batch 115, LR 0.355302 Loss 3.715564, Accuracy 92.643%\n",
      "Epoch 33, Batch 116, LR 0.355208 Loss 3.711085, Accuracy 92.666%\n",
      "Epoch 33, Batch 117, LR 0.355115 Loss 3.709097, Accuracy 92.655%\n",
      "Epoch 33, Batch 118, LR 0.355021 Loss 3.707181, Accuracy 92.671%\n",
      "Epoch 33, Batch 119, LR 0.354928 Loss 3.712096, Accuracy 92.680%\n",
      "Epoch 33, Batch 120, LR 0.354834 Loss 3.705097, Accuracy 92.708%\n",
      "Epoch 33, Batch 121, LR 0.354741 Loss 3.702068, Accuracy 92.698%\n",
      "Epoch 33, Batch 122, LR 0.354647 Loss 3.704037, Accuracy 92.681%\n",
      "Epoch 33, Batch 123, LR 0.354554 Loss 3.709078, Accuracy 92.670%\n",
      "Epoch 33, Batch 124, LR 0.354460 Loss 3.712883, Accuracy 92.647%\n",
      "Epoch 33, Batch 125, LR 0.354367 Loss 3.712243, Accuracy 92.631%\n",
      "Epoch 33, Batch 126, LR 0.354274 Loss 3.710596, Accuracy 92.622%\n",
      "Epoch 33, Batch 127, LR 0.354180 Loss 3.708979, Accuracy 92.643%\n",
      "Epoch 33, Batch 128, LR 0.354087 Loss 3.707877, Accuracy 92.651%\n",
      "Epoch 33, Batch 129, LR 0.353993 Loss 3.711858, Accuracy 92.624%\n",
      "Epoch 33, Batch 130, LR 0.353900 Loss 3.717160, Accuracy 92.572%\n",
      "Epoch 33, Batch 131, LR 0.353806 Loss 3.712691, Accuracy 92.587%\n",
      "Epoch 33, Batch 132, LR 0.353713 Loss 3.715833, Accuracy 92.578%\n",
      "Epoch 33, Batch 133, LR 0.353620 Loss 3.717325, Accuracy 92.563%\n",
      "Epoch 33, Batch 134, LR 0.353526 Loss 3.714457, Accuracy 92.584%\n",
      "Epoch 33, Batch 135, LR 0.353433 Loss 3.710091, Accuracy 92.604%\n",
      "Epoch 33, Batch 136, LR 0.353340 Loss 3.710441, Accuracy 92.595%\n",
      "Epoch 33, Batch 137, LR 0.353246 Loss 3.711914, Accuracy 92.575%\n",
      "Epoch 33, Batch 138, LR 0.353153 Loss 3.707840, Accuracy 92.606%\n",
      "Epoch 33, Batch 139, LR 0.353060 Loss 3.707990, Accuracy 92.592%\n",
      "Epoch 33, Batch 140, LR 0.352966 Loss 3.710505, Accuracy 92.578%\n",
      "Epoch 33, Batch 141, LR 0.352873 Loss 3.712180, Accuracy 92.564%\n",
      "Epoch 33, Batch 142, LR 0.352780 Loss 3.707800, Accuracy 92.584%\n",
      "Epoch 33, Batch 143, LR 0.352687 Loss 3.709744, Accuracy 92.581%\n",
      "Epoch 33, Batch 144, LR 0.352593 Loss 3.705951, Accuracy 92.594%\n",
      "Epoch 33, Batch 145, LR 0.352500 Loss 3.709054, Accuracy 92.581%\n",
      "Epoch 33, Batch 146, LR 0.352407 Loss 3.705041, Accuracy 92.600%\n",
      "Epoch 33, Batch 147, LR 0.352314 Loss 3.707777, Accuracy 92.607%\n",
      "Epoch 33, Batch 148, LR 0.352221 Loss 3.707769, Accuracy 92.631%\n",
      "Epoch 33, Batch 149, LR 0.352127 Loss 3.708588, Accuracy 92.617%\n",
      "Epoch 33, Batch 150, LR 0.352034 Loss 3.710480, Accuracy 92.620%\n",
      "Epoch 33, Batch 151, LR 0.351941 Loss 3.707885, Accuracy 92.632%\n",
      "Epoch 33, Batch 152, LR 0.351848 Loss 3.710230, Accuracy 92.619%\n",
      "Epoch 33, Batch 153, LR 0.351755 Loss 3.709223, Accuracy 92.627%\n",
      "Epoch 33, Batch 154, LR 0.351661 Loss 3.713559, Accuracy 92.639%\n",
      "Epoch 33, Batch 155, LR 0.351568 Loss 3.712322, Accuracy 92.651%\n",
      "Epoch 33, Batch 156, LR 0.351475 Loss 3.714262, Accuracy 92.648%\n",
      "Epoch 33, Batch 157, LR 0.351382 Loss 3.711175, Accuracy 92.660%\n",
      "Epoch 33, Batch 158, LR 0.351289 Loss 3.712734, Accuracy 92.637%\n",
      "Epoch 33, Batch 159, LR 0.351196 Loss 3.707552, Accuracy 92.664%\n",
      "Epoch 33, Batch 160, LR 0.351103 Loss 3.710131, Accuracy 92.642%\n",
      "Epoch 33, Batch 161, LR 0.351010 Loss 3.709029, Accuracy 92.639%\n",
      "Epoch 33, Batch 162, LR 0.350917 Loss 3.711905, Accuracy 92.617%\n",
      "Epoch 33, Batch 163, LR 0.350824 Loss 3.712098, Accuracy 92.624%\n",
      "Epoch 33, Batch 164, LR 0.350731 Loss 3.711115, Accuracy 92.616%\n",
      "Epoch 33, Batch 165, LR 0.350638 Loss 3.713368, Accuracy 92.618%\n",
      "Epoch 33, Batch 166, LR 0.350544 Loss 3.711612, Accuracy 92.635%\n",
      "Epoch 33, Batch 167, LR 0.350451 Loss 3.708929, Accuracy 92.637%\n",
      "Epoch 33, Batch 168, LR 0.350358 Loss 3.710864, Accuracy 92.625%\n",
      "Epoch 33, Batch 169, LR 0.350265 Loss 3.712111, Accuracy 92.617%\n",
      "Epoch 33, Batch 170, LR 0.350172 Loss 3.710791, Accuracy 92.629%\n",
      "Epoch 33, Batch 171, LR 0.350080 Loss 3.713174, Accuracy 92.640%\n",
      "Epoch 33, Batch 172, LR 0.349987 Loss 3.709425, Accuracy 92.655%\n",
      "Epoch 33, Batch 173, LR 0.349894 Loss 3.709535, Accuracy 92.657%\n",
      "Epoch 33, Batch 174, LR 0.349801 Loss 3.710445, Accuracy 92.659%\n",
      "Epoch 33, Batch 175, LR 0.349708 Loss 3.714433, Accuracy 92.643%\n",
      "Epoch 33, Batch 176, LR 0.349615 Loss 3.713469, Accuracy 92.654%\n",
      "Epoch 33, Batch 177, LR 0.349522 Loss 3.714795, Accuracy 92.673%\n",
      "Epoch 33, Batch 178, LR 0.349429 Loss 3.713990, Accuracy 92.670%\n",
      "Epoch 33, Batch 179, LR 0.349336 Loss 3.709172, Accuracy 92.681%\n",
      "Epoch 33, Batch 180, LR 0.349243 Loss 3.708558, Accuracy 92.674%\n",
      "Epoch 33, Batch 181, LR 0.349150 Loss 3.704068, Accuracy 92.697%\n",
      "Epoch 33, Batch 182, LR 0.349057 Loss 3.706134, Accuracy 92.694%\n",
      "Epoch 33, Batch 183, LR 0.348965 Loss 3.709607, Accuracy 92.691%\n",
      "Epoch 33, Batch 184, LR 0.348872 Loss 3.710057, Accuracy 92.680%\n",
      "Epoch 33, Batch 185, LR 0.348779 Loss 3.711002, Accuracy 92.682%\n",
      "Epoch 33, Batch 186, LR 0.348686 Loss 3.711721, Accuracy 92.679%\n",
      "Epoch 33, Batch 187, LR 0.348593 Loss 3.712726, Accuracy 92.672%\n",
      "Epoch 33, Batch 188, LR 0.348501 Loss 3.712856, Accuracy 92.670%\n",
      "Epoch 33, Batch 189, LR 0.348408 Loss 3.713959, Accuracy 92.667%\n",
      "Epoch 33, Batch 190, LR 0.348315 Loss 3.714913, Accuracy 92.652%\n",
      "Epoch 33, Batch 191, LR 0.348222 Loss 3.714096, Accuracy 92.646%\n",
      "Epoch 33, Batch 192, LR 0.348129 Loss 3.714755, Accuracy 92.647%\n",
      "Epoch 33, Batch 193, LR 0.348037 Loss 3.713681, Accuracy 92.657%\n",
      "Epoch 33, Batch 194, LR 0.347944 Loss 3.711738, Accuracy 92.675%\n",
      "Epoch 33, Batch 195, LR 0.347851 Loss 3.708971, Accuracy 92.672%\n",
      "Epoch 33, Batch 196, LR 0.347759 Loss 3.712733, Accuracy 92.654%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 197, LR 0.347666 Loss 3.714420, Accuracy 92.624%\n",
      "Epoch 33, Batch 198, LR 0.347573 Loss 3.712934, Accuracy 92.637%\n",
      "Epoch 33, Batch 199, LR 0.347480 Loss 3.711531, Accuracy 92.639%\n",
      "Epoch 33, Batch 200, LR 0.347388 Loss 3.711373, Accuracy 92.641%\n",
      "Epoch 33, Batch 201, LR 0.347295 Loss 3.711371, Accuracy 92.642%\n",
      "Epoch 33, Batch 202, LR 0.347202 Loss 3.710450, Accuracy 92.644%\n",
      "Epoch 33, Batch 203, LR 0.347110 Loss 3.712292, Accuracy 92.638%\n",
      "Epoch 33, Batch 204, LR 0.347017 Loss 3.711304, Accuracy 92.655%\n",
      "Epoch 33, Batch 205, LR 0.346925 Loss 3.711161, Accuracy 92.668%\n",
      "Epoch 33, Batch 206, LR 0.346832 Loss 3.713051, Accuracy 92.658%\n",
      "Epoch 33, Batch 207, LR 0.346739 Loss 3.712845, Accuracy 92.667%\n",
      "Epoch 33, Batch 208, LR 0.346647 Loss 3.712686, Accuracy 92.668%\n",
      "Epoch 33, Batch 209, LR 0.346554 Loss 3.713101, Accuracy 92.658%\n",
      "Epoch 33, Batch 210, LR 0.346462 Loss 3.712628, Accuracy 92.656%\n",
      "Epoch 33, Batch 211, LR 0.346369 Loss 3.709943, Accuracy 92.665%\n",
      "Epoch 33, Batch 212, LR 0.346277 Loss 3.709430, Accuracy 92.678%\n",
      "Epoch 33, Batch 213, LR 0.346184 Loss 3.709764, Accuracy 92.672%\n",
      "Epoch 33, Batch 214, LR 0.346091 Loss 3.706693, Accuracy 92.695%\n",
      "Epoch 33, Batch 215, LR 0.345999 Loss 3.706693, Accuracy 92.689%\n",
      "Epoch 33, Batch 216, LR 0.345906 Loss 3.705453, Accuracy 92.690%\n",
      "Epoch 33, Batch 217, LR 0.345814 Loss 3.707376, Accuracy 92.684%\n",
      "Epoch 33, Batch 218, LR 0.345721 Loss 3.709755, Accuracy 92.682%\n",
      "Epoch 33, Batch 219, LR 0.345629 Loss 3.709558, Accuracy 92.683%\n",
      "Epoch 33, Batch 220, LR 0.345537 Loss 3.710208, Accuracy 92.692%\n",
      "Epoch 33, Batch 221, LR 0.345444 Loss 3.711890, Accuracy 92.697%\n",
      "Epoch 33, Batch 222, LR 0.345352 Loss 3.712205, Accuracy 92.701%\n",
      "Epoch 33, Batch 223, LR 0.345259 Loss 3.710341, Accuracy 92.713%\n",
      "Epoch 33, Batch 224, LR 0.345167 Loss 3.708680, Accuracy 92.725%\n",
      "Epoch 33, Batch 225, LR 0.345074 Loss 3.704977, Accuracy 92.736%\n",
      "Epoch 33, Batch 226, LR 0.344982 Loss 3.704633, Accuracy 92.741%\n",
      "Epoch 33, Batch 227, LR 0.344890 Loss 3.706408, Accuracy 92.735%\n",
      "Epoch 33, Batch 228, LR 0.344797 Loss 3.706955, Accuracy 92.736%\n",
      "Epoch 33, Batch 229, LR 0.344705 Loss 3.710865, Accuracy 92.706%\n",
      "Epoch 33, Batch 230, LR 0.344612 Loss 3.710376, Accuracy 92.704%\n",
      "Epoch 33, Batch 231, LR 0.344520 Loss 3.710795, Accuracy 92.712%\n",
      "Epoch 33, Batch 232, LR 0.344428 Loss 3.710586, Accuracy 92.720%\n",
      "Epoch 33, Batch 233, LR 0.344335 Loss 3.707652, Accuracy 92.724%\n",
      "Epoch 33, Batch 234, LR 0.344243 Loss 3.707483, Accuracy 92.725%\n",
      "Epoch 33, Batch 235, LR 0.344151 Loss 3.708181, Accuracy 92.726%\n",
      "Epoch 33, Batch 236, LR 0.344058 Loss 3.707349, Accuracy 92.730%\n",
      "Epoch 33, Batch 237, LR 0.343966 Loss 3.706253, Accuracy 92.728%\n",
      "Epoch 33, Batch 238, LR 0.343874 Loss 3.707780, Accuracy 92.723%\n",
      "Epoch 33, Batch 239, LR 0.343782 Loss 3.707860, Accuracy 92.724%\n",
      "Epoch 33, Batch 240, LR 0.343689 Loss 3.705535, Accuracy 92.725%\n",
      "Epoch 33, Batch 241, LR 0.343597 Loss 3.704898, Accuracy 92.719%\n",
      "Epoch 33, Batch 242, LR 0.343505 Loss 3.707026, Accuracy 92.710%\n",
      "Epoch 33, Batch 243, LR 0.343413 Loss 3.708828, Accuracy 92.699%\n",
      "Epoch 33, Batch 244, LR 0.343320 Loss 3.708287, Accuracy 92.713%\n",
      "Epoch 33, Batch 245, LR 0.343228 Loss 3.708831, Accuracy 92.730%\n",
      "Epoch 33, Batch 246, LR 0.343136 Loss 3.707320, Accuracy 92.737%\n",
      "Epoch 33, Batch 247, LR 0.343044 Loss 3.707695, Accuracy 92.738%\n",
      "Epoch 33, Batch 248, LR 0.342952 Loss 3.711575, Accuracy 92.723%\n",
      "Epoch 33, Batch 249, LR 0.342860 Loss 3.710065, Accuracy 92.737%\n",
      "Epoch 33, Batch 250, LR 0.342767 Loss 3.710249, Accuracy 92.750%\n",
      "Epoch 33, Batch 251, LR 0.342675 Loss 3.710241, Accuracy 92.760%\n",
      "Epoch 33, Batch 252, LR 0.342583 Loss 3.712619, Accuracy 92.752%\n",
      "Epoch 33, Batch 253, LR 0.342491 Loss 3.713698, Accuracy 92.740%\n",
      "Epoch 33, Batch 254, LR 0.342399 Loss 3.714862, Accuracy 92.735%\n",
      "Epoch 33, Batch 255, LR 0.342307 Loss 3.714054, Accuracy 92.730%\n",
      "Epoch 33, Batch 256, LR 0.342215 Loss 3.715942, Accuracy 92.731%\n",
      "Epoch 33, Batch 257, LR 0.342123 Loss 3.717325, Accuracy 92.719%\n",
      "Epoch 33, Batch 258, LR 0.342031 Loss 3.716205, Accuracy 92.720%\n",
      "Epoch 33, Batch 259, LR 0.341938 Loss 3.714759, Accuracy 92.721%\n",
      "Epoch 33, Batch 260, LR 0.341846 Loss 3.713486, Accuracy 92.725%\n",
      "Epoch 33, Batch 261, LR 0.341754 Loss 3.713364, Accuracy 92.726%\n",
      "Epoch 33, Batch 262, LR 0.341662 Loss 3.714101, Accuracy 92.712%\n",
      "Epoch 33, Batch 263, LR 0.341570 Loss 3.713726, Accuracy 92.713%\n",
      "Epoch 33, Batch 264, LR 0.341478 Loss 3.712117, Accuracy 92.720%\n",
      "Epoch 33, Batch 265, LR 0.341386 Loss 3.710453, Accuracy 92.727%\n",
      "Epoch 33, Batch 266, LR 0.341294 Loss 3.709046, Accuracy 92.728%\n",
      "Epoch 33, Batch 267, LR 0.341202 Loss 3.709937, Accuracy 92.729%\n",
      "Epoch 33, Batch 268, LR 0.341110 Loss 3.711179, Accuracy 92.715%\n",
      "Epoch 33, Batch 269, LR 0.341018 Loss 3.711064, Accuracy 92.710%\n",
      "Epoch 33, Batch 270, LR 0.340927 Loss 3.712184, Accuracy 92.697%\n",
      "Epoch 33, Batch 271, LR 0.340835 Loss 3.712358, Accuracy 92.704%\n",
      "Epoch 33, Batch 272, LR 0.340743 Loss 3.713230, Accuracy 92.702%\n",
      "Epoch 33, Batch 273, LR 0.340651 Loss 3.713784, Accuracy 92.705%\n",
      "Epoch 33, Batch 274, LR 0.340559 Loss 3.712735, Accuracy 92.706%\n",
      "Epoch 33, Batch 275, LR 0.340467 Loss 3.711423, Accuracy 92.713%\n",
      "Epoch 33, Batch 276, LR 0.340375 Loss 3.711609, Accuracy 92.708%\n",
      "Epoch 33, Batch 277, LR 0.340283 Loss 3.713704, Accuracy 92.698%\n",
      "Epoch 33, Batch 278, LR 0.340191 Loss 3.713791, Accuracy 92.685%\n",
      "Epoch 33, Batch 279, LR 0.340100 Loss 3.712623, Accuracy 92.689%\n",
      "Epoch 33, Batch 280, LR 0.340008 Loss 3.712916, Accuracy 92.695%\n",
      "Epoch 33, Batch 281, LR 0.339916 Loss 3.713166, Accuracy 92.688%\n",
      "Epoch 33, Batch 282, LR 0.339824 Loss 3.714553, Accuracy 92.694%\n",
      "Epoch 33, Batch 283, LR 0.339732 Loss 3.713236, Accuracy 92.695%\n",
      "Epoch 33, Batch 284, LR 0.339640 Loss 3.711701, Accuracy 92.702%\n",
      "Epoch 33, Batch 285, LR 0.339549 Loss 3.713335, Accuracy 92.700%\n",
      "Epoch 33, Batch 286, LR 0.339457 Loss 3.714307, Accuracy 92.690%\n",
      "Epoch 33, Batch 287, LR 0.339365 Loss 3.714651, Accuracy 92.675%\n",
      "Epoch 33, Batch 288, LR 0.339273 Loss 3.717457, Accuracy 92.665%\n",
      "Epoch 33, Batch 289, LR 0.339182 Loss 3.718637, Accuracy 92.663%\n",
      "Epoch 33, Batch 290, LR 0.339090 Loss 3.721117, Accuracy 92.648%\n",
      "Epoch 33, Batch 291, LR 0.338998 Loss 3.720220, Accuracy 92.655%\n",
      "Epoch 33, Batch 292, LR 0.338906 Loss 3.721218, Accuracy 92.645%\n",
      "Epoch 33, Batch 293, LR 0.338815 Loss 3.721903, Accuracy 92.641%\n",
      "Epoch 33, Batch 294, LR 0.338723 Loss 3.721293, Accuracy 92.645%\n",
      "Epoch 33, Batch 295, LR 0.338631 Loss 3.721252, Accuracy 92.643%\n",
      "Epoch 33, Batch 296, LR 0.338540 Loss 3.719521, Accuracy 92.649%\n",
      "Epoch 33, Batch 297, LR 0.338448 Loss 3.718564, Accuracy 92.653%\n",
      "Epoch 33, Batch 298, LR 0.338356 Loss 3.719702, Accuracy 92.644%\n",
      "Epoch 33, Batch 299, LR 0.338265 Loss 3.719190, Accuracy 92.634%\n",
      "Epoch 33, Batch 300, LR 0.338173 Loss 3.719711, Accuracy 92.633%\n",
      "Epoch 33, Batch 301, LR 0.338081 Loss 3.719459, Accuracy 92.634%\n",
      "Epoch 33, Batch 302, LR 0.337990 Loss 3.717512, Accuracy 92.648%\n",
      "Epoch 33, Batch 303, LR 0.337898 Loss 3.715816, Accuracy 92.662%\n",
      "Epoch 33, Batch 304, LR 0.337807 Loss 3.716976, Accuracy 92.658%\n",
      "Epoch 33, Batch 305, LR 0.337715 Loss 3.717418, Accuracy 92.651%\n",
      "Epoch 33, Batch 306, LR 0.337623 Loss 3.718297, Accuracy 92.650%\n",
      "Epoch 33, Batch 307, LR 0.337532 Loss 3.719488, Accuracy 92.640%\n",
      "Epoch 33, Batch 308, LR 0.337440 Loss 3.720061, Accuracy 92.636%\n",
      "Epoch 33, Batch 309, LR 0.337349 Loss 3.720143, Accuracy 92.638%\n",
      "Epoch 33, Batch 310, LR 0.337257 Loss 3.719507, Accuracy 92.641%\n",
      "Epoch 33, Batch 311, LR 0.337166 Loss 3.718504, Accuracy 92.642%\n",
      "Epoch 33, Batch 312, LR 0.337074 Loss 3.718294, Accuracy 92.648%\n",
      "Epoch 33, Batch 313, LR 0.336983 Loss 3.717661, Accuracy 92.652%\n",
      "Epoch 33, Batch 314, LR 0.336891 Loss 3.718491, Accuracy 92.645%\n",
      "Epoch 33, Batch 315, LR 0.336800 Loss 3.720116, Accuracy 92.649%\n",
      "Epoch 33, Batch 316, LR 0.336708 Loss 3.720133, Accuracy 92.657%\n",
      "Epoch 33, Batch 317, LR 0.336617 Loss 3.719105, Accuracy 92.658%\n",
      "Epoch 33, Batch 318, LR 0.336525 Loss 3.718880, Accuracy 92.659%\n",
      "Epoch 33, Batch 319, LR 0.336434 Loss 3.718478, Accuracy 92.660%\n",
      "Epoch 33, Batch 320, LR 0.336343 Loss 3.719090, Accuracy 92.654%\n",
      "Epoch 33, Batch 321, LR 0.336251 Loss 3.720802, Accuracy 92.655%\n",
      "Epoch 33, Batch 322, LR 0.336160 Loss 3.719998, Accuracy 92.656%\n",
      "Epoch 33, Batch 323, LR 0.336068 Loss 3.721093, Accuracy 92.654%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 324, LR 0.335977 Loss 3.719635, Accuracy 92.643%\n",
      "Epoch 33, Batch 325, LR 0.335886 Loss 3.717207, Accuracy 92.656%\n",
      "Epoch 33, Batch 326, LR 0.335794 Loss 3.716877, Accuracy 92.657%\n",
      "Epoch 33, Batch 327, LR 0.335703 Loss 3.718915, Accuracy 92.646%\n",
      "Epoch 33, Batch 328, LR 0.335612 Loss 3.719539, Accuracy 92.647%\n",
      "Epoch 33, Batch 329, LR 0.335520 Loss 3.719459, Accuracy 92.651%\n",
      "Epoch 33, Batch 330, LR 0.335429 Loss 3.719869, Accuracy 92.647%\n",
      "Epoch 33, Batch 331, LR 0.335338 Loss 3.719805, Accuracy 92.655%\n",
      "Epoch 33, Batch 332, LR 0.335246 Loss 3.720881, Accuracy 92.656%\n",
      "Epoch 33, Batch 333, LR 0.335155 Loss 3.723707, Accuracy 92.631%\n",
      "Epoch 33, Batch 334, LR 0.335064 Loss 3.724730, Accuracy 92.623%\n",
      "Epoch 33, Batch 335, LR 0.334973 Loss 3.724235, Accuracy 92.626%\n",
      "Epoch 33, Batch 336, LR 0.334881 Loss 3.724337, Accuracy 92.632%\n",
      "Epoch 33, Batch 337, LR 0.334790 Loss 3.724821, Accuracy 92.623%\n",
      "Epoch 33, Batch 338, LR 0.334699 Loss 3.723536, Accuracy 92.624%\n",
      "Epoch 33, Batch 339, LR 0.334608 Loss 3.723529, Accuracy 92.623%\n",
      "Epoch 33, Batch 340, LR 0.334516 Loss 3.722985, Accuracy 92.638%\n",
      "Epoch 33, Batch 341, LR 0.334425 Loss 3.723116, Accuracy 92.639%\n",
      "Epoch 33, Batch 342, LR 0.334334 Loss 3.722173, Accuracy 92.649%\n",
      "Epoch 33, Batch 343, LR 0.334243 Loss 3.722694, Accuracy 92.657%\n",
      "Epoch 33, Batch 344, LR 0.334152 Loss 3.723492, Accuracy 92.653%\n",
      "Epoch 33, Batch 345, LR 0.334061 Loss 3.722590, Accuracy 92.652%\n",
      "Epoch 33, Batch 346, LR 0.333969 Loss 3.722037, Accuracy 92.655%\n",
      "Epoch 33, Batch 347, LR 0.333878 Loss 3.721437, Accuracy 92.651%\n",
      "Epoch 33, Batch 348, LR 0.333787 Loss 3.719650, Accuracy 92.652%\n",
      "Epoch 33, Batch 349, LR 0.333696 Loss 3.719461, Accuracy 92.655%\n",
      "Epoch 33, Batch 350, LR 0.333605 Loss 3.718121, Accuracy 92.661%\n",
      "Epoch 33, Batch 351, LR 0.333514 Loss 3.715872, Accuracy 92.664%\n",
      "Epoch 33, Batch 352, LR 0.333423 Loss 3.715365, Accuracy 92.665%\n",
      "Epoch 33, Batch 353, LR 0.333332 Loss 3.713738, Accuracy 92.663%\n",
      "Epoch 33, Batch 354, LR 0.333241 Loss 3.711265, Accuracy 92.675%\n",
      "Epoch 33, Batch 355, LR 0.333150 Loss 3.709502, Accuracy 92.678%\n",
      "Epoch 33, Batch 356, LR 0.333058 Loss 3.709450, Accuracy 92.672%\n",
      "Epoch 33, Batch 357, LR 0.332967 Loss 3.710367, Accuracy 92.671%\n",
      "Epoch 33, Batch 358, LR 0.332876 Loss 3.710055, Accuracy 92.672%\n",
      "Epoch 33, Batch 359, LR 0.332785 Loss 3.710961, Accuracy 92.671%\n",
      "Epoch 33, Batch 360, LR 0.332694 Loss 3.713153, Accuracy 92.661%\n",
      "Epoch 33, Batch 361, LR 0.332603 Loss 3.712416, Accuracy 92.666%\n",
      "Epoch 33, Batch 362, LR 0.332512 Loss 3.712949, Accuracy 92.667%\n",
      "Epoch 33, Batch 363, LR 0.332421 Loss 3.712609, Accuracy 92.665%\n",
      "Epoch 33, Batch 364, LR 0.332331 Loss 3.711948, Accuracy 92.662%\n",
      "Epoch 33, Batch 365, LR 0.332240 Loss 3.711589, Accuracy 92.663%\n",
      "Epoch 33, Batch 366, LR 0.332149 Loss 3.711842, Accuracy 92.664%\n",
      "Epoch 33, Batch 367, LR 0.332058 Loss 3.711698, Accuracy 92.658%\n",
      "Epoch 33, Batch 368, LR 0.331967 Loss 3.712941, Accuracy 92.648%\n",
      "Epoch 33, Batch 369, LR 0.331876 Loss 3.713238, Accuracy 92.651%\n",
      "Epoch 33, Batch 370, LR 0.331785 Loss 3.713559, Accuracy 92.637%\n",
      "Epoch 33, Batch 371, LR 0.331694 Loss 3.712327, Accuracy 92.644%\n",
      "Epoch 33, Batch 372, LR 0.331603 Loss 3.713225, Accuracy 92.643%\n",
      "Epoch 33, Batch 373, LR 0.331512 Loss 3.713881, Accuracy 92.632%\n",
      "Epoch 33, Batch 374, LR 0.331422 Loss 3.712396, Accuracy 92.630%\n",
      "Epoch 33, Batch 375, LR 0.331331 Loss 3.711077, Accuracy 92.631%\n",
      "Epoch 33, Batch 376, LR 0.331240 Loss 3.713694, Accuracy 92.624%\n",
      "Epoch 33, Batch 377, LR 0.331149 Loss 3.713869, Accuracy 92.616%\n",
      "Epoch 33, Batch 378, LR 0.331058 Loss 3.714011, Accuracy 92.615%\n",
      "Epoch 33, Batch 379, LR 0.330967 Loss 3.713026, Accuracy 92.620%\n",
      "Epoch 33, Batch 380, LR 0.330877 Loss 3.714098, Accuracy 92.613%\n",
      "Epoch 33, Batch 381, LR 0.330786 Loss 3.713365, Accuracy 92.616%\n",
      "Epoch 33, Batch 382, LR 0.330695 Loss 3.715934, Accuracy 92.609%\n",
      "Epoch 33, Batch 383, LR 0.330604 Loss 3.716409, Accuracy 92.608%\n",
      "Epoch 33, Batch 384, LR 0.330514 Loss 3.715859, Accuracy 92.609%\n",
      "Epoch 33, Batch 385, LR 0.330423 Loss 3.716270, Accuracy 92.606%\n",
      "Epoch 33, Batch 386, LR 0.330332 Loss 3.717237, Accuracy 92.598%\n",
      "Epoch 33, Batch 387, LR 0.330241 Loss 3.717798, Accuracy 92.599%\n",
      "Epoch 33, Batch 388, LR 0.330151 Loss 3.716979, Accuracy 92.602%\n",
      "Epoch 33, Batch 389, LR 0.330060 Loss 3.717019, Accuracy 92.601%\n",
      "Epoch 33, Batch 390, LR 0.329969 Loss 3.715044, Accuracy 92.616%\n",
      "Epoch 33, Batch 391, LR 0.329879 Loss 3.714205, Accuracy 92.617%\n",
      "Epoch 33, Batch 392, LR 0.329788 Loss 3.714299, Accuracy 92.620%\n",
      "Epoch 33, Batch 393, LR 0.329697 Loss 3.713654, Accuracy 92.627%\n",
      "Epoch 33, Batch 394, LR 0.329607 Loss 3.714691, Accuracy 92.620%\n",
      "Epoch 33, Batch 395, LR 0.329516 Loss 3.713681, Accuracy 92.623%\n",
      "Epoch 33, Batch 396, LR 0.329425 Loss 3.713036, Accuracy 92.622%\n",
      "Epoch 33, Batch 397, LR 0.329335 Loss 3.711904, Accuracy 92.628%\n",
      "Epoch 33, Batch 398, LR 0.329244 Loss 3.710641, Accuracy 92.639%\n",
      "Epoch 33, Batch 399, LR 0.329154 Loss 3.710716, Accuracy 92.634%\n",
      "Epoch 33, Batch 400, LR 0.329063 Loss 3.711341, Accuracy 92.629%\n",
      "Epoch 33, Batch 401, LR 0.328972 Loss 3.711852, Accuracy 92.628%\n",
      "Epoch 33, Batch 402, LR 0.328882 Loss 3.711680, Accuracy 92.629%\n",
      "Epoch 33, Batch 403, LR 0.328791 Loss 3.712314, Accuracy 92.628%\n",
      "Epoch 33, Batch 404, LR 0.328701 Loss 3.711358, Accuracy 92.638%\n",
      "Epoch 33, Batch 405, LR 0.328610 Loss 3.710332, Accuracy 92.649%\n",
      "Epoch 33, Batch 406, LR 0.328520 Loss 3.710885, Accuracy 92.638%\n",
      "Epoch 33, Batch 407, LR 0.328429 Loss 3.711076, Accuracy 92.639%\n",
      "Epoch 33, Batch 408, LR 0.328339 Loss 3.712556, Accuracy 92.632%\n",
      "Epoch 33, Batch 409, LR 0.328248 Loss 3.712162, Accuracy 92.631%\n",
      "Epoch 33, Batch 410, LR 0.328158 Loss 3.712896, Accuracy 92.626%\n",
      "Epoch 33, Batch 411, LR 0.328067 Loss 3.713341, Accuracy 92.625%\n",
      "Epoch 33, Batch 412, LR 0.327977 Loss 3.713950, Accuracy 92.631%\n",
      "Epoch 33, Batch 413, LR 0.327886 Loss 3.714076, Accuracy 92.634%\n",
      "Epoch 33, Batch 414, LR 0.327796 Loss 3.713700, Accuracy 92.639%\n",
      "Epoch 33, Batch 415, LR 0.327706 Loss 3.713602, Accuracy 92.639%\n",
      "Epoch 33, Batch 416, LR 0.327615 Loss 3.714158, Accuracy 92.642%\n",
      "Epoch 33, Batch 417, LR 0.327525 Loss 3.714221, Accuracy 92.639%\n",
      "Epoch 33, Batch 418, LR 0.327434 Loss 3.713441, Accuracy 92.642%\n",
      "Epoch 33, Batch 419, LR 0.327344 Loss 3.714188, Accuracy 92.629%\n",
      "Epoch 33, Batch 420, LR 0.327254 Loss 3.714665, Accuracy 92.630%\n",
      "Epoch 33, Batch 421, LR 0.327163 Loss 3.714111, Accuracy 92.627%\n",
      "Epoch 33, Batch 422, LR 0.327073 Loss 3.714239, Accuracy 92.632%\n",
      "Epoch 33, Batch 423, LR 0.326983 Loss 3.712983, Accuracy 92.642%\n",
      "Epoch 33, Batch 424, LR 0.326892 Loss 3.713315, Accuracy 92.646%\n",
      "Epoch 33, Batch 425, LR 0.326802 Loss 3.713993, Accuracy 92.647%\n",
      "Epoch 33, Batch 426, LR 0.326712 Loss 3.715869, Accuracy 92.639%\n",
      "Epoch 33, Batch 427, LR 0.326621 Loss 3.715751, Accuracy 92.645%\n",
      "Epoch 33, Batch 428, LR 0.326531 Loss 3.716177, Accuracy 92.642%\n",
      "Epoch 33, Batch 429, LR 0.326441 Loss 3.716329, Accuracy 92.641%\n",
      "Epoch 33, Batch 430, LR 0.326351 Loss 3.716565, Accuracy 92.649%\n",
      "Epoch 33, Batch 431, LR 0.326260 Loss 3.716512, Accuracy 92.641%\n",
      "Epoch 33, Batch 432, LR 0.326170 Loss 3.717675, Accuracy 92.636%\n",
      "Epoch 33, Batch 433, LR 0.326080 Loss 3.717461, Accuracy 92.637%\n",
      "Epoch 33, Batch 434, LR 0.325990 Loss 3.716669, Accuracy 92.636%\n",
      "Epoch 33, Batch 435, LR 0.325899 Loss 3.717745, Accuracy 92.629%\n",
      "Epoch 33, Batch 436, LR 0.325809 Loss 3.716893, Accuracy 92.632%\n",
      "Epoch 33, Batch 437, LR 0.325719 Loss 3.717010, Accuracy 92.634%\n",
      "Epoch 33, Batch 438, LR 0.325629 Loss 3.717699, Accuracy 92.625%\n",
      "Epoch 33, Batch 439, LR 0.325539 Loss 3.717335, Accuracy 92.615%\n",
      "Epoch 33, Batch 440, LR 0.325449 Loss 3.716364, Accuracy 92.619%\n",
      "Epoch 33, Batch 441, LR 0.325358 Loss 3.716271, Accuracy 92.622%\n",
      "Epoch 33, Batch 442, LR 0.325268 Loss 3.715212, Accuracy 92.628%\n",
      "Epoch 33, Batch 443, LR 0.325178 Loss 3.714839, Accuracy 92.628%\n",
      "Epoch 33, Batch 444, LR 0.325088 Loss 3.714992, Accuracy 92.629%\n",
      "Epoch 33, Batch 445, LR 0.324998 Loss 3.714815, Accuracy 92.632%\n",
      "Epoch 33, Batch 446, LR 0.324908 Loss 3.715787, Accuracy 92.629%\n",
      "Epoch 33, Batch 447, LR 0.324818 Loss 3.714499, Accuracy 92.624%\n",
      "Epoch 33, Batch 448, LR 0.324728 Loss 3.713556, Accuracy 92.625%\n",
      "Epoch 33, Batch 449, LR 0.324638 Loss 3.712314, Accuracy 92.628%\n",
      "Epoch 33, Batch 450, LR 0.324548 Loss 3.711759, Accuracy 92.632%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 451, LR 0.324458 Loss 3.712408, Accuracy 92.629%\n",
      "Epoch 33, Batch 452, LR 0.324368 Loss 3.713742, Accuracy 92.630%\n",
      "Epoch 33, Batch 453, LR 0.324278 Loss 3.714713, Accuracy 92.627%\n",
      "Epoch 33, Batch 454, LR 0.324188 Loss 3.714838, Accuracy 92.626%\n",
      "Epoch 33, Batch 455, LR 0.324098 Loss 3.714344, Accuracy 92.625%\n",
      "Epoch 33, Batch 456, LR 0.324008 Loss 3.714991, Accuracy 92.630%\n",
      "Epoch 33, Batch 457, LR 0.323918 Loss 3.715624, Accuracy 92.629%\n",
      "Epoch 33, Batch 458, LR 0.323828 Loss 3.714399, Accuracy 92.638%\n",
      "Epoch 33, Batch 459, LR 0.323738 Loss 3.715581, Accuracy 92.627%\n",
      "Epoch 33, Batch 460, LR 0.323648 Loss 3.713183, Accuracy 92.636%\n",
      "Epoch 33, Batch 461, LR 0.323558 Loss 3.714514, Accuracy 92.625%\n",
      "Epoch 33, Batch 462, LR 0.323468 Loss 3.715508, Accuracy 92.622%\n",
      "Epoch 33, Batch 463, LR 0.323378 Loss 3.715889, Accuracy 92.628%\n",
      "Epoch 33, Batch 464, LR 0.323288 Loss 3.714977, Accuracy 92.632%\n",
      "Epoch 33, Batch 465, LR 0.323198 Loss 3.714538, Accuracy 92.638%\n",
      "Epoch 33, Batch 466, LR 0.323108 Loss 3.713775, Accuracy 92.644%\n",
      "Epoch 33, Batch 467, LR 0.323018 Loss 3.713294, Accuracy 92.646%\n",
      "Epoch 33, Batch 468, LR 0.322929 Loss 3.714046, Accuracy 92.635%\n",
      "Epoch 33, Batch 469, LR 0.322839 Loss 3.714711, Accuracy 92.631%\n",
      "Epoch 33, Batch 470, LR 0.322749 Loss 3.715841, Accuracy 92.633%\n",
      "Epoch 33, Batch 471, LR 0.322659 Loss 3.716462, Accuracy 92.625%\n",
      "Epoch 33, Batch 472, LR 0.322569 Loss 3.715884, Accuracy 92.626%\n",
      "Epoch 33, Batch 473, LR 0.322479 Loss 3.716328, Accuracy 92.629%\n",
      "Epoch 33, Batch 474, LR 0.322390 Loss 3.715477, Accuracy 92.631%\n",
      "Epoch 33, Batch 475, LR 0.322300 Loss 3.715982, Accuracy 92.628%\n",
      "Epoch 33, Batch 476, LR 0.322210 Loss 3.717116, Accuracy 92.624%\n",
      "Epoch 33, Batch 477, LR 0.322120 Loss 3.717524, Accuracy 92.623%\n",
      "Epoch 33, Batch 478, LR 0.322031 Loss 3.717545, Accuracy 92.626%\n",
      "Epoch 33, Batch 479, LR 0.321941 Loss 3.716961, Accuracy 92.626%\n",
      "Epoch 33, Batch 480, LR 0.321851 Loss 3.716259, Accuracy 92.630%\n",
      "Epoch 33, Batch 481, LR 0.321761 Loss 3.715444, Accuracy 92.636%\n",
      "Epoch 33, Batch 482, LR 0.321672 Loss 3.715381, Accuracy 92.633%\n",
      "Epoch 33, Batch 483, LR 0.321582 Loss 3.714435, Accuracy 92.627%\n",
      "Epoch 33, Batch 484, LR 0.321492 Loss 3.713845, Accuracy 92.636%\n",
      "Epoch 33, Batch 485, LR 0.321403 Loss 3.714321, Accuracy 92.640%\n",
      "Epoch 33, Batch 486, LR 0.321313 Loss 3.713501, Accuracy 92.642%\n",
      "Epoch 33, Batch 487, LR 0.321223 Loss 3.713930, Accuracy 92.641%\n",
      "Epoch 33, Batch 488, LR 0.321134 Loss 3.712896, Accuracy 92.647%\n",
      "Epoch 33, Batch 489, LR 0.321044 Loss 3.713886, Accuracy 92.640%\n",
      "Epoch 33, Batch 490, LR 0.320954 Loss 3.712776, Accuracy 92.643%\n",
      "Epoch 33, Batch 491, LR 0.320865 Loss 3.713751, Accuracy 92.643%\n",
      "Epoch 33, Batch 492, LR 0.320775 Loss 3.713522, Accuracy 92.645%\n",
      "Epoch 33, Batch 493, LR 0.320686 Loss 3.713384, Accuracy 92.645%\n",
      "Epoch 33, Batch 494, LR 0.320596 Loss 3.713552, Accuracy 92.648%\n",
      "Epoch 33, Batch 495, LR 0.320506 Loss 3.713861, Accuracy 92.644%\n",
      "Epoch 33, Batch 496, LR 0.320417 Loss 3.713968, Accuracy 92.644%\n",
      "Epoch 33, Batch 497, LR 0.320327 Loss 3.714332, Accuracy 92.640%\n",
      "Epoch 33, Batch 498, LR 0.320238 Loss 3.715904, Accuracy 92.635%\n",
      "Epoch 33, Batch 499, LR 0.320148 Loss 3.714592, Accuracy 92.638%\n",
      "Epoch 33, Batch 500, LR 0.320059 Loss 3.713213, Accuracy 92.641%\n",
      "Epoch 33, Batch 501, LR 0.319969 Loss 3.712802, Accuracy 92.648%\n",
      "Epoch 33, Batch 502, LR 0.319880 Loss 3.712844, Accuracy 92.647%\n",
      "Epoch 33, Batch 503, LR 0.319790 Loss 3.712902, Accuracy 92.647%\n",
      "Epoch 33, Batch 504, LR 0.319701 Loss 3.714909, Accuracy 92.635%\n",
      "Epoch 33, Batch 505, LR 0.319611 Loss 3.714662, Accuracy 92.638%\n",
      "Epoch 33, Batch 506, LR 0.319522 Loss 3.714563, Accuracy 92.638%\n",
      "Epoch 33, Batch 507, LR 0.319432 Loss 3.714783, Accuracy 92.634%\n",
      "Epoch 33, Batch 508, LR 0.319343 Loss 3.715075, Accuracy 92.635%\n",
      "Epoch 33, Batch 509, LR 0.319254 Loss 3.714549, Accuracy 92.634%\n",
      "Epoch 33, Batch 510, LR 0.319164 Loss 3.713543, Accuracy 92.641%\n",
      "Epoch 33, Batch 511, LR 0.319075 Loss 3.713288, Accuracy 92.646%\n",
      "Epoch 33, Batch 512, LR 0.318985 Loss 3.713068, Accuracy 92.644%\n",
      "Epoch 33, Batch 513, LR 0.318896 Loss 3.712854, Accuracy 92.644%\n",
      "Epoch 33, Batch 514, LR 0.318807 Loss 3.713703, Accuracy 92.639%\n",
      "Epoch 33, Batch 515, LR 0.318717 Loss 3.715692, Accuracy 92.630%\n",
      "Epoch 33, Batch 516, LR 0.318628 Loss 3.715003, Accuracy 92.637%\n",
      "Epoch 33, Batch 517, LR 0.318539 Loss 3.716632, Accuracy 92.626%\n",
      "Epoch 33, Batch 518, LR 0.318449 Loss 3.715491, Accuracy 92.629%\n",
      "Epoch 33, Batch 519, LR 0.318360 Loss 3.715854, Accuracy 92.633%\n",
      "Epoch 33, Batch 520, LR 0.318271 Loss 3.717281, Accuracy 92.625%\n",
      "Epoch 33, Batch 521, LR 0.318181 Loss 3.716782, Accuracy 92.625%\n",
      "Epoch 33, Batch 522, LR 0.318092 Loss 3.716939, Accuracy 92.625%\n",
      "Epoch 33, Batch 523, LR 0.318003 Loss 3.718058, Accuracy 92.621%\n",
      "Epoch 33, Batch 524, LR 0.317914 Loss 3.717707, Accuracy 92.623%\n",
      "Epoch 33, Batch 525, LR 0.317824 Loss 3.717477, Accuracy 92.624%\n",
      "Epoch 33, Batch 526, LR 0.317735 Loss 3.717383, Accuracy 92.627%\n",
      "Epoch 33, Batch 527, LR 0.317646 Loss 3.716906, Accuracy 92.628%\n",
      "Epoch 33, Batch 528, LR 0.317557 Loss 3.716882, Accuracy 92.630%\n",
      "Epoch 33, Batch 529, LR 0.317467 Loss 3.716410, Accuracy 92.636%\n",
      "Epoch 33, Batch 530, LR 0.317378 Loss 3.715091, Accuracy 92.642%\n",
      "Epoch 33, Batch 531, LR 0.317289 Loss 3.714954, Accuracy 92.641%\n",
      "Epoch 33, Batch 532, LR 0.317200 Loss 3.715418, Accuracy 92.640%\n",
      "Epoch 33, Batch 533, LR 0.317111 Loss 3.716092, Accuracy 92.635%\n",
      "Epoch 33, Batch 534, LR 0.317022 Loss 3.716388, Accuracy 92.628%\n",
      "Epoch 33, Batch 535, LR 0.316932 Loss 3.716424, Accuracy 92.626%\n",
      "Epoch 33, Batch 536, LR 0.316843 Loss 3.716654, Accuracy 92.628%\n",
      "Epoch 33, Batch 537, LR 0.316754 Loss 3.717842, Accuracy 92.618%\n",
      "Epoch 33, Batch 538, LR 0.316665 Loss 3.718343, Accuracy 92.617%\n",
      "Epoch 33, Batch 539, LR 0.316576 Loss 3.717835, Accuracy 92.618%\n",
      "Epoch 33, Batch 540, LR 0.316487 Loss 3.718187, Accuracy 92.611%\n",
      "Epoch 33, Batch 541, LR 0.316398 Loss 3.717369, Accuracy 92.616%\n",
      "Epoch 33, Batch 542, LR 0.316309 Loss 3.718976, Accuracy 92.610%\n",
      "Epoch 33, Batch 543, LR 0.316220 Loss 3.718971, Accuracy 92.615%\n",
      "Epoch 33, Batch 544, LR 0.316131 Loss 3.717641, Accuracy 92.623%\n",
      "Epoch 33, Batch 545, LR 0.316042 Loss 3.717894, Accuracy 92.620%\n",
      "Epoch 33, Batch 546, LR 0.315953 Loss 3.718379, Accuracy 92.615%\n",
      "Epoch 33, Batch 547, LR 0.315864 Loss 3.717457, Accuracy 92.617%\n",
      "Epoch 33, Batch 548, LR 0.315775 Loss 3.717445, Accuracy 92.618%\n",
      "Epoch 33, Batch 549, LR 0.315686 Loss 3.716198, Accuracy 92.619%\n",
      "Epoch 33, Batch 550, LR 0.315597 Loss 3.716174, Accuracy 92.614%\n",
      "Epoch 33, Batch 551, LR 0.315508 Loss 3.716352, Accuracy 92.617%\n",
      "Epoch 33, Batch 552, LR 0.315419 Loss 3.716454, Accuracy 92.615%\n",
      "Epoch 33, Batch 553, LR 0.315330 Loss 3.716286, Accuracy 92.614%\n",
      "Epoch 33, Batch 554, LR 0.315241 Loss 3.716111, Accuracy 92.613%\n",
      "Epoch 33, Batch 555, LR 0.315152 Loss 3.715833, Accuracy 92.611%\n",
      "Epoch 33, Batch 556, LR 0.315063 Loss 3.716530, Accuracy 92.612%\n",
      "Epoch 33, Batch 557, LR 0.314974 Loss 3.717064, Accuracy 92.611%\n",
      "Epoch 33, Batch 558, LR 0.314885 Loss 3.717689, Accuracy 92.605%\n",
      "Epoch 33, Batch 559, LR 0.314796 Loss 3.717816, Accuracy 92.601%\n",
      "Epoch 33, Batch 560, LR 0.314707 Loss 3.717980, Accuracy 92.592%\n",
      "Epoch 33, Batch 561, LR 0.314619 Loss 3.719214, Accuracy 92.586%\n",
      "Epoch 33, Batch 562, LR 0.314530 Loss 3.718199, Accuracy 92.585%\n",
      "Epoch 33, Batch 563, LR 0.314441 Loss 3.718120, Accuracy 92.589%\n",
      "Epoch 33, Batch 564, LR 0.314352 Loss 3.717828, Accuracy 92.588%\n",
      "Epoch 33, Batch 565, LR 0.314263 Loss 3.717901, Accuracy 92.590%\n",
      "Epoch 33, Batch 566, LR 0.314174 Loss 3.718170, Accuracy 92.586%\n",
      "Epoch 33, Batch 567, LR 0.314086 Loss 3.718034, Accuracy 92.587%\n",
      "Epoch 33, Batch 568, LR 0.313997 Loss 3.718165, Accuracy 92.588%\n",
      "Epoch 33, Batch 569, LR 0.313908 Loss 3.717837, Accuracy 92.593%\n",
      "Epoch 33, Batch 570, LR 0.313819 Loss 3.718391, Accuracy 92.595%\n",
      "Epoch 33, Batch 571, LR 0.313730 Loss 3.718997, Accuracy 92.588%\n",
      "Epoch 33, Batch 572, LR 0.313642 Loss 3.720000, Accuracy 92.579%\n",
      "Epoch 33, Batch 573, LR 0.313553 Loss 3.719767, Accuracy 92.582%\n",
      "Epoch 33, Batch 574, LR 0.313464 Loss 3.719211, Accuracy 92.585%\n",
      "Epoch 33, Batch 575, LR 0.313376 Loss 3.719351, Accuracy 92.582%\n",
      "Epoch 33, Batch 576, LR 0.313287 Loss 3.720388, Accuracy 92.581%\n",
      "Epoch 33, Batch 577, LR 0.313198 Loss 3.720226, Accuracy 92.584%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 578, LR 0.313109 Loss 3.721073, Accuracy 92.579%\n",
      "Epoch 33, Batch 579, LR 0.313021 Loss 3.720904, Accuracy 92.583%\n",
      "Epoch 33, Batch 580, LR 0.312932 Loss 3.720990, Accuracy 92.582%\n",
      "Epoch 33, Batch 581, LR 0.312844 Loss 3.719961, Accuracy 92.587%\n",
      "Epoch 33, Batch 582, LR 0.312755 Loss 3.719733, Accuracy 92.592%\n",
      "Epoch 33, Batch 583, LR 0.312666 Loss 3.719745, Accuracy 92.592%\n",
      "Epoch 33, Batch 584, LR 0.312578 Loss 3.720286, Accuracy 92.589%\n",
      "Epoch 33, Batch 585, LR 0.312489 Loss 3.719810, Accuracy 92.592%\n",
      "Epoch 33, Batch 586, LR 0.312400 Loss 3.720855, Accuracy 92.585%\n",
      "Epoch 33, Batch 587, LR 0.312312 Loss 3.721592, Accuracy 92.580%\n",
      "Epoch 33, Batch 588, LR 0.312223 Loss 3.720137, Accuracy 92.586%\n",
      "Epoch 33, Batch 589, LR 0.312135 Loss 3.720170, Accuracy 92.585%\n",
      "Epoch 33, Batch 590, LR 0.312046 Loss 3.720309, Accuracy 92.582%\n",
      "Epoch 33, Batch 591, LR 0.311958 Loss 3.721195, Accuracy 92.580%\n",
      "Epoch 33, Batch 592, LR 0.311869 Loss 3.720657, Accuracy 92.582%\n",
      "Epoch 33, Batch 593, LR 0.311781 Loss 3.721634, Accuracy 92.575%\n",
      "Epoch 33, Batch 594, LR 0.311692 Loss 3.721206, Accuracy 92.577%\n",
      "Epoch 33, Batch 595, LR 0.311604 Loss 3.722266, Accuracy 92.571%\n",
      "Epoch 33, Batch 596, LR 0.311515 Loss 3.722227, Accuracy 92.573%\n",
      "Epoch 33, Batch 597, LR 0.311427 Loss 3.722431, Accuracy 92.570%\n",
      "Epoch 33, Batch 598, LR 0.311338 Loss 3.723236, Accuracy 92.561%\n",
      "Epoch 33, Batch 599, LR 0.311250 Loss 3.723351, Accuracy 92.561%\n",
      "Epoch 33, Batch 600, LR 0.311161 Loss 3.723481, Accuracy 92.560%\n",
      "Epoch 33, Batch 601, LR 0.311073 Loss 3.724186, Accuracy 92.561%\n",
      "Epoch 33, Batch 602, LR 0.310984 Loss 3.724501, Accuracy 92.565%\n",
      "Epoch 33, Batch 603, LR 0.310896 Loss 3.724064, Accuracy 92.567%\n",
      "Epoch 33, Batch 604, LR 0.310808 Loss 3.724555, Accuracy 92.565%\n",
      "Epoch 33, Batch 605, LR 0.310719 Loss 3.724607, Accuracy 92.561%\n",
      "Epoch 33, Batch 606, LR 0.310631 Loss 3.725337, Accuracy 92.550%\n",
      "Epoch 33, Batch 607, LR 0.310542 Loss 3.724891, Accuracy 92.552%\n",
      "Epoch 33, Batch 608, LR 0.310454 Loss 3.725483, Accuracy 92.554%\n",
      "Epoch 33, Batch 609, LR 0.310366 Loss 3.724973, Accuracy 92.557%\n",
      "Epoch 33, Batch 610, LR 0.310277 Loss 3.724649, Accuracy 92.553%\n",
      "Epoch 33, Batch 611, LR 0.310189 Loss 3.725215, Accuracy 92.556%\n",
      "Epoch 33, Batch 612, LR 0.310101 Loss 3.725317, Accuracy 92.556%\n",
      "Epoch 33, Batch 613, LR 0.310012 Loss 3.724530, Accuracy 92.557%\n",
      "Epoch 33, Batch 614, LR 0.309924 Loss 3.724019, Accuracy 92.554%\n",
      "Epoch 33, Batch 615, LR 0.309836 Loss 3.724320, Accuracy 92.555%\n",
      "Epoch 33, Batch 616, LR 0.309748 Loss 3.724882, Accuracy 92.549%\n",
      "Epoch 33, Batch 617, LR 0.309659 Loss 3.724453, Accuracy 92.551%\n",
      "Epoch 33, Batch 618, LR 0.309571 Loss 3.723786, Accuracy 92.554%\n",
      "Epoch 33, Batch 619, LR 0.309483 Loss 3.724466, Accuracy 92.554%\n",
      "Epoch 33, Batch 620, LR 0.309395 Loss 3.724342, Accuracy 92.550%\n",
      "Epoch 33, Batch 621, LR 0.309306 Loss 3.723142, Accuracy 92.562%\n",
      "Epoch 33, Batch 622, LR 0.309218 Loss 3.723207, Accuracy 92.557%\n",
      "Epoch 33, Batch 623, LR 0.309130 Loss 3.723041, Accuracy 92.557%\n",
      "Epoch 33, Batch 624, LR 0.309042 Loss 3.722477, Accuracy 92.561%\n",
      "Epoch 33, Batch 625, LR 0.308954 Loss 3.723830, Accuracy 92.551%\n",
      "Epoch 33, Batch 626, LR 0.308866 Loss 3.723529, Accuracy 92.557%\n",
      "Epoch 33, Batch 627, LR 0.308777 Loss 3.723312, Accuracy 92.560%\n",
      "Epoch 33, Batch 628, LR 0.308689 Loss 3.724344, Accuracy 92.556%\n",
      "Epoch 33, Batch 629, LR 0.308601 Loss 3.724604, Accuracy 92.556%\n",
      "Epoch 33, Batch 630, LR 0.308513 Loss 3.725386, Accuracy 92.551%\n",
      "Epoch 33, Batch 631, LR 0.308425 Loss 3.726300, Accuracy 92.545%\n",
      "Epoch 33, Batch 632, LR 0.308337 Loss 3.725858, Accuracy 92.544%\n",
      "Epoch 33, Batch 633, LR 0.308249 Loss 3.725852, Accuracy 92.547%\n",
      "Epoch 33, Batch 634, LR 0.308161 Loss 3.726764, Accuracy 92.544%\n",
      "Epoch 33, Batch 635, LR 0.308073 Loss 3.726819, Accuracy 92.543%\n",
      "Epoch 33, Batch 636, LR 0.307985 Loss 3.726153, Accuracy 92.544%\n",
      "Epoch 33, Batch 637, LR 0.307896 Loss 3.726087, Accuracy 92.542%\n",
      "Epoch 33, Batch 638, LR 0.307808 Loss 3.726239, Accuracy 92.541%\n",
      "Epoch 33, Batch 639, LR 0.307720 Loss 3.726401, Accuracy 92.542%\n",
      "Epoch 33, Batch 640, LR 0.307632 Loss 3.727876, Accuracy 92.534%\n",
      "Epoch 33, Batch 641, LR 0.307544 Loss 3.728317, Accuracy 92.534%\n",
      "Epoch 33, Batch 642, LR 0.307456 Loss 3.727797, Accuracy 92.536%\n",
      "Epoch 33, Batch 643, LR 0.307368 Loss 3.727895, Accuracy 92.530%\n",
      "Epoch 33, Batch 644, LR 0.307280 Loss 3.728258, Accuracy 92.527%\n",
      "Epoch 33, Batch 645, LR 0.307193 Loss 3.727991, Accuracy 92.529%\n",
      "Epoch 33, Batch 646, LR 0.307105 Loss 3.728495, Accuracy 92.526%\n",
      "Epoch 33, Batch 647, LR 0.307017 Loss 3.729214, Accuracy 92.517%\n",
      "Epoch 33, Batch 648, LR 0.306929 Loss 3.728851, Accuracy 92.518%\n",
      "Epoch 33, Batch 649, LR 0.306841 Loss 3.727211, Accuracy 92.526%\n",
      "Epoch 33, Batch 650, LR 0.306753 Loss 3.726983, Accuracy 92.524%\n",
      "Epoch 33, Batch 651, LR 0.306665 Loss 3.727026, Accuracy 92.521%\n",
      "Epoch 33, Batch 652, LR 0.306577 Loss 3.726837, Accuracy 92.522%\n",
      "Epoch 33, Batch 653, LR 0.306489 Loss 3.726593, Accuracy 92.521%\n",
      "Epoch 33, Batch 654, LR 0.306401 Loss 3.727787, Accuracy 92.516%\n",
      "Epoch 33, Batch 655, LR 0.306314 Loss 3.728231, Accuracy 92.517%\n",
      "Epoch 33, Batch 656, LR 0.306226 Loss 3.728679, Accuracy 92.519%\n",
      "Epoch 33, Batch 657, LR 0.306138 Loss 3.728685, Accuracy 92.520%\n",
      "Epoch 33, Batch 658, LR 0.306050 Loss 3.728594, Accuracy 92.522%\n",
      "Epoch 33, Batch 659, LR 0.305962 Loss 3.728371, Accuracy 92.523%\n",
      "Epoch 33, Batch 660, LR 0.305874 Loss 3.728903, Accuracy 92.520%\n",
      "Epoch 33, Batch 661, LR 0.305787 Loss 3.727669, Accuracy 92.522%\n",
      "Epoch 33, Batch 662, LR 0.305699 Loss 3.727144, Accuracy 92.523%\n",
      "Epoch 33, Batch 663, LR 0.305611 Loss 3.726409, Accuracy 92.525%\n",
      "Epoch 33, Batch 664, LR 0.305523 Loss 3.725908, Accuracy 92.528%\n",
      "Epoch 33, Batch 665, LR 0.305436 Loss 3.725260, Accuracy 92.533%\n",
      "Epoch 33, Batch 666, LR 0.305348 Loss 3.726136, Accuracy 92.528%\n",
      "Epoch 33, Batch 667, LR 0.305260 Loss 3.726719, Accuracy 92.526%\n",
      "Epoch 33, Batch 668, LR 0.305173 Loss 3.727205, Accuracy 92.522%\n",
      "Epoch 33, Batch 669, LR 0.305085 Loss 3.727366, Accuracy 92.523%\n",
      "Epoch 33, Batch 670, LR 0.304997 Loss 3.727614, Accuracy 92.522%\n",
      "Epoch 33, Batch 671, LR 0.304909 Loss 3.728033, Accuracy 92.520%\n",
      "Epoch 33, Batch 672, LR 0.304822 Loss 3.728908, Accuracy 92.513%\n",
      "Epoch 33, Batch 673, LR 0.304734 Loss 3.729155, Accuracy 92.507%\n",
      "Epoch 33, Batch 674, LR 0.304646 Loss 3.729566, Accuracy 92.503%\n",
      "Epoch 33, Batch 675, LR 0.304559 Loss 3.729572, Accuracy 92.503%\n",
      "Epoch 33, Batch 676, LR 0.304471 Loss 3.729577, Accuracy 92.503%\n",
      "Epoch 33, Batch 677, LR 0.304384 Loss 3.729413, Accuracy 92.505%\n",
      "Epoch 33, Batch 678, LR 0.304296 Loss 3.729189, Accuracy 92.507%\n",
      "Epoch 33, Batch 679, LR 0.304208 Loss 3.729698, Accuracy 92.509%\n",
      "Epoch 33, Batch 680, LR 0.304121 Loss 3.730227, Accuracy 92.509%\n",
      "Epoch 33, Batch 681, LR 0.304033 Loss 3.728861, Accuracy 92.514%\n",
      "Epoch 33, Batch 682, LR 0.303946 Loss 3.729830, Accuracy 92.513%\n",
      "Epoch 33, Batch 683, LR 0.303858 Loss 3.729012, Accuracy 92.516%\n",
      "Epoch 33, Batch 684, LR 0.303771 Loss 3.729466, Accuracy 92.516%\n",
      "Epoch 33, Batch 685, LR 0.303683 Loss 3.728547, Accuracy 92.524%\n",
      "Epoch 33, Batch 686, LR 0.303596 Loss 3.728735, Accuracy 92.527%\n",
      "Epoch 33, Batch 687, LR 0.303508 Loss 3.728752, Accuracy 92.528%\n",
      "Epoch 33, Batch 688, LR 0.303421 Loss 3.729495, Accuracy 92.526%\n",
      "Epoch 33, Batch 689, LR 0.303333 Loss 3.728823, Accuracy 92.527%\n",
      "Epoch 33, Batch 690, LR 0.303246 Loss 3.728047, Accuracy 92.531%\n",
      "Epoch 33, Batch 691, LR 0.303158 Loss 3.728170, Accuracy 92.531%\n",
      "Epoch 33, Batch 692, LR 0.303071 Loss 3.728365, Accuracy 92.536%\n",
      "Epoch 33, Batch 693, LR 0.302983 Loss 3.728255, Accuracy 92.538%\n",
      "Epoch 33, Batch 694, LR 0.302896 Loss 3.728410, Accuracy 92.535%\n",
      "Epoch 33, Batch 695, LR 0.302809 Loss 3.727654, Accuracy 92.534%\n",
      "Epoch 33, Batch 696, LR 0.302721 Loss 3.727472, Accuracy 92.537%\n",
      "Epoch 33, Batch 697, LR 0.302634 Loss 3.727477, Accuracy 92.537%\n",
      "Epoch 33, Batch 698, LR 0.302546 Loss 3.726872, Accuracy 92.540%\n",
      "Epoch 33, Batch 699, LR 0.302459 Loss 3.726435, Accuracy 92.541%\n",
      "Epoch 33, Batch 700, LR 0.302372 Loss 3.726333, Accuracy 92.537%\n",
      "Epoch 33, Batch 701, LR 0.302284 Loss 3.726908, Accuracy 92.535%\n",
      "Epoch 33, Batch 702, LR 0.302197 Loss 3.727850, Accuracy 92.530%\n",
      "Epoch 33, Batch 703, LR 0.302110 Loss 3.728320, Accuracy 92.525%\n",
      "Epoch 33, Batch 704, LR 0.302022 Loss 3.728367, Accuracy 92.526%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 705, LR 0.301935 Loss 3.729325, Accuracy 92.522%\n",
      "Epoch 33, Batch 706, LR 0.301848 Loss 3.729919, Accuracy 92.519%\n",
      "Epoch 33, Batch 707, LR 0.301760 Loss 3.729876, Accuracy 92.519%\n",
      "Epoch 33, Batch 708, LR 0.301673 Loss 3.729672, Accuracy 92.517%\n",
      "Epoch 33, Batch 709, LR 0.301586 Loss 3.729540, Accuracy 92.519%\n",
      "Epoch 33, Batch 710, LR 0.301499 Loss 3.729352, Accuracy 92.524%\n",
      "Epoch 33, Batch 711, LR 0.301411 Loss 3.728933, Accuracy 92.526%\n",
      "Epoch 33, Batch 712, LR 0.301324 Loss 3.728546, Accuracy 92.528%\n",
      "Epoch 33, Batch 713, LR 0.301237 Loss 3.728637, Accuracy 92.524%\n",
      "Epoch 33, Batch 714, LR 0.301150 Loss 3.728333, Accuracy 92.526%\n",
      "Epoch 33, Batch 715, LR 0.301063 Loss 3.728884, Accuracy 92.527%\n",
      "Epoch 33, Batch 716, LR 0.300975 Loss 3.729275, Accuracy 92.528%\n",
      "Epoch 33, Batch 717, LR 0.300888 Loss 3.730019, Accuracy 92.529%\n",
      "Epoch 33, Batch 718, LR 0.300801 Loss 3.730837, Accuracy 92.520%\n",
      "Epoch 33, Batch 719, LR 0.300714 Loss 3.730637, Accuracy 92.518%\n",
      "Epoch 33, Batch 720, LR 0.300627 Loss 3.730241, Accuracy 92.516%\n",
      "Epoch 33, Batch 721, LR 0.300540 Loss 3.730259, Accuracy 92.513%\n",
      "Epoch 33, Batch 722, LR 0.300452 Loss 3.730145, Accuracy 92.515%\n",
      "Epoch 33, Batch 723, LR 0.300365 Loss 3.729771, Accuracy 92.520%\n",
      "Epoch 33, Batch 724, LR 0.300278 Loss 3.729520, Accuracy 92.522%\n",
      "Epoch 33, Batch 725, LR 0.300191 Loss 3.729334, Accuracy 92.525%\n",
      "Epoch 33, Batch 726, LR 0.300104 Loss 3.728995, Accuracy 92.528%\n",
      "Epoch 33, Batch 727, LR 0.300017 Loss 3.729063, Accuracy 92.527%\n",
      "Epoch 33, Batch 728, LR 0.299930 Loss 3.729062, Accuracy 92.524%\n",
      "Epoch 33, Batch 729, LR 0.299843 Loss 3.729209, Accuracy 92.524%\n",
      "Epoch 33, Batch 730, LR 0.299756 Loss 3.728845, Accuracy 92.527%\n",
      "Epoch 33, Batch 731, LR 0.299669 Loss 3.728568, Accuracy 92.531%\n",
      "Epoch 33, Batch 732, LR 0.299582 Loss 3.728598, Accuracy 92.533%\n",
      "Epoch 33, Batch 733, LR 0.299495 Loss 3.728415, Accuracy 92.532%\n",
      "Epoch 33, Batch 734, LR 0.299408 Loss 3.728132, Accuracy 92.536%\n",
      "Epoch 33, Batch 735, LR 0.299321 Loss 3.727704, Accuracy 92.533%\n",
      "Epoch 33, Batch 736, LR 0.299234 Loss 3.728167, Accuracy 92.532%\n",
      "Epoch 33, Batch 737, LR 0.299147 Loss 3.727313, Accuracy 92.538%\n",
      "Epoch 33, Batch 738, LR 0.299060 Loss 3.726819, Accuracy 92.542%\n",
      "Epoch 33, Batch 739, LR 0.298973 Loss 3.726709, Accuracy 92.540%\n",
      "Epoch 33, Batch 740, LR 0.298886 Loss 3.725819, Accuracy 92.546%\n",
      "Epoch 33, Batch 741, LR 0.298799 Loss 3.726068, Accuracy 92.546%\n",
      "Epoch 33, Batch 742, LR 0.298712 Loss 3.725606, Accuracy 92.547%\n",
      "Epoch 33, Batch 743, LR 0.298625 Loss 3.725345, Accuracy 92.548%\n",
      "Epoch 33, Batch 744, LR 0.298539 Loss 3.725168, Accuracy 92.550%\n",
      "Epoch 33, Batch 745, LR 0.298452 Loss 3.724031, Accuracy 92.558%\n",
      "Epoch 33, Batch 746, LR 0.298365 Loss 3.723859, Accuracy 92.559%\n",
      "Epoch 33, Batch 747, LR 0.298278 Loss 3.723287, Accuracy 92.561%\n",
      "Epoch 33, Batch 748, LR 0.298191 Loss 3.723119, Accuracy 92.560%\n",
      "Epoch 33, Batch 749, LR 0.298104 Loss 3.723158, Accuracy 92.561%\n",
      "Epoch 33, Batch 750, LR 0.298018 Loss 3.722960, Accuracy 92.559%\n",
      "Epoch 33, Batch 751, LR 0.297931 Loss 3.724498, Accuracy 92.553%\n",
      "Epoch 33, Batch 752, LR 0.297844 Loss 3.724735, Accuracy 92.550%\n",
      "Epoch 33, Batch 753, LR 0.297757 Loss 3.724195, Accuracy 92.546%\n",
      "Epoch 33, Batch 754, LR 0.297670 Loss 3.723727, Accuracy 92.547%\n",
      "Epoch 33, Batch 755, LR 0.297584 Loss 3.724623, Accuracy 92.538%\n",
      "Epoch 33, Batch 756, LR 0.297497 Loss 3.725054, Accuracy 92.539%\n",
      "Epoch 33, Batch 757, LR 0.297410 Loss 3.724241, Accuracy 92.539%\n",
      "Epoch 33, Batch 758, LR 0.297323 Loss 3.724392, Accuracy 92.540%\n",
      "Epoch 33, Batch 759, LR 0.297237 Loss 3.725044, Accuracy 92.537%\n",
      "Epoch 33, Batch 760, LR 0.297150 Loss 3.725292, Accuracy 92.540%\n",
      "Epoch 33, Batch 761, LR 0.297063 Loss 3.724868, Accuracy 92.542%\n",
      "Epoch 33, Batch 762, LR 0.296977 Loss 3.725042, Accuracy 92.539%\n",
      "Epoch 33, Batch 763, LR 0.296890 Loss 3.725298, Accuracy 92.535%\n",
      "Epoch 33, Batch 764, LR 0.296803 Loss 3.724681, Accuracy 92.536%\n",
      "Epoch 33, Batch 765, LR 0.296717 Loss 3.724651, Accuracy 92.540%\n",
      "Epoch 33, Batch 766, LR 0.296630 Loss 3.724465, Accuracy 92.541%\n",
      "Epoch 33, Batch 767, LR 0.296543 Loss 3.725214, Accuracy 92.536%\n",
      "Epoch 33, Batch 768, LR 0.296457 Loss 3.726038, Accuracy 92.531%\n",
      "Epoch 33, Batch 769, LR 0.296370 Loss 3.726297, Accuracy 92.531%\n",
      "Epoch 33, Batch 770, LR 0.296284 Loss 3.725993, Accuracy 92.533%\n",
      "Epoch 33, Batch 771, LR 0.296197 Loss 3.726116, Accuracy 92.533%\n",
      "Epoch 33, Batch 772, LR 0.296110 Loss 3.725936, Accuracy 92.532%\n",
      "Epoch 33, Batch 773, LR 0.296024 Loss 3.726197, Accuracy 92.529%\n",
      "Epoch 33, Batch 774, LR 0.295937 Loss 3.726525, Accuracy 92.527%\n",
      "Epoch 33, Batch 775, LR 0.295851 Loss 3.726900, Accuracy 92.525%\n",
      "Epoch 33, Batch 776, LR 0.295764 Loss 3.726459, Accuracy 92.529%\n",
      "Epoch 33, Batch 777, LR 0.295678 Loss 3.726496, Accuracy 92.530%\n",
      "Epoch 33, Batch 778, LR 0.295591 Loss 3.726792, Accuracy 92.533%\n",
      "Epoch 33, Batch 779, LR 0.295505 Loss 3.725762, Accuracy 92.536%\n",
      "Epoch 33, Batch 780, LR 0.295418 Loss 3.726335, Accuracy 92.538%\n",
      "Epoch 33, Batch 781, LR 0.295332 Loss 3.725946, Accuracy 92.542%\n",
      "Epoch 33, Batch 782, LR 0.295245 Loss 3.726090, Accuracy 92.538%\n",
      "Epoch 33, Batch 783, LR 0.295159 Loss 3.726030, Accuracy 92.539%\n",
      "Epoch 33, Batch 784, LR 0.295072 Loss 3.726068, Accuracy 92.539%\n",
      "Epoch 33, Batch 785, LR 0.294986 Loss 3.725688, Accuracy 92.543%\n",
      "Epoch 33, Batch 786, LR 0.294899 Loss 3.725945, Accuracy 92.541%\n",
      "Epoch 33, Batch 787, LR 0.294813 Loss 3.725550, Accuracy 92.543%\n",
      "Epoch 33, Batch 788, LR 0.294727 Loss 3.725956, Accuracy 92.543%\n",
      "Epoch 33, Batch 789, LR 0.294640 Loss 3.726130, Accuracy 92.545%\n",
      "Epoch 33, Batch 790, LR 0.294554 Loss 3.726198, Accuracy 92.545%\n",
      "Epoch 33, Batch 791, LR 0.294468 Loss 3.725379, Accuracy 92.545%\n",
      "Epoch 33, Batch 792, LR 0.294381 Loss 3.725591, Accuracy 92.545%\n",
      "Epoch 33, Batch 793, LR 0.294295 Loss 3.725971, Accuracy 92.547%\n",
      "Epoch 33, Batch 794, LR 0.294209 Loss 3.725719, Accuracy 92.547%\n",
      "Epoch 33, Batch 795, LR 0.294122 Loss 3.725730, Accuracy 92.545%\n",
      "Epoch 33, Batch 796, LR 0.294036 Loss 3.725456, Accuracy 92.547%\n",
      "Epoch 33, Batch 797, LR 0.293950 Loss 3.725319, Accuracy 92.547%\n",
      "Epoch 33, Batch 798, LR 0.293863 Loss 3.725263, Accuracy 92.541%\n",
      "Epoch 33, Batch 799, LR 0.293777 Loss 3.724942, Accuracy 92.540%\n",
      "Epoch 33, Batch 800, LR 0.293691 Loss 3.724257, Accuracy 92.542%\n",
      "Epoch 33, Batch 801, LR 0.293605 Loss 3.724121, Accuracy 92.541%\n",
      "Epoch 33, Batch 802, LR 0.293518 Loss 3.723817, Accuracy 92.540%\n",
      "Epoch 33, Batch 803, LR 0.293432 Loss 3.723909, Accuracy 92.543%\n",
      "Epoch 33, Batch 804, LR 0.293346 Loss 3.723877, Accuracy 92.542%\n",
      "Epoch 33, Batch 805, LR 0.293260 Loss 3.723547, Accuracy 92.543%\n",
      "Epoch 33, Batch 806, LR 0.293173 Loss 3.723730, Accuracy 92.542%\n",
      "Epoch 33, Batch 807, LR 0.293087 Loss 3.724217, Accuracy 92.541%\n",
      "Epoch 33, Batch 808, LR 0.293001 Loss 3.723505, Accuracy 92.545%\n",
      "Epoch 33, Batch 809, LR 0.292915 Loss 3.723349, Accuracy 92.545%\n",
      "Epoch 33, Batch 810, LR 0.292829 Loss 3.723883, Accuracy 92.542%\n",
      "Epoch 33, Batch 811, LR 0.292743 Loss 3.724054, Accuracy 92.545%\n",
      "Epoch 33, Batch 812, LR 0.292656 Loss 3.724237, Accuracy 92.544%\n",
      "Epoch 33, Batch 813, LR 0.292570 Loss 3.724293, Accuracy 92.544%\n",
      "Epoch 33, Batch 814, LR 0.292484 Loss 3.723657, Accuracy 92.545%\n",
      "Epoch 33, Batch 815, LR 0.292398 Loss 3.723813, Accuracy 92.545%\n",
      "Epoch 33, Batch 816, LR 0.292312 Loss 3.724090, Accuracy 92.546%\n",
      "Epoch 33, Batch 817, LR 0.292226 Loss 3.724172, Accuracy 92.545%\n",
      "Epoch 33, Batch 818, LR 0.292140 Loss 3.724572, Accuracy 92.540%\n",
      "Epoch 33, Batch 819, LR 0.292054 Loss 3.723573, Accuracy 92.545%\n",
      "Epoch 33, Batch 820, LR 0.291968 Loss 3.723003, Accuracy 92.549%\n",
      "Epoch 33, Batch 821, LR 0.291882 Loss 3.723252, Accuracy 92.549%\n",
      "Epoch 33, Batch 822, LR 0.291796 Loss 3.724309, Accuracy 92.543%\n",
      "Epoch 33, Batch 823, LR 0.291710 Loss 3.725032, Accuracy 92.541%\n",
      "Epoch 33, Batch 824, LR 0.291624 Loss 3.725641, Accuracy 92.538%\n",
      "Epoch 33, Batch 825, LR 0.291538 Loss 3.725775, Accuracy 92.537%\n",
      "Epoch 33, Batch 826, LR 0.291452 Loss 3.725792, Accuracy 92.539%\n",
      "Epoch 33, Batch 827, LR 0.291366 Loss 3.725376, Accuracy 92.541%\n",
      "Epoch 33, Batch 828, LR 0.291280 Loss 3.725596, Accuracy 92.539%\n",
      "Epoch 33, Batch 829, LR 0.291194 Loss 3.725662, Accuracy 92.538%\n",
      "Epoch 33, Batch 830, LR 0.291108 Loss 3.725343, Accuracy 92.540%\n",
      "Epoch 33, Batch 831, LR 0.291022 Loss 3.726146, Accuracy 92.538%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 832, LR 0.290936 Loss 3.726781, Accuracy 92.537%\n",
      "Epoch 33, Batch 833, LR 0.290850 Loss 3.726638, Accuracy 92.539%\n",
      "Epoch 33, Batch 834, LR 0.290764 Loss 3.726569, Accuracy 92.542%\n",
      "Epoch 33, Batch 835, LR 0.290678 Loss 3.727069, Accuracy 92.538%\n",
      "Epoch 33, Batch 836, LR 0.290592 Loss 3.726506, Accuracy 92.538%\n",
      "Epoch 33, Batch 837, LR 0.290507 Loss 3.725841, Accuracy 92.542%\n",
      "Epoch 33, Batch 838, LR 0.290421 Loss 3.725998, Accuracy 92.540%\n",
      "Epoch 33, Batch 839, LR 0.290335 Loss 3.726186, Accuracy 92.539%\n",
      "Epoch 33, Batch 840, LR 0.290249 Loss 3.726107, Accuracy 92.541%\n",
      "Epoch 33, Batch 841, LR 0.290163 Loss 3.725419, Accuracy 92.543%\n",
      "Epoch 33, Batch 842, LR 0.290078 Loss 3.725080, Accuracy 92.546%\n",
      "Epoch 33, Batch 843, LR 0.289992 Loss 3.725260, Accuracy 92.546%\n",
      "Epoch 33, Batch 844, LR 0.289906 Loss 3.725380, Accuracy 92.546%\n",
      "Epoch 33, Batch 845, LR 0.289820 Loss 3.725948, Accuracy 92.545%\n",
      "Epoch 33, Batch 846, LR 0.289734 Loss 3.725615, Accuracy 92.545%\n",
      "Epoch 33, Batch 847, LR 0.289649 Loss 3.725737, Accuracy 92.547%\n",
      "Epoch 33, Batch 848, LR 0.289563 Loss 3.724845, Accuracy 92.547%\n",
      "Epoch 33, Batch 849, LR 0.289477 Loss 3.724659, Accuracy 92.548%\n",
      "Epoch 33, Batch 850, LR 0.289391 Loss 3.724981, Accuracy 92.551%\n",
      "Epoch 33, Batch 851, LR 0.289306 Loss 3.724382, Accuracy 92.553%\n",
      "Epoch 33, Batch 852, LR 0.289220 Loss 3.724572, Accuracy 92.551%\n",
      "Epoch 33, Batch 853, LR 0.289134 Loss 3.724666, Accuracy 92.547%\n",
      "Epoch 33, Batch 854, LR 0.289049 Loss 3.725062, Accuracy 92.543%\n",
      "Epoch 33, Batch 855, LR 0.288963 Loss 3.724824, Accuracy 92.547%\n",
      "Epoch 33, Batch 856, LR 0.288877 Loss 3.724445, Accuracy 92.547%\n",
      "Epoch 33, Batch 857, LR 0.288792 Loss 3.723682, Accuracy 92.550%\n",
      "Epoch 33, Batch 858, LR 0.288706 Loss 3.723841, Accuracy 92.552%\n",
      "Epoch 33, Batch 859, LR 0.288621 Loss 3.723380, Accuracy 92.551%\n",
      "Epoch 33, Batch 860, LR 0.288535 Loss 3.723295, Accuracy 92.551%\n",
      "Epoch 33, Batch 861, LR 0.288449 Loss 3.723593, Accuracy 92.551%\n",
      "Epoch 33, Batch 862, LR 0.288364 Loss 3.724559, Accuracy 92.546%\n",
      "Epoch 33, Batch 863, LR 0.288278 Loss 3.723589, Accuracy 92.548%\n",
      "Epoch 33, Batch 864, LR 0.288193 Loss 3.723713, Accuracy 92.546%\n",
      "Epoch 33, Batch 865, LR 0.288107 Loss 3.724199, Accuracy 92.543%\n",
      "Epoch 33, Batch 866, LR 0.288022 Loss 3.725151, Accuracy 92.539%\n",
      "Epoch 33, Batch 867, LR 0.287936 Loss 3.725105, Accuracy 92.540%\n",
      "Epoch 33, Batch 868, LR 0.287850 Loss 3.724893, Accuracy 92.540%\n",
      "Epoch 33, Batch 869, LR 0.287765 Loss 3.725851, Accuracy 92.537%\n",
      "Epoch 33, Batch 870, LR 0.287679 Loss 3.726296, Accuracy 92.537%\n",
      "Epoch 33, Batch 871, LR 0.287594 Loss 3.726094, Accuracy 92.539%\n",
      "Epoch 33, Batch 872, LR 0.287509 Loss 3.725601, Accuracy 92.540%\n",
      "Epoch 33, Batch 873, LR 0.287423 Loss 3.725760, Accuracy 92.537%\n",
      "Epoch 33, Batch 874, LR 0.287338 Loss 3.726786, Accuracy 92.533%\n",
      "Epoch 33, Batch 875, LR 0.287252 Loss 3.726709, Accuracy 92.537%\n",
      "Epoch 33, Batch 876, LR 0.287167 Loss 3.726698, Accuracy 92.534%\n",
      "Epoch 33, Batch 877, LR 0.287081 Loss 3.726503, Accuracy 92.536%\n",
      "Epoch 33, Batch 878, LR 0.286996 Loss 3.727220, Accuracy 92.530%\n",
      "Epoch 33, Batch 879, LR 0.286910 Loss 3.726935, Accuracy 92.530%\n",
      "Epoch 33, Batch 880, LR 0.286825 Loss 3.727310, Accuracy 92.528%\n",
      "Epoch 33, Batch 881, LR 0.286740 Loss 3.727428, Accuracy 92.525%\n",
      "Epoch 33, Batch 882, LR 0.286654 Loss 3.728263, Accuracy 92.521%\n",
      "Epoch 33, Batch 883, LR 0.286569 Loss 3.728366, Accuracy 92.519%\n",
      "Epoch 33, Batch 884, LR 0.286484 Loss 3.728305, Accuracy 92.519%\n",
      "Epoch 33, Batch 885, LR 0.286398 Loss 3.728331, Accuracy 92.519%\n",
      "Epoch 33, Batch 886, LR 0.286313 Loss 3.728300, Accuracy 92.517%\n",
      "Epoch 33, Batch 887, LR 0.286228 Loss 3.727549, Accuracy 92.520%\n",
      "Epoch 33, Batch 888, LR 0.286142 Loss 3.727483, Accuracy 92.522%\n",
      "Epoch 33, Batch 889, LR 0.286057 Loss 3.727653, Accuracy 92.524%\n",
      "Epoch 33, Batch 890, LR 0.285972 Loss 3.727629, Accuracy 92.523%\n",
      "Epoch 33, Batch 891, LR 0.285887 Loss 3.727290, Accuracy 92.526%\n",
      "Epoch 33, Batch 892, LR 0.285801 Loss 3.727863, Accuracy 92.521%\n",
      "Epoch 33, Batch 893, LR 0.285716 Loss 3.727969, Accuracy 92.524%\n",
      "Epoch 33, Batch 894, LR 0.285631 Loss 3.728424, Accuracy 92.526%\n",
      "Epoch 33, Batch 895, LR 0.285546 Loss 3.728311, Accuracy 92.525%\n",
      "Epoch 33, Batch 896, LR 0.285460 Loss 3.728571, Accuracy 92.523%\n",
      "Epoch 33, Batch 897, LR 0.285375 Loss 3.728198, Accuracy 92.523%\n",
      "Epoch 33, Batch 898, LR 0.285290 Loss 3.728096, Accuracy 92.522%\n",
      "Epoch 33, Batch 899, LR 0.285205 Loss 3.728056, Accuracy 92.524%\n",
      "Epoch 33, Batch 900, LR 0.285120 Loss 3.727224, Accuracy 92.527%\n",
      "Epoch 33, Batch 901, LR 0.285035 Loss 3.727540, Accuracy 92.523%\n",
      "Epoch 33, Batch 902, LR 0.284949 Loss 3.727560, Accuracy 92.524%\n",
      "Epoch 33, Batch 903, LR 0.284864 Loss 3.727005, Accuracy 92.524%\n",
      "Epoch 33, Batch 904, LR 0.284779 Loss 3.726408, Accuracy 92.527%\n",
      "Epoch 33, Batch 905, LR 0.284694 Loss 3.726615, Accuracy 92.525%\n",
      "Epoch 33, Batch 906, LR 0.284609 Loss 3.726096, Accuracy 92.529%\n",
      "Epoch 33, Batch 907, LR 0.284524 Loss 3.726667, Accuracy 92.527%\n",
      "Epoch 33, Batch 908, LR 0.284439 Loss 3.726682, Accuracy 92.529%\n",
      "Epoch 33, Batch 909, LR 0.284354 Loss 3.726636, Accuracy 92.530%\n",
      "Epoch 33, Batch 910, LR 0.284269 Loss 3.726922, Accuracy 92.527%\n",
      "Epoch 33, Batch 911, LR 0.284184 Loss 3.726925, Accuracy 92.525%\n",
      "Epoch 33, Batch 912, LR 0.284099 Loss 3.726605, Accuracy 92.522%\n",
      "Epoch 33, Batch 913, LR 0.284014 Loss 3.727055, Accuracy 92.520%\n",
      "Epoch 33, Batch 914, LR 0.283929 Loss 3.726325, Accuracy 92.524%\n",
      "Epoch 33, Batch 915, LR 0.283844 Loss 3.726088, Accuracy 92.523%\n",
      "Epoch 33, Batch 916, LR 0.283759 Loss 3.726923, Accuracy 92.518%\n",
      "Epoch 33, Batch 917, LR 0.283674 Loss 3.727194, Accuracy 92.520%\n",
      "Epoch 33, Batch 918, LR 0.283589 Loss 3.727507, Accuracy 92.517%\n",
      "Epoch 33, Batch 919, LR 0.283504 Loss 3.726816, Accuracy 92.522%\n",
      "Epoch 33, Batch 920, LR 0.283419 Loss 3.728378, Accuracy 92.512%\n",
      "Epoch 33, Batch 921, LR 0.283334 Loss 3.728219, Accuracy 92.512%\n",
      "Epoch 33, Batch 922, LR 0.283249 Loss 3.728060, Accuracy 92.511%\n",
      "Epoch 33, Batch 923, LR 0.283164 Loss 3.727452, Accuracy 92.510%\n",
      "Epoch 33, Batch 924, LR 0.283079 Loss 3.727814, Accuracy 92.510%\n",
      "Epoch 33, Batch 925, LR 0.282994 Loss 3.728063, Accuracy 92.509%\n",
      "Epoch 33, Batch 926, LR 0.282909 Loss 3.728526, Accuracy 92.510%\n",
      "Epoch 33, Batch 927, LR 0.282825 Loss 3.727780, Accuracy 92.514%\n",
      "Epoch 33, Batch 928, LR 0.282740 Loss 3.728112, Accuracy 92.518%\n",
      "Epoch 33, Batch 929, LR 0.282655 Loss 3.727738, Accuracy 92.517%\n",
      "Epoch 33, Batch 930, LR 0.282570 Loss 3.728244, Accuracy 92.511%\n",
      "Epoch 33, Batch 931, LR 0.282485 Loss 3.727965, Accuracy 92.512%\n",
      "Epoch 33, Batch 932, LR 0.282400 Loss 3.728025, Accuracy 92.513%\n",
      "Epoch 33, Batch 933, LR 0.282316 Loss 3.728230, Accuracy 92.512%\n",
      "Epoch 33, Batch 934, LR 0.282231 Loss 3.728234, Accuracy 92.509%\n",
      "Epoch 33, Batch 935, LR 0.282146 Loss 3.727578, Accuracy 92.513%\n",
      "Epoch 33, Batch 936, LR 0.282061 Loss 3.727571, Accuracy 92.511%\n",
      "Epoch 33, Batch 937, LR 0.281976 Loss 3.727505, Accuracy 92.513%\n",
      "Epoch 33, Batch 938, LR 0.281892 Loss 3.727515, Accuracy 92.511%\n",
      "Epoch 33, Batch 939, LR 0.281807 Loss 3.727565, Accuracy 92.511%\n",
      "Epoch 33, Batch 940, LR 0.281722 Loss 3.727263, Accuracy 92.513%\n",
      "Epoch 33, Batch 941, LR 0.281638 Loss 3.726802, Accuracy 92.515%\n",
      "Epoch 33, Batch 942, LR 0.281553 Loss 3.726491, Accuracy 92.514%\n",
      "Epoch 33, Batch 943, LR 0.281468 Loss 3.726329, Accuracy 92.516%\n",
      "Epoch 33, Batch 944, LR 0.281384 Loss 3.726168, Accuracy 92.516%\n",
      "Epoch 33, Batch 945, LR 0.281299 Loss 3.726465, Accuracy 92.512%\n",
      "Epoch 33, Batch 946, LR 0.281214 Loss 3.726756, Accuracy 92.513%\n",
      "Epoch 33, Batch 947, LR 0.281130 Loss 3.726784, Accuracy 92.516%\n",
      "Epoch 33, Batch 948, LR 0.281045 Loss 3.726235, Accuracy 92.519%\n",
      "Epoch 33, Batch 949, LR 0.280960 Loss 3.726112, Accuracy 92.521%\n",
      "Epoch 33, Batch 950, LR 0.280876 Loss 3.725876, Accuracy 92.521%\n",
      "Epoch 33, Batch 951, LR 0.280791 Loss 3.725937, Accuracy 92.520%\n",
      "Epoch 33, Batch 952, LR 0.280707 Loss 3.725760, Accuracy 92.522%\n",
      "Epoch 33, Batch 953, LR 0.280622 Loss 3.724805, Accuracy 92.527%\n",
      "Epoch 33, Batch 954, LR 0.280537 Loss 3.725224, Accuracy 92.523%\n",
      "Epoch 33, Batch 955, LR 0.280453 Loss 3.724836, Accuracy 92.525%\n",
      "Epoch 33, Batch 956, LR 0.280368 Loss 3.724564, Accuracy 92.526%\n",
      "Epoch 33, Batch 957, LR 0.280284 Loss 3.724386, Accuracy 92.527%\n",
      "Epoch 33, Batch 958, LR 0.280199 Loss 3.724215, Accuracy 92.528%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 959, LR 0.280115 Loss 3.724790, Accuracy 92.526%\n",
      "Epoch 33, Batch 960, LR 0.280030 Loss 3.724055, Accuracy 92.529%\n",
      "Epoch 33, Batch 961, LR 0.279946 Loss 3.723690, Accuracy 92.531%\n",
      "Epoch 33, Batch 962, LR 0.279861 Loss 3.723197, Accuracy 92.535%\n",
      "Epoch 33, Batch 963, LR 0.279777 Loss 3.723674, Accuracy 92.531%\n",
      "Epoch 33, Batch 964, LR 0.279692 Loss 3.723571, Accuracy 92.533%\n",
      "Epoch 33, Batch 965, LR 0.279608 Loss 3.724363, Accuracy 92.528%\n",
      "Epoch 33, Batch 966, LR 0.279524 Loss 3.724452, Accuracy 92.528%\n",
      "Epoch 33, Batch 967, LR 0.279439 Loss 3.724361, Accuracy 92.532%\n",
      "Epoch 33, Batch 968, LR 0.279355 Loss 3.724426, Accuracy 92.531%\n",
      "Epoch 33, Batch 969, LR 0.279270 Loss 3.724826, Accuracy 92.530%\n",
      "Epoch 33, Batch 970, LR 0.279186 Loss 3.724764, Accuracy 92.530%\n",
      "Epoch 33, Batch 971, LR 0.279102 Loss 3.724832, Accuracy 92.526%\n",
      "Epoch 33, Batch 972, LR 0.279017 Loss 3.724550, Accuracy 92.527%\n",
      "Epoch 33, Batch 973, LR 0.278933 Loss 3.724364, Accuracy 92.525%\n",
      "Epoch 33, Batch 974, LR 0.278848 Loss 3.724694, Accuracy 92.524%\n",
      "Epoch 33, Batch 975, LR 0.278764 Loss 3.724775, Accuracy 92.526%\n",
      "Epoch 33, Batch 976, LR 0.278680 Loss 3.724436, Accuracy 92.529%\n",
      "Epoch 33, Batch 977, LR 0.278596 Loss 3.724263, Accuracy 92.532%\n",
      "Epoch 33, Batch 978, LR 0.278511 Loss 3.724313, Accuracy 92.532%\n",
      "Epoch 33, Batch 979, LR 0.278427 Loss 3.723844, Accuracy 92.532%\n",
      "Epoch 33, Batch 980, LR 0.278343 Loss 3.723735, Accuracy 92.534%\n",
      "Epoch 33, Batch 981, LR 0.278258 Loss 3.723764, Accuracy 92.536%\n",
      "Epoch 33, Batch 982, LR 0.278174 Loss 3.723849, Accuracy 92.537%\n",
      "Epoch 33, Batch 983, LR 0.278090 Loss 3.723542, Accuracy 92.537%\n",
      "Epoch 33, Batch 984, LR 0.278006 Loss 3.723333, Accuracy 92.539%\n",
      "Epoch 33, Batch 985, LR 0.277921 Loss 3.723242, Accuracy 92.540%\n",
      "Epoch 33, Batch 986, LR 0.277837 Loss 3.722534, Accuracy 92.545%\n",
      "Epoch 33, Batch 987, LR 0.277753 Loss 3.722090, Accuracy 92.546%\n",
      "Epoch 33, Batch 988, LR 0.277669 Loss 3.721986, Accuracy 92.546%\n",
      "Epoch 33, Batch 989, LR 0.277585 Loss 3.721806, Accuracy 92.547%\n",
      "Epoch 33, Batch 990, LR 0.277501 Loss 3.721728, Accuracy 92.551%\n",
      "Epoch 33, Batch 991, LR 0.277416 Loss 3.721354, Accuracy 92.553%\n",
      "Epoch 33, Batch 992, LR 0.277332 Loss 3.721652, Accuracy 92.549%\n",
      "Epoch 33, Batch 993, LR 0.277248 Loss 3.721903, Accuracy 92.549%\n",
      "Epoch 33, Batch 994, LR 0.277164 Loss 3.721547, Accuracy 92.551%\n",
      "Epoch 33, Batch 995, LR 0.277080 Loss 3.722455, Accuracy 92.547%\n",
      "Epoch 33, Batch 996, LR 0.276996 Loss 3.722513, Accuracy 92.547%\n",
      "Epoch 33, Batch 997, LR 0.276912 Loss 3.722516, Accuracy 92.546%\n",
      "Epoch 33, Batch 998, LR 0.276828 Loss 3.722877, Accuracy 92.542%\n",
      "Epoch 33, Batch 999, LR 0.276744 Loss 3.723429, Accuracy 92.539%\n",
      "Epoch 33, Batch 1000, LR 0.276660 Loss 3.722921, Accuracy 92.542%\n",
      "Epoch 33, Batch 1001, LR 0.276575 Loss 3.723227, Accuracy 92.539%\n",
      "Epoch 33, Batch 1002, LR 0.276491 Loss 3.723702, Accuracy 92.536%\n",
      "Epoch 33, Batch 1003, LR 0.276407 Loss 3.723999, Accuracy 92.538%\n",
      "Epoch 33, Batch 1004, LR 0.276323 Loss 3.723810, Accuracy 92.538%\n",
      "Epoch 33, Batch 1005, LR 0.276239 Loss 3.723576, Accuracy 92.542%\n",
      "Epoch 33, Batch 1006, LR 0.276155 Loss 3.724070, Accuracy 92.538%\n",
      "Epoch 33, Batch 1007, LR 0.276071 Loss 3.723787, Accuracy 92.541%\n",
      "Epoch 33, Batch 1008, LR 0.275988 Loss 3.724113, Accuracy 92.541%\n",
      "Epoch 33, Batch 1009, LR 0.275904 Loss 3.724672, Accuracy 92.537%\n",
      "Epoch 33, Batch 1010, LR 0.275820 Loss 3.724684, Accuracy 92.539%\n",
      "Epoch 33, Batch 1011, LR 0.275736 Loss 3.724374, Accuracy 92.540%\n",
      "Epoch 33, Batch 1012, LR 0.275652 Loss 3.723872, Accuracy 92.543%\n",
      "Epoch 33, Batch 1013, LR 0.275568 Loss 3.723610, Accuracy 92.543%\n",
      "Epoch 33, Batch 1014, LR 0.275484 Loss 3.723229, Accuracy 92.546%\n",
      "Epoch 33, Batch 1015, LR 0.275400 Loss 3.722889, Accuracy 92.546%\n",
      "Epoch 33, Batch 1016, LR 0.275316 Loss 3.723026, Accuracy 92.545%\n",
      "Epoch 33, Batch 1017, LR 0.275232 Loss 3.723203, Accuracy 92.546%\n",
      "Epoch 33, Batch 1018, LR 0.275149 Loss 3.723099, Accuracy 92.545%\n",
      "Epoch 33, Batch 1019, LR 0.275065 Loss 3.723183, Accuracy 92.544%\n",
      "Epoch 33, Batch 1020, LR 0.274981 Loss 3.723605, Accuracy 92.543%\n",
      "Epoch 33, Batch 1021, LR 0.274897 Loss 3.723638, Accuracy 92.544%\n",
      "Epoch 33, Batch 1022, LR 0.274813 Loss 3.723792, Accuracy 92.545%\n",
      "Epoch 33, Batch 1023, LR 0.274729 Loss 3.724452, Accuracy 92.542%\n",
      "Epoch 33, Batch 1024, LR 0.274646 Loss 3.724008, Accuracy 92.542%\n",
      "Epoch 33, Batch 1025, LR 0.274562 Loss 3.723763, Accuracy 92.543%\n",
      "Epoch 33, Batch 1026, LR 0.274478 Loss 3.724025, Accuracy 92.542%\n",
      "Epoch 33, Batch 1027, LR 0.274394 Loss 3.723785, Accuracy 92.542%\n",
      "Epoch 33, Batch 1028, LR 0.274311 Loss 3.724205, Accuracy 92.539%\n",
      "Epoch 33, Batch 1029, LR 0.274227 Loss 3.724204, Accuracy 92.536%\n",
      "Epoch 33, Batch 1030, LR 0.274143 Loss 3.723943, Accuracy 92.539%\n",
      "Epoch 33, Batch 1031, LR 0.274060 Loss 3.724343, Accuracy 92.536%\n",
      "Epoch 33, Batch 1032, LR 0.273976 Loss 3.724021, Accuracy 92.537%\n",
      "Epoch 33, Batch 1033, LR 0.273892 Loss 3.723804, Accuracy 92.536%\n",
      "Epoch 33, Batch 1034, LR 0.273808 Loss 3.724057, Accuracy 92.535%\n",
      "Epoch 33, Batch 1035, LR 0.273725 Loss 3.724169, Accuracy 92.534%\n",
      "Epoch 33, Batch 1036, LR 0.273641 Loss 3.723459, Accuracy 92.540%\n",
      "Epoch 33, Batch 1037, LR 0.273558 Loss 3.723870, Accuracy 92.537%\n",
      "Epoch 33, Batch 1038, LR 0.273474 Loss 3.723552, Accuracy 92.538%\n",
      "Epoch 33, Batch 1039, LR 0.273390 Loss 3.723184, Accuracy 92.540%\n",
      "Epoch 33, Batch 1040, LR 0.273307 Loss 3.722849, Accuracy 92.541%\n",
      "Epoch 33, Batch 1041, LR 0.273223 Loss 3.722637, Accuracy 92.543%\n",
      "Epoch 33, Batch 1042, LR 0.273140 Loss 3.722431, Accuracy 92.546%\n",
      "Epoch 33, Batch 1043, LR 0.273056 Loss 3.722375, Accuracy 92.549%\n",
      "Epoch 33, Batch 1044, LR 0.272972 Loss 3.722257, Accuracy 92.550%\n",
      "Epoch 33, Batch 1045, LR 0.272889 Loss 3.721947, Accuracy 92.552%\n",
      "Epoch 33, Batch 1046, LR 0.272805 Loss 3.721439, Accuracy 92.551%\n",
      "Epoch 33, Batch 1047, LR 0.272722 Loss 3.722209, Accuracy 92.546%\n",
      "Epoch 33, Loss (train set) 3.722209, Accuracy (train set) 92.546%\n",
      "Epoch 34, Batch 1, LR 0.272638 Loss 4.138532, Accuracy 92.188%\n",
      "Epoch 34, Batch 2, LR 0.272555 Loss 3.682713, Accuracy 91.797%\n",
      "Epoch 34, Batch 3, LR 0.272471 Loss 3.952101, Accuracy 89.583%\n",
      "Epoch 34, Batch 4, LR 0.272388 Loss 3.830665, Accuracy 91.016%\n",
      "Epoch 34, Batch 5, LR 0.272304 Loss 3.888699, Accuracy 90.938%\n",
      "Epoch 34, Batch 6, LR 0.272221 Loss 3.958255, Accuracy 90.104%\n",
      "Epoch 34, Batch 7, LR 0.272137 Loss 3.916941, Accuracy 90.402%\n",
      "Epoch 34, Batch 8, LR 0.272054 Loss 3.861840, Accuracy 90.820%\n",
      "Epoch 34, Batch 9, LR 0.271971 Loss 3.873370, Accuracy 90.799%\n",
      "Epoch 34, Batch 10, LR 0.271887 Loss 3.823302, Accuracy 91.250%\n",
      "Epoch 34, Batch 11, LR 0.271804 Loss 3.888407, Accuracy 91.264%\n",
      "Epoch 34, Batch 12, LR 0.271720 Loss 3.858986, Accuracy 91.471%\n",
      "Epoch 34, Batch 13, LR 0.271637 Loss 3.827575, Accuracy 91.647%\n",
      "Epoch 34, Batch 14, LR 0.271554 Loss 3.799904, Accuracy 91.853%\n",
      "Epoch 34, Batch 15, LR 0.271470 Loss 3.784091, Accuracy 91.823%\n",
      "Epoch 34, Batch 16, LR 0.271387 Loss 3.750308, Accuracy 91.992%\n",
      "Epoch 34, Batch 17, LR 0.271304 Loss 3.717901, Accuracy 92.096%\n",
      "Epoch 34, Batch 18, LR 0.271220 Loss 3.743472, Accuracy 92.188%\n",
      "Epoch 34, Batch 19, LR 0.271137 Loss 3.704258, Accuracy 92.393%\n",
      "Epoch 34, Batch 20, LR 0.271054 Loss 3.706319, Accuracy 92.422%\n",
      "Epoch 34, Batch 21, LR 0.270970 Loss 3.706514, Accuracy 92.411%\n",
      "Epoch 34, Batch 22, LR 0.270887 Loss 3.709484, Accuracy 92.472%\n",
      "Epoch 34, Batch 23, LR 0.270804 Loss 3.688352, Accuracy 92.493%\n",
      "Epoch 34, Batch 24, LR 0.270721 Loss 3.683236, Accuracy 92.546%\n",
      "Epoch 34, Batch 25, LR 0.270637 Loss 3.693663, Accuracy 92.594%\n",
      "Epoch 34, Batch 26, LR 0.270554 Loss 3.678441, Accuracy 92.698%\n",
      "Epoch 34, Batch 27, LR 0.270471 Loss 3.667454, Accuracy 92.795%\n",
      "Epoch 34, Batch 28, LR 0.270388 Loss 3.673940, Accuracy 92.746%\n",
      "Epoch 34, Batch 29, LR 0.270304 Loss 3.658801, Accuracy 92.753%\n",
      "Epoch 34, Batch 30, LR 0.270221 Loss 3.639414, Accuracy 92.865%\n",
      "Epoch 34, Batch 31, LR 0.270138 Loss 3.638952, Accuracy 92.767%\n",
      "Epoch 34, Batch 32, LR 0.270055 Loss 3.648254, Accuracy 92.773%\n",
      "Epoch 34, Batch 33, LR 0.269972 Loss 3.654071, Accuracy 92.779%\n",
      "Epoch 34, Batch 34, LR 0.269889 Loss 3.638081, Accuracy 92.877%\n",
      "Epoch 34, Batch 35, LR 0.269806 Loss 3.633286, Accuracy 92.857%\n",
      "Epoch 34, Batch 36, LR 0.269722 Loss 3.643760, Accuracy 92.839%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 37, LR 0.269639 Loss 3.655245, Accuracy 92.863%\n",
      "Epoch 34, Batch 38, LR 0.269556 Loss 3.653973, Accuracy 92.845%\n",
      "Epoch 34, Batch 39, LR 0.269473 Loss 3.650239, Accuracy 92.869%\n",
      "Epoch 34, Batch 40, LR 0.269390 Loss 3.660967, Accuracy 92.754%\n",
      "Epoch 34, Batch 41, LR 0.269307 Loss 3.661666, Accuracy 92.797%\n",
      "Epoch 34, Batch 42, LR 0.269224 Loss 3.675365, Accuracy 92.708%\n",
      "Epoch 34, Batch 43, LR 0.269141 Loss 3.667444, Accuracy 92.733%\n",
      "Epoch 34, Batch 44, LR 0.269058 Loss 3.674391, Accuracy 92.720%\n",
      "Epoch 34, Batch 45, LR 0.268975 Loss 3.676241, Accuracy 92.691%\n",
      "Epoch 34, Batch 46, LR 0.268892 Loss 3.679776, Accuracy 92.629%\n",
      "Epoch 34, Batch 47, LR 0.268809 Loss 3.680435, Accuracy 92.553%\n",
      "Epoch 34, Batch 48, LR 0.268726 Loss 3.683453, Accuracy 92.546%\n",
      "Epoch 34, Batch 49, LR 0.268643 Loss 3.678452, Accuracy 92.586%\n",
      "Epoch 34, Batch 50, LR 0.268560 Loss 3.675576, Accuracy 92.547%\n",
      "Epoch 34, Batch 51, LR 0.268477 Loss 3.678662, Accuracy 92.509%\n",
      "Epoch 34, Batch 52, LR 0.268394 Loss 3.668346, Accuracy 92.533%\n",
      "Epoch 34, Batch 53, LR 0.268311 Loss 3.666985, Accuracy 92.556%\n",
      "Epoch 34, Batch 54, LR 0.268228 Loss 3.665403, Accuracy 92.593%\n",
      "Epoch 34, Batch 55, LR 0.268145 Loss 3.673950, Accuracy 92.585%\n",
      "Epoch 34, Batch 56, LR 0.268062 Loss 3.687421, Accuracy 92.550%\n",
      "Epoch 34, Batch 57, LR 0.267979 Loss 3.689081, Accuracy 92.571%\n",
      "Epoch 34, Batch 58, LR 0.267897 Loss 3.680879, Accuracy 92.605%\n",
      "Epoch 34, Batch 59, LR 0.267814 Loss 3.696373, Accuracy 92.572%\n",
      "Epoch 34, Batch 60, LR 0.267731 Loss 3.691555, Accuracy 92.604%\n",
      "Epoch 34, Batch 61, LR 0.267648 Loss 3.687768, Accuracy 92.649%\n",
      "Epoch 34, Batch 62, LR 0.267565 Loss 3.696072, Accuracy 92.616%\n",
      "Epoch 34, Batch 63, LR 0.267482 Loss 3.704615, Accuracy 92.560%\n",
      "Epoch 34, Batch 64, LR 0.267400 Loss 3.694865, Accuracy 92.590%\n",
      "Epoch 34, Batch 65, LR 0.267317 Loss 3.699197, Accuracy 92.560%\n",
      "Epoch 34, Batch 66, LR 0.267234 Loss 3.704279, Accuracy 92.543%\n",
      "Epoch 34, Batch 67, LR 0.267151 Loss 3.706748, Accuracy 92.526%\n",
      "Epoch 34, Batch 68, LR 0.267069 Loss 3.708341, Accuracy 92.509%\n",
      "Epoch 34, Batch 69, LR 0.266986 Loss 3.698175, Accuracy 92.561%\n",
      "Epoch 34, Batch 70, LR 0.266903 Loss 3.695520, Accuracy 92.578%\n",
      "Epoch 34, Batch 71, LR 0.266820 Loss 3.690198, Accuracy 92.617%\n",
      "Epoch 34, Batch 72, LR 0.266738 Loss 3.684475, Accuracy 92.632%\n",
      "Epoch 34, Batch 73, LR 0.266655 Loss 3.680329, Accuracy 92.658%\n",
      "Epoch 34, Batch 74, LR 0.266572 Loss 3.680139, Accuracy 92.631%\n",
      "Epoch 34, Batch 75, LR 0.266490 Loss 3.675423, Accuracy 92.656%\n",
      "Epoch 34, Batch 76, LR 0.266407 Loss 3.672211, Accuracy 92.681%\n",
      "Epoch 34, Batch 77, LR 0.266324 Loss 3.678105, Accuracy 92.644%\n",
      "Epoch 34, Batch 78, LR 0.266242 Loss 3.680981, Accuracy 92.658%\n",
      "Epoch 34, Batch 79, LR 0.266159 Loss 3.675032, Accuracy 92.702%\n",
      "Epoch 34, Batch 80, LR 0.266076 Loss 3.672925, Accuracy 92.715%\n",
      "Epoch 34, Batch 81, LR 0.265994 Loss 3.670621, Accuracy 92.737%\n",
      "Epoch 34, Batch 82, LR 0.265911 Loss 3.671589, Accuracy 92.721%\n",
      "Epoch 34, Batch 83, LR 0.265829 Loss 3.673141, Accuracy 92.658%\n",
      "Epoch 34, Batch 84, LR 0.265746 Loss 3.667083, Accuracy 92.680%\n",
      "Epoch 34, Batch 85, LR 0.265663 Loss 3.666255, Accuracy 92.647%\n",
      "Epoch 34, Batch 86, LR 0.265581 Loss 3.664994, Accuracy 92.678%\n",
      "Epoch 34, Batch 87, LR 0.265498 Loss 3.669159, Accuracy 92.645%\n",
      "Epoch 34, Batch 88, LR 0.265416 Loss 3.675568, Accuracy 92.623%\n",
      "Epoch 34, Batch 89, LR 0.265333 Loss 3.672947, Accuracy 92.644%\n",
      "Epoch 34, Batch 90, LR 0.265251 Loss 3.666749, Accuracy 92.656%\n",
      "Epoch 34, Batch 91, LR 0.265168 Loss 3.669321, Accuracy 92.660%\n",
      "Epoch 34, Batch 92, LR 0.265086 Loss 3.673896, Accuracy 92.672%\n",
      "Epoch 34, Batch 93, LR 0.265003 Loss 3.671996, Accuracy 92.675%\n",
      "Epoch 34, Batch 94, LR 0.264921 Loss 3.674943, Accuracy 92.653%\n",
      "Epoch 34, Batch 95, LR 0.264838 Loss 3.676566, Accuracy 92.648%\n",
      "Epoch 34, Batch 96, LR 0.264756 Loss 3.668623, Accuracy 92.708%\n",
      "Epoch 34, Batch 97, LR 0.264673 Loss 3.670625, Accuracy 92.671%\n",
      "Epoch 34, Batch 98, LR 0.264591 Loss 3.678812, Accuracy 92.618%\n",
      "Epoch 34, Batch 99, LR 0.264509 Loss 3.673385, Accuracy 92.645%\n",
      "Epoch 34, Batch 100, LR 0.264426 Loss 3.676631, Accuracy 92.656%\n",
      "Epoch 34, Batch 101, LR 0.264344 Loss 3.677138, Accuracy 92.667%\n",
      "Epoch 34, Batch 102, LR 0.264261 Loss 3.677252, Accuracy 92.670%\n",
      "Epoch 34, Batch 103, LR 0.264179 Loss 3.673712, Accuracy 92.711%\n",
      "Epoch 34, Batch 104, LR 0.264097 Loss 3.671406, Accuracy 92.736%\n",
      "Epoch 34, Batch 105, LR 0.264014 Loss 3.671369, Accuracy 92.746%\n",
      "Epoch 34, Batch 106, LR 0.263932 Loss 3.675003, Accuracy 92.726%\n",
      "Epoch 34, Batch 107, LR 0.263850 Loss 3.674160, Accuracy 92.728%\n",
      "Epoch 34, Batch 108, LR 0.263767 Loss 3.682838, Accuracy 92.701%\n",
      "Epoch 34, Batch 109, LR 0.263685 Loss 3.680956, Accuracy 92.725%\n",
      "Epoch 34, Batch 110, LR 0.263603 Loss 3.683012, Accuracy 92.678%\n",
      "Epoch 34, Batch 111, LR 0.263521 Loss 3.679110, Accuracy 92.673%\n",
      "Epoch 34, Batch 112, LR 0.263438 Loss 3.678217, Accuracy 92.669%\n",
      "Epoch 34, Batch 113, LR 0.263356 Loss 3.680988, Accuracy 92.637%\n",
      "Epoch 34, Batch 114, LR 0.263274 Loss 3.678076, Accuracy 92.647%\n",
      "Epoch 34, Batch 115, LR 0.263192 Loss 3.680009, Accuracy 92.649%\n",
      "Epoch 34, Batch 116, LR 0.263109 Loss 3.679326, Accuracy 92.645%\n",
      "Epoch 34, Batch 117, LR 0.263027 Loss 3.674855, Accuracy 92.655%\n",
      "Epoch 34, Batch 118, LR 0.262945 Loss 3.673117, Accuracy 92.671%\n",
      "Epoch 34, Batch 119, LR 0.262863 Loss 3.674094, Accuracy 92.673%\n",
      "Epoch 34, Batch 120, LR 0.262781 Loss 3.674251, Accuracy 92.689%\n",
      "Epoch 34, Batch 121, LR 0.262698 Loss 3.671254, Accuracy 92.691%\n",
      "Epoch 34, Batch 122, LR 0.262616 Loss 3.671751, Accuracy 92.661%\n",
      "Epoch 34, Batch 123, LR 0.262534 Loss 3.673708, Accuracy 92.632%\n",
      "Epoch 34, Batch 124, LR 0.262452 Loss 3.676011, Accuracy 92.610%\n",
      "Epoch 34, Batch 125, LR 0.262370 Loss 3.677907, Accuracy 92.600%\n",
      "Epoch 34, Batch 126, LR 0.262288 Loss 3.675653, Accuracy 92.609%\n",
      "Epoch 34, Batch 127, LR 0.262206 Loss 3.675633, Accuracy 92.637%\n",
      "Epoch 34, Batch 128, LR 0.262124 Loss 3.677965, Accuracy 92.645%\n",
      "Epoch 34, Batch 129, LR 0.262042 Loss 3.674729, Accuracy 92.642%\n",
      "Epoch 34, Batch 130, LR 0.261960 Loss 3.673917, Accuracy 92.644%\n",
      "Epoch 34, Batch 131, LR 0.261878 Loss 3.673479, Accuracy 92.665%\n",
      "Epoch 34, Batch 132, LR 0.261795 Loss 3.674177, Accuracy 92.673%\n",
      "Epoch 34, Batch 133, LR 0.261713 Loss 3.671054, Accuracy 92.687%\n",
      "Epoch 34, Batch 134, LR 0.261631 Loss 3.672767, Accuracy 92.701%\n",
      "Epoch 34, Batch 135, LR 0.261549 Loss 3.676827, Accuracy 92.674%\n",
      "Epoch 34, Batch 136, LR 0.261467 Loss 3.678690, Accuracy 92.641%\n",
      "Epoch 34, Batch 137, LR 0.261385 Loss 3.679210, Accuracy 92.632%\n",
      "Epoch 34, Batch 138, LR 0.261304 Loss 3.678076, Accuracy 92.629%\n",
      "Epoch 34, Batch 139, LR 0.261222 Loss 3.675401, Accuracy 92.632%\n",
      "Epoch 34, Batch 140, LR 0.261140 Loss 3.676297, Accuracy 92.640%\n",
      "Epoch 34, Batch 141, LR 0.261058 Loss 3.673343, Accuracy 92.636%\n",
      "Epoch 34, Batch 142, LR 0.260976 Loss 3.669830, Accuracy 92.661%\n",
      "Epoch 34, Batch 143, LR 0.260894 Loss 3.671690, Accuracy 92.668%\n",
      "Epoch 34, Batch 144, LR 0.260812 Loss 3.668882, Accuracy 92.692%\n",
      "Epoch 34, Batch 145, LR 0.260730 Loss 3.667908, Accuracy 92.678%\n",
      "Epoch 34, Batch 146, LR 0.260648 Loss 3.668192, Accuracy 92.669%\n",
      "Epoch 34, Batch 147, LR 0.260566 Loss 3.666906, Accuracy 92.682%\n",
      "Epoch 34, Batch 148, LR 0.260484 Loss 3.669302, Accuracy 92.668%\n",
      "Epoch 34, Batch 149, LR 0.260403 Loss 3.664949, Accuracy 92.686%\n",
      "Epoch 34, Batch 150, LR 0.260321 Loss 3.665905, Accuracy 92.661%\n",
      "Epoch 34, Batch 151, LR 0.260239 Loss 3.668154, Accuracy 92.648%\n",
      "Epoch 34, Batch 152, LR 0.260157 Loss 3.674082, Accuracy 92.614%\n",
      "Epoch 34, Batch 153, LR 0.260075 Loss 3.675962, Accuracy 92.622%\n",
      "Epoch 34, Batch 154, LR 0.259994 Loss 3.677095, Accuracy 92.619%\n",
      "Epoch 34, Batch 155, LR 0.259912 Loss 3.676608, Accuracy 92.621%\n",
      "Epoch 34, Batch 156, LR 0.259830 Loss 3.670381, Accuracy 92.648%\n",
      "Epoch 34, Batch 157, LR 0.259748 Loss 3.668302, Accuracy 92.665%\n",
      "Epoch 34, Batch 158, LR 0.259667 Loss 3.668784, Accuracy 92.657%\n",
      "Epoch 34, Batch 159, LR 0.259585 Loss 3.667858, Accuracy 92.664%\n",
      "Epoch 34, Batch 160, LR 0.259503 Loss 3.669345, Accuracy 92.661%\n",
      "Epoch 34, Batch 161, LR 0.259421 Loss 3.671312, Accuracy 92.653%\n",
      "Epoch 34, Batch 162, LR 0.259340 Loss 3.670760, Accuracy 92.655%\n",
      "Epoch 34, Batch 163, LR 0.259258 Loss 3.668641, Accuracy 92.657%\n",
      "Epoch 34, Batch 164, LR 0.259176 Loss 3.665992, Accuracy 92.673%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 165, LR 0.259095 Loss 3.662829, Accuracy 92.689%\n",
      "Epoch 34, Batch 166, LR 0.259013 Loss 3.663502, Accuracy 92.686%\n",
      "Epoch 34, Batch 167, LR 0.258931 Loss 3.662253, Accuracy 92.707%\n",
      "Epoch 34, Batch 168, LR 0.258850 Loss 3.662318, Accuracy 92.708%\n",
      "Epoch 34, Batch 169, LR 0.258768 Loss 3.659566, Accuracy 92.719%\n",
      "Epoch 34, Batch 170, LR 0.258687 Loss 3.657384, Accuracy 92.730%\n",
      "Epoch 34, Batch 171, LR 0.258605 Loss 3.656272, Accuracy 92.717%\n",
      "Epoch 34, Batch 172, LR 0.258523 Loss 3.656521, Accuracy 92.714%\n",
      "Epoch 34, Batch 173, LR 0.258442 Loss 3.660839, Accuracy 92.702%\n",
      "Epoch 34, Batch 174, LR 0.258360 Loss 3.657329, Accuracy 92.717%\n",
      "Epoch 34, Batch 175, LR 0.258279 Loss 3.655256, Accuracy 92.714%\n",
      "Epoch 34, Batch 176, LR 0.258197 Loss 3.652147, Accuracy 92.711%\n",
      "Epoch 34, Batch 177, LR 0.258116 Loss 3.653322, Accuracy 92.722%\n",
      "Epoch 34, Batch 178, LR 0.258034 Loss 3.655013, Accuracy 92.723%\n",
      "Epoch 34, Batch 179, LR 0.257953 Loss 3.656193, Accuracy 92.724%\n",
      "Epoch 34, Batch 180, LR 0.257871 Loss 3.652699, Accuracy 92.739%\n",
      "Epoch 34, Batch 181, LR 0.257790 Loss 3.652095, Accuracy 92.731%\n",
      "Epoch 34, Batch 182, LR 0.257708 Loss 3.649501, Accuracy 92.754%\n",
      "Epoch 34, Batch 183, LR 0.257627 Loss 3.648609, Accuracy 92.755%\n",
      "Epoch 34, Batch 184, LR 0.257545 Loss 3.648853, Accuracy 92.752%\n",
      "Epoch 34, Batch 185, LR 0.257464 Loss 3.651992, Accuracy 92.728%\n",
      "Epoch 34, Batch 186, LR 0.257383 Loss 3.651705, Accuracy 92.742%\n",
      "Epoch 34, Batch 187, LR 0.257301 Loss 3.652949, Accuracy 92.735%\n",
      "Epoch 34, Batch 188, LR 0.257220 Loss 3.653425, Accuracy 92.736%\n",
      "Epoch 34, Batch 189, LR 0.257138 Loss 3.653937, Accuracy 92.717%\n",
      "Epoch 34, Batch 190, LR 0.257057 Loss 3.652901, Accuracy 92.730%\n",
      "Epoch 34, Batch 191, LR 0.256976 Loss 3.653526, Accuracy 92.715%\n",
      "Epoch 34, Batch 192, LR 0.256894 Loss 3.654498, Accuracy 92.708%\n",
      "Epoch 34, Batch 193, LR 0.256813 Loss 3.657232, Accuracy 92.677%\n",
      "Epoch 34, Batch 194, LR 0.256732 Loss 3.658673, Accuracy 92.663%\n",
      "Epoch 34, Batch 195, LR 0.256650 Loss 3.658939, Accuracy 92.668%\n",
      "Epoch 34, Batch 196, LR 0.256569 Loss 3.661965, Accuracy 92.650%\n",
      "Epoch 34, Batch 197, LR 0.256488 Loss 3.662524, Accuracy 92.640%\n",
      "Epoch 34, Batch 198, LR 0.256406 Loss 3.661945, Accuracy 92.637%\n",
      "Epoch 34, Batch 199, LR 0.256325 Loss 3.659209, Accuracy 92.651%\n",
      "Epoch 34, Batch 200, LR 0.256244 Loss 3.659466, Accuracy 92.660%\n",
      "Epoch 34, Batch 201, LR 0.256163 Loss 3.658885, Accuracy 92.658%\n",
      "Epoch 34, Batch 202, LR 0.256081 Loss 3.657130, Accuracy 92.659%\n",
      "Epoch 34, Batch 203, LR 0.256000 Loss 3.658347, Accuracy 92.669%\n",
      "Epoch 34, Batch 204, LR 0.255919 Loss 3.658398, Accuracy 92.662%\n",
      "Epoch 34, Batch 205, LR 0.255838 Loss 3.659717, Accuracy 92.656%\n",
      "Epoch 34, Batch 206, LR 0.255757 Loss 3.660263, Accuracy 92.673%\n",
      "Epoch 34, Batch 207, LR 0.255675 Loss 3.659629, Accuracy 92.674%\n",
      "Epoch 34, Batch 208, LR 0.255594 Loss 3.658289, Accuracy 92.683%\n",
      "Epoch 34, Batch 209, LR 0.255513 Loss 3.656801, Accuracy 92.692%\n",
      "Epoch 34, Batch 210, LR 0.255432 Loss 3.659828, Accuracy 92.686%\n",
      "Epoch 34, Batch 211, LR 0.255351 Loss 3.659619, Accuracy 92.691%\n",
      "Epoch 34, Batch 212, LR 0.255270 Loss 3.663679, Accuracy 92.670%\n",
      "Epoch 34, Batch 213, LR 0.255188 Loss 3.661534, Accuracy 92.683%\n",
      "Epoch 34, Batch 214, LR 0.255107 Loss 3.663859, Accuracy 92.680%\n",
      "Epoch 34, Batch 215, LR 0.255026 Loss 3.661646, Accuracy 92.693%\n",
      "Epoch 34, Batch 216, LR 0.254945 Loss 3.661727, Accuracy 92.687%\n",
      "Epoch 34, Batch 217, LR 0.254864 Loss 3.663311, Accuracy 92.684%\n",
      "Epoch 34, Batch 218, LR 0.254783 Loss 3.665535, Accuracy 92.671%\n",
      "Epoch 34, Batch 219, LR 0.254702 Loss 3.663405, Accuracy 92.687%\n",
      "Epoch 34, Batch 220, LR 0.254621 Loss 3.665775, Accuracy 92.688%\n",
      "Epoch 34, Batch 221, LR 0.254540 Loss 3.664413, Accuracy 92.700%\n",
      "Epoch 34, Batch 222, LR 0.254459 Loss 3.664739, Accuracy 92.705%\n",
      "Epoch 34, Batch 223, LR 0.254378 Loss 3.665241, Accuracy 92.702%\n",
      "Epoch 34, Batch 224, LR 0.254297 Loss 3.664069, Accuracy 92.700%\n",
      "Epoch 34, Batch 225, LR 0.254216 Loss 3.664540, Accuracy 92.708%\n",
      "Epoch 34, Batch 226, LR 0.254135 Loss 3.664164, Accuracy 92.696%\n",
      "Epoch 34, Batch 227, LR 0.254054 Loss 3.665797, Accuracy 92.687%\n",
      "Epoch 34, Batch 228, LR 0.253973 Loss 3.668257, Accuracy 92.691%\n",
      "Epoch 34, Batch 229, LR 0.253892 Loss 3.668762, Accuracy 92.672%\n",
      "Epoch 34, Batch 230, LR 0.253811 Loss 3.668277, Accuracy 92.663%\n",
      "Epoch 34, Batch 231, LR 0.253730 Loss 3.668656, Accuracy 92.668%\n",
      "Epoch 34, Batch 232, LR 0.253650 Loss 3.668374, Accuracy 92.659%\n",
      "Epoch 34, Batch 233, LR 0.253569 Loss 3.671734, Accuracy 92.644%\n",
      "Epoch 34, Batch 234, LR 0.253488 Loss 3.676640, Accuracy 92.618%\n",
      "Epoch 34, Batch 235, LR 0.253407 Loss 3.677021, Accuracy 92.623%\n",
      "Epoch 34, Batch 236, LR 0.253326 Loss 3.678147, Accuracy 92.628%\n",
      "Epoch 34, Batch 237, LR 0.253245 Loss 3.679155, Accuracy 92.633%\n",
      "Epoch 34, Batch 238, LR 0.253164 Loss 3.678189, Accuracy 92.631%\n",
      "Epoch 34, Batch 239, LR 0.253084 Loss 3.679967, Accuracy 92.616%\n",
      "Epoch 34, Batch 240, LR 0.253003 Loss 3.680491, Accuracy 92.607%\n",
      "Epoch 34, Batch 241, LR 0.252922 Loss 3.681057, Accuracy 92.593%\n",
      "Epoch 34, Batch 242, LR 0.252841 Loss 3.682490, Accuracy 92.597%\n",
      "Epoch 34, Batch 243, LR 0.252760 Loss 3.684946, Accuracy 92.586%\n",
      "Epoch 34, Batch 244, LR 0.252680 Loss 3.681408, Accuracy 92.601%\n",
      "Epoch 34, Batch 245, LR 0.252599 Loss 3.682302, Accuracy 92.596%\n",
      "Epoch 34, Batch 246, LR 0.252518 Loss 3.681318, Accuracy 92.600%\n",
      "Epoch 34, Batch 247, LR 0.252437 Loss 3.681479, Accuracy 92.602%\n",
      "Epoch 34, Batch 248, LR 0.252357 Loss 3.679755, Accuracy 92.629%\n",
      "Epoch 34, Batch 249, LR 0.252276 Loss 3.678567, Accuracy 92.627%\n",
      "Epoch 34, Batch 250, LR 0.252195 Loss 3.675874, Accuracy 92.631%\n",
      "Epoch 34, Batch 251, LR 0.252115 Loss 3.674604, Accuracy 92.636%\n",
      "Epoch 34, Batch 252, LR 0.252034 Loss 3.675304, Accuracy 92.625%\n",
      "Epoch 34, Batch 253, LR 0.251953 Loss 3.674615, Accuracy 92.629%\n",
      "Epoch 34, Batch 254, LR 0.251873 Loss 3.673908, Accuracy 92.637%\n",
      "Epoch 34, Batch 255, LR 0.251792 Loss 3.674272, Accuracy 92.629%\n",
      "Epoch 34, Batch 256, LR 0.251711 Loss 3.674642, Accuracy 92.624%\n",
      "Epoch 34, Batch 257, LR 0.251631 Loss 3.673852, Accuracy 92.622%\n",
      "Epoch 34, Batch 258, LR 0.251550 Loss 3.673128, Accuracy 92.633%\n",
      "Epoch 34, Batch 259, LR 0.251470 Loss 3.670962, Accuracy 92.646%\n",
      "Epoch 34, Batch 260, LR 0.251389 Loss 3.672384, Accuracy 92.644%\n",
      "Epoch 34, Batch 261, LR 0.251309 Loss 3.671478, Accuracy 92.642%\n",
      "Epoch 34, Batch 262, LR 0.251228 Loss 3.671567, Accuracy 92.650%\n",
      "Epoch 34, Batch 263, LR 0.251147 Loss 3.673360, Accuracy 92.660%\n",
      "Epoch 34, Batch 264, LR 0.251067 Loss 3.672360, Accuracy 92.661%\n",
      "Epoch 34, Batch 265, LR 0.250986 Loss 3.671602, Accuracy 92.665%\n",
      "Epoch 34, Batch 266, LR 0.250906 Loss 3.672886, Accuracy 92.669%\n",
      "Epoch 34, Batch 267, LR 0.250825 Loss 3.676150, Accuracy 92.650%\n",
      "Epoch 34, Batch 268, LR 0.250745 Loss 3.676563, Accuracy 92.642%\n",
      "Epoch 34, Batch 269, LR 0.250664 Loss 3.676946, Accuracy 92.646%\n",
      "Epoch 34, Batch 270, LR 0.250584 Loss 3.677739, Accuracy 92.648%\n",
      "Epoch 34, Batch 271, LR 0.250504 Loss 3.678666, Accuracy 92.652%\n",
      "Epoch 34, Batch 272, LR 0.250423 Loss 3.678351, Accuracy 92.650%\n",
      "Epoch 34, Batch 273, LR 0.250343 Loss 3.680303, Accuracy 92.634%\n",
      "Epoch 34, Batch 274, LR 0.250262 Loss 3.680277, Accuracy 92.629%\n",
      "Epoch 34, Batch 275, LR 0.250182 Loss 3.680954, Accuracy 92.628%\n",
      "Epoch 34, Batch 276, LR 0.250101 Loss 3.680373, Accuracy 92.623%\n",
      "Epoch 34, Batch 277, LR 0.250021 Loss 3.680624, Accuracy 92.622%\n",
      "Epoch 34, Batch 278, LR 0.249941 Loss 3.679445, Accuracy 92.617%\n",
      "Epoch 34, Batch 279, LR 0.249860 Loss 3.680894, Accuracy 92.613%\n",
      "Epoch 34, Batch 280, LR 0.249780 Loss 3.682468, Accuracy 92.609%\n",
      "Epoch 34, Batch 281, LR 0.249700 Loss 3.680916, Accuracy 92.616%\n",
      "Epoch 34, Batch 282, LR 0.249619 Loss 3.683765, Accuracy 92.609%\n",
      "Epoch 34, Batch 283, LR 0.249539 Loss 3.682186, Accuracy 92.610%\n",
      "Epoch 34, Batch 284, LR 0.249459 Loss 3.684670, Accuracy 92.592%\n",
      "Epoch 34, Batch 285, LR 0.249378 Loss 3.684980, Accuracy 92.588%\n",
      "Epoch 34, Batch 286, LR 0.249298 Loss 3.684515, Accuracy 92.586%\n",
      "Epoch 34, Batch 287, LR 0.249218 Loss 3.687313, Accuracy 92.571%\n",
      "Epoch 34, Batch 288, LR 0.249138 Loss 3.687381, Accuracy 92.573%\n",
      "Epoch 34, Batch 289, LR 0.249057 Loss 3.688000, Accuracy 92.571%\n",
      "Epoch 34, Batch 290, LR 0.248977 Loss 3.688902, Accuracy 92.567%\n",
      "Epoch 34, Batch 291, LR 0.248897 Loss 3.688647, Accuracy 92.574%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 292, LR 0.248817 Loss 3.689061, Accuracy 92.573%\n",
      "Epoch 34, Batch 293, LR 0.248737 Loss 3.688311, Accuracy 92.582%\n",
      "Epoch 34, Batch 294, LR 0.248656 Loss 3.685701, Accuracy 92.589%\n",
      "Epoch 34, Batch 295, LR 0.248576 Loss 3.684504, Accuracy 92.587%\n",
      "Epoch 34, Batch 296, LR 0.248496 Loss 3.685988, Accuracy 92.578%\n",
      "Epoch 34, Batch 297, LR 0.248416 Loss 3.686004, Accuracy 92.587%\n",
      "Epoch 34, Batch 298, LR 0.248336 Loss 3.685303, Accuracy 92.596%\n",
      "Epoch 34, Batch 299, LR 0.248256 Loss 3.685658, Accuracy 92.603%\n",
      "Epoch 34, Batch 300, LR 0.248176 Loss 3.684859, Accuracy 92.607%\n",
      "Epoch 34, Batch 301, LR 0.248095 Loss 3.683249, Accuracy 92.618%\n",
      "Epoch 34, Batch 302, LR 0.248015 Loss 3.681507, Accuracy 92.625%\n",
      "Epoch 34, Batch 303, LR 0.247935 Loss 3.681166, Accuracy 92.626%\n",
      "Epoch 34, Batch 304, LR 0.247855 Loss 3.679925, Accuracy 92.627%\n",
      "Epoch 34, Batch 305, LR 0.247775 Loss 3.680903, Accuracy 92.620%\n",
      "Epoch 34, Batch 306, LR 0.247695 Loss 3.682154, Accuracy 92.611%\n",
      "Epoch 34, Batch 307, LR 0.247615 Loss 3.681519, Accuracy 92.612%\n",
      "Epoch 34, Batch 308, LR 0.247535 Loss 3.680212, Accuracy 92.619%\n",
      "Epoch 34, Batch 309, LR 0.247455 Loss 3.679497, Accuracy 92.622%\n",
      "Epoch 34, Batch 310, LR 0.247375 Loss 3.678202, Accuracy 92.636%\n",
      "Epoch 34, Batch 311, LR 0.247295 Loss 3.677549, Accuracy 92.647%\n",
      "Epoch 34, Batch 312, LR 0.247215 Loss 3.675415, Accuracy 92.658%\n",
      "Epoch 34, Batch 313, LR 0.247135 Loss 3.674705, Accuracy 92.664%\n",
      "Epoch 34, Batch 314, LR 0.247055 Loss 3.673855, Accuracy 92.670%\n",
      "Epoch 34, Batch 315, LR 0.246975 Loss 3.673710, Accuracy 92.669%\n",
      "Epoch 34, Batch 316, LR 0.246895 Loss 3.673655, Accuracy 92.667%\n",
      "Epoch 34, Batch 317, LR 0.246815 Loss 3.675280, Accuracy 92.658%\n",
      "Epoch 34, Batch 318, LR 0.246735 Loss 3.674992, Accuracy 92.669%\n",
      "Epoch 34, Batch 319, LR 0.246656 Loss 3.673221, Accuracy 92.675%\n",
      "Epoch 34, Batch 320, LR 0.246576 Loss 3.675536, Accuracy 92.664%\n",
      "Epoch 34, Batch 321, LR 0.246496 Loss 3.674515, Accuracy 92.672%\n",
      "Epoch 34, Batch 322, LR 0.246416 Loss 3.675740, Accuracy 92.658%\n",
      "Epoch 34, Batch 323, LR 0.246336 Loss 3.675765, Accuracy 92.654%\n",
      "Epoch 34, Batch 324, LR 0.246256 Loss 3.675539, Accuracy 92.655%\n",
      "Epoch 34, Batch 325, LR 0.246176 Loss 3.677292, Accuracy 92.647%\n",
      "Epoch 34, Batch 326, LR 0.246097 Loss 3.675338, Accuracy 92.655%\n",
      "Epoch 34, Batch 327, LR 0.246017 Loss 3.675271, Accuracy 92.646%\n",
      "Epoch 34, Batch 328, LR 0.245937 Loss 3.674400, Accuracy 92.654%\n",
      "Epoch 34, Batch 329, LR 0.245857 Loss 3.673097, Accuracy 92.665%\n",
      "Epoch 34, Batch 330, LR 0.245777 Loss 3.673026, Accuracy 92.663%\n",
      "Epoch 34, Batch 331, LR 0.245698 Loss 3.673368, Accuracy 92.660%\n",
      "Epoch 34, Batch 332, LR 0.245618 Loss 3.675721, Accuracy 92.663%\n",
      "Epoch 34, Batch 333, LR 0.245538 Loss 3.674921, Accuracy 92.666%\n",
      "Epoch 34, Batch 334, LR 0.245459 Loss 3.675766, Accuracy 92.672%\n",
      "Epoch 34, Batch 335, LR 0.245379 Loss 3.676220, Accuracy 92.663%\n",
      "Epoch 34, Batch 336, LR 0.245299 Loss 3.674818, Accuracy 92.676%\n",
      "Epoch 34, Batch 337, LR 0.245219 Loss 3.675221, Accuracy 92.670%\n",
      "Epoch 34, Batch 338, LR 0.245140 Loss 3.673330, Accuracy 92.682%\n",
      "Epoch 34, Batch 339, LR 0.245060 Loss 3.675571, Accuracy 92.667%\n",
      "Epoch 34, Batch 340, LR 0.244980 Loss 3.675736, Accuracy 92.665%\n",
      "Epoch 34, Batch 341, LR 0.244901 Loss 3.676394, Accuracy 92.671%\n",
      "Epoch 34, Batch 342, LR 0.244821 Loss 3.675610, Accuracy 92.679%\n",
      "Epoch 34, Batch 343, LR 0.244742 Loss 3.675417, Accuracy 92.684%\n",
      "Epoch 34, Batch 344, LR 0.244662 Loss 3.675365, Accuracy 92.683%\n",
      "Epoch 34, Batch 345, LR 0.244582 Loss 3.674112, Accuracy 92.688%\n",
      "Epoch 34, Batch 346, LR 0.244503 Loss 3.672706, Accuracy 92.691%\n",
      "Epoch 34, Batch 347, LR 0.244423 Loss 3.671565, Accuracy 92.703%\n",
      "Epoch 34, Batch 348, LR 0.244344 Loss 3.672455, Accuracy 92.699%\n",
      "Epoch 34, Batch 349, LR 0.244264 Loss 3.671247, Accuracy 92.707%\n",
      "Epoch 34, Batch 350, LR 0.244185 Loss 3.670583, Accuracy 92.694%\n",
      "Epoch 34, Batch 351, LR 0.244105 Loss 3.672442, Accuracy 92.688%\n",
      "Epoch 34, Batch 352, LR 0.244025 Loss 3.672134, Accuracy 92.687%\n",
      "Epoch 34, Batch 353, LR 0.243946 Loss 3.672387, Accuracy 92.685%\n",
      "Epoch 34, Batch 354, LR 0.243866 Loss 3.672176, Accuracy 92.691%\n",
      "Epoch 34, Batch 355, LR 0.243787 Loss 3.672398, Accuracy 92.694%\n",
      "Epoch 34, Batch 356, LR 0.243708 Loss 3.673228, Accuracy 92.694%\n",
      "Epoch 34, Batch 357, LR 0.243628 Loss 3.672152, Accuracy 92.700%\n",
      "Epoch 34, Batch 358, LR 0.243549 Loss 3.672872, Accuracy 92.696%\n",
      "Epoch 34, Batch 359, LR 0.243469 Loss 3.673146, Accuracy 92.697%\n",
      "Epoch 34, Batch 360, LR 0.243390 Loss 3.673837, Accuracy 92.695%\n",
      "Epoch 34, Batch 361, LR 0.243310 Loss 3.675939, Accuracy 92.683%\n",
      "Epoch 34, Batch 362, LR 0.243231 Loss 3.677006, Accuracy 92.680%\n",
      "Epoch 34, Batch 363, LR 0.243152 Loss 3.676634, Accuracy 92.678%\n",
      "Epoch 34, Batch 364, LR 0.243072 Loss 3.677195, Accuracy 92.681%\n",
      "Epoch 34, Batch 365, LR 0.242993 Loss 3.679348, Accuracy 92.678%\n",
      "Epoch 34, Batch 366, LR 0.242913 Loss 3.679179, Accuracy 92.676%\n",
      "Epoch 34, Batch 367, LR 0.242834 Loss 3.678040, Accuracy 92.679%\n",
      "Epoch 34, Batch 368, LR 0.242755 Loss 3.677549, Accuracy 92.686%\n",
      "Epoch 34, Batch 369, LR 0.242675 Loss 3.676741, Accuracy 92.694%\n",
      "Epoch 34, Batch 370, LR 0.242596 Loss 3.677510, Accuracy 92.694%\n",
      "Epoch 34, Batch 371, LR 0.242517 Loss 3.675391, Accuracy 92.703%\n",
      "Epoch 34, Batch 372, LR 0.242438 Loss 3.674057, Accuracy 92.715%\n",
      "Epoch 34, Batch 373, LR 0.242358 Loss 3.673541, Accuracy 92.715%\n",
      "Epoch 34, Batch 374, LR 0.242279 Loss 3.672186, Accuracy 92.726%\n",
      "Epoch 34, Batch 375, LR 0.242200 Loss 3.672286, Accuracy 92.731%\n",
      "Epoch 34, Batch 376, LR 0.242121 Loss 3.672949, Accuracy 92.734%\n",
      "Epoch 34, Batch 377, LR 0.242041 Loss 3.673373, Accuracy 92.737%\n",
      "Epoch 34, Batch 378, LR 0.241962 Loss 3.672302, Accuracy 92.743%\n",
      "Epoch 34, Batch 379, LR 0.241883 Loss 3.673257, Accuracy 92.736%\n",
      "Epoch 34, Batch 380, LR 0.241804 Loss 3.672686, Accuracy 92.738%\n",
      "Epoch 34, Batch 381, LR 0.241725 Loss 3.673195, Accuracy 92.737%\n",
      "Epoch 34, Batch 382, LR 0.241645 Loss 3.673635, Accuracy 92.738%\n",
      "Epoch 34, Batch 383, LR 0.241566 Loss 3.673438, Accuracy 92.736%\n",
      "Epoch 34, Batch 384, LR 0.241487 Loss 3.674181, Accuracy 92.731%\n",
      "Epoch 34, Batch 385, LR 0.241408 Loss 3.674588, Accuracy 92.731%\n",
      "Epoch 34, Batch 386, LR 0.241329 Loss 3.673403, Accuracy 92.738%\n",
      "Epoch 34, Batch 387, LR 0.241250 Loss 3.672132, Accuracy 92.749%\n",
      "Epoch 34, Batch 388, LR 0.241171 Loss 3.673140, Accuracy 92.747%\n",
      "Epoch 34, Batch 389, LR 0.241092 Loss 3.671501, Accuracy 92.758%\n",
      "Epoch 34, Batch 390, LR 0.241012 Loss 3.672182, Accuracy 92.752%\n",
      "Epoch 34, Batch 391, LR 0.240933 Loss 3.673072, Accuracy 92.749%\n",
      "Epoch 34, Batch 392, LR 0.240854 Loss 3.673110, Accuracy 92.752%\n",
      "Epoch 34, Batch 393, LR 0.240775 Loss 3.672769, Accuracy 92.754%\n",
      "Epoch 34, Batch 394, LR 0.240696 Loss 3.674787, Accuracy 92.739%\n",
      "Epoch 34, Batch 395, LR 0.240617 Loss 3.675121, Accuracy 92.739%\n",
      "Epoch 34, Batch 396, LR 0.240538 Loss 3.673760, Accuracy 92.740%\n",
      "Epoch 34, Batch 397, LR 0.240459 Loss 3.673650, Accuracy 92.733%\n",
      "Epoch 34, Batch 398, LR 0.240380 Loss 3.676378, Accuracy 92.714%\n",
      "Epoch 34, Batch 399, LR 0.240301 Loss 3.674567, Accuracy 92.724%\n",
      "Epoch 34, Batch 400, LR 0.240222 Loss 3.673813, Accuracy 92.725%\n",
      "Epoch 34, Batch 401, LR 0.240143 Loss 3.672567, Accuracy 92.733%\n",
      "Epoch 34, Batch 402, LR 0.240064 Loss 3.672304, Accuracy 92.736%\n",
      "Epoch 34, Batch 403, LR 0.239986 Loss 3.672293, Accuracy 92.734%\n",
      "Epoch 34, Batch 404, LR 0.239907 Loss 3.672318, Accuracy 92.733%\n",
      "Epoch 34, Batch 405, LR 0.239828 Loss 3.670953, Accuracy 92.735%\n",
      "Epoch 34, Batch 406, LR 0.239749 Loss 3.670873, Accuracy 92.734%\n",
      "Epoch 34, Batch 407, LR 0.239670 Loss 3.672591, Accuracy 92.723%\n",
      "Epoch 34, Batch 408, LR 0.239591 Loss 3.671879, Accuracy 92.720%\n",
      "Epoch 34, Batch 409, LR 0.239512 Loss 3.671885, Accuracy 92.720%\n",
      "Epoch 34, Batch 410, LR 0.239433 Loss 3.671521, Accuracy 92.725%\n",
      "Epoch 34, Batch 411, LR 0.239355 Loss 3.670908, Accuracy 92.722%\n",
      "Epoch 34, Batch 412, LR 0.239276 Loss 3.671454, Accuracy 92.718%\n",
      "Epoch 34, Batch 413, LR 0.239197 Loss 3.672953, Accuracy 92.711%\n",
      "Epoch 34, Batch 414, LR 0.239118 Loss 3.673347, Accuracy 92.718%\n",
      "Epoch 34, Batch 415, LR 0.239039 Loss 3.674381, Accuracy 92.709%\n",
      "Epoch 34, Batch 416, LR 0.238961 Loss 3.674259, Accuracy 92.710%\n",
      "Epoch 34, Batch 417, LR 0.238882 Loss 3.674641, Accuracy 92.712%\n",
      "Epoch 34, Batch 418, LR 0.238803 Loss 3.675191, Accuracy 92.720%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 419, LR 0.238724 Loss 3.673589, Accuracy 92.724%\n",
      "Epoch 34, Batch 420, LR 0.238646 Loss 3.674076, Accuracy 92.725%\n",
      "Epoch 34, Batch 421, LR 0.238567 Loss 3.674078, Accuracy 92.728%\n",
      "Epoch 34, Batch 422, LR 0.238488 Loss 3.674071, Accuracy 92.732%\n",
      "Epoch 34, Batch 423, LR 0.238409 Loss 3.674189, Accuracy 92.732%\n",
      "Epoch 34, Batch 424, LR 0.238331 Loss 3.672261, Accuracy 92.740%\n",
      "Epoch 34, Batch 425, LR 0.238252 Loss 3.672457, Accuracy 92.732%\n",
      "Epoch 34, Batch 426, LR 0.238173 Loss 3.672620, Accuracy 92.734%\n",
      "Epoch 34, Batch 427, LR 0.238095 Loss 3.671022, Accuracy 92.742%\n",
      "Epoch 34, Batch 428, LR 0.238016 Loss 3.670808, Accuracy 92.741%\n",
      "Epoch 34, Batch 429, LR 0.237938 Loss 3.671463, Accuracy 92.734%\n",
      "Epoch 34, Batch 430, LR 0.237859 Loss 3.670332, Accuracy 92.736%\n",
      "Epoch 34, Batch 431, LR 0.237780 Loss 3.669390, Accuracy 92.748%\n",
      "Epoch 34, Batch 432, LR 0.237702 Loss 3.667862, Accuracy 92.752%\n",
      "Epoch 34, Batch 433, LR 0.237623 Loss 3.667920, Accuracy 92.750%\n",
      "Epoch 34, Batch 434, LR 0.237545 Loss 3.667984, Accuracy 92.753%\n",
      "Epoch 34, Batch 435, LR 0.237466 Loss 3.667208, Accuracy 92.757%\n",
      "Epoch 34, Batch 436, LR 0.237388 Loss 3.667053, Accuracy 92.752%\n",
      "Epoch 34, Batch 437, LR 0.237309 Loss 3.667270, Accuracy 92.751%\n",
      "Epoch 34, Batch 438, LR 0.237230 Loss 3.666871, Accuracy 92.751%\n",
      "Epoch 34, Batch 439, LR 0.237152 Loss 3.668336, Accuracy 92.745%\n",
      "Epoch 34, Batch 440, LR 0.237073 Loss 3.668472, Accuracy 92.738%\n",
      "Epoch 34, Batch 441, LR 0.236995 Loss 3.666911, Accuracy 92.738%\n",
      "Epoch 34, Batch 442, LR 0.236917 Loss 3.665922, Accuracy 92.748%\n",
      "Epoch 34, Batch 443, LR 0.236838 Loss 3.665252, Accuracy 92.748%\n",
      "Epoch 34, Batch 444, LR 0.236760 Loss 3.665208, Accuracy 92.749%\n",
      "Epoch 34, Batch 445, LR 0.236681 Loss 3.664497, Accuracy 92.753%\n",
      "Epoch 34, Batch 446, LR 0.236603 Loss 3.665839, Accuracy 92.745%\n",
      "Epoch 34, Batch 447, LR 0.236524 Loss 3.667142, Accuracy 92.738%\n",
      "Epoch 34, Batch 448, LR 0.236446 Loss 3.665644, Accuracy 92.747%\n",
      "Epoch 34, Batch 449, LR 0.236368 Loss 3.665437, Accuracy 92.746%\n",
      "Epoch 34, Batch 450, LR 0.236289 Loss 3.666104, Accuracy 92.747%\n",
      "Epoch 34, Batch 451, LR 0.236211 Loss 3.666551, Accuracy 92.740%\n",
      "Epoch 34, Batch 452, LR 0.236132 Loss 3.666088, Accuracy 92.732%\n",
      "Epoch 34, Batch 453, LR 0.236054 Loss 3.665063, Accuracy 92.739%\n",
      "Epoch 34, Batch 454, LR 0.235976 Loss 3.665039, Accuracy 92.738%\n",
      "Epoch 34, Batch 455, LR 0.235897 Loss 3.666129, Accuracy 92.734%\n",
      "Epoch 34, Batch 456, LR 0.235819 Loss 3.665962, Accuracy 92.734%\n",
      "Epoch 34, Batch 457, LR 0.235741 Loss 3.666966, Accuracy 92.731%\n",
      "Epoch 34, Batch 458, LR 0.235663 Loss 3.666309, Accuracy 92.737%\n",
      "Epoch 34, Batch 459, LR 0.235584 Loss 3.666266, Accuracy 92.739%\n",
      "Epoch 34, Batch 460, LR 0.235506 Loss 3.666919, Accuracy 92.739%\n",
      "Epoch 34, Batch 461, LR 0.235428 Loss 3.667492, Accuracy 92.731%\n",
      "Epoch 34, Batch 462, LR 0.235350 Loss 3.667924, Accuracy 92.727%\n",
      "Epoch 34, Batch 463, LR 0.235271 Loss 3.667478, Accuracy 92.727%\n",
      "Epoch 34, Batch 464, LR 0.235193 Loss 3.666508, Accuracy 92.730%\n",
      "Epoch 34, Batch 465, LR 0.235115 Loss 3.666688, Accuracy 92.732%\n",
      "Epoch 34, Batch 466, LR 0.235037 Loss 3.667276, Accuracy 92.729%\n",
      "Epoch 34, Batch 467, LR 0.234959 Loss 3.668077, Accuracy 92.725%\n",
      "Epoch 34, Batch 468, LR 0.234880 Loss 3.670115, Accuracy 92.718%\n",
      "Epoch 34, Batch 469, LR 0.234802 Loss 3.670217, Accuracy 92.717%\n",
      "Epoch 34, Batch 470, LR 0.234724 Loss 3.671406, Accuracy 92.706%\n",
      "Epoch 34, Batch 471, LR 0.234646 Loss 3.671046, Accuracy 92.707%\n",
      "Epoch 34, Batch 472, LR 0.234568 Loss 3.670274, Accuracy 92.706%\n",
      "Epoch 34, Batch 473, LR 0.234490 Loss 3.671539, Accuracy 92.700%\n",
      "Epoch 34, Batch 474, LR 0.234412 Loss 3.671074, Accuracy 92.703%\n",
      "Epoch 34, Batch 475, LR 0.234333 Loss 3.670760, Accuracy 92.709%\n",
      "Epoch 34, Batch 476, LR 0.234255 Loss 3.670991, Accuracy 92.708%\n",
      "Epoch 34, Batch 477, LR 0.234177 Loss 3.671647, Accuracy 92.707%\n",
      "Epoch 34, Batch 478, LR 0.234099 Loss 3.671571, Accuracy 92.709%\n",
      "Epoch 34, Batch 479, LR 0.234021 Loss 3.670950, Accuracy 92.714%\n",
      "Epoch 34, Batch 480, LR 0.233943 Loss 3.670682, Accuracy 92.710%\n",
      "Epoch 34, Batch 481, LR 0.233865 Loss 3.672476, Accuracy 92.701%\n",
      "Epoch 34, Batch 482, LR 0.233787 Loss 3.671979, Accuracy 92.706%\n",
      "Epoch 34, Batch 483, LR 0.233709 Loss 3.673497, Accuracy 92.695%\n",
      "Epoch 34, Batch 484, LR 0.233631 Loss 3.673618, Accuracy 92.691%\n",
      "Epoch 34, Batch 485, LR 0.233553 Loss 3.673259, Accuracy 92.692%\n",
      "Epoch 34, Batch 486, LR 0.233475 Loss 3.673687, Accuracy 92.691%\n",
      "Epoch 34, Batch 487, LR 0.233397 Loss 3.672689, Accuracy 92.694%\n",
      "Epoch 34, Batch 488, LR 0.233319 Loss 3.672187, Accuracy 92.700%\n",
      "Epoch 34, Batch 489, LR 0.233241 Loss 3.671374, Accuracy 92.704%\n",
      "Epoch 34, Batch 490, LR 0.233164 Loss 3.670447, Accuracy 92.709%\n",
      "Epoch 34, Batch 491, LR 0.233086 Loss 3.669927, Accuracy 92.714%\n",
      "Epoch 34, Batch 492, LR 0.233008 Loss 3.670392, Accuracy 92.710%\n",
      "Epoch 34, Batch 493, LR 0.232930 Loss 3.670359, Accuracy 92.712%\n",
      "Epoch 34, Batch 494, LR 0.232852 Loss 3.670644, Accuracy 92.709%\n",
      "Epoch 34, Batch 495, LR 0.232774 Loss 3.670012, Accuracy 92.710%\n",
      "Epoch 34, Batch 496, LR 0.232696 Loss 3.670861, Accuracy 92.709%\n",
      "Epoch 34, Batch 497, LR 0.232619 Loss 3.671114, Accuracy 92.709%\n",
      "Epoch 34, Batch 498, LR 0.232541 Loss 3.671733, Accuracy 92.704%\n",
      "Epoch 34, Batch 499, LR 0.232463 Loss 3.671055, Accuracy 92.707%\n",
      "Epoch 34, Batch 500, LR 0.232385 Loss 3.670824, Accuracy 92.711%\n",
      "Epoch 34, Batch 501, LR 0.232307 Loss 3.671049, Accuracy 92.708%\n",
      "Epoch 34, Batch 502, LR 0.232230 Loss 3.670077, Accuracy 92.715%\n",
      "Epoch 34, Batch 503, LR 0.232152 Loss 3.669262, Accuracy 92.716%\n",
      "Epoch 34, Batch 504, LR 0.232074 Loss 3.669222, Accuracy 92.722%\n",
      "Epoch 34, Batch 505, LR 0.231996 Loss 3.667443, Accuracy 92.732%\n",
      "Epoch 34, Batch 506, LR 0.231919 Loss 3.667358, Accuracy 92.736%\n",
      "Epoch 34, Batch 507, LR 0.231841 Loss 3.667151, Accuracy 92.742%\n",
      "Epoch 34, Batch 508, LR 0.231763 Loss 3.667904, Accuracy 92.737%\n",
      "Epoch 34, Batch 509, LR 0.231685 Loss 3.668826, Accuracy 92.729%\n",
      "Epoch 34, Batch 510, LR 0.231608 Loss 3.667399, Accuracy 92.736%\n",
      "Epoch 34, Batch 511, LR 0.231530 Loss 3.667435, Accuracy 92.738%\n",
      "Epoch 34, Batch 512, LR 0.231452 Loss 3.668405, Accuracy 92.734%\n",
      "Epoch 34, Batch 513, LR 0.231375 Loss 3.668527, Accuracy 92.739%\n",
      "Epoch 34, Batch 514, LR 0.231297 Loss 3.668415, Accuracy 92.742%\n",
      "Epoch 34, Batch 515, LR 0.231220 Loss 3.668944, Accuracy 92.746%\n",
      "Epoch 34, Batch 516, LR 0.231142 Loss 3.668862, Accuracy 92.751%\n",
      "Epoch 34, Batch 517, LR 0.231064 Loss 3.668430, Accuracy 92.751%\n",
      "Epoch 34, Batch 518, LR 0.230987 Loss 3.668551, Accuracy 92.752%\n",
      "Epoch 34, Batch 519, LR 0.230909 Loss 3.668438, Accuracy 92.757%\n",
      "Epoch 34, Batch 520, LR 0.230832 Loss 3.668796, Accuracy 92.761%\n",
      "Epoch 34, Batch 521, LR 0.230754 Loss 3.668572, Accuracy 92.762%\n",
      "Epoch 34, Batch 522, LR 0.230677 Loss 3.668813, Accuracy 92.759%\n",
      "Epoch 34, Batch 523, LR 0.230599 Loss 3.669136, Accuracy 92.758%\n",
      "Epoch 34, Batch 524, LR 0.230522 Loss 3.667341, Accuracy 92.764%\n",
      "Epoch 34, Batch 525, LR 0.230444 Loss 3.666379, Accuracy 92.768%\n",
      "Epoch 34, Batch 526, LR 0.230367 Loss 3.665593, Accuracy 92.768%\n",
      "Epoch 34, Batch 527, LR 0.230289 Loss 3.665366, Accuracy 92.770%\n",
      "Epoch 34, Batch 528, LR 0.230212 Loss 3.664393, Accuracy 92.773%\n",
      "Epoch 34, Batch 529, LR 0.230134 Loss 3.664383, Accuracy 92.772%\n",
      "Epoch 34, Batch 530, LR 0.230057 Loss 3.663646, Accuracy 92.774%\n",
      "Epoch 34, Batch 531, LR 0.229979 Loss 3.662898, Accuracy 92.777%\n",
      "Epoch 34, Batch 532, LR 0.229902 Loss 3.664207, Accuracy 92.773%\n",
      "Epoch 34, Batch 533, LR 0.229824 Loss 3.663362, Accuracy 92.775%\n",
      "Epoch 34, Batch 534, LR 0.229747 Loss 3.663406, Accuracy 92.771%\n",
      "Epoch 34, Batch 535, LR 0.229670 Loss 3.662486, Accuracy 92.775%\n",
      "Epoch 34, Batch 536, LR 0.229592 Loss 3.662305, Accuracy 92.775%\n",
      "Epoch 34, Batch 537, LR 0.229515 Loss 3.662345, Accuracy 92.780%\n",
      "Epoch 34, Batch 538, LR 0.229438 Loss 3.664082, Accuracy 92.773%\n",
      "Epoch 34, Batch 539, LR 0.229360 Loss 3.664584, Accuracy 92.767%\n",
      "Epoch 34, Batch 540, LR 0.229283 Loss 3.663410, Accuracy 92.772%\n",
      "Epoch 34, Batch 541, LR 0.229206 Loss 3.663910, Accuracy 92.768%\n",
      "Epoch 34, Batch 542, LR 0.229128 Loss 3.664382, Accuracy 92.763%\n",
      "Epoch 34, Batch 543, LR 0.229051 Loss 3.664966, Accuracy 92.766%\n",
      "Epoch 34, Batch 544, LR 0.228974 Loss 3.664388, Accuracy 92.769%\n",
      "Epoch 34, Batch 545, LR 0.228896 Loss 3.664216, Accuracy 92.772%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 546, LR 0.228819 Loss 3.664023, Accuracy 92.777%\n",
      "Epoch 34, Batch 547, LR 0.228742 Loss 3.665090, Accuracy 92.777%\n",
      "Epoch 34, Batch 548, LR 0.228665 Loss 3.665303, Accuracy 92.775%\n",
      "Epoch 34, Batch 549, LR 0.228587 Loss 3.665313, Accuracy 92.774%\n",
      "Epoch 34, Batch 550, LR 0.228510 Loss 3.665309, Accuracy 92.773%\n",
      "Epoch 34, Batch 551, LR 0.228433 Loss 3.666127, Accuracy 92.772%\n",
      "Epoch 34, Batch 552, LR 0.228356 Loss 3.665948, Accuracy 92.769%\n",
      "Epoch 34, Batch 553, LR 0.228279 Loss 3.667228, Accuracy 92.765%\n",
      "Epoch 34, Batch 554, LR 0.228202 Loss 3.666048, Accuracy 92.773%\n",
      "Epoch 34, Batch 555, LR 0.228124 Loss 3.666491, Accuracy 92.779%\n",
      "Epoch 34, Batch 556, LR 0.228047 Loss 3.666628, Accuracy 92.785%\n",
      "Epoch 34, Batch 557, LR 0.227970 Loss 3.667495, Accuracy 92.784%\n",
      "Epoch 34, Batch 558, LR 0.227893 Loss 3.666919, Accuracy 92.787%\n",
      "Epoch 34, Batch 559, LR 0.227816 Loss 3.665936, Accuracy 92.791%\n",
      "Epoch 34, Batch 560, LR 0.227739 Loss 3.666200, Accuracy 92.787%\n",
      "Epoch 34, Batch 561, LR 0.227662 Loss 3.666082, Accuracy 92.788%\n",
      "Epoch 34, Batch 562, LR 0.227585 Loss 3.665868, Accuracy 92.792%\n",
      "Epoch 34, Batch 563, LR 0.227508 Loss 3.664846, Accuracy 92.793%\n",
      "Epoch 34, Batch 564, LR 0.227431 Loss 3.665304, Accuracy 92.798%\n",
      "Epoch 34, Batch 565, LR 0.227353 Loss 3.664993, Accuracy 92.799%\n",
      "Epoch 34, Batch 566, LR 0.227276 Loss 3.664817, Accuracy 92.796%\n",
      "Epoch 34, Batch 567, LR 0.227199 Loss 3.665104, Accuracy 92.799%\n",
      "Epoch 34, Batch 568, LR 0.227122 Loss 3.663656, Accuracy 92.806%\n",
      "Epoch 34, Batch 569, LR 0.227045 Loss 3.663604, Accuracy 92.805%\n",
      "Epoch 34, Batch 570, LR 0.226968 Loss 3.663412, Accuracy 92.803%\n",
      "Epoch 34, Batch 571, LR 0.226892 Loss 3.663211, Accuracy 92.805%\n",
      "Epoch 34, Batch 572, LR 0.226815 Loss 3.663506, Accuracy 92.803%\n",
      "Epoch 34, Batch 573, LR 0.226738 Loss 3.663773, Accuracy 92.805%\n",
      "Epoch 34, Batch 574, LR 0.226661 Loss 3.664216, Accuracy 92.800%\n",
      "Epoch 34, Batch 575, LR 0.226584 Loss 3.663843, Accuracy 92.800%\n",
      "Epoch 34, Batch 576, LR 0.226507 Loss 3.662844, Accuracy 92.807%\n",
      "Epoch 34, Batch 577, LR 0.226430 Loss 3.663781, Accuracy 92.800%\n",
      "Epoch 34, Batch 578, LR 0.226353 Loss 3.663564, Accuracy 92.800%\n",
      "Epoch 34, Batch 579, LR 0.226276 Loss 3.663944, Accuracy 92.796%\n",
      "Epoch 34, Batch 580, LR 0.226199 Loss 3.663541, Accuracy 92.795%\n",
      "Epoch 34, Batch 581, LR 0.226123 Loss 3.662877, Accuracy 92.801%\n",
      "Epoch 34, Batch 582, LR 0.226046 Loss 3.663344, Accuracy 92.805%\n",
      "Epoch 34, Batch 583, LR 0.225969 Loss 3.664367, Accuracy 92.808%\n",
      "Epoch 34, Batch 584, LR 0.225892 Loss 3.664061, Accuracy 92.811%\n",
      "Epoch 34, Batch 585, LR 0.225815 Loss 3.663525, Accuracy 92.812%\n",
      "Epoch 34, Batch 586, LR 0.225738 Loss 3.662507, Accuracy 92.818%\n",
      "Epoch 34, Batch 587, LR 0.225662 Loss 3.661519, Accuracy 92.821%\n",
      "Epoch 34, Batch 588, LR 0.225585 Loss 3.662180, Accuracy 92.823%\n",
      "Epoch 34, Batch 589, LR 0.225508 Loss 3.661226, Accuracy 92.828%\n",
      "Epoch 34, Batch 590, LR 0.225431 Loss 3.661722, Accuracy 92.823%\n",
      "Epoch 34, Batch 591, LR 0.225355 Loss 3.661754, Accuracy 92.826%\n",
      "Epoch 34, Batch 592, LR 0.225278 Loss 3.662269, Accuracy 92.820%\n",
      "Epoch 34, Batch 593, LR 0.225201 Loss 3.662718, Accuracy 92.817%\n",
      "Epoch 34, Batch 594, LR 0.225125 Loss 3.662998, Accuracy 92.820%\n",
      "Epoch 34, Batch 595, LR 0.225048 Loss 3.663460, Accuracy 92.815%\n",
      "Epoch 34, Batch 596, LR 0.224971 Loss 3.663598, Accuracy 92.814%\n",
      "Epoch 34, Batch 597, LR 0.224895 Loss 3.665231, Accuracy 92.806%\n",
      "Epoch 34, Batch 598, LR 0.224818 Loss 3.664430, Accuracy 92.812%\n",
      "Epoch 34, Batch 599, LR 0.224741 Loss 3.665540, Accuracy 92.802%\n",
      "Epoch 34, Batch 600, LR 0.224665 Loss 3.666481, Accuracy 92.802%\n",
      "Epoch 34, Batch 601, LR 0.224588 Loss 3.665862, Accuracy 92.806%\n",
      "Epoch 34, Batch 602, LR 0.224511 Loss 3.664521, Accuracy 92.809%\n",
      "Epoch 34, Batch 603, LR 0.224435 Loss 3.663869, Accuracy 92.815%\n",
      "Epoch 34, Batch 604, LR 0.224358 Loss 3.664209, Accuracy 92.811%\n",
      "Epoch 34, Batch 605, LR 0.224282 Loss 3.664754, Accuracy 92.809%\n",
      "Epoch 34, Batch 606, LR 0.224205 Loss 3.664873, Accuracy 92.809%\n",
      "Epoch 34, Batch 607, LR 0.224129 Loss 3.666016, Accuracy 92.805%\n",
      "Epoch 34, Batch 608, LR 0.224052 Loss 3.666272, Accuracy 92.806%\n",
      "Epoch 34, Batch 609, LR 0.223976 Loss 3.665577, Accuracy 92.808%\n",
      "Epoch 34, Batch 610, LR 0.223899 Loss 3.665175, Accuracy 92.814%\n",
      "Epoch 34, Batch 611, LR 0.223823 Loss 3.664351, Accuracy 92.815%\n",
      "Epoch 34, Batch 612, LR 0.223746 Loss 3.664943, Accuracy 92.810%\n",
      "Epoch 34, Batch 613, LR 0.223670 Loss 3.664775, Accuracy 92.809%\n",
      "Epoch 34, Batch 614, LR 0.223593 Loss 3.664326, Accuracy 92.815%\n",
      "Epoch 34, Batch 615, LR 0.223517 Loss 3.663751, Accuracy 92.816%\n",
      "Epoch 34, Batch 616, LR 0.223440 Loss 3.661954, Accuracy 92.823%\n",
      "Epoch 34, Batch 617, LR 0.223364 Loss 3.662027, Accuracy 92.819%\n",
      "Epoch 34, Batch 618, LR 0.223288 Loss 3.661975, Accuracy 92.820%\n",
      "Epoch 34, Batch 619, LR 0.223211 Loss 3.661746, Accuracy 92.826%\n",
      "Epoch 34, Batch 620, LR 0.223135 Loss 3.661827, Accuracy 92.824%\n",
      "Epoch 34, Batch 621, LR 0.223058 Loss 3.661959, Accuracy 92.822%\n",
      "Epoch 34, Batch 622, LR 0.222982 Loss 3.662425, Accuracy 92.823%\n",
      "Epoch 34, Batch 623, LR 0.222906 Loss 3.662956, Accuracy 92.820%\n",
      "Epoch 34, Batch 624, LR 0.222829 Loss 3.662298, Accuracy 92.822%\n",
      "Epoch 34, Batch 625, LR 0.222753 Loss 3.662763, Accuracy 92.821%\n",
      "Epoch 34, Batch 626, LR 0.222677 Loss 3.662587, Accuracy 92.823%\n",
      "Epoch 34, Batch 627, LR 0.222600 Loss 3.662509, Accuracy 92.815%\n",
      "Epoch 34, Batch 628, LR 0.222524 Loss 3.662581, Accuracy 92.811%\n",
      "Epoch 34, Batch 629, LR 0.222448 Loss 3.662255, Accuracy 92.813%\n",
      "Epoch 34, Batch 630, LR 0.222372 Loss 3.662952, Accuracy 92.814%\n",
      "Epoch 34, Batch 631, LR 0.222295 Loss 3.661870, Accuracy 92.818%\n",
      "Epoch 34, Batch 632, LR 0.222219 Loss 3.662296, Accuracy 92.813%\n",
      "Epoch 34, Batch 633, LR 0.222143 Loss 3.661249, Accuracy 92.814%\n",
      "Epoch 34, Batch 634, LR 0.222067 Loss 3.659328, Accuracy 92.821%\n",
      "Epoch 34, Batch 635, LR 0.221990 Loss 3.659362, Accuracy 92.820%\n",
      "Epoch 34, Batch 636, LR 0.221914 Loss 3.659391, Accuracy 92.821%\n",
      "Epoch 34, Batch 637, LR 0.221838 Loss 3.659679, Accuracy 92.819%\n",
      "Epoch 34, Batch 638, LR 0.221762 Loss 3.659530, Accuracy 92.819%\n",
      "Epoch 34, Batch 639, LR 0.221686 Loss 3.658566, Accuracy 92.823%\n",
      "Epoch 34, Batch 640, LR 0.221609 Loss 3.658549, Accuracy 92.820%\n",
      "Epoch 34, Batch 641, LR 0.221533 Loss 3.659407, Accuracy 92.820%\n",
      "Epoch 34, Batch 642, LR 0.221457 Loss 3.659377, Accuracy 92.819%\n",
      "Epoch 34, Batch 643, LR 0.221381 Loss 3.659824, Accuracy 92.817%\n",
      "Epoch 34, Batch 644, LR 0.221305 Loss 3.659406, Accuracy 92.818%\n",
      "Epoch 34, Batch 645, LR 0.221229 Loss 3.659162, Accuracy 92.819%\n",
      "Epoch 34, Batch 646, LR 0.221153 Loss 3.658738, Accuracy 92.818%\n",
      "Epoch 34, Batch 647, LR 0.221077 Loss 3.658739, Accuracy 92.820%\n",
      "Epoch 34, Batch 648, LR 0.221001 Loss 3.658239, Accuracy 92.820%\n",
      "Epoch 34, Batch 649, LR 0.220925 Loss 3.658624, Accuracy 92.816%\n",
      "Epoch 34, Batch 650, LR 0.220849 Loss 3.658125, Accuracy 92.814%\n",
      "Epoch 34, Batch 651, LR 0.220773 Loss 3.657771, Accuracy 92.815%\n",
      "Epoch 34, Batch 652, LR 0.220697 Loss 3.657231, Accuracy 92.821%\n",
      "Epoch 34, Batch 653, LR 0.220621 Loss 3.656628, Accuracy 92.824%\n",
      "Epoch 34, Batch 654, LR 0.220545 Loss 3.655510, Accuracy 92.829%\n",
      "Epoch 34, Batch 655, LR 0.220469 Loss 3.656385, Accuracy 92.826%\n",
      "Epoch 34, Batch 656, LR 0.220393 Loss 3.656071, Accuracy 92.827%\n",
      "Epoch 34, Batch 657, LR 0.220317 Loss 3.656472, Accuracy 92.824%\n",
      "Epoch 34, Batch 658, LR 0.220241 Loss 3.655911, Accuracy 92.824%\n",
      "Epoch 34, Batch 659, LR 0.220165 Loss 3.656111, Accuracy 92.823%\n",
      "Epoch 34, Batch 660, LR 0.220089 Loss 3.655491, Accuracy 92.828%\n",
      "Epoch 34, Batch 661, LR 0.220013 Loss 3.654759, Accuracy 92.829%\n",
      "Epoch 34, Batch 662, LR 0.219937 Loss 3.654366, Accuracy 92.832%\n",
      "Epoch 34, Batch 663, LR 0.219861 Loss 3.653840, Accuracy 92.834%\n",
      "Epoch 34, Batch 664, LR 0.219785 Loss 3.654161, Accuracy 92.837%\n",
      "Epoch 34, Batch 665, LR 0.219710 Loss 3.654605, Accuracy 92.838%\n",
      "Epoch 34, Batch 666, LR 0.219634 Loss 3.654690, Accuracy 92.836%\n",
      "Epoch 34, Batch 667, LR 0.219558 Loss 3.654619, Accuracy 92.833%\n",
      "Epoch 34, Batch 668, LR 0.219482 Loss 3.654831, Accuracy 92.831%\n",
      "Epoch 34, Batch 669, LR 0.219406 Loss 3.654433, Accuracy 92.833%\n",
      "Epoch 34, Batch 670, LR 0.219330 Loss 3.654808, Accuracy 92.832%\n",
      "Epoch 34, Batch 671, LR 0.219255 Loss 3.655159, Accuracy 92.829%\n",
      "Epoch 34, Batch 672, LR 0.219179 Loss 3.656017, Accuracy 92.823%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 673, LR 0.219103 Loss 3.656511, Accuracy 92.822%\n",
      "Epoch 34, Batch 674, LR 0.219027 Loss 3.656218, Accuracy 92.823%\n",
      "Epoch 34, Batch 675, LR 0.218952 Loss 3.655751, Accuracy 92.824%\n",
      "Epoch 34, Batch 676, LR 0.218876 Loss 3.655953, Accuracy 92.823%\n",
      "Epoch 34, Batch 677, LR 0.218800 Loss 3.656161, Accuracy 92.821%\n",
      "Epoch 34, Batch 678, LR 0.218724 Loss 3.656074, Accuracy 92.824%\n",
      "Epoch 34, Batch 679, LR 0.218649 Loss 3.656391, Accuracy 92.823%\n",
      "Epoch 34, Batch 680, LR 0.218573 Loss 3.655859, Accuracy 92.825%\n",
      "Epoch 34, Batch 681, LR 0.218497 Loss 3.655429, Accuracy 92.825%\n",
      "Epoch 34, Batch 682, LR 0.218422 Loss 3.655003, Accuracy 92.827%\n",
      "Epoch 34, Batch 683, LR 0.218346 Loss 3.654703, Accuracy 92.826%\n",
      "Epoch 34, Batch 684, LR 0.218271 Loss 3.654030, Accuracy 92.828%\n",
      "Epoch 34, Batch 685, LR 0.218195 Loss 3.653983, Accuracy 92.825%\n",
      "Epoch 34, Batch 686, LR 0.218119 Loss 3.653588, Accuracy 92.828%\n",
      "Epoch 34, Batch 687, LR 0.218044 Loss 3.654130, Accuracy 92.824%\n",
      "Epoch 34, Batch 688, LR 0.217968 Loss 3.653583, Accuracy 92.825%\n",
      "Epoch 34, Batch 689, LR 0.217893 Loss 3.652657, Accuracy 92.829%\n",
      "Epoch 34, Batch 690, LR 0.217817 Loss 3.652230, Accuracy 92.834%\n",
      "Epoch 34, Batch 691, LR 0.217741 Loss 3.651240, Accuracy 92.840%\n",
      "Epoch 34, Batch 692, LR 0.217666 Loss 3.651109, Accuracy 92.842%\n",
      "Epoch 34, Batch 693, LR 0.217590 Loss 3.650940, Accuracy 92.847%\n",
      "Epoch 34, Batch 694, LR 0.217515 Loss 3.650332, Accuracy 92.852%\n",
      "Epoch 34, Batch 695, LR 0.217439 Loss 3.651330, Accuracy 92.851%\n",
      "Epoch 34, Batch 696, LR 0.217364 Loss 3.650509, Accuracy 92.858%\n",
      "Epoch 34, Batch 697, LR 0.217288 Loss 3.649872, Accuracy 92.857%\n",
      "Epoch 34, Batch 698, LR 0.217213 Loss 3.649488, Accuracy 92.859%\n",
      "Epoch 34, Batch 699, LR 0.217137 Loss 3.648591, Accuracy 92.868%\n",
      "Epoch 34, Batch 700, LR 0.217062 Loss 3.648196, Accuracy 92.869%\n",
      "Epoch 34, Batch 701, LR 0.216987 Loss 3.648034, Accuracy 92.872%\n",
      "Epoch 34, Batch 702, LR 0.216911 Loss 3.647841, Accuracy 92.873%\n",
      "Epoch 34, Batch 703, LR 0.216836 Loss 3.648206, Accuracy 92.869%\n",
      "Epoch 34, Batch 704, LR 0.216760 Loss 3.649149, Accuracy 92.866%\n",
      "Epoch 34, Batch 705, LR 0.216685 Loss 3.649154, Accuracy 92.866%\n",
      "Epoch 34, Batch 706, LR 0.216610 Loss 3.649585, Accuracy 92.864%\n",
      "Epoch 34, Batch 707, LR 0.216534 Loss 3.649588, Accuracy 92.865%\n",
      "Epoch 34, Batch 708, LR 0.216459 Loss 3.650153, Accuracy 92.858%\n",
      "Epoch 34, Batch 709, LR 0.216384 Loss 3.650565, Accuracy 92.857%\n",
      "Epoch 34, Batch 710, LR 0.216308 Loss 3.650601, Accuracy 92.858%\n",
      "Epoch 34, Batch 711, LR 0.216233 Loss 3.650600, Accuracy 92.859%\n",
      "Epoch 34, Batch 712, LR 0.216158 Loss 3.650891, Accuracy 92.859%\n",
      "Epoch 34, Batch 713, LR 0.216082 Loss 3.650117, Accuracy 92.860%\n",
      "Epoch 34, Batch 714, LR 0.216007 Loss 3.650177, Accuracy 92.855%\n",
      "Epoch 34, Batch 715, LR 0.215932 Loss 3.650468, Accuracy 92.851%\n",
      "Epoch 34, Batch 716, LR 0.215857 Loss 3.650099, Accuracy 92.854%\n",
      "Epoch 34, Batch 717, LR 0.215781 Loss 3.651123, Accuracy 92.849%\n",
      "Epoch 34, Batch 718, LR 0.215706 Loss 3.651050, Accuracy 92.848%\n",
      "Epoch 34, Batch 719, LR 0.215631 Loss 3.651496, Accuracy 92.846%\n",
      "Epoch 34, Batch 720, LR 0.215556 Loss 3.651168, Accuracy 92.849%\n",
      "Epoch 34, Batch 721, LR 0.215481 Loss 3.650889, Accuracy 92.854%\n",
      "Epoch 34, Batch 722, LR 0.215405 Loss 3.651526, Accuracy 92.853%\n",
      "Epoch 34, Batch 723, LR 0.215330 Loss 3.652670, Accuracy 92.846%\n",
      "Epoch 34, Batch 724, LR 0.215255 Loss 3.652916, Accuracy 92.846%\n",
      "Epoch 34, Batch 725, LR 0.215180 Loss 3.652667, Accuracy 92.846%\n",
      "Epoch 34, Batch 726, LR 0.215105 Loss 3.653047, Accuracy 92.844%\n",
      "Epoch 34, Batch 727, LR 0.215030 Loss 3.652872, Accuracy 92.846%\n",
      "Epoch 34, Batch 728, LR 0.214955 Loss 3.652791, Accuracy 92.849%\n",
      "Epoch 34, Batch 729, LR 0.214879 Loss 3.652595, Accuracy 92.849%\n",
      "Epoch 34, Batch 730, LR 0.214804 Loss 3.652544, Accuracy 92.850%\n",
      "Epoch 34, Batch 731, LR 0.214729 Loss 3.651846, Accuracy 92.850%\n",
      "Epoch 34, Batch 732, LR 0.214654 Loss 3.651954, Accuracy 92.847%\n",
      "Epoch 34, Batch 733, LR 0.214579 Loss 3.652134, Accuracy 92.847%\n",
      "Epoch 34, Batch 734, LR 0.214504 Loss 3.651711, Accuracy 92.847%\n",
      "Epoch 34, Batch 735, LR 0.214429 Loss 3.651183, Accuracy 92.850%\n",
      "Epoch 34, Batch 736, LR 0.214354 Loss 3.651447, Accuracy 92.852%\n",
      "Epoch 34, Batch 737, LR 0.214279 Loss 3.652187, Accuracy 92.846%\n",
      "Epoch 34, Batch 738, LR 0.214204 Loss 3.653221, Accuracy 92.842%\n",
      "Epoch 34, Batch 739, LR 0.214129 Loss 3.653128, Accuracy 92.838%\n",
      "Epoch 34, Batch 740, LR 0.214054 Loss 3.653535, Accuracy 92.838%\n",
      "Epoch 34, Batch 741, LR 0.213979 Loss 3.654423, Accuracy 92.836%\n",
      "Epoch 34, Batch 742, LR 0.213904 Loss 3.653777, Accuracy 92.840%\n",
      "Epoch 34, Batch 743, LR 0.213829 Loss 3.654711, Accuracy 92.838%\n",
      "Epoch 34, Batch 744, LR 0.213754 Loss 3.653958, Accuracy 92.843%\n",
      "Epoch 34, Batch 745, LR 0.213680 Loss 3.653075, Accuracy 92.849%\n",
      "Epoch 34, Batch 746, LR 0.213605 Loss 3.653124, Accuracy 92.847%\n",
      "Epoch 34, Batch 747, LR 0.213530 Loss 3.652869, Accuracy 92.846%\n",
      "Epoch 34, Batch 748, LR 0.213455 Loss 3.652393, Accuracy 92.849%\n",
      "Epoch 34, Batch 749, LR 0.213380 Loss 3.653365, Accuracy 92.847%\n",
      "Epoch 34, Batch 750, LR 0.213305 Loss 3.653140, Accuracy 92.845%\n",
      "Epoch 34, Batch 751, LR 0.213230 Loss 3.653853, Accuracy 92.837%\n",
      "Epoch 34, Batch 752, LR 0.213156 Loss 3.653726, Accuracy 92.836%\n",
      "Epoch 34, Batch 753, LR 0.213081 Loss 3.652809, Accuracy 92.840%\n",
      "Epoch 34, Batch 754, LR 0.213006 Loss 3.653653, Accuracy 92.836%\n",
      "Epoch 34, Batch 755, LR 0.212931 Loss 3.654313, Accuracy 92.833%\n",
      "Epoch 34, Batch 756, LR 0.212856 Loss 3.654445, Accuracy 92.832%\n",
      "Epoch 34, Batch 757, LR 0.212782 Loss 3.655042, Accuracy 92.831%\n",
      "Epoch 34, Batch 758, LR 0.212707 Loss 3.655162, Accuracy 92.834%\n",
      "Epoch 34, Batch 759, LR 0.212632 Loss 3.655222, Accuracy 92.833%\n",
      "Epoch 34, Batch 760, LR 0.212557 Loss 3.654922, Accuracy 92.835%\n",
      "Epoch 34, Batch 761, LR 0.212483 Loss 3.654908, Accuracy 92.835%\n",
      "Epoch 34, Batch 762, LR 0.212408 Loss 3.654516, Accuracy 92.840%\n",
      "Epoch 34, Batch 763, LR 0.212333 Loss 3.654299, Accuracy 92.838%\n",
      "Epoch 34, Batch 764, LR 0.212259 Loss 3.653755, Accuracy 92.842%\n",
      "Epoch 34, Batch 765, LR 0.212184 Loss 3.653983, Accuracy 92.839%\n",
      "Epoch 34, Batch 766, LR 0.212109 Loss 3.652936, Accuracy 92.843%\n",
      "Epoch 34, Batch 767, LR 0.212035 Loss 3.652922, Accuracy 92.844%\n",
      "Epoch 34, Batch 768, LR 0.211960 Loss 3.651677, Accuracy 92.850%\n",
      "Epoch 34, Batch 769, LR 0.211885 Loss 3.652072, Accuracy 92.847%\n",
      "Epoch 34, Batch 770, LR 0.211811 Loss 3.651894, Accuracy 92.847%\n",
      "Epoch 34, Batch 771, LR 0.211736 Loss 3.652488, Accuracy 92.844%\n",
      "Epoch 34, Batch 772, LR 0.211662 Loss 3.652581, Accuracy 92.845%\n",
      "Epoch 34, Batch 773, LR 0.211587 Loss 3.651903, Accuracy 92.848%\n",
      "Epoch 34, Batch 774, LR 0.211512 Loss 3.652104, Accuracy 92.846%\n",
      "Epoch 34, Batch 775, LR 0.211438 Loss 3.651653, Accuracy 92.849%\n",
      "Epoch 34, Batch 776, LR 0.211363 Loss 3.652117, Accuracy 92.849%\n",
      "Epoch 34, Batch 777, LR 0.211289 Loss 3.652117, Accuracy 92.852%\n",
      "Epoch 34, Batch 778, LR 0.211214 Loss 3.651825, Accuracy 92.855%\n",
      "Epoch 34, Batch 779, LR 0.211140 Loss 3.651578, Accuracy 92.857%\n",
      "Epoch 34, Batch 780, LR 0.211065 Loss 3.651098, Accuracy 92.857%\n",
      "Epoch 34, Batch 781, LR 0.210991 Loss 3.651175, Accuracy 92.855%\n",
      "Epoch 34, Batch 782, LR 0.210916 Loss 3.651012, Accuracy 92.855%\n",
      "Epoch 34, Batch 783, LR 0.210842 Loss 3.651750, Accuracy 92.850%\n",
      "Epoch 34, Batch 784, LR 0.210767 Loss 3.651194, Accuracy 92.851%\n",
      "Epoch 34, Batch 785, LR 0.210693 Loss 3.651135, Accuracy 92.849%\n",
      "Epoch 34, Batch 786, LR 0.210619 Loss 3.650853, Accuracy 92.854%\n",
      "Epoch 34, Batch 787, LR 0.210544 Loss 3.650745, Accuracy 92.855%\n",
      "Epoch 34, Batch 788, LR 0.210470 Loss 3.649947, Accuracy 92.857%\n",
      "Epoch 34, Batch 789, LR 0.210395 Loss 3.650802, Accuracy 92.850%\n",
      "Epoch 34, Batch 790, LR 0.210321 Loss 3.652946, Accuracy 92.841%\n",
      "Epoch 34, Batch 791, LR 0.210247 Loss 3.652866, Accuracy 92.844%\n",
      "Epoch 34, Batch 792, LR 0.210172 Loss 3.652849, Accuracy 92.847%\n",
      "Epoch 34, Batch 793, LR 0.210098 Loss 3.653172, Accuracy 92.846%\n",
      "Epoch 34, Batch 794, LR 0.210024 Loss 3.652775, Accuracy 92.850%\n",
      "Epoch 34, Batch 795, LR 0.209949 Loss 3.652717, Accuracy 92.850%\n",
      "Epoch 34, Batch 796, LR 0.209875 Loss 3.652322, Accuracy 92.855%\n",
      "Epoch 34, Batch 797, LR 0.209801 Loss 3.653548, Accuracy 92.848%\n",
      "Epoch 34, Batch 798, LR 0.209727 Loss 3.653381, Accuracy 92.849%\n",
      "Epoch 34, Batch 799, LR 0.209652 Loss 3.653405, Accuracy 92.848%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 800, LR 0.209578 Loss 3.653405, Accuracy 92.846%\n",
      "Epoch 34, Batch 801, LR 0.209504 Loss 3.652944, Accuracy 92.851%\n",
      "Epoch 34, Batch 802, LR 0.209430 Loss 3.653613, Accuracy 92.850%\n",
      "Epoch 34, Batch 803, LR 0.209355 Loss 3.654339, Accuracy 92.847%\n",
      "Epoch 34, Batch 804, LR 0.209281 Loss 3.654198, Accuracy 92.844%\n",
      "Epoch 34, Batch 805, LR 0.209207 Loss 3.653645, Accuracy 92.849%\n",
      "Epoch 34, Batch 806, LR 0.209133 Loss 3.653411, Accuracy 92.848%\n",
      "Epoch 34, Batch 807, LR 0.209059 Loss 3.653768, Accuracy 92.844%\n",
      "Epoch 34, Batch 808, LR 0.208984 Loss 3.653317, Accuracy 92.845%\n",
      "Epoch 34, Batch 809, LR 0.208910 Loss 3.653929, Accuracy 92.842%\n",
      "Epoch 34, Batch 810, LR 0.208836 Loss 3.653658, Accuracy 92.842%\n",
      "Epoch 34, Batch 811, LR 0.208762 Loss 3.654489, Accuracy 92.843%\n",
      "Epoch 34, Batch 812, LR 0.208688 Loss 3.653316, Accuracy 92.848%\n",
      "Epoch 34, Batch 813, LR 0.208614 Loss 3.653555, Accuracy 92.849%\n",
      "Epoch 34, Batch 814, LR 0.208540 Loss 3.653635, Accuracy 92.847%\n",
      "Epoch 34, Batch 815, LR 0.208466 Loss 3.653643, Accuracy 92.842%\n",
      "Epoch 34, Batch 816, LR 0.208392 Loss 3.653900, Accuracy 92.839%\n",
      "Epoch 34, Batch 817, LR 0.208318 Loss 3.653634, Accuracy 92.840%\n",
      "Epoch 34, Batch 818, LR 0.208244 Loss 3.653603, Accuracy 92.839%\n",
      "Epoch 34, Batch 819, LR 0.208170 Loss 3.653733, Accuracy 92.835%\n",
      "Epoch 34, Batch 820, LR 0.208096 Loss 3.652992, Accuracy 92.838%\n",
      "Epoch 34, Batch 821, LR 0.208022 Loss 3.653557, Accuracy 92.837%\n",
      "Epoch 34, Batch 822, LR 0.207948 Loss 3.653737, Accuracy 92.838%\n",
      "Epoch 34, Batch 823, LR 0.207874 Loss 3.653454, Accuracy 92.836%\n",
      "Epoch 34, Batch 824, LR 0.207800 Loss 3.653025, Accuracy 92.837%\n",
      "Epoch 34, Batch 825, LR 0.207726 Loss 3.653364, Accuracy 92.835%\n",
      "Epoch 34, Batch 826, LR 0.207652 Loss 3.653912, Accuracy 92.834%\n",
      "Epoch 34, Batch 827, LR 0.207578 Loss 3.652958, Accuracy 92.838%\n",
      "Epoch 34, Batch 828, LR 0.207504 Loss 3.652899, Accuracy 92.838%\n",
      "Epoch 34, Batch 829, LR 0.207430 Loss 3.652698, Accuracy 92.837%\n",
      "Epoch 34, Batch 830, LR 0.207356 Loss 3.653081, Accuracy 92.835%\n",
      "Epoch 34, Batch 831, LR 0.207282 Loss 3.654235, Accuracy 92.828%\n",
      "Epoch 34, Batch 832, LR 0.207208 Loss 3.653694, Accuracy 92.830%\n",
      "Epoch 34, Batch 833, LR 0.207134 Loss 3.653634, Accuracy 92.827%\n",
      "Epoch 34, Batch 834, LR 0.207061 Loss 3.653242, Accuracy 92.829%\n",
      "Epoch 34, Batch 835, LR 0.206987 Loss 3.653017, Accuracy 92.831%\n",
      "Epoch 34, Batch 836, LR 0.206913 Loss 3.653292, Accuracy 92.832%\n",
      "Epoch 34, Batch 837, LR 0.206839 Loss 3.652431, Accuracy 92.835%\n",
      "Epoch 34, Batch 838, LR 0.206765 Loss 3.652865, Accuracy 92.835%\n",
      "Epoch 34, Batch 839, LR 0.206692 Loss 3.652483, Accuracy 92.838%\n",
      "Epoch 34, Batch 840, LR 0.206618 Loss 3.652503, Accuracy 92.838%\n",
      "Epoch 34, Batch 841, LR 0.206544 Loss 3.653161, Accuracy 92.834%\n",
      "Epoch 34, Batch 842, LR 0.206470 Loss 3.652514, Accuracy 92.837%\n",
      "Epoch 34, Batch 843, LR 0.206397 Loss 3.651869, Accuracy 92.839%\n",
      "Epoch 34, Batch 844, LR 0.206323 Loss 3.652300, Accuracy 92.837%\n",
      "Epoch 34, Batch 845, LR 0.206249 Loss 3.652334, Accuracy 92.839%\n",
      "Epoch 34, Batch 846, LR 0.206175 Loss 3.652485, Accuracy 92.839%\n",
      "Epoch 34, Batch 847, LR 0.206102 Loss 3.652836, Accuracy 92.837%\n",
      "Epoch 34, Batch 848, LR 0.206028 Loss 3.652624, Accuracy 92.837%\n",
      "Epoch 34, Batch 849, LR 0.205954 Loss 3.653101, Accuracy 92.834%\n",
      "Epoch 34, Batch 850, LR 0.205881 Loss 3.653095, Accuracy 92.835%\n",
      "Epoch 34, Batch 851, LR 0.205807 Loss 3.653068, Accuracy 92.836%\n",
      "Epoch 34, Batch 852, LR 0.205734 Loss 3.653464, Accuracy 92.833%\n",
      "Epoch 34, Batch 853, LR 0.205660 Loss 3.653870, Accuracy 92.834%\n",
      "Epoch 34, Batch 854, LR 0.205586 Loss 3.654518, Accuracy 92.832%\n",
      "Epoch 34, Batch 855, LR 0.205513 Loss 3.653923, Accuracy 92.833%\n",
      "Epoch 34, Batch 856, LR 0.205439 Loss 3.654122, Accuracy 92.834%\n",
      "Epoch 34, Batch 857, LR 0.205366 Loss 3.654209, Accuracy 92.834%\n",
      "Epoch 34, Batch 858, LR 0.205292 Loss 3.653520, Accuracy 92.839%\n",
      "Epoch 34, Batch 859, LR 0.205218 Loss 3.653541, Accuracy 92.839%\n",
      "Epoch 34, Batch 860, LR 0.205145 Loss 3.652985, Accuracy 92.839%\n",
      "Epoch 34, Batch 861, LR 0.205071 Loss 3.653256, Accuracy 92.835%\n",
      "Epoch 34, Batch 862, LR 0.204998 Loss 3.653695, Accuracy 92.832%\n",
      "Epoch 34, Batch 863, LR 0.204924 Loss 3.653466, Accuracy 92.835%\n",
      "Epoch 34, Batch 864, LR 0.204851 Loss 3.653379, Accuracy 92.831%\n",
      "Epoch 34, Batch 865, LR 0.204777 Loss 3.653705, Accuracy 92.831%\n",
      "Epoch 34, Batch 866, LR 0.204704 Loss 3.653936, Accuracy 92.827%\n",
      "Epoch 34, Batch 867, LR 0.204631 Loss 3.654151, Accuracy 92.825%\n",
      "Epoch 34, Batch 868, LR 0.204557 Loss 3.654315, Accuracy 92.826%\n",
      "Epoch 34, Batch 869, LR 0.204484 Loss 3.654179, Accuracy 92.829%\n",
      "Epoch 34, Batch 870, LR 0.204410 Loss 3.653790, Accuracy 92.830%\n",
      "Epoch 34, Batch 871, LR 0.204337 Loss 3.653353, Accuracy 92.831%\n",
      "Epoch 34, Batch 872, LR 0.204263 Loss 3.653680, Accuracy 92.828%\n",
      "Epoch 34, Batch 873, LR 0.204190 Loss 3.653621, Accuracy 92.825%\n",
      "Epoch 34, Batch 874, LR 0.204117 Loss 3.653493, Accuracy 92.825%\n",
      "Epoch 34, Batch 875, LR 0.204043 Loss 3.653437, Accuracy 92.823%\n",
      "Epoch 34, Batch 876, LR 0.203970 Loss 3.653191, Accuracy 92.825%\n",
      "Epoch 34, Batch 877, LR 0.203897 Loss 3.652811, Accuracy 92.824%\n",
      "Epoch 34, Batch 878, LR 0.203823 Loss 3.653521, Accuracy 92.823%\n",
      "Epoch 34, Batch 879, LR 0.203750 Loss 3.653864, Accuracy 92.820%\n",
      "Epoch 34, Batch 880, LR 0.203677 Loss 3.653947, Accuracy 92.819%\n",
      "Epoch 34, Batch 881, LR 0.203604 Loss 3.654354, Accuracy 92.817%\n",
      "Epoch 34, Batch 882, LR 0.203530 Loss 3.654138, Accuracy 92.818%\n",
      "Epoch 34, Batch 883, LR 0.203457 Loss 3.654187, Accuracy 92.820%\n",
      "Epoch 34, Batch 884, LR 0.203384 Loss 3.654031, Accuracy 92.823%\n",
      "Epoch 34, Batch 885, LR 0.203311 Loss 3.654101, Accuracy 92.821%\n",
      "Epoch 34, Batch 886, LR 0.203237 Loss 3.654242, Accuracy 92.816%\n",
      "Epoch 34, Batch 887, LR 0.203164 Loss 3.654392, Accuracy 92.815%\n",
      "Epoch 34, Batch 888, LR 0.203091 Loss 3.654506, Accuracy 92.815%\n",
      "Epoch 34, Batch 889, LR 0.203018 Loss 3.653999, Accuracy 92.818%\n",
      "Epoch 34, Batch 890, LR 0.202945 Loss 3.654337, Accuracy 92.817%\n",
      "Epoch 34, Batch 891, LR 0.202871 Loss 3.655438, Accuracy 92.813%\n",
      "Epoch 34, Batch 892, LR 0.202798 Loss 3.655598, Accuracy 92.816%\n",
      "Epoch 34, Batch 893, LR 0.202725 Loss 3.654757, Accuracy 92.821%\n",
      "Epoch 34, Batch 894, LR 0.202652 Loss 3.655325, Accuracy 92.816%\n",
      "Epoch 34, Batch 895, LR 0.202579 Loss 3.654935, Accuracy 92.814%\n",
      "Epoch 34, Batch 896, LR 0.202506 Loss 3.654645, Accuracy 92.814%\n",
      "Epoch 34, Batch 897, LR 0.202433 Loss 3.655363, Accuracy 92.811%\n",
      "Epoch 34, Batch 898, LR 0.202360 Loss 3.654548, Accuracy 92.814%\n",
      "Epoch 34, Batch 899, LR 0.202287 Loss 3.654691, Accuracy 92.816%\n",
      "Epoch 34, Batch 900, LR 0.202213 Loss 3.654897, Accuracy 92.816%\n",
      "Epoch 34, Batch 901, LR 0.202140 Loss 3.654926, Accuracy 92.819%\n",
      "Epoch 34, Batch 902, LR 0.202067 Loss 3.654528, Accuracy 92.822%\n",
      "Epoch 34, Batch 903, LR 0.201994 Loss 3.653874, Accuracy 92.823%\n",
      "Epoch 34, Batch 904, LR 0.201921 Loss 3.653882, Accuracy 92.824%\n",
      "Epoch 34, Batch 905, LR 0.201848 Loss 3.654011, Accuracy 92.822%\n",
      "Epoch 34, Batch 906, LR 0.201775 Loss 3.653559, Accuracy 92.824%\n",
      "Epoch 34, Batch 907, LR 0.201702 Loss 3.653422, Accuracy 92.823%\n",
      "Epoch 34, Batch 908, LR 0.201630 Loss 3.654212, Accuracy 92.820%\n",
      "Epoch 34, Batch 909, LR 0.201557 Loss 3.654146, Accuracy 92.820%\n",
      "Epoch 34, Batch 910, LR 0.201484 Loss 3.653691, Accuracy 92.825%\n",
      "Epoch 34, Batch 911, LR 0.201411 Loss 3.654434, Accuracy 92.821%\n",
      "Epoch 34, Batch 912, LR 0.201338 Loss 3.654794, Accuracy 92.821%\n",
      "Epoch 34, Batch 913, LR 0.201265 Loss 3.654625, Accuracy 92.822%\n",
      "Epoch 34, Batch 914, LR 0.201192 Loss 3.654111, Accuracy 92.822%\n",
      "Epoch 34, Batch 915, LR 0.201119 Loss 3.653939, Accuracy 92.824%\n",
      "Epoch 34, Batch 916, LR 0.201046 Loss 3.654435, Accuracy 92.819%\n",
      "Epoch 34, Batch 917, LR 0.200973 Loss 3.654218, Accuracy 92.820%\n",
      "Epoch 34, Batch 918, LR 0.200901 Loss 3.653322, Accuracy 92.821%\n",
      "Epoch 34, Batch 919, LR 0.200828 Loss 3.653189, Accuracy 92.821%\n",
      "Epoch 34, Batch 920, LR 0.200755 Loss 3.653548, Accuracy 92.820%\n",
      "Epoch 34, Batch 921, LR 0.200682 Loss 3.653821, Accuracy 92.820%\n",
      "Epoch 34, Batch 922, LR 0.200609 Loss 3.653269, Accuracy 92.824%\n",
      "Epoch 34, Batch 923, LR 0.200537 Loss 3.653306, Accuracy 92.825%\n",
      "Epoch 34, Batch 924, LR 0.200464 Loss 3.653612, Accuracy 92.825%\n",
      "Epoch 34, Batch 925, LR 0.200391 Loss 3.654072, Accuracy 92.823%\n",
      "Epoch 34, Batch 926, LR 0.200318 Loss 3.654366, Accuracy 92.817%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 927, LR 0.200246 Loss 3.654570, Accuracy 92.815%\n",
      "Epoch 34, Batch 928, LR 0.200173 Loss 3.653984, Accuracy 92.818%\n",
      "Epoch 34, Batch 929, LR 0.200100 Loss 3.653875, Accuracy 92.822%\n",
      "Epoch 34, Batch 930, LR 0.200028 Loss 3.654384, Accuracy 92.822%\n",
      "Epoch 34, Batch 931, LR 0.199955 Loss 3.654581, Accuracy 92.819%\n",
      "Epoch 34, Batch 932, LR 0.199882 Loss 3.655102, Accuracy 92.820%\n",
      "Epoch 34, Batch 933, LR 0.199810 Loss 3.654698, Accuracy 92.825%\n",
      "Epoch 34, Batch 934, LR 0.199737 Loss 3.654295, Accuracy 92.825%\n",
      "Epoch 34, Batch 935, LR 0.199664 Loss 3.654408, Accuracy 92.824%\n",
      "Epoch 34, Batch 936, LR 0.199592 Loss 3.654046, Accuracy 92.827%\n",
      "Epoch 34, Batch 937, LR 0.199519 Loss 3.654112, Accuracy 92.828%\n",
      "Epoch 34, Batch 938, LR 0.199446 Loss 3.654631, Accuracy 92.821%\n",
      "Epoch 34, Batch 939, LR 0.199374 Loss 3.654997, Accuracy 92.822%\n",
      "Epoch 34, Batch 940, LR 0.199301 Loss 3.654376, Accuracy 92.827%\n",
      "Epoch 34, Batch 941, LR 0.199229 Loss 3.653830, Accuracy 92.830%\n",
      "Epoch 34, Batch 942, LR 0.199156 Loss 3.653885, Accuracy 92.832%\n",
      "Epoch 34, Batch 943, LR 0.199084 Loss 3.653081, Accuracy 92.835%\n",
      "Epoch 34, Batch 944, LR 0.199011 Loss 3.652984, Accuracy 92.839%\n",
      "Epoch 34, Batch 945, LR 0.198939 Loss 3.653072, Accuracy 92.836%\n",
      "Epoch 34, Batch 946, LR 0.198866 Loss 3.653052, Accuracy 92.836%\n",
      "Epoch 34, Batch 947, LR 0.198794 Loss 3.653183, Accuracy 92.836%\n",
      "Epoch 34, Batch 948, LR 0.198721 Loss 3.653139, Accuracy 92.837%\n",
      "Epoch 34, Batch 949, LR 0.198649 Loss 3.652848, Accuracy 92.838%\n",
      "Epoch 34, Batch 950, LR 0.198576 Loss 3.652993, Accuracy 92.836%\n",
      "Epoch 34, Batch 951, LR 0.198504 Loss 3.653296, Accuracy 92.836%\n",
      "Epoch 34, Batch 952, LR 0.198431 Loss 3.653542, Accuracy 92.835%\n",
      "Epoch 34, Batch 953, LR 0.198359 Loss 3.653662, Accuracy 92.835%\n",
      "Epoch 34, Batch 954, LR 0.198287 Loss 3.653598, Accuracy 92.836%\n",
      "Epoch 34, Batch 955, LR 0.198214 Loss 3.653911, Accuracy 92.839%\n",
      "Epoch 34, Batch 956, LR 0.198142 Loss 3.654435, Accuracy 92.837%\n",
      "Epoch 34, Batch 957, LR 0.198069 Loss 3.654834, Accuracy 92.835%\n",
      "Epoch 34, Batch 958, LR 0.197997 Loss 3.654636, Accuracy 92.833%\n",
      "Epoch 34, Batch 959, LR 0.197925 Loss 3.654629, Accuracy 92.834%\n",
      "Epoch 34, Batch 960, LR 0.197852 Loss 3.654473, Accuracy 92.834%\n",
      "Epoch 34, Batch 961, LR 0.197780 Loss 3.654647, Accuracy 92.833%\n",
      "Epoch 34, Batch 962, LR 0.197708 Loss 3.654814, Accuracy 92.832%\n",
      "Epoch 34, Batch 963, LR 0.197636 Loss 3.654393, Accuracy 92.832%\n",
      "Epoch 34, Batch 964, LR 0.197563 Loss 3.654135, Accuracy 92.837%\n",
      "Epoch 34, Batch 965, LR 0.197491 Loss 3.653894, Accuracy 92.839%\n",
      "Epoch 34, Batch 966, LR 0.197419 Loss 3.654117, Accuracy 92.838%\n",
      "Epoch 34, Batch 967, LR 0.197347 Loss 3.654701, Accuracy 92.837%\n",
      "Epoch 34, Batch 968, LR 0.197274 Loss 3.655006, Accuracy 92.837%\n",
      "Epoch 34, Batch 969, LR 0.197202 Loss 3.654342, Accuracy 92.839%\n",
      "Epoch 34, Batch 970, LR 0.197130 Loss 3.652893, Accuracy 92.845%\n",
      "Epoch 34, Batch 971, LR 0.197058 Loss 3.652594, Accuracy 92.844%\n",
      "Epoch 34, Batch 972, LR 0.196985 Loss 3.652842, Accuracy 92.844%\n",
      "Epoch 34, Batch 973, LR 0.196913 Loss 3.652565, Accuracy 92.848%\n",
      "Epoch 34, Batch 974, LR 0.196841 Loss 3.652285, Accuracy 92.849%\n",
      "Epoch 34, Batch 975, LR 0.196769 Loss 3.652414, Accuracy 92.849%\n",
      "Epoch 34, Batch 976, LR 0.196697 Loss 3.652052, Accuracy 92.850%\n",
      "Epoch 34, Batch 977, LR 0.196625 Loss 3.652400, Accuracy 92.850%\n",
      "Epoch 34, Batch 978, LR 0.196553 Loss 3.651685, Accuracy 92.851%\n",
      "Epoch 34, Batch 979, LR 0.196481 Loss 3.652015, Accuracy 92.851%\n",
      "Epoch 34, Batch 980, LR 0.196408 Loss 3.651794, Accuracy 92.852%\n",
      "Epoch 34, Batch 981, LR 0.196336 Loss 3.651436, Accuracy 92.852%\n",
      "Epoch 34, Batch 982, LR 0.196264 Loss 3.651789, Accuracy 92.849%\n",
      "Epoch 34, Batch 983, LR 0.196192 Loss 3.651565, Accuracy 92.850%\n",
      "Epoch 34, Batch 984, LR 0.196120 Loss 3.651104, Accuracy 92.851%\n",
      "Epoch 34, Batch 985, LR 0.196048 Loss 3.651119, Accuracy 92.851%\n",
      "Epoch 34, Batch 986, LR 0.195976 Loss 3.650281, Accuracy 92.854%\n",
      "Epoch 34, Batch 987, LR 0.195904 Loss 3.650674, Accuracy 92.852%\n",
      "Epoch 34, Batch 988, LR 0.195832 Loss 3.650471, Accuracy 92.853%\n",
      "Epoch 34, Batch 989, LR 0.195760 Loss 3.650008, Accuracy 92.859%\n",
      "Epoch 34, Batch 990, LR 0.195688 Loss 3.650169, Accuracy 92.861%\n",
      "Epoch 34, Batch 991, LR 0.195616 Loss 3.649898, Accuracy 92.862%\n",
      "Epoch 34, Batch 992, LR 0.195544 Loss 3.650510, Accuracy 92.856%\n",
      "Epoch 34, Batch 993, LR 0.195472 Loss 3.650390, Accuracy 92.855%\n",
      "Epoch 34, Batch 994, LR 0.195401 Loss 3.650044, Accuracy 92.854%\n",
      "Epoch 34, Batch 995, LR 0.195329 Loss 3.649386, Accuracy 92.858%\n",
      "Epoch 34, Batch 996, LR 0.195257 Loss 3.650065, Accuracy 92.853%\n",
      "Epoch 34, Batch 997, LR 0.195185 Loss 3.650631, Accuracy 92.848%\n",
      "Epoch 34, Batch 998, LR 0.195113 Loss 3.650404, Accuracy 92.851%\n",
      "Epoch 34, Batch 999, LR 0.195041 Loss 3.650698, Accuracy 92.851%\n",
      "Epoch 34, Batch 1000, LR 0.194969 Loss 3.651023, Accuracy 92.852%\n",
      "Epoch 34, Batch 1001, LR 0.194897 Loss 3.651554, Accuracy 92.850%\n",
      "Epoch 34, Batch 1002, LR 0.194826 Loss 3.651627, Accuracy 92.848%\n",
      "Epoch 34, Batch 1003, LR 0.194754 Loss 3.651600, Accuracy 92.846%\n",
      "Epoch 34, Batch 1004, LR 0.194682 Loss 3.651477, Accuracy 92.848%\n",
      "Epoch 34, Batch 1005, LR 0.194610 Loss 3.650960, Accuracy 92.850%\n",
      "Epoch 34, Batch 1006, LR 0.194538 Loss 3.650940, Accuracy 92.848%\n",
      "Epoch 34, Batch 1007, LR 0.194467 Loss 3.650777, Accuracy 92.848%\n",
      "Epoch 34, Batch 1008, LR 0.194395 Loss 3.650473, Accuracy 92.851%\n",
      "Epoch 34, Batch 1009, LR 0.194323 Loss 3.649821, Accuracy 92.854%\n",
      "Epoch 34, Batch 1010, LR 0.194251 Loss 3.649833, Accuracy 92.855%\n",
      "Epoch 34, Batch 1011, LR 0.194180 Loss 3.649954, Accuracy 92.852%\n",
      "Epoch 34, Batch 1012, LR 0.194108 Loss 3.649126, Accuracy 92.855%\n",
      "Epoch 34, Batch 1013, LR 0.194036 Loss 3.649131, Accuracy 92.858%\n",
      "Epoch 34, Batch 1014, LR 0.193965 Loss 3.649387, Accuracy 92.857%\n",
      "Epoch 34, Batch 1015, LR 0.193893 Loss 3.649373, Accuracy 92.859%\n",
      "Epoch 34, Batch 1016, LR 0.193821 Loss 3.649395, Accuracy 92.856%\n",
      "Epoch 34, Batch 1017, LR 0.193750 Loss 3.649219, Accuracy 92.854%\n",
      "Epoch 34, Batch 1018, LR 0.193678 Loss 3.649670, Accuracy 92.854%\n",
      "Epoch 34, Batch 1019, LR 0.193607 Loss 3.649633, Accuracy 92.852%\n",
      "Epoch 34, Batch 1020, LR 0.193535 Loss 3.649846, Accuracy 92.852%\n",
      "Epoch 34, Batch 1021, LR 0.193463 Loss 3.649887, Accuracy 92.852%\n",
      "Epoch 34, Batch 1022, LR 0.193392 Loss 3.649181, Accuracy 92.856%\n",
      "Epoch 34, Batch 1023, LR 0.193320 Loss 3.649515, Accuracy 92.855%\n",
      "Epoch 34, Batch 1024, LR 0.193249 Loss 3.649306, Accuracy 92.852%\n",
      "Epoch 34, Batch 1025, LR 0.193177 Loss 3.649261, Accuracy 92.854%\n",
      "Epoch 34, Batch 1026, LR 0.193106 Loss 3.648933, Accuracy 92.855%\n",
      "Epoch 34, Batch 1027, LR 0.193034 Loss 3.649041, Accuracy 92.855%\n",
      "Epoch 34, Batch 1028, LR 0.192963 Loss 3.649006, Accuracy 92.857%\n",
      "Epoch 34, Batch 1029, LR 0.192891 Loss 3.649631, Accuracy 92.852%\n",
      "Epoch 34, Batch 1030, LR 0.192820 Loss 3.649462, Accuracy 92.853%\n",
      "Epoch 34, Batch 1031, LR 0.192748 Loss 3.649786, Accuracy 92.852%\n",
      "Epoch 34, Batch 1032, LR 0.192677 Loss 3.650097, Accuracy 92.850%\n",
      "Epoch 34, Batch 1033, LR 0.192605 Loss 3.650622, Accuracy 92.849%\n",
      "Epoch 34, Batch 1034, LR 0.192534 Loss 3.650308, Accuracy 92.850%\n",
      "Epoch 34, Batch 1035, LR 0.192462 Loss 3.650805, Accuracy 92.849%\n",
      "Epoch 34, Batch 1036, LR 0.192391 Loss 3.650954, Accuracy 92.847%\n",
      "Epoch 34, Batch 1037, LR 0.192320 Loss 3.651251, Accuracy 92.844%\n",
      "Epoch 34, Batch 1038, LR 0.192248 Loss 3.650677, Accuracy 92.845%\n",
      "Epoch 34, Batch 1039, LR 0.192177 Loss 3.651011, Accuracy 92.845%\n",
      "Epoch 34, Batch 1040, LR 0.192105 Loss 3.650902, Accuracy 92.843%\n",
      "Epoch 34, Batch 1041, LR 0.192034 Loss 3.650863, Accuracy 92.843%\n",
      "Epoch 34, Batch 1042, LR 0.191963 Loss 3.651077, Accuracy 92.843%\n",
      "Epoch 34, Batch 1043, LR 0.191891 Loss 3.650917, Accuracy 92.844%\n",
      "Epoch 34, Batch 1044, LR 0.191820 Loss 3.650958, Accuracy 92.844%\n",
      "Epoch 34, Batch 1045, LR 0.191749 Loss 3.650932, Accuracy 92.842%\n",
      "Epoch 34, Batch 1046, LR 0.191678 Loss 3.650685, Accuracy 92.844%\n",
      "Epoch 34, Batch 1047, LR 0.191606 Loss 3.650275, Accuracy 92.847%\n",
      "Epoch 34, Loss (train set) 3.650275, Accuracy (train set) 92.847%\n",
      "Epoch 34, Accuracy (validation set) 90.949%\n",
      "Epoch 35, Batch 1, LR 0.191535 Loss 4.041152, Accuracy 90.625%\n",
      "Epoch 35, Batch 2, LR 0.191464 Loss 3.974538, Accuracy 90.625%\n",
      "Epoch 35, Batch 3, LR 0.191393 Loss 3.858974, Accuracy 91.927%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 4, LR 0.191321 Loss 3.872541, Accuracy 91.016%\n",
      "Epoch 35, Batch 5, LR 0.191250 Loss 3.797791, Accuracy 91.562%\n",
      "Epoch 35, Batch 6, LR 0.191179 Loss 3.708201, Accuracy 92.318%\n",
      "Epoch 35, Batch 7, LR 0.191108 Loss 3.704818, Accuracy 92.746%\n",
      "Epoch 35, Batch 8, LR 0.191037 Loss 3.688915, Accuracy 92.578%\n",
      "Epoch 35, Batch 9, LR 0.190965 Loss 3.623338, Accuracy 92.622%\n",
      "Epoch 35, Batch 10, LR 0.190894 Loss 3.612986, Accuracy 92.578%\n",
      "Epoch 35, Batch 11, LR 0.190823 Loss 3.556860, Accuracy 93.040%\n",
      "Epoch 35, Batch 12, LR 0.190752 Loss 3.595965, Accuracy 93.099%\n",
      "Epoch 35, Batch 13, LR 0.190681 Loss 3.639930, Accuracy 92.788%\n",
      "Epoch 35, Batch 14, LR 0.190610 Loss 3.640186, Accuracy 92.690%\n",
      "Epoch 35, Batch 15, LR 0.190539 Loss 3.608112, Accuracy 92.812%\n",
      "Epoch 35, Batch 16, LR 0.190468 Loss 3.624878, Accuracy 92.822%\n",
      "Epoch 35, Batch 17, LR 0.190397 Loss 3.636712, Accuracy 92.877%\n",
      "Epoch 35, Batch 18, LR 0.190325 Loss 3.582604, Accuracy 92.969%\n",
      "Epoch 35, Batch 19, LR 0.190254 Loss 3.591150, Accuracy 92.804%\n",
      "Epoch 35, Batch 20, LR 0.190183 Loss 3.558390, Accuracy 92.930%\n",
      "Epoch 35, Batch 21, LR 0.190112 Loss 3.547540, Accuracy 93.043%\n",
      "Epoch 35, Batch 22, LR 0.190041 Loss 3.566265, Accuracy 93.004%\n",
      "Epoch 35, Batch 23, LR 0.189970 Loss 3.540008, Accuracy 92.969%\n",
      "Epoch 35, Batch 24, LR 0.189899 Loss 3.546731, Accuracy 92.871%\n",
      "Epoch 35, Batch 25, LR 0.189828 Loss 3.556306, Accuracy 92.875%\n",
      "Epoch 35, Batch 26, LR 0.189757 Loss 3.538441, Accuracy 92.909%\n",
      "Epoch 35, Batch 27, LR 0.189687 Loss 3.523109, Accuracy 93.056%\n",
      "Epoch 35, Batch 28, LR 0.189616 Loss 3.518490, Accuracy 93.108%\n",
      "Epoch 35, Batch 29, LR 0.189545 Loss 3.523279, Accuracy 93.103%\n",
      "Epoch 35, Batch 30, LR 0.189474 Loss 3.514969, Accuracy 93.229%\n",
      "Epoch 35, Batch 31, LR 0.189403 Loss 3.526801, Accuracy 93.170%\n",
      "Epoch 35, Batch 32, LR 0.189332 Loss 3.530427, Accuracy 93.164%\n",
      "Epoch 35, Batch 33, LR 0.189261 Loss 3.527919, Accuracy 93.205%\n",
      "Epoch 35, Batch 34, LR 0.189190 Loss 3.539666, Accuracy 93.130%\n",
      "Epoch 35, Batch 35, LR 0.189119 Loss 3.534357, Accuracy 93.125%\n",
      "Epoch 35, Batch 36, LR 0.189049 Loss 3.521416, Accuracy 93.294%\n",
      "Epoch 35, Batch 37, LR 0.188978 Loss 3.512853, Accuracy 93.349%\n",
      "Epoch 35, Batch 38, LR 0.188907 Loss 3.507295, Accuracy 93.400%\n",
      "Epoch 35, Batch 39, LR 0.188836 Loss 3.500805, Accuracy 93.490%\n",
      "Epoch 35, Batch 40, LR 0.188765 Loss 3.502270, Accuracy 93.457%\n",
      "Epoch 35, Batch 41, LR 0.188695 Loss 3.493484, Accuracy 93.502%\n",
      "Epoch 35, Batch 42, LR 0.188624 Loss 3.485208, Accuracy 93.564%\n",
      "Epoch 35, Batch 43, LR 0.188553 Loss 3.501679, Accuracy 93.514%\n",
      "Epoch 35, Batch 44, LR 0.188482 Loss 3.488363, Accuracy 93.537%\n",
      "Epoch 35, Batch 45, LR 0.188412 Loss 3.497101, Accuracy 93.507%\n",
      "Epoch 35, Batch 46, LR 0.188341 Loss 3.496093, Accuracy 93.427%\n",
      "Epoch 35, Batch 47, LR 0.188270 Loss 3.508242, Accuracy 93.384%\n",
      "Epoch 35, Batch 48, LR 0.188199 Loss 3.507956, Accuracy 93.343%\n",
      "Epoch 35, Batch 49, LR 0.188129 Loss 3.506743, Accuracy 93.383%\n",
      "Epoch 35, Batch 50, LR 0.188058 Loss 3.522262, Accuracy 93.312%\n",
      "Epoch 35, Batch 51, LR 0.187987 Loss 3.513282, Accuracy 93.336%\n",
      "Epoch 35, Batch 52, LR 0.187917 Loss 3.515575, Accuracy 93.374%\n",
      "Epoch 35, Batch 53, LR 0.187846 Loss 3.519939, Accuracy 93.323%\n",
      "Epoch 35, Batch 54, LR 0.187776 Loss 3.528576, Accuracy 93.258%\n",
      "Epoch 35, Batch 55, LR 0.187705 Loss 3.535041, Accuracy 93.210%\n",
      "Epoch 35, Batch 56, LR 0.187634 Loss 3.534435, Accuracy 93.234%\n",
      "Epoch 35, Batch 57, LR 0.187564 Loss 3.528383, Accuracy 93.257%\n",
      "Epoch 35, Batch 58, LR 0.187493 Loss 3.516563, Accuracy 93.292%\n",
      "Epoch 35, Batch 59, LR 0.187423 Loss 3.513395, Accuracy 93.340%\n",
      "Epoch 35, Batch 60, LR 0.187352 Loss 3.506123, Accuracy 93.372%\n",
      "Epoch 35, Batch 61, LR 0.187282 Loss 3.514128, Accuracy 93.276%\n",
      "Epoch 35, Batch 62, LR 0.187211 Loss 3.506990, Accuracy 93.309%\n",
      "Epoch 35, Batch 63, LR 0.187141 Loss 3.514117, Accuracy 93.279%\n",
      "Epoch 35, Batch 64, LR 0.187070 Loss 3.518714, Accuracy 93.286%\n",
      "Epoch 35, Batch 65, LR 0.187000 Loss 3.529257, Accuracy 93.293%\n",
      "Epoch 35, Batch 66, LR 0.186929 Loss 3.522217, Accuracy 93.336%\n",
      "Epoch 35, Batch 67, LR 0.186859 Loss 3.516379, Accuracy 93.319%\n",
      "Epoch 35, Batch 68, LR 0.186788 Loss 3.518517, Accuracy 93.325%\n",
      "Epoch 35, Batch 69, LR 0.186718 Loss 3.513876, Accuracy 93.342%\n",
      "Epoch 35, Batch 70, LR 0.186647 Loss 3.517412, Accuracy 93.326%\n",
      "Epoch 35, Batch 71, LR 0.186577 Loss 3.523783, Accuracy 93.343%\n",
      "Epoch 35, Batch 72, LR 0.186507 Loss 3.524553, Accuracy 93.392%\n",
      "Epoch 35, Batch 73, LR 0.186436 Loss 3.520707, Accuracy 93.354%\n",
      "Epoch 35, Batch 74, LR 0.186366 Loss 3.527810, Accuracy 93.296%\n",
      "Epoch 35, Batch 75, LR 0.186295 Loss 3.525399, Accuracy 93.333%\n",
      "Epoch 35, Batch 76, LR 0.186225 Loss 3.526773, Accuracy 93.308%\n",
      "Epoch 35, Batch 77, LR 0.186155 Loss 3.523990, Accuracy 93.334%\n",
      "Epoch 35, Batch 78, LR 0.186084 Loss 3.526217, Accuracy 93.319%\n",
      "Epoch 35, Batch 79, LR 0.186014 Loss 3.519199, Accuracy 93.354%\n",
      "Epoch 35, Batch 80, LR 0.185944 Loss 3.515937, Accuracy 93.350%\n",
      "Epoch 35, Batch 81, LR 0.185874 Loss 3.521022, Accuracy 93.355%\n",
      "Epoch 35, Batch 82, LR 0.185803 Loss 3.520155, Accuracy 93.388%\n",
      "Epoch 35, Batch 83, LR 0.185733 Loss 3.519341, Accuracy 93.373%\n",
      "Epoch 35, Batch 84, LR 0.185663 Loss 3.516809, Accuracy 93.378%\n",
      "Epoch 35, Batch 85, LR 0.185593 Loss 3.522877, Accuracy 93.401%\n",
      "Epoch 35, Batch 86, LR 0.185522 Loss 3.523783, Accuracy 93.405%\n",
      "Epoch 35, Batch 87, LR 0.185452 Loss 3.524783, Accuracy 93.391%\n",
      "Epoch 35, Batch 88, LR 0.185382 Loss 3.529481, Accuracy 93.350%\n",
      "Epoch 35, Batch 89, LR 0.185312 Loss 3.535559, Accuracy 93.337%\n",
      "Epoch 35, Batch 90, LR 0.185242 Loss 3.531164, Accuracy 93.342%\n",
      "Epoch 35, Batch 91, LR 0.185171 Loss 3.529184, Accuracy 93.346%\n",
      "Epoch 35, Batch 92, LR 0.185101 Loss 3.528463, Accuracy 93.351%\n",
      "Epoch 35, Batch 93, LR 0.185031 Loss 3.523811, Accuracy 93.389%\n",
      "Epoch 35, Batch 94, LR 0.184961 Loss 3.522077, Accuracy 93.401%\n",
      "Epoch 35, Batch 95, LR 0.184891 Loss 3.521487, Accuracy 93.396%\n",
      "Epoch 35, Batch 96, LR 0.184821 Loss 3.524691, Accuracy 93.392%\n",
      "Epoch 35, Batch 97, LR 0.184751 Loss 3.519378, Accuracy 93.428%\n",
      "Epoch 35, Batch 98, LR 0.184681 Loss 3.515773, Accuracy 93.431%\n",
      "Epoch 35, Batch 99, LR 0.184611 Loss 3.516782, Accuracy 93.426%\n",
      "Epoch 35, Batch 100, LR 0.184540 Loss 3.521762, Accuracy 93.430%\n",
      "Epoch 35, Batch 101, LR 0.184470 Loss 3.524431, Accuracy 93.472%\n",
      "Epoch 35, Batch 102, LR 0.184400 Loss 3.529697, Accuracy 93.444%\n",
      "Epoch 35, Batch 103, LR 0.184330 Loss 3.527221, Accuracy 93.447%\n",
      "Epoch 35, Batch 104, LR 0.184260 Loss 3.534427, Accuracy 93.382%\n",
      "Epoch 35, Batch 105, LR 0.184190 Loss 3.533871, Accuracy 93.400%\n",
      "Epoch 35, Batch 106, LR 0.184120 Loss 3.531165, Accuracy 93.396%\n",
      "Epoch 35, Batch 107, LR 0.184050 Loss 3.537103, Accuracy 93.348%\n",
      "Epoch 35, Batch 108, LR 0.183980 Loss 3.535156, Accuracy 93.338%\n",
      "Epoch 35, Batch 109, LR 0.183911 Loss 3.533216, Accuracy 93.334%\n",
      "Epoch 35, Batch 110, LR 0.183841 Loss 3.535419, Accuracy 93.338%\n",
      "Epoch 35, Batch 111, LR 0.183771 Loss 3.539749, Accuracy 93.285%\n",
      "Epoch 35, Batch 112, LR 0.183701 Loss 3.536148, Accuracy 93.297%\n",
      "Epoch 35, Batch 113, LR 0.183631 Loss 3.536024, Accuracy 93.287%\n",
      "Epoch 35, Batch 114, LR 0.183561 Loss 3.529958, Accuracy 93.332%\n",
      "Epoch 35, Batch 115, LR 0.183491 Loss 3.527564, Accuracy 93.349%\n",
      "Epoch 35, Batch 116, LR 0.183421 Loss 3.533183, Accuracy 93.319%\n",
      "Epoch 35, Batch 117, LR 0.183351 Loss 3.526462, Accuracy 93.356%\n",
      "Epoch 35, Batch 118, LR 0.183282 Loss 3.531222, Accuracy 93.326%\n",
      "Epoch 35, Batch 119, LR 0.183212 Loss 3.530223, Accuracy 93.343%\n",
      "Epoch 35, Batch 120, LR 0.183142 Loss 3.527745, Accuracy 93.359%\n",
      "Epoch 35, Batch 121, LR 0.183072 Loss 3.529758, Accuracy 93.350%\n",
      "Epoch 35, Batch 122, LR 0.183002 Loss 3.529289, Accuracy 93.372%\n",
      "Epoch 35, Batch 123, LR 0.182933 Loss 3.528200, Accuracy 93.350%\n",
      "Epoch 35, Batch 124, LR 0.182863 Loss 3.525730, Accuracy 93.366%\n",
      "Epoch 35, Batch 125, LR 0.182793 Loss 3.527454, Accuracy 93.369%\n",
      "Epoch 35, Batch 126, LR 0.182723 Loss 3.528557, Accuracy 93.372%\n",
      "Epoch 35, Batch 127, LR 0.182654 Loss 3.529722, Accuracy 93.381%\n",
      "Epoch 35, Batch 128, LR 0.182584 Loss 3.534693, Accuracy 93.359%\n",
      "Epoch 35, Batch 129, LR 0.182514 Loss 3.535178, Accuracy 93.350%\n",
      "Epoch 35, Batch 130, LR 0.182445 Loss 3.535200, Accuracy 93.323%\n",
      "Epoch 35, Batch 131, LR 0.182375 Loss 3.534004, Accuracy 93.344%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 132, LR 0.182305 Loss 3.536799, Accuracy 93.324%\n",
      "Epoch 35, Batch 133, LR 0.182236 Loss 3.537556, Accuracy 93.309%\n",
      "Epoch 35, Batch 134, LR 0.182166 Loss 3.537094, Accuracy 93.324%\n",
      "Epoch 35, Batch 135, LR 0.182096 Loss 3.539404, Accuracy 93.316%\n",
      "Epoch 35, Batch 136, LR 0.182027 Loss 3.535093, Accuracy 93.331%\n",
      "Epoch 35, Batch 137, LR 0.181957 Loss 3.536949, Accuracy 93.322%\n",
      "Epoch 35, Batch 138, LR 0.181887 Loss 3.533571, Accuracy 93.337%\n",
      "Epoch 35, Batch 139, LR 0.181818 Loss 3.541720, Accuracy 93.295%\n",
      "Epoch 35, Batch 140, LR 0.181748 Loss 3.539158, Accuracy 93.320%\n",
      "Epoch 35, Batch 141, LR 0.181679 Loss 3.538610, Accuracy 93.312%\n",
      "Epoch 35, Batch 142, LR 0.181609 Loss 3.540835, Accuracy 93.304%\n",
      "Epoch 35, Batch 143, LR 0.181540 Loss 3.540198, Accuracy 93.318%\n",
      "Epoch 35, Batch 144, LR 0.181470 Loss 3.540168, Accuracy 93.300%\n",
      "Epoch 35, Batch 145, LR 0.181401 Loss 3.544596, Accuracy 93.276%\n",
      "Epoch 35, Batch 146, LR 0.181331 Loss 3.548020, Accuracy 93.258%\n",
      "Epoch 35, Batch 147, LR 0.181262 Loss 3.545508, Accuracy 93.261%\n",
      "Epoch 35, Batch 148, LR 0.181192 Loss 3.545951, Accuracy 93.270%\n",
      "Epoch 35, Batch 149, LR 0.181123 Loss 3.546859, Accuracy 93.252%\n",
      "Epoch 35, Batch 150, LR 0.181053 Loss 3.545858, Accuracy 93.260%\n",
      "Epoch 35, Batch 151, LR 0.180984 Loss 3.546405, Accuracy 93.269%\n",
      "Epoch 35, Batch 152, LR 0.180915 Loss 3.547630, Accuracy 93.251%\n",
      "Epoch 35, Batch 153, LR 0.180845 Loss 3.546217, Accuracy 93.234%\n",
      "Epoch 35, Batch 154, LR 0.180776 Loss 3.544365, Accuracy 93.258%\n",
      "Epoch 35, Batch 155, LR 0.180706 Loss 3.542051, Accuracy 93.271%\n",
      "Epoch 35, Batch 156, LR 0.180637 Loss 3.542573, Accuracy 93.264%\n",
      "Epoch 35, Batch 157, LR 0.180568 Loss 3.545217, Accuracy 93.257%\n",
      "Epoch 35, Batch 158, LR 0.180498 Loss 3.551842, Accuracy 93.231%\n",
      "Epoch 35, Batch 159, LR 0.180429 Loss 3.552012, Accuracy 93.239%\n",
      "Epoch 35, Batch 160, LR 0.180360 Loss 3.554945, Accuracy 93.228%\n",
      "Epoch 35, Batch 161, LR 0.180290 Loss 3.553854, Accuracy 93.231%\n",
      "Epoch 35, Batch 162, LR 0.180221 Loss 3.553582, Accuracy 93.220%\n",
      "Epoch 35, Batch 163, LR 0.180152 Loss 3.549905, Accuracy 93.232%\n",
      "Epoch 35, Batch 164, LR 0.180082 Loss 3.547857, Accuracy 93.236%\n",
      "Epoch 35, Batch 165, LR 0.180013 Loss 3.546675, Accuracy 93.253%\n",
      "Epoch 35, Batch 166, LR 0.179944 Loss 3.547358, Accuracy 93.251%\n",
      "Epoch 35, Batch 167, LR 0.179875 Loss 3.548266, Accuracy 93.254%\n",
      "Epoch 35, Batch 168, LR 0.179806 Loss 3.548295, Accuracy 93.262%\n",
      "Epoch 35, Batch 169, LR 0.179736 Loss 3.546890, Accuracy 93.278%\n",
      "Epoch 35, Batch 170, LR 0.179667 Loss 3.546835, Accuracy 93.286%\n",
      "Epoch 35, Batch 171, LR 0.179598 Loss 3.545609, Accuracy 93.275%\n",
      "Epoch 35, Batch 172, LR 0.179529 Loss 3.548096, Accuracy 93.259%\n",
      "Epoch 35, Batch 173, LR 0.179460 Loss 3.547562, Accuracy 93.267%\n",
      "Epoch 35, Batch 174, LR 0.179390 Loss 3.550950, Accuracy 93.252%\n",
      "Epoch 35, Batch 175, LR 0.179321 Loss 3.549819, Accuracy 93.263%\n",
      "Epoch 35, Batch 176, LR 0.179252 Loss 3.548087, Accuracy 93.266%\n",
      "Epoch 35, Batch 177, LR 0.179183 Loss 3.547732, Accuracy 93.278%\n",
      "Epoch 35, Batch 178, LR 0.179114 Loss 3.545102, Accuracy 93.272%\n",
      "Epoch 35, Batch 179, LR 0.179045 Loss 3.546227, Accuracy 93.257%\n",
      "Epoch 35, Batch 180, LR 0.178976 Loss 3.550224, Accuracy 93.238%\n",
      "Epoch 35, Batch 181, LR 0.178907 Loss 3.553603, Accuracy 93.232%\n",
      "Epoch 35, Batch 182, LR 0.178838 Loss 3.554752, Accuracy 93.222%\n",
      "Epoch 35, Batch 183, LR 0.178769 Loss 3.552906, Accuracy 93.233%\n",
      "Epoch 35, Batch 184, LR 0.178700 Loss 3.554902, Accuracy 93.249%\n",
      "Epoch 35, Batch 185, LR 0.178631 Loss 3.555387, Accuracy 93.247%\n",
      "Epoch 35, Batch 186, LR 0.178562 Loss 3.556702, Accuracy 93.250%\n",
      "Epoch 35, Batch 187, LR 0.178493 Loss 3.557514, Accuracy 93.244%\n",
      "Epoch 35, Batch 188, LR 0.178424 Loss 3.559867, Accuracy 93.247%\n",
      "Epoch 35, Batch 189, LR 0.178355 Loss 3.560606, Accuracy 93.242%\n",
      "Epoch 35, Batch 190, LR 0.178286 Loss 3.559993, Accuracy 93.244%\n",
      "Epoch 35, Batch 191, LR 0.178217 Loss 3.562508, Accuracy 93.239%\n",
      "Epoch 35, Batch 192, LR 0.178148 Loss 3.563459, Accuracy 93.233%\n",
      "Epoch 35, Batch 193, LR 0.178079 Loss 3.560212, Accuracy 93.228%\n",
      "Epoch 35, Batch 194, LR 0.178010 Loss 3.557963, Accuracy 93.243%\n",
      "Epoch 35, Batch 195, LR 0.177941 Loss 3.554617, Accuracy 93.253%\n",
      "Epoch 35, Batch 196, LR 0.177872 Loss 3.554966, Accuracy 93.244%\n",
      "Epoch 35, Batch 197, LR 0.177804 Loss 3.555570, Accuracy 93.246%\n",
      "Epoch 35, Batch 198, LR 0.177735 Loss 3.556692, Accuracy 93.245%\n",
      "Epoch 35, Batch 199, LR 0.177666 Loss 3.555184, Accuracy 93.255%\n",
      "Epoch 35, Batch 200, LR 0.177597 Loss 3.556423, Accuracy 93.262%\n",
      "Epoch 35, Batch 201, LR 0.177528 Loss 3.559715, Accuracy 93.260%\n",
      "Epoch 35, Batch 202, LR 0.177459 Loss 3.558928, Accuracy 93.270%\n",
      "Epoch 35, Batch 203, LR 0.177391 Loss 3.560698, Accuracy 93.269%\n",
      "Epoch 35, Batch 204, LR 0.177322 Loss 3.561828, Accuracy 93.264%\n",
      "Epoch 35, Batch 205, LR 0.177253 Loss 3.563171, Accuracy 93.255%\n",
      "Epoch 35, Batch 206, LR 0.177184 Loss 3.567672, Accuracy 93.223%\n",
      "Epoch 35, Batch 207, LR 0.177116 Loss 3.565750, Accuracy 93.225%\n",
      "Epoch 35, Batch 208, LR 0.177047 Loss 3.565614, Accuracy 93.232%\n",
      "Epoch 35, Batch 209, LR 0.176978 Loss 3.566189, Accuracy 93.227%\n",
      "Epoch 35, Batch 210, LR 0.176909 Loss 3.569578, Accuracy 93.222%\n",
      "Epoch 35, Batch 211, LR 0.176841 Loss 3.568743, Accuracy 93.228%\n",
      "Epoch 35, Batch 212, LR 0.176772 Loss 3.567540, Accuracy 93.227%\n",
      "Epoch 35, Batch 213, LR 0.176703 Loss 3.565774, Accuracy 93.229%\n",
      "Epoch 35, Batch 214, LR 0.176635 Loss 3.565192, Accuracy 93.224%\n",
      "Epoch 35, Batch 215, LR 0.176566 Loss 3.564569, Accuracy 93.230%\n",
      "Epoch 35, Batch 216, LR 0.176497 Loss 3.566387, Accuracy 93.229%\n",
      "Epoch 35, Batch 217, LR 0.176429 Loss 3.566524, Accuracy 93.239%\n",
      "Epoch 35, Batch 218, LR 0.176360 Loss 3.563086, Accuracy 93.252%\n",
      "Epoch 35, Batch 219, LR 0.176292 Loss 3.566950, Accuracy 93.247%\n",
      "Epoch 35, Batch 220, LR 0.176223 Loss 3.567623, Accuracy 93.253%\n",
      "Epoch 35, Batch 221, LR 0.176154 Loss 3.568171, Accuracy 93.252%\n",
      "Epoch 35, Batch 222, LR 0.176086 Loss 3.566700, Accuracy 93.250%\n",
      "Epoch 35, Batch 223, LR 0.176017 Loss 3.568068, Accuracy 93.239%\n",
      "Epoch 35, Batch 224, LR 0.175949 Loss 3.570180, Accuracy 93.234%\n",
      "Epoch 35, Batch 225, LR 0.175880 Loss 3.571167, Accuracy 93.229%\n",
      "Epoch 35, Batch 226, LR 0.175812 Loss 3.568893, Accuracy 93.231%\n",
      "Epoch 35, Batch 227, LR 0.175743 Loss 3.568865, Accuracy 93.241%\n",
      "Epoch 35, Batch 228, LR 0.175675 Loss 3.571581, Accuracy 93.233%\n",
      "Epoch 35, Batch 229, LR 0.175606 Loss 3.571884, Accuracy 93.235%\n",
      "Epoch 35, Batch 230, LR 0.175538 Loss 3.573053, Accuracy 93.220%\n",
      "Epoch 35, Batch 231, LR 0.175469 Loss 3.577728, Accuracy 93.195%\n",
      "Epoch 35, Batch 232, LR 0.175401 Loss 3.576488, Accuracy 93.194%\n",
      "Epoch 35, Batch 233, LR 0.175333 Loss 3.576986, Accuracy 93.193%\n",
      "Epoch 35, Batch 234, LR 0.175264 Loss 3.577386, Accuracy 93.202%\n",
      "Epoch 35, Batch 235, LR 0.175196 Loss 3.579060, Accuracy 93.195%\n",
      "Epoch 35, Batch 236, LR 0.175127 Loss 3.579314, Accuracy 93.191%\n",
      "Epoch 35, Batch 237, LR 0.175059 Loss 3.580952, Accuracy 93.176%\n",
      "Epoch 35, Batch 238, LR 0.174991 Loss 3.578829, Accuracy 93.192%\n",
      "Epoch 35, Batch 239, LR 0.174922 Loss 3.578666, Accuracy 93.194%\n",
      "Epoch 35, Batch 240, LR 0.174854 Loss 3.580882, Accuracy 93.190%\n",
      "Epoch 35, Batch 241, LR 0.174786 Loss 3.580283, Accuracy 93.196%\n",
      "Epoch 35, Batch 242, LR 0.174717 Loss 3.577936, Accuracy 93.208%\n",
      "Epoch 35, Batch 243, LR 0.174649 Loss 3.577529, Accuracy 93.207%\n",
      "Epoch 35, Batch 244, LR 0.174581 Loss 3.579124, Accuracy 93.206%\n",
      "Epoch 35, Batch 245, LR 0.174513 Loss 3.580812, Accuracy 93.211%\n",
      "Epoch 35, Batch 246, LR 0.174444 Loss 3.580020, Accuracy 93.207%\n",
      "Epoch 35, Batch 247, LR 0.174376 Loss 3.579902, Accuracy 93.212%\n",
      "Epoch 35, Batch 248, LR 0.174308 Loss 3.578621, Accuracy 93.224%\n",
      "Epoch 35, Batch 249, LR 0.174240 Loss 3.576855, Accuracy 93.232%\n",
      "Epoch 35, Batch 250, LR 0.174171 Loss 3.577555, Accuracy 93.225%\n",
      "Epoch 35, Batch 251, LR 0.174103 Loss 3.576990, Accuracy 93.224%\n",
      "Epoch 35, Batch 252, LR 0.174035 Loss 3.575438, Accuracy 93.229%\n",
      "Epoch 35, Batch 253, LR 0.173967 Loss 3.575209, Accuracy 93.231%\n",
      "Epoch 35, Batch 254, LR 0.173899 Loss 3.574425, Accuracy 93.239%\n",
      "Epoch 35, Batch 255, LR 0.173831 Loss 3.574630, Accuracy 93.254%\n",
      "Epoch 35, Batch 256, LR 0.173762 Loss 3.576868, Accuracy 93.246%\n",
      "Epoch 35, Batch 257, LR 0.173694 Loss 3.577234, Accuracy 93.248%\n",
      "Epoch 35, Batch 258, LR 0.173626 Loss 3.576522, Accuracy 93.259%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 259, LR 0.173558 Loss 3.575783, Accuracy 93.261%\n",
      "Epoch 35, Batch 260, LR 0.173490 Loss 3.575394, Accuracy 93.266%\n",
      "Epoch 35, Batch 261, LR 0.173422 Loss 3.574940, Accuracy 93.256%\n",
      "Epoch 35, Batch 262, LR 0.173354 Loss 3.575100, Accuracy 93.255%\n",
      "Epoch 35, Batch 263, LR 0.173286 Loss 3.574157, Accuracy 93.263%\n",
      "Epoch 35, Batch 264, LR 0.173218 Loss 3.573872, Accuracy 93.268%\n",
      "Epoch 35, Batch 265, LR 0.173150 Loss 3.576579, Accuracy 93.243%\n",
      "Epoch 35, Batch 266, LR 0.173082 Loss 3.575944, Accuracy 93.257%\n",
      "Epoch 35, Batch 267, LR 0.173014 Loss 3.574839, Accuracy 93.261%\n",
      "Epoch 35, Batch 268, LR 0.172946 Loss 3.574591, Accuracy 93.275%\n",
      "Epoch 35, Batch 269, LR 0.172878 Loss 3.573611, Accuracy 93.274%\n",
      "Epoch 35, Batch 270, LR 0.172810 Loss 3.575393, Accuracy 93.275%\n",
      "Epoch 35, Batch 271, LR 0.172742 Loss 3.574778, Accuracy 93.277%\n",
      "Epoch 35, Batch 272, LR 0.172674 Loss 3.574395, Accuracy 93.276%\n",
      "Epoch 35, Batch 273, LR 0.172606 Loss 3.573898, Accuracy 93.284%\n",
      "Epoch 35, Batch 274, LR 0.172538 Loss 3.575638, Accuracy 93.274%\n",
      "Epoch 35, Batch 275, LR 0.172470 Loss 3.574777, Accuracy 93.273%\n",
      "Epoch 35, Batch 276, LR 0.172402 Loss 3.574967, Accuracy 93.277%\n",
      "Epoch 35, Batch 277, LR 0.172334 Loss 3.575652, Accuracy 93.268%\n",
      "Epoch 35, Batch 278, LR 0.172267 Loss 3.574421, Accuracy 93.281%\n",
      "Epoch 35, Batch 279, LR 0.172199 Loss 3.573398, Accuracy 93.274%\n",
      "Epoch 35, Batch 280, LR 0.172131 Loss 3.572263, Accuracy 93.273%\n",
      "Epoch 35, Batch 281, LR 0.172063 Loss 3.572822, Accuracy 93.261%\n",
      "Epoch 35, Batch 282, LR 0.171995 Loss 3.573052, Accuracy 93.251%\n",
      "Epoch 35, Batch 283, LR 0.171927 Loss 3.573017, Accuracy 93.256%\n",
      "Epoch 35, Batch 284, LR 0.171860 Loss 3.574810, Accuracy 93.238%\n",
      "Epoch 35, Batch 285, LR 0.171792 Loss 3.573921, Accuracy 93.237%\n",
      "Epoch 35, Batch 286, LR 0.171724 Loss 3.574111, Accuracy 93.242%\n",
      "Epoch 35, Batch 287, LR 0.171656 Loss 3.575190, Accuracy 93.236%\n",
      "Epoch 35, Batch 288, LR 0.171589 Loss 3.574783, Accuracy 93.248%\n",
      "Epoch 35, Batch 289, LR 0.171521 Loss 3.576065, Accuracy 93.239%\n",
      "Epoch 35, Batch 290, LR 0.171453 Loss 3.576294, Accuracy 93.233%\n",
      "Epoch 35, Batch 291, LR 0.171385 Loss 3.575118, Accuracy 93.235%\n",
      "Epoch 35, Batch 292, LR 0.171318 Loss 3.573504, Accuracy 93.239%\n",
      "Epoch 35, Batch 293, LR 0.171250 Loss 3.575588, Accuracy 93.235%\n",
      "Epoch 35, Batch 294, LR 0.171182 Loss 3.572632, Accuracy 93.248%\n",
      "Epoch 35, Batch 295, LR 0.171115 Loss 3.571685, Accuracy 93.252%\n",
      "Epoch 35, Batch 296, LR 0.171047 Loss 3.571514, Accuracy 93.254%\n",
      "Epoch 35, Batch 297, LR 0.170979 Loss 3.572043, Accuracy 93.237%\n",
      "Epoch 35, Batch 298, LR 0.170912 Loss 3.573634, Accuracy 93.220%\n",
      "Epoch 35, Batch 299, LR 0.170844 Loss 3.575460, Accuracy 93.212%\n",
      "Epoch 35, Batch 300, LR 0.170777 Loss 3.574786, Accuracy 93.224%\n",
      "Epoch 35, Batch 301, LR 0.170709 Loss 3.576175, Accuracy 93.218%\n",
      "Epoch 35, Batch 302, LR 0.170642 Loss 3.576706, Accuracy 93.215%\n",
      "Epoch 35, Batch 303, LR 0.170574 Loss 3.576923, Accuracy 93.209%\n",
      "Epoch 35, Batch 304, LR 0.170506 Loss 3.577732, Accuracy 93.213%\n",
      "Epoch 35, Batch 305, LR 0.170439 Loss 3.578427, Accuracy 93.202%\n",
      "Epoch 35, Batch 306, LR 0.170371 Loss 3.578549, Accuracy 93.193%\n",
      "Epoch 35, Batch 307, LR 0.170304 Loss 3.577027, Accuracy 93.193%\n",
      "Epoch 35, Batch 308, LR 0.170236 Loss 3.577804, Accuracy 93.197%\n",
      "Epoch 35, Batch 309, LR 0.170169 Loss 3.578645, Accuracy 93.199%\n",
      "Epoch 35, Batch 310, LR 0.170101 Loss 3.578201, Accuracy 93.193%\n",
      "Epoch 35, Batch 311, LR 0.170034 Loss 3.578283, Accuracy 93.205%\n",
      "Epoch 35, Batch 312, LR 0.169967 Loss 3.579403, Accuracy 93.182%\n",
      "Epoch 35, Batch 313, LR 0.169899 Loss 3.578244, Accuracy 93.183%\n",
      "Epoch 35, Batch 314, LR 0.169832 Loss 3.578386, Accuracy 93.180%\n",
      "Epoch 35, Batch 315, LR 0.169764 Loss 3.577765, Accuracy 93.182%\n",
      "Epoch 35, Batch 316, LR 0.169697 Loss 3.578034, Accuracy 93.184%\n",
      "Epoch 35, Batch 317, LR 0.169630 Loss 3.580777, Accuracy 93.181%\n",
      "Epoch 35, Batch 318, LR 0.169562 Loss 3.580066, Accuracy 93.185%\n",
      "Epoch 35, Batch 319, LR 0.169495 Loss 3.582288, Accuracy 93.182%\n",
      "Epoch 35, Batch 320, LR 0.169427 Loss 3.581508, Accuracy 93.186%\n",
      "Epoch 35, Batch 321, LR 0.169360 Loss 3.582744, Accuracy 93.185%\n",
      "Epoch 35, Batch 322, LR 0.169293 Loss 3.583763, Accuracy 93.185%\n",
      "Epoch 35, Batch 323, LR 0.169225 Loss 3.583734, Accuracy 93.182%\n",
      "Epoch 35, Batch 324, LR 0.169158 Loss 3.583358, Accuracy 93.191%\n",
      "Epoch 35, Batch 325, LR 0.169091 Loss 3.582812, Accuracy 93.190%\n",
      "Epoch 35, Batch 326, LR 0.169024 Loss 3.582552, Accuracy 93.194%\n",
      "Epoch 35, Batch 327, LR 0.168956 Loss 3.583355, Accuracy 93.186%\n",
      "Epoch 35, Batch 328, LR 0.168889 Loss 3.582312, Accuracy 93.185%\n",
      "Epoch 35, Batch 329, LR 0.168822 Loss 3.582993, Accuracy 93.182%\n",
      "Epoch 35, Batch 330, LR 0.168755 Loss 3.582928, Accuracy 93.184%\n",
      "Epoch 35, Batch 331, LR 0.168687 Loss 3.581591, Accuracy 93.191%\n",
      "Epoch 35, Batch 332, LR 0.168620 Loss 3.582385, Accuracy 93.192%\n",
      "Epoch 35, Batch 333, LR 0.168553 Loss 3.581702, Accuracy 93.194%\n",
      "Epoch 35, Batch 334, LR 0.168486 Loss 3.581588, Accuracy 93.193%\n",
      "Epoch 35, Batch 335, LR 0.168419 Loss 3.581806, Accuracy 93.190%\n",
      "Epoch 35, Batch 336, LR 0.168352 Loss 3.581465, Accuracy 93.185%\n",
      "Epoch 35, Batch 337, LR 0.168285 Loss 3.582058, Accuracy 93.173%\n",
      "Epoch 35, Batch 338, LR 0.168217 Loss 3.581225, Accuracy 93.181%\n",
      "Epoch 35, Batch 339, LR 0.168150 Loss 3.579593, Accuracy 93.192%\n",
      "Epoch 35, Batch 340, LR 0.168083 Loss 3.579240, Accuracy 93.196%\n",
      "Epoch 35, Batch 341, LR 0.168016 Loss 3.580174, Accuracy 93.186%\n",
      "Epoch 35, Batch 342, LR 0.167949 Loss 3.579559, Accuracy 93.186%\n",
      "Epoch 35, Batch 343, LR 0.167882 Loss 3.580166, Accuracy 93.185%\n",
      "Epoch 35, Batch 344, LR 0.167815 Loss 3.577889, Accuracy 93.191%\n",
      "Epoch 35, Batch 345, LR 0.167748 Loss 3.578178, Accuracy 93.191%\n",
      "Epoch 35, Batch 346, LR 0.167681 Loss 3.578417, Accuracy 93.195%\n",
      "Epoch 35, Batch 347, LR 0.167614 Loss 3.578097, Accuracy 93.196%\n",
      "Epoch 35, Batch 348, LR 0.167547 Loss 3.578734, Accuracy 93.193%\n",
      "Epoch 35, Batch 349, LR 0.167480 Loss 3.578015, Accuracy 93.197%\n",
      "Epoch 35, Batch 350, LR 0.167413 Loss 3.578675, Accuracy 93.192%\n",
      "Epoch 35, Batch 351, LR 0.167346 Loss 3.578234, Accuracy 93.194%\n",
      "Epoch 35, Batch 352, LR 0.167279 Loss 3.579115, Accuracy 93.188%\n",
      "Epoch 35, Batch 353, LR 0.167212 Loss 3.580005, Accuracy 93.177%\n",
      "Epoch 35, Batch 354, LR 0.167145 Loss 3.579817, Accuracy 93.178%\n",
      "Epoch 35, Batch 355, LR 0.167078 Loss 3.580220, Accuracy 93.180%\n",
      "Epoch 35, Batch 356, LR 0.167011 Loss 3.581231, Accuracy 93.177%\n",
      "Epoch 35, Batch 357, LR 0.166944 Loss 3.580967, Accuracy 93.179%\n",
      "Epoch 35, Batch 358, LR 0.166878 Loss 3.581314, Accuracy 93.187%\n",
      "Epoch 35, Batch 359, LR 0.166811 Loss 3.581083, Accuracy 93.189%\n",
      "Epoch 35, Batch 360, LR 0.166744 Loss 3.579204, Accuracy 93.199%\n",
      "Epoch 35, Batch 361, LR 0.166677 Loss 3.579208, Accuracy 93.196%\n",
      "Epoch 35, Batch 362, LR 0.166610 Loss 3.580691, Accuracy 93.193%\n",
      "Epoch 35, Batch 363, LR 0.166543 Loss 3.584118, Accuracy 93.186%\n",
      "Epoch 35, Batch 364, LR 0.166477 Loss 3.583792, Accuracy 93.188%\n",
      "Epoch 35, Batch 365, LR 0.166410 Loss 3.583298, Accuracy 93.191%\n",
      "Epoch 35, Batch 366, LR 0.166343 Loss 3.581815, Accuracy 93.199%\n",
      "Epoch 35, Batch 367, LR 0.166276 Loss 3.580270, Accuracy 93.207%\n",
      "Epoch 35, Batch 368, LR 0.166210 Loss 3.579481, Accuracy 93.204%\n",
      "Epoch 35, Batch 369, LR 0.166143 Loss 3.578219, Accuracy 93.212%\n",
      "Epoch 35, Batch 370, LR 0.166076 Loss 3.576723, Accuracy 93.222%\n",
      "Epoch 35, Batch 371, LR 0.166009 Loss 3.576129, Accuracy 93.228%\n",
      "Epoch 35, Batch 372, LR 0.165943 Loss 3.576835, Accuracy 93.231%\n",
      "Epoch 35, Batch 373, LR 0.165876 Loss 3.576738, Accuracy 93.226%\n",
      "Epoch 35, Batch 374, LR 0.165809 Loss 3.576520, Accuracy 93.219%\n",
      "Epoch 35, Batch 375, LR 0.165743 Loss 3.575672, Accuracy 93.221%\n",
      "Epoch 35, Batch 376, LR 0.165676 Loss 3.576140, Accuracy 93.224%\n",
      "Epoch 35, Batch 377, LR 0.165609 Loss 3.575708, Accuracy 93.222%\n",
      "Epoch 35, Batch 378, LR 0.165543 Loss 3.575501, Accuracy 93.223%\n",
      "Epoch 35, Batch 379, LR 0.165476 Loss 3.574952, Accuracy 93.237%\n",
      "Epoch 35, Batch 380, LR 0.165410 Loss 3.574840, Accuracy 93.234%\n",
      "Epoch 35, Batch 381, LR 0.165343 Loss 3.574410, Accuracy 93.235%\n",
      "Epoch 35, Batch 382, LR 0.165276 Loss 3.574907, Accuracy 93.233%\n",
      "Epoch 35, Batch 383, LR 0.165210 Loss 3.574401, Accuracy 93.234%\n",
      "Epoch 35, Batch 384, LR 0.165143 Loss 3.575418, Accuracy 93.231%\n",
      "Epoch 35, Batch 385, LR 0.165077 Loss 3.577639, Accuracy 93.222%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 386, LR 0.165010 Loss 3.577910, Accuracy 93.222%\n",
      "Epoch 35, Batch 387, LR 0.164944 Loss 3.577960, Accuracy 93.223%\n",
      "Epoch 35, Batch 388, LR 0.164877 Loss 3.578639, Accuracy 93.220%\n",
      "Epoch 35, Batch 389, LR 0.164811 Loss 3.577905, Accuracy 93.222%\n",
      "Epoch 35, Batch 390, LR 0.164744 Loss 3.576984, Accuracy 93.223%\n",
      "Epoch 35, Batch 391, LR 0.164678 Loss 3.576013, Accuracy 93.223%\n",
      "Epoch 35, Batch 392, LR 0.164611 Loss 3.576435, Accuracy 93.226%\n",
      "Epoch 35, Batch 393, LR 0.164545 Loss 3.576994, Accuracy 93.225%\n",
      "Epoch 35, Batch 394, LR 0.164479 Loss 3.577064, Accuracy 93.227%\n",
      "Epoch 35, Batch 395, LR 0.164412 Loss 3.576061, Accuracy 93.224%\n",
      "Epoch 35, Batch 396, LR 0.164346 Loss 3.574510, Accuracy 93.229%\n",
      "Epoch 35, Batch 397, LR 0.164279 Loss 3.573755, Accuracy 93.229%\n",
      "Epoch 35, Batch 398, LR 0.164213 Loss 3.573037, Accuracy 93.232%\n",
      "Epoch 35, Batch 399, LR 0.164147 Loss 3.574198, Accuracy 93.225%\n",
      "Epoch 35, Batch 400, LR 0.164080 Loss 3.573738, Accuracy 93.223%\n",
      "Epoch 35, Batch 401, LR 0.164014 Loss 3.572781, Accuracy 93.228%\n",
      "Epoch 35, Batch 402, LR 0.163948 Loss 3.574189, Accuracy 93.221%\n",
      "Epoch 35, Batch 403, LR 0.163881 Loss 3.572798, Accuracy 93.221%\n",
      "Epoch 35, Batch 404, LR 0.163815 Loss 3.573782, Accuracy 93.224%\n",
      "Epoch 35, Batch 405, LR 0.163749 Loss 3.573812, Accuracy 93.223%\n",
      "Epoch 35, Batch 406, LR 0.163682 Loss 3.574793, Accuracy 93.219%\n",
      "Epoch 35, Batch 407, LR 0.163616 Loss 3.575075, Accuracy 93.220%\n",
      "Epoch 35, Batch 408, LR 0.163550 Loss 3.574081, Accuracy 93.227%\n",
      "Epoch 35, Batch 409, LR 0.163484 Loss 3.573531, Accuracy 93.227%\n",
      "Epoch 35, Batch 410, LR 0.163417 Loss 3.573059, Accuracy 93.224%\n",
      "Epoch 35, Batch 411, LR 0.163351 Loss 3.572864, Accuracy 93.233%\n",
      "Epoch 35, Batch 412, LR 0.163285 Loss 3.570863, Accuracy 93.238%\n",
      "Epoch 35, Batch 413, LR 0.163219 Loss 3.570430, Accuracy 93.239%\n",
      "Epoch 35, Batch 414, LR 0.163153 Loss 3.568983, Accuracy 93.246%\n",
      "Epoch 35, Batch 415, LR 0.163087 Loss 3.568401, Accuracy 93.251%\n",
      "Epoch 35, Batch 416, LR 0.163020 Loss 3.567340, Accuracy 93.250%\n",
      "Epoch 35, Batch 417, LR 0.162954 Loss 3.565683, Accuracy 93.255%\n",
      "Epoch 35, Batch 418, LR 0.162888 Loss 3.565057, Accuracy 93.258%\n",
      "Epoch 35, Batch 419, LR 0.162822 Loss 3.563983, Accuracy 93.265%\n",
      "Epoch 35, Batch 420, LR 0.162756 Loss 3.563745, Accuracy 93.263%\n",
      "Epoch 35, Batch 421, LR 0.162690 Loss 3.561892, Accuracy 93.275%\n",
      "Epoch 35, Batch 422, LR 0.162624 Loss 3.561142, Accuracy 93.285%\n",
      "Epoch 35, Batch 423, LR 0.162558 Loss 3.562107, Accuracy 93.285%\n",
      "Epoch 35, Batch 424, LR 0.162492 Loss 3.560717, Accuracy 93.289%\n",
      "Epoch 35, Batch 425, LR 0.162426 Loss 3.560656, Accuracy 93.298%\n",
      "Epoch 35, Batch 426, LR 0.162360 Loss 3.560222, Accuracy 93.301%\n",
      "Epoch 35, Batch 427, LR 0.162294 Loss 3.560479, Accuracy 93.294%\n",
      "Epoch 35, Batch 428, LR 0.162228 Loss 3.560077, Accuracy 93.292%\n",
      "Epoch 35, Batch 429, LR 0.162162 Loss 3.561091, Accuracy 93.284%\n",
      "Epoch 35, Batch 430, LR 0.162096 Loss 3.562526, Accuracy 93.276%\n",
      "Epoch 35, Batch 431, LR 0.162030 Loss 3.562638, Accuracy 93.271%\n",
      "Epoch 35, Batch 432, LR 0.161964 Loss 3.561791, Accuracy 93.271%\n",
      "Epoch 35, Batch 433, LR 0.161898 Loss 3.561823, Accuracy 93.272%\n",
      "Epoch 35, Batch 434, LR 0.161832 Loss 3.561957, Accuracy 93.269%\n",
      "Epoch 35, Batch 435, LR 0.161766 Loss 3.560396, Accuracy 93.274%\n",
      "Epoch 35, Batch 436, LR 0.161700 Loss 3.561703, Accuracy 93.268%\n",
      "Epoch 35, Batch 437, LR 0.161634 Loss 3.562191, Accuracy 93.260%\n",
      "Epoch 35, Batch 438, LR 0.161568 Loss 3.561173, Accuracy 93.267%\n",
      "Epoch 35, Batch 439, LR 0.161502 Loss 3.559989, Accuracy 93.270%\n",
      "Epoch 35, Batch 440, LR 0.161437 Loss 3.560444, Accuracy 93.267%\n",
      "Epoch 35, Batch 441, LR 0.161371 Loss 3.559281, Accuracy 93.275%\n",
      "Epoch 35, Batch 442, LR 0.161305 Loss 3.558275, Accuracy 93.285%\n",
      "Epoch 35, Batch 443, LR 0.161239 Loss 3.557201, Accuracy 93.290%\n",
      "Epoch 35, Batch 444, LR 0.161173 Loss 3.557272, Accuracy 93.293%\n",
      "Epoch 35, Batch 445, LR 0.161108 Loss 3.557379, Accuracy 93.297%\n",
      "Epoch 35, Batch 446, LR 0.161042 Loss 3.557752, Accuracy 93.295%\n",
      "Epoch 35, Batch 447, LR 0.160976 Loss 3.558348, Accuracy 93.296%\n",
      "Epoch 35, Batch 448, LR 0.160910 Loss 3.557173, Accuracy 93.300%\n",
      "Epoch 35, Batch 449, LR 0.160845 Loss 3.558444, Accuracy 93.291%\n",
      "Epoch 35, Batch 450, LR 0.160779 Loss 3.559167, Accuracy 93.295%\n",
      "Epoch 35, Batch 451, LR 0.160713 Loss 3.559352, Accuracy 93.293%\n",
      "Epoch 35, Batch 452, LR 0.160647 Loss 3.559278, Accuracy 93.289%\n",
      "Epoch 35, Batch 453, LR 0.160582 Loss 3.559564, Accuracy 93.293%\n",
      "Epoch 35, Batch 454, LR 0.160516 Loss 3.560836, Accuracy 93.285%\n",
      "Epoch 35, Batch 455, LR 0.160450 Loss 3.561063, Accuracy 93.288%\n",
      "Epoch 35, Batch 456, LR 0.160385 Loss 3.561281, Accuracy 93.286%\n",
      "Epoch 35, Batch 457, LR 0.160319 Loss 3.561279, Accuracy 93.282%\n",
      "Epoch 35, Batch 458, LR 0.160253 Loss 3.562256, Accuracy 93.277%\n",
      "Epoch 35, Batch 459, LR 0.160188 Loss 3.562895, Accuracy 93.279%\n",
      "Epoch 35, Batch 460, LR 0.160122 Loss 3.562628, Accuracy 93.283%\n",
      "Epoch 35, Batch 461, LR 0.160057 Loss 3.563368, Accuracy 93.279%\n",
      "Epoch 35, Batch 462, LR 0.159991 Loss 3.562912, Accuracy 93.283%\n",
      "Epoch 35, Batch 463, LR 0.159926 Loss 3.562955, Accuracy 93.281%\n",
      "Epoch 35, Batch 464, LR 0.159860 Loss 3.564313, Accuracy 93.272%\n",
      "Epoch 35, Batch 465, LR 0.159794 Loss 3.564355, Accuracy 93.271%\n",
      "Epoch 35, Batch 466, LR 0.159729 Loss 3.564095, Accuracy 93.276%\n",
      "Epoch 35, Batch 467, LR 0.159663 Loss 3.565252, Accuracy 93.273%\n",
      "Epoch 35, Batch 468, LR 0.159598 Loss 3.566047, Accuracy 93.264%\n",
      "Epoch 35, Batch 469, LR 0.159532 Loss 3.565435, Accuracy 93.267%\n",
      "Epoch 35, Batch 470, LR 0.159467 Loss 3.565268, Accuracy 93.273%\n",
      "Epoch 35, Batch 471, LR 0.159402 Loss 3.565196, Accuracy 93.274%\n",
      "Epoch 35, Batch 472, LR 0.159336 Loss 3.564912, Accuracy 93.275%\n",
      "Epoch 35, Batch 473, LR 0.159271 Loss 3.563383, Accuracy 93.279%\n",
      "Epoch 35, Batch 474, LR 0.159205 Loss 3.561592, Accuracy 93.289%\n",
      "Epoch 35, Batch 475, LR 0.159140 Loss 3.561442, Accuracy 93.293%\n",
      "Epoch 35, Batch 476, LR 0.159074 Loss 3.561958, Accuracy 93.295%\n",
      "Epoch 35, Batch 477, LR 0.159009 Loss 3.563052, Accuracy 93.296%\n",
      "Epoch 35, Batch 478, LR 0.158944 Loss 3.563093, Accuracy 93.296%\n",
      "Epoch 35, Batch 479, LR 0.158878 Loss 3.564071, Accuracy 93.293%\n",
      "Epoch 35, Batch 480, LR 0.158813 Loss 3.563322, Accuracy 93.301%\n",
      "Epoch 35, Batch 481, LR 0.158748 Loss 3.563469, Accuracy 93.303%\n",
      "Epoch 35, Batch 482, LR 0.158682 Loss 3.564042, Accuracy 93.301%\n",
      "Epoch 35, Batch 483, LR 0.158617 Loss 3.563182, Accuracy 93.297%\n",
      "Epoch 35, Batch 484, LR 0.158552 Loss 3.563333, Accuracy 93.292%\n",
      "Epoch 35, Batch 485, LR 0.158486 Loss 3.564469, Accuracy 93.284%\n",
      "Epoch 35, Batch 486, LR 0.158421 Loss 3.565490, Accuracy 93.285%\n",
      "Epoch 35, Batch 487, LR 0.158356 Loss 3.565586, Accuracy 93.290%\n",
      "Epoch 35, Batch 488, LR 0.158291 Loss 3.566331, Accuracy 93.294%\n",
      "Epoch 35, Batch 489, LR 0.158225 Loss 3.566065, Accuracy 93.291%\n",
      "Epoch 35, Batch 490, LR 0.158160 Loss 3.566116, Accuracy 93.291%\n",
      "Epoch 35, Batch 491, LR 0.158095 Loss 3.564637, Accuracy 93.293%\n",
      "Epoch 35, Batch 492, LR 0.158030 Loss 3.564718, Accuracy 93.291%\n",
      "Epoch 35, Batch 493, LR 0.157965 Loss 3.564710, Accuracy 93.290%\n",
      "Epoch 35, Batch 494, LR 0.157899 Loss 3.565535, Accuracy 93.282%\n",
      "Epoch 35, Batch 495, LR 0.157834 Loss 3.567251, Accuracy 93.270%\n",
      "Epoch 35, Batch 496, LR 0.157769 Loss 3.565377, Accuracy 93.274%\n",
      "Epoch 35, Batch 497, LR 0.157704 Loss 3.565665, Accuracy 93.272%\n",
      "Epoch 35, Batch 498, LR 0.157639 Loss 3.565196, Accuracy 93.273%\n",
      "Epoch 35, Batch 499, LR 0.157574 Loss 3.565927, Accuracy 93.268%\n",
      "Epoch 35, Batch 500, LR 0.157509 Loss 3.565520, Accuracy 93.266%\n",
      "Epoch 35, Batch 501, LR 0.157444 Loss 3.566132, Accuracy 93.260%\n",
      "Epoch 35, Batch 502, LR 0.157378 Loss 3.565928, Accuracy 93.266%\n",
      "Epoch 35, Batch 503, LR 0.157313 Loss 3.565385, Accuracy 93.267%\n",
      "Epoch 35, Batch 504, LR 0.157248 Loss 3.566102, Accuracy 93.266%\n",
      "Epoch 35, Batch 505, LR 0.157183 Loss 3.565840, Accuracy 93.264%\n",
      "Epoch 35, Batch 506, LR 0.157118 Loss 3.566483, Accuracy 93.265%\n",
      "Epoch 35, Batch 507, LR 0.157053 Loss 3.566126, Accuracy 93.265%\n",
      "Epoch 35, Batch 508, LR 0.156988 Loss 3.566830, Accuracy 93.261%\n",
      "Epoch 35, Batch 509, LR 0.156923 Loss 3.567607, Accuracy 93.265%\n",
      "Epoch 35, Batch 510, LR 0.156858 Loss 3.567271, Accuracy 93.266%\n",
      "Epoch 35, Batch 511, LR 0.156793 Loss 3.566922, Accuracy 93.265%\n",
      "Epoch 35, Batch 512, LR 0.156728 Loss 3.565922, Accuracy 93.265%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 513, LR 0.156663 Loss 3.566153, Accuracy 93.266%\n",
      "Epoch 35, Batch 514, LR 0.156599 Loss 3.566978, Accuracy 93.265%\n",
      "Epoch 35, Batch 515, LR 0.156534 Loss 3.567159, Accuracy 93.262%\n",
      "Epoch 35, Batch 516, LR 0.156469 Loss 3.568427, Accuracy 93.261%\n",
      "Epoch 35, Batch 517, LR 0.156404 Loss 3.570490, Accuracy 93.262%\n",
      "Epoch 35, Batch 518, LR 0.156339 Loss 3.569527, Accuracy 93.269%\n",
      "Epoch 35, Batch 519, LR 0.156274 Loss 3.569917, Accuracy 93.267%\n",
      "Epoch 35, Batch 520, LR 0.156209 Loss 3.569433, Accuracy 93.271%\n",
      "Epoch 35, Batch 521, LR 0.156144 Loss 3.569271, Accuracy 93.267%\n",
      "Epoch 35, Batch 522, LR 0.156080 Loss 3.569949, Accuracy 93.264%\n",
      "Epoch 35, Batch 523, LR 0.156015 Loss 3.570114, Accuracy 93.259%\n",
      "Epoch 35, Batch 524, LR 0.155950 Loss 3.569537, Accuracy 93.265%\n",
      "Epoch 35, Batch 525, LR 0.155885 Loss 3.570368, Accuracy 93.268%\n",
      "Epoch 35, Batch 526, LR 0.155820 Loss 3.571523, Accuracy 93.261%\n",
      "Epoch 35, Batch 527, LR 0.155756 Loss 3.570869, Accuracy 93.265%\n",
      "Epoch 35, Batch 528, LR 0.155691 Loss 3.572207, Accuracy 93.254%\n",
      "Epoch 35, Batch 529, LR 0.155626 Loss 3.572968, Accuracy 93.255%\n",
      "Epoch 35, Batch 530, LR 0.155562 Loss 3.573002, Accuracy 93.256%\n",
      "Epoch 35, Batch 531, LR 0.155497 Loss 3.572718, Accuracy 93.253%\n",
      "Epoch 35, Batch 532, LR 0.155432 Loss 3.572304, Accuracy 93.258%\n",
      "Epoch 35, Batch 533, LR 0.155367 Loss 3.572475, Accuracy 93.259%\n",
      "Epoch 35, Batch 534, LR 0.155303 Loss 3.572954, Accuracy 93.254%\n",
      "Epoch 35, Batch 535, LR 0.155238 Loss 3.573103, Accuracy 93.251%\n",
      "Epoch 35, Batch 536, LR 0.155173 Loss 3.572824, Accuracy 93.253%\n",
      "Epoch 35, Batch 537, LR 0.155109 Loss 3.572070, Accuracy 93.254%\n",
      "Epoch 35, Batch 538, LR 0.155044 Loss 3.571097, Accuracy 93.256%\n",
      "Epoch 35, Batch 539, LR 0.154980 Loss 3.571587, Accuracy 93.251%\n",
      "Epoch 35, Batch 540, LR 0.154915 Loss 3.572016, Accuracy 93.252%\n",
      "Epoch 35, Batch 541, LR 0.154850 Loss 3.572234, Accuracy 93.247%\n",
      "Epoch 35, Batch 542, LR 0.154786 Loss 3.572061, Accuracy 93.244%\n",
      "Epoch 35, Batch 543, LR 0.154721 Loss 3.573636, Accuracy 93.241%\n",
      "Epoch 35, Batch 544, LR 0.154657 Loss 3.574003, Accuracy 93.233%\n",
      "Epoch 35, Batch 545, LR 0.154592 Loss 3.573505, Accuracy 93.233%\n",
      "Epoch 35, Batch 546, LR 0.154528 Loss 3.574116, Accuracy 93.231%\n",
      "Epoch 35, Batch 547, LR 0.154463 Loss 3.574221, Accuracy 93.233%\n",
      "Epoch 35, Batch 548, LR 0.154399 Loss 3.573991, Accuracy 93.234%\n",
      "Epoch 35, Batch 549, LR 0.154334 Loss 3.574316, Accuracy 93.229%\n",
      "Epoch 35, Batch 550, LR 0.154270 Loss 3.574967, Accuracy 93.223%\n",
      "Epoch 35, Batch 551, LR 0.154205 Loss 3.574234, Accuracy 93.225%\n",
      "Epoch 35, Batch 552, LR 0.154141 Loss 3.574207, Accuracy 93.224%\n",
      "Epoch 35, Batch 553, LR 0.154076 Loss 3.574428, Accuracy 93.223%\n",
      "Epoch 35, Batch 554, LR 0.154012 Loss 3.574133, Accuracy 93.223%\n",
      "Epoch 35, Batch 555, LR 0.153948 Loss 3.574028, Accuracy 93.229%\n",
      "Epoch 35, Batch 556, LR 0.153883 Loss 3.574188, Accuracy 93.232%\n",
      "Epoch 35, Batch 557, LR 0.153819 Loss 3.574845, Accuracy 93.234%\n",
      "Epoch 35, Batch 558, LR 0.153754 Loss 3.575813, Accuracy 93.229%\n",
      "Epoch 35, Batch 559, LR 0.153690 Loss 3.576477, Accuracy 93.225%\n",
      "Epoch 35, Batch 560, LR 0.153626 Loss 3.575726, Accuracy 93.230%\n",
      "Epoch 35, Batch 561, LR 0.153561 Loss 3.575502, Accuracy 93.229%\n",
      "Epoch 35, Batch 562, LR 0.153497 Loss 3.576342, Accuracy 93.227%\n",
      "Epoch 35, Batch 563, LR 0.153433 Loss 3.577210, Accuracy 93.225%\n",
      "Epoch 35, Batch 564, LR 0.153369 Loss 3.576921, Accuracy 93.225%\n",
      "Epoch 35, Batch 565, LR 0.153304 Loss 3.577375, Accuracy 93.222%\n",
      "Epoch 35, Batch 566, LR 0.153240 Loss 3.577444, Accuracy 93.224%\n",
      "Epoch 35, Batch 567, LR 0.153176 Loss 3.576469, Accuracy 93.226%\n",
      "Epoch 35, Batch 568, LR 0.153111 Loss 3.575956, Accuracy 93.236%\n",
      "Epoch 35, Batch 569, LR 0.153047 Loss 3.577429, Accuracy 93.231%\n",
      "Epoch 35, Batch 570, LR 0.152983 Loss 3.577223, Accuracy 93.231%\n",
      "Epoch 35, Batch 571, LR 0.152919 Loss 3.576821, Accuracy 93.236%\n",
      "Epoch 35, Batch 572, LR 0.152855 Loss 3.576685, Accuracy 93.238%\n",
      "Epoch 35, Batch 573, LR 0.152790 Loss 3.577103, Accuracy 93.236%\n",
      "Epoch 35, Batch 574, LR 0.152726 Loss 3.577667, Accuracy 93.237%\n",
      "Epoch 35, Batch 575, LR 0.152662 Loss 3.576830, Accuracy 93.245%\n",
      "Epoch 35, Batch 576, LR 0.152598 Loss 3.576603, Accuracy 93.248%\n",
      "Epoch 35, Batch 577, LR 0.152534 Loss 3.577248, Accuracy 93.245%\n",
      "Epoch 35, Batch 578, LR 0.152470 Loss 3.577353, Accuracy 93.244%\n",
      "Epoch 35, Batch 579, LR 0.152406 Loss 3.577227, Accuracy 93.241%\n",
      "Epoch 35, Batch 580, LR 0.152342 Loss 3.576503, Accuracy 93.244%\n",
      "Epoch 35, Batch 581, LR 0.152277 Loss 3.576521, Accuracy 93.244%\n",
      "Epoch 35, Batch 582, LR 0.152213 Loss 3.576103, Accuracy 93.240%\n",
      "Epoch 35, Batch 583, LR 0.152149 Loss 3.575877, Accuracy 93.243%\n",
      "Epoch 35, Batch 584, LR 0.152085 Loss 3.576188, Accuracy 93.239%\n",
      "Epoch 35, Batch 585, LR 0.152021 Loss 3.576616, Accuracy 93.233%\n",
      "Epoch 35, Batch 586, LR 0.151957 Loss 3.576129, Accuracy 93.241%\n",
      "Epoch 35, Batch 587, LR 0.151893 Loss 3.576110, Accuracy 93.239%\n",
      "Epoch 35, Batch 588, LR 0.151829 Loss 3.576372, Accuracy 93.240%\n",
      "Epoch 35, Batch 589, LR 0.151765 Loss 3.576183, Accuracy 93.245%\n",
      "Epoch 35, Batch 590, LR 0.151701 Loss 3.575952, Accuracy 93.244%\n",
      "Epoch 35, Batch 591, LR 0.151637 Loss 3.576413, Accuracy 93.242%\n",
      "Epoch 35, Batch 592, LR 0.151573 Loss 3.576136, Accuracy 93.249%\n",
      "Epoch 35, Batch 593, LR 0.151510 Loss 3.575966, Accuracy 93.252%\n",
      "Epoch 35, Batch 594, LR 0.151446 Loss 3.576490, Accuracy 93.250%\n",
      "Epoch 35, Batch 595, LR 0.151382 Loss 3.576396, Accuracy 93.250%\n",
      "Epoch 35, Batch 596, LR 0.151318 Loss 3.575728, Accuracy 93.249%\n",
      "Epoch 35, Batch 597, LR 0.151254 Loss 3.575260, Accuracy 93.249%\n",
      "Epoch 35, Batch 598, LR 0.151190 Loss 3.576362, Accuracy 93.242%\n",
      "Epoch 35, Batch 599, LR 0.151126 Loss 3.576657, Accuracy 93.248%\n",
      "Epoch 35, Batch 600, LR 0.151062 Loss 3.575709, Accuracy 93.254%\n",
      "Epoch 35, Batch 601, LR 0.150999 Loss 3.575145, Accuracy 93.255%\n",
      "Epoch 35, Batch 602, LR 0.150935 Loss 3.575658, Accuracy 93.253%\n",
      "Epoch 35, Batch 603, LR 0.150871 Loss 3.575851, Accuracy 93.251%\n",
      "Epoch 35, Batch 604, LR 0.150807 Loss 3.576246, Accuracy 93.246%\n",
      "Epoch 35, Batch 605, LR 0.150743 Loss 3.575774, Accuracy 93.249%\n",
      "Epoch 35, Batch 606, LR 0.150680 Loss 3.576109, Accuracy 93.247%\n",
      "Epoch 35, Batch 607, LR 0.150616 Loss 3.575471, Accuracy 93.253%\n",
      "Epoch 35, Batch 608, LR 0.150552 Loss 3.575812, Accuracy 93.253%\n",
      "Epoch 35, Batch 609, LR 0.150488 Loss 3.576214, Accuracy 93.255%\n",
      "Epoch 35, Batch 610, LR 0.150425 Loss 3.576419, Accuracy 93.253%\n",
      "Epoch 35, Batch 611, LR 0.150361 Loss 3.576980, Accuracy 93.250%\n",
      "Epoch 35, Batch 612, LR 0.150297 Loss 3.576644, Accuracy 93.255%\n",
      "Epoch 35, Batch 613, LR 0.150234 Loss 3.577089, Accuracy 93.245%\n",
      "Epoch 35, Batch 614, LR 0.150170 Loss 3.577037, Accuracy 93.247%\n",
      "Epoch 35, Batch 615, LR 0.150106 Loss 3.577055, Accuracy 93.248%\n",
      "Epoch 35, Batch 616, LR 0.150043 Loss 3.577216, Accuracy 93.249%\n",
      "Epoch 35, Batch 617, LR 0.149979 Loss 3.576776, Accuracy 93.255%\n",
      "Epoch 35, Batch 618, LR 0.149915 Loss 3.576441, Accuracy 93.257%\n",
      "Epoch 35, Batch 619, LR 0.149852 Loss 3.575552, Accuracy 93.257%\n",
      "Epoch 35, Batch 620, LR 0.149788 Loss 3.575955, Accuracy 93.254%\n",
      "Epoch 35, Batch 621, LR 0.149725 Loss 3.577036, Accuracy 93.248%\n",
      "Epoch 35, Batch 622, LR 0.149661 Loss 3.577419, Accuracy 93.250%\n",
      "Epoch 35, Batch 623, LR 0.149598 Loss 3.577540, Accuracy 93.251%\n",
      "Epoch 35, Batch 624, LR 0.149534 Loss 3.577599, Accuracy 93.252%\n",
      "Epoch 35, Batch 625, LR 0.149471 Loss 3.576309, Accuracy 93.255%\n",
      "Epoch 35, Batch 626, LR 0.149407 Loss 3.576427, Accuracy 93.255%\n",
      "Epoch 35, Batch 627, LR 0.149344 Loss 3.576073, Accuracy 93.262%\n",
      "Epoch 35, Batch 628, LR 0.149280 Loss 3.576021, Accuracy 93.265%\n",
      "Epoch 35, Batch 629, LR 0.149217 Loss 3.575396, Accuracy 93.267%\n",
      "Epoch 35, Batch 630, LR 0.149153 Loss 3.576159, Accuracy 93.260%\n",
      "Epoch 35, Batch 631, LR 0.149090 Loss 3.575369, Accuracy 93.265%\n",
      "Epoch 35, Batch 632, LR 0.149026 Loss 3.576037, Accuracy 93.264%\n",
      "Epoch 35, Batch 633, LR 0.148963 Loss 3.576010, Accuracy 93.262%\n",
      "Epoch 35, Batch 634, LR 0.148899 Loss 3.576380, Accuracy 93.260%\n",
      "Epoch 35, Batch 635, LR 0.148836 Loss 3.576581, Accuracy 93.264%\n",
      "Epoch 35, Batch 636, LR 0.148773 Loss 3.577168, Accuracy 93.264%\n",
      "Epoch 35, Batch 637, LR 0.148709 Loss 3.576485, Accuracy 93.267%\n",
      "Epoch 35, Batch 638, LR 0.148646 Loss 3.576193, Accuracy 93.265%\n",
      "Epoch 35, Batch 639, LR 0.148583 Loss 3.575511, Accuracy 93.267%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 640, LR 0.148519 Loss 3.576285, Accuracy 93.265%\n",
      "Epoch 35, Batch 641, LR 0.148456 Loss 3.576144, Accuracy 93.262%\n",
      "Epoch 35, Batch 642, LR 0.148393 Loss 3.575934, Accuracy 93.262%\n",
      "Epoch 35, Batch 643, LR 0.148329 Loss 3.576873, Accuracy 93.254%\n",
      "Epoch 35, Batch 644, LR 0.148266 Loss 3.576840, Accuracy 93.254%\n",
      "Epoch 35, Batch 645, LR 0.148203 Loss 3.576454, Accuracy 93.256%\n",
      "Epoch 35, Batch 646, LR 0.148139 Loss 3.575920, Accuracy 93.257%\n",
      "Epoch 35, Batch 647, LR 0.148076 Loss 3.576353, Accuracy 93.257%\n",
      "Epoch 35, Batch 648, LR 0.148013 Loss 3.576779, Accuracy 93.254%\n",
      "Epoch 35, Batch 649, LR 0.147950 Loss 3.575279, Accuracy 93.259%\n",
      "Epoch 35, Batch 650, LR 0.147887 Loss 3.575185, Accuracy 93.262%\n",
      "Epoch 35, Batch 651, LR 0.147823 Loss 3.575500, Accuracy 93.259%\n",
      "Epoch 35, Batch 652, LR 0.147760 Loss 3.575178, Accuracy 93.256%\n",
      "Epoch 35, Batch 653, LR 0.147697 Loss 3.575518, Accuracy 93.253%\n",
      "Epoch 35, Batch 654, LR 0.147634 Loss 3.576416, Accuracy 93.248%\n",
      "Epoch 35, Batch 655, LR 0.147571 Loss 3.576586, Accuracy 93.247%\n",
      "Epoch 35, Batch 656, LR 0.147508 Loss 3.576786, Accuracy 93.245%\n",
      "Epoch 35, Batch 657, LR 0.147444 Loss 3.575729, Accuracy 93.245%\n",
      "Epoch 35, Batch 658, LR 0.147381 Loss 3.576072, Accuracy 93.245%\n",
      "Epoch 35, Batch 659, LR 0.147318 Loss 3.575646, Accuracy 93.250%\n",
      "Epoch 35, Batch 660, LR 0.147255 Loss 3.575822, Accuracy 93.248%\n",
      "Epoch 35, Batch 661, LR 0.147192 Loss 3.575783, Accuracy 93.249%\n",
      "Epoch 35, Batch 662, LR 0.147129 Loss 3.576658, Accuracy 93.243%\n",
      "Epoch 35, Batch 663, LR 0.147066 Loss 3.577300, Accuracy 93.240%\n",
      "Epoch 35, Batch 664, LR 0.147003 Loss 3.577425, Accuracy 93.241%\n",
      "Epoch 35, Batch 665, LR 0.146940 Loss 3.577065, Accuracy 93.240%\n",
      "Epoch 35, Batch 666, LR 0.146877 Loss 3.577603, Accuracy 93.240%\n",
      "Epoch 35, Batch 667, LR 0.146814 Loss 3.577658, Accuracy 93.237%\n",
      "Epoch 35, Batch 668, LR 0.146751 Loss 3.578245, Accuracy 93.228%\n",
      "Epoch 35, Batch 669, LR 0.146688 Loss 3.578157, Accuracy 93.224%\n",
      "Epoch 35, Batch 670, LR 0.146625 Loss 3.578139, Accuracy 93.223%\n",
      "Epoch 35, Batch 671, LR 0.146562 Loss 3.578927, Accuracy 93.220%\n",
      "Epoch 35, Batch 672, LR 0.146499 Loss 3.579021, Accuracy 93.214%\n",
      "Epoch 35, Batch 673, LR 0.146436 Loss 3.579139, Accuracy 93.213%\n",
      "Epoch 35, Batch 674, LR 0.146373 Loss 3.578896, Accuracy 93.214%\n",
      "Epoch 35, Batch 675, LR 0.146311 Loss 3.580339, Accuracy 93.205%\n",
      "Epoch 35, Batch 676, LR 0.146248 Loss 3.580065, Accuracy 93.205%\n",
      "Epoch 35, Batch 677, LR 0.146185 Loss 3.580771, Accuracy 93.203%\n",
      "Epoch 35, Batch 678, LR 0.146122 Loss 3.580364, Accuracy 93.203%\n",
      "Epoch 35, Batch 679, LR 0.146059 Loss 3.580717, Accuracy 93.202%\n",
      "Epoch 35, Batch 680, LR 0.145996 Loss 3.579761, Accuracy 93.209%\n",
      "Epoch 35, Batch 681, LR 0.145933 Loss 3.580651, Accuracy 93.206%\n",
      "Epoch 35, Batch 682, LR 0.145871 Loss 3.581453, Accuracy 93.202%\n",
      "Epoch 35, Batch 683, LR 0.145808 Loss 3.581268, Accuracy 93.200%\n",
      "Epoch 35, Batch 684, LR 0.145745 Loss 3.581947, Accuracy 93.194%\n",
      "Epoch 35, Batch 685, LR 0.145682 Loss 3.582724, Accuracy 93.191%\n",
      "Epoch 35, Batch 686, LR 0.145620 Loss 3.583637, Accuracy 93.187%\n",
      "Epoch 35, Batch 687, LR 0.145557 Loss 3.583263, Accuracy 93.188%\n",
      "Epoch 35, Batch 688, LR 0.145494 Loss 3.583063, Accuracy 93.189%\n",
      "Epoch 35, Batch 689, LR 0.145431 Loss 3.583217, Accuracy 93.185%\n",
      "Epoch 35, Batch 690, LR 0.145369 Loss 3.583513, Accuracy 93.178%\n",
      "Epoch 35, Batch 691, LR 0.145306 Loss 3.584037, Accuracy 93.176%\n",
      "Epoch 35, Batch 692, LR 0.145243 Loss 3.584755, Accuracy 93.173%\n",
      "Epoch 35, Batch 693, LR 0.145181 Loss 3.585320, Accuracy 93.169%\n",
      "Epoch 35, Batch 694, LR 0.145118 Loss 3.584853, Accuracy 93.168%\n",
      "Epoch 35, Batch 695, LR 0.145055 Loss 3.583974, Accuracy 93.170%\n",
      "Epoch 35, Batch 696, LR 0.144993 Loss 3.583904, Accuracy 93.175%\n",
      "Epoch 35, Batch 697, LR 0.144930 Loss 3.583514, Accuracy 93.178%\n",
      "Epoch 35, Batch 698, LR 0.144868 Loss 3.583527, Accuracy 93.177%\n",
      "Epoch 35, Batch 699, LR 0.144805 Loss 3.582364, Accuracy 93.184%\n",
      "Epoch 35, Batch 700, LR 0.144742 Loss 3.581348, Accuracy 93.188%\n",
      "Epoch 35, Batch 701, LR 0.144680 Loss 3.581577, Accuracy 93.189%\n",
      "Epoch 35, Batch 702, LR 0.144617 Loss 3.580626, Accuracy 93.194%\n",
      "Epoch 35, Batch 703, LR 0.144555 Loss 3.580942, Accuracy 93.194%\n",
      "Epoch 35, Batch 704, LR 0.144492 Loss 3.580206, Accuracy 93.197%\n",
      "Epoch 35, Batch 705, LR 0.144430 Loss 3.579964, Accuracy 93.196%\n",
      "Epoch 35, Batch 706, LR 0.144367 Loss 3.579485, Accuracy 93.196%\n",
      "Epoch 35, Batch 707, LR 0.144305 Loss 3.578802, Accuracy 93.201%\n",
      "Epoch 35, Batch 708, LR 0.144242 Loss 3.578280, Accuracy 93.204%\n",
      "Epoch 35, Batch 709, LR 0.144180 Loss 3.577542, Accuracy 93.208%\n",
      "Epoch 35, Batch 710, LR 0.144117 Loss 3.577088, Accuracy 93.210%\n",
      "Epoch 35, Batch 711, LR 0.144055 Loss 3.577080, Accuracy 93.206%\n",
      "Epoch 35, Batch 712, LR 0.143992 Loss 3.576398, Accuracy 93.206%\n",
      "Epoch 35, Batch 713, LR 0.143930 Loss 3.575899, Accuracy 93.209%\n",
      "Epoch 35, Batch 714, LR 0.143868 Loss 3.576301, Accuracy 93.206%\n",
      "Epoch 35, Batch 715, LR 0.143805 Loss 3.576674, Accuracy 93.203%\n",
      "Epoch 35, Batch 716, LR 0.143743 Loss 3.576250, Accuracy 93.203%\n",
      "Epoch 35, Batch 717, LR 0.143681 Loss 3.576660, Accuracy 93.201%\n",
      "Epoch 35, Batch 718, LR 0.143618 Loss 3.576503, Accuracy 93.204%\n",
      "Epoch 35, Batch 719, LR 0.143556 Loss 3.576583, Accuracy 93.202%\n",
      "Epoch 35, Batch 720, LR 0.143494 Loss 3.577772, Accuracy 93.199%\n",
      "Epoch 35, Batch 721, LR 0.143431 Loss 3.577837, Accuracy 93.200%\n",
      "Epoch 35, Batch 722, LR 0.143369 Loss 3.577911, Accuracy 93.198%\n",
      "Epoch 35, Batch 723, LR 0.143307 Loss 3.577959, Accuracy 93.200%\n",
      "Epoch 35, Batch 724, LR 0.143244 Loss 3.577881, Accuracy 93.198%\n",
      "Epoch 35, Batch 725, LR 0.143182 Loss 3.578290, Accuracy 93.194%\n",
      "Epoch 35, Batch 726, LR 0.143120 Loss 3.578478, Accuracy 93.194%\n",
      "Epoch 35, Batch 727, LR 0.143058 Loss 3.578642, Accuracy 93.193%\n",
      "Epoch 35, Batch 728, LR 0.142995 Loss 3.577555, Accuracy 93.198%\n",
      "Epoch 35, Batch 729, LR 0.142933 Loss 3.577599, Accuracy 93.201%\n",
      "Epoch 35, Batch 730, LR 0.142871 Loss 3.577931, Accuracy 93.198%\n",
      "Epoch 35, Batch 731, LR 0.142809 Loss 3.577935, Accuracy 93.200%\n",
      "Epoch 35, Batch 732, LR 0.142747 Loss 3.577378, Accuracy 93.201%\n",
      "Epoch 35, Batch 733, LR 0.142685 Loss 3.577101, Accuracy 93.201%\n",
      "Epoch 35, Batch 734, LR 0.142622 Loss 3.576374, Accuracy 93.204%\n",
      "Epoch 35, Batch 735, LR 0.142560 Loss 3.576959, Accuracy 93.204%\n",
      "Epoch 35, Batch 736, LR 0.142498 Loss 3.577112, Accuracy 93.204%\n",
      "Epoch 35, Batch 737, LR 0.142436 Loss 3.576314, Accuracy 93.209%\n",
      "Epoch 35, Batch 738, LR 0.142374 Loss 3.576522, Accuracy 93.209%\n",
      "Epoch 35, Batch 739, LR 0.142312 Loss 3.577255, Accuracy 93.203%\n",
      "Epoch 35, Batch 740, LR 0.142250 Loss 3.577390, Accuracy 93.201%\n",
      "Epoch 35, Batch 741, LR 0.142188 Loss 3.577132, Accuracy 93.200%\n",
      "Epoch 35, Batch 742, LR 0.142126 Loss 3.576885, Accuracy 93.202%\n",
      "Epoch 35, Batch 743, LR 0.142064 Loss 3.576716, Accuracy 93.199%\n",
      "Epoch 35, Batch 744, LR 0.142002 Loss 3.575348, Accuracy 93.206%\n",
      "Epoch 35, Batch 745, LR 0.141940 Loss 3.575301, Accuracy 93.205%\n",
      "Epoch 35, Batch 746, LR 0.141878 Loss 3.575133, Accuracy 93.200%\n",
      "Epoch 35, Batch 747, LR 0.141816 Loss 3.574977, Accuracy 93.202%\n",
      "Epoch 35, Batch 748, LR 0.141754 Loss 3.575264, Accuracy 93.204%\n",
      "Epoch 35, Batch 749, LR 0.141692 Loss 3.575920, Accuracy 93.198%\n",
      "Epoch 35, Batch 750, LR 0.141630 Loss 3.575816, Accuracy 93.194%\n",
      "Epoch 35, Batch 751, LR 0.141568 Loss 3.575397, Accuracy 93.192%\n",
      "Epoch 35, Batch 752, LR 0.141506 Loss 3.575166, Accuracy 93.191%\n",
      "Epoch 35, Batch 753, LR 0.141444 Loss 3.575191, Accuracy 93.192%\n",
      "Epoch 35, Batch 754, LR 0.141382 Loss 3.575009, Accuracy 93.193%\n",
      "Epoch 35, Batch 755, LR 0.141320 Loss 3.575552, Accuracy 93.187%\n",
      "Epoch 35, Batch 756, LR 0.141259 Loss 3.576374, Accuracy 93.181%\n",
      "Epoch 35, Batch 757, LR 0.141197 Loss 3.576277, Accuracy 93.181%\n",
      "Epoch 35, Batch 758, LR 0.141135 Loss 3.576632, Accuracy 93.179%\n",
      "Epoch 35, Batch 759, LR 0.141073 Loss 3.577117, Accuracy 93.175%\n",
      "Epoch 35, Batch 760, LR 0.141011 Loss 3.577401, Accuracy 93.177%\n",
      "Epoch 35, Batch 761, LR 0.140949 Loss 3.576983, Accuracy 93.182%\n",
      "Epoch 35, Batch 762, LR 0.140888 Loss 3.577058, Accuracy 93.181%\n",
      "Epoch 35, Batch 763, LR 0.140826 Loss 3.577018, Accuracy 93.183%\n",
      "Epoch 35, Batch 764, LR 0.140764 Loss 3.577657, Accuracy 93.182%\n",
      "Epoch 35, Batch 765, LR 0.140702 Loss 3.577157, Accuracy 93.185%\n",
      "Epoch 35, Batch 766, LR 0.140641 Loss 3.576477, Accuracy 93.186%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 767, LR 0.140579 Loss 3.576669, Accuracy 93.185%\n",
      "Epoch 35, Batch 768, LR 0.140517 Loss 3.576482, Accuracy 93.187%\n",
      "Epoch 35, Batch 769, LR 0.140456 Loss 3.576046, Accuracy 93.191%\n",
      "Epoch 35, Batch 770, LR 0.140394 Loss 3.575778, Accuracy 93.190%\n",
      "Epoch 35, Batch 771, LR 0.140332 Loss 3.574524, Accuracy 93.192%\n",
      "Epoch 35, Batch 772, LR 0.140271 Loss 3.573695, Accuracy 93.196%\n",
      "Epoch 35, Batch 773, LR 0.140209 Loss 3.573318, Accuracy 93.197%\n",
      "Epoch 35, Batch 774, LR 0.140147 Loss 3.572505, Accuracy 93.203%\n",
      "Epoch 35, Batch 775, LR 0.140086 Loss 3.572322, Accuracy 93.203%\n",
      "Epoch 35, Batch 776, LR 0.140024 Loss 3.573000, Accuracy 93.199%\n",
      "Epoch 35, Batch 777, LR 0.139962 Loss 3.573135, Accuracy 93.199%\n",
      "Epoch 35, Batch 778, LR 0.139901 Loss 3.573613, Accuracy 93.198%\n",
      "Epoch 35, Batch 779, LR 0.139839 Loss 3.573578, Accuracy 93.201%\n",
      "Epoch 35, Batch 780, LR 0.139778 Loss 3.574250, Accuracy 93.198%\n",
      "Epoch 35, Batch 781, LR 0.139716 Loss 3.574143, Accuracy 93.195%\n",
      "Epoch 35, Batch 782, LR 0.139655 Loss 3.574128, Accuracy 93.198%\n",
      "Epoch 35, Batch 783, LR 0.139593 Loss 3.573791, Accuracy 93.201%\n",
      "Epoch 35, Batch 784, LR 0.139532 Loss 3.574446, Accuracy 93.199%\n",
      "Epoch 35, Batch 785, LR 0.139470 Loss 3.574716, Accuracy 93.199%\n",
      "Epoch 35, Batch 786, LR 0.139409 Loss 3.574291, Accuracy 93.199%\n",
      "Epoch 35, Batch 787, LR 0.139347 Loss 3.574336, Accuracy 93.196%\n",
      "Epoch 35, Batch 788, LR 0.139286 Loss 3.574386, Accuracy 93.196%\n",
      "Epoch 35, Batch 789, LR 0.139224 Loss 3.574082, Accuracy 93.196%\n",
      "Epoch 35, Batch 790, LR 0.139163 Loss 3.573523, Accuracy 93.197%\n",
      "Epoch 35, Batch 791, LR 0.139101 Loss 3.573382, Accuracy 93.197%\n",
      "Epoch 35, Batch 792, LR 0.139040 Loss 3.573824, Accuracy 93.197%\n",
      "Epoch 35, Batch 793, LR 0.138979 Loss 3.575817, Accuracy 93.186%\n",
      "Epoch 35, Batch 794, LR 0.138917 Loss 3.575113, Accuracy 93.187%\n",
      "Epoch 35, Batch 795, LR 0.138856 Loss 3.575909, Accuracy 93.187%\n",
      "Epoch 35, Batch 796, LR 0.138795 Loss 3.576325, Accuracy 93.185%\n",
      "Epoch 35, Batch 797, LR 0.138733 Loss 3.577180, Accuracy 93.182%\n",
      "Epoch 35, Batch 798, LR 0.138672 Loss 3.577065, Accuracy 93.183%\n",
      "Epoch 35, Batch 799, LR 0.138611 Loss 3.577418, Accuracy 93.181%\n",
      "Epoch 35, Batch 800, LR 0.138549 Loss 3.576669, Accuracy 93.184%\n",
      "Epoch 35, Batch 801, LR 0.138488 Loss 3.576137, Accuracy 93.186%\n",
      "Epoch 35, Batch 802, LR 0.138427 Loss 3.575956, Accuracy 93.185%\n",
      "Epoch 35, Batch 803, LR 0.138365 Loss 3.575655, Accuracy 93.188%\n",
      "Epoch 35, Batch 804, LR 0.138304 Loss 3.575228, Accuracy 93.185%\n",
      "Epoch 35, Batch 805, LR 0.138243 Loss 3.575082, Accuracy 93.186%\n",
      "Epoch 35, Batch 806, LR 0.138182 Loss 3.575486, Accuracy 93.184%\n",
      "Epoch 35, Batch 807, LR 0.138121 Loss 3.576671, Accuracy 93.184%\n",
      "Epoch 35, Batch 808, LR 0.138059 Loss 3.576816, Accuracy 93.181%\n",
      "Epoch 35, Batch 809, LR 0.137998 Loss 3.576403, Accuracy 93.182%\n",
      "Epoch 35, Batch 810, LR 0.137937 Loss 3.575926, Accuracy 93.184%\n",
      "Epoch 35, Batch 811, LR 0.137876 Loss 3.575941, Accuracy 93.185%\n",
      "Epoch 35, Batch 812, LR 0.137815 Loss 3.576449, Accuracy 93.179%\n",
      "Epoch 35, Batch 813, LR 0.137754 Loss 3.576734, Accuracy 93.176%\n",
      "Epoch 35, Batch 814, LR 0.137692 Loss 3.576357, Accuracy 93.180%\n",
      "Epoch 35, Batch 815, LR 0.137631 Loss 3.576332, Accuracy 93.181%\n",
      "Epoch 35, Batch 816, LR 0.137570 Loss 3.576758, Accuracy 93.183%\n",
      "Epoch 35, Batch 817, LR 0.137509 Loss 3.575737, Accuracy 93.187%\n",
      "Epoch 35, Batch 818, LR 0.137448 Loss 3.576007, Accuracy 93.187%\n",
      "Epoch 35, Batch 819, LR 0.137387 Loss 3.575694, Accuracy 93.189%\n",
      "Epoch 35, Batch 820, LR 0.137326 Loss 3.575614, Accuracy 93.190%\n",
      "Epoch 35, Batch 821, LR 0.137265 Loss 3.575344, Accuracy 93.190%\n",
      "Epoch 35, Batch 822, LR 0.137204 Loss 3.575277, Accuracy 93.192%\n",
      "Epoch 35, Batch 823, LR 0.137143 Loss 3.575510, Accuracy 93.191%\n",
      "Epoch 35, Batch 824, LR 0.137082 Loss 3.575425, Accuracy 93.189%\n",
      "Epoch 35, Batch 825, LR 0.137021 Loss 3.575536, Accuracy 93.188%\n",
      "Epoch 35, Batch 826, LR 0.136960 Loss 3.575105, Accuracy 93.190%\n",
      "Epoch 35, Batch 827, LR 0.136899 Loss 3.574823, Accuracy 93.192%\n",
      "Epoch 35, Batch 828, LR 0.136838 Loss 3.574678, Accuracy 93.194%\n",
      "Epoch 35, Batch 829, LR 0.136777 Loss 3.574183, Accuracy 93.199%\n",
      "Epoch 35, Batch 830, LR 0.136716 Loss 3.574364, Accuracy 93.197%\n",
      "Epoch 35, Batch 831, LR 0.136655 Loss 3.574532, Accuracy 93.198%\n",
      "Epoch 35, Batch 832, LR 0.136594 Loss 3.575969, Accuracy 93.192%\n",
      "Epoch 35, Batch 833, LR 0.136534 Loss 3.576577, Accuracy 93.190%\n",
      "Epoch 35, Batch 834, LR 0.136473 Loss 3.576178, Accuracy 93.191%\n",
      "Epoch 35, Batch 835, LR 0.136412 Loss 3.576125, Accuracy 93.190%\n",
      "Epoch 35, Batch 836, LR 0.136351 Loss 3.575868, Accuracy 93.191%\n",
      "Epoch 35, Batch 837, LR 0.136290 Loss 3.575340, Accuracy 93.196%\n",
      "Epoch 35, Batch 838, LR 0.136229 Loss 3.574835, Accuracy 93.200%\n",
      "Epoch 35, Batch 839, LR 0.136169 Loss 3.575258, Accuracy 93.202%\n",
      "Epoch 35, Batch 840, LR 0.136108 Loss 3.574634, Accuracy 93.207%\n",
      "Epoch 35, Batch 841, LR 0.136047 Loss 3.574130, Accuracy 93.209%\n",
      "Epoch 35, Batch 842, LR 0.135986 Loss 3.574751, Accuracy 93.206%\n",
      "Epoch 35, Batch 843, LR 0.135925 Loss 3.575213, Accuracy 93.203%\n",
      "Epoch 35, Batch 844, LR 0.135865 Loss 3.575670, Accuracy 93.202%\n",
      "Epoch 35, Batch 845, LR 0.135804 Loss 3.575916, Accuracy 93.201%\n",
      "Epoch 35, Batch 846, LR 0.135743 Loss 3.575830, Accuracy 93.203%\n",
      "Epoch 35, Batch 847, LR 0.135683 Loss 3.576373, Accuracy 93.200%\n",
      "Epoch 35, Batch 848, LR 0.135622 Loss 3.576288, Accuracy 93.203%\n",
      "Epoch 35, Batch 849, LR 0.135561 Loss 3.576496, Accuracy 93.196%\n",
      "Epoch 35, Batch 850, LR 0.135501 Loss 3.577095, Accuracy 93.193%\n",
      "Epoch 35, Batch 851, LR 0.135440 Loss 3.576971, Accuracy 93.193%\n",
      "Epoch 35, Batch 852, LR 0.135379 Loss 3.576780, Accuracy 93.192%\n",
      "Epoch 35, Batch 853, LR 0.135319 Loss 3.576636, Accuracy 93.190%\n",
      "Epoch 35, Batch 854, LR 0.135258 Loss 3.576285, Accuracy 93.187%\n",
      "Epoch 35, Batch 855, LR 0.135197 Loss 3.576025, Accuracy 93.190%\n",
      "Epoch 35, Batch 856, LR 0.135137 Loss 3.576154, Accuracy 93.191%\n",
      "Epoch 35, Batch 857, LR 0.135076 Loss 3.577050, Accuracy 93.188%\n",
      "Epoch 35, Batch 858, LR 0.135016 Loss 3.577003, Accuracy 93.188%\n",
      "Epoch 35, Batch 859, LR 0.134955 Loss 3.576696, Accuracy 93.189%\n",
      "Epoch 35, Batch 860, LR 0.134895 Loss 3.576753, Accuracy 93.189%\n",
      "Epoch 35, Batch 861, LR 0.134834 Loss 3.577048, Accuracy 93.189%\n",
      "Epoch 35, Batch 862, LR 0.134774 Loss 3.576628, Accuracy 93.193%\n",
      "Epoch 35, Batch 863, LR 0.134713 Loss 3.576423, Accuracy 93.193%\n",
      "Epoch 35, Batch 864, LR 0.134653 Loss 3.576620, Accuracy 93.190%\n",
      "Epoch 35, Batch 865, LR 0.134592 Loss 3.576287, Accuracy 93.192%\n",
      "Epoch 35, Batch 866, LR 0.134532 Loss 3.576361, Accuracy 93.192%\n",
      "Epoch 35, Batch 867, LR 0.134471 Loss 3.576871, Accuracy 93.190%\n",
      "Epoch 35, Batch 868, LR 0.134411 Loss 3.575996, Accuracy 93.191%\n",
      "Epoch 35, Batch 869, LR 0.134350 Loss 3.576162, Accuracy 93.190%\n",
      "Epoch 35, Batch 870, LR 0.134290 Loss 3.576897, Accuracy 93.188%\n",
      "Epoch 35, Batch 871, LR 0.134230 Loss 3.576413, Accuracy 93.189%\n",
      "Epoch 35, Batch 872, LR 0.134169 Loss 3.575661, Accuracy 93.191%\n",
      "Epoch 35, Batch 873, LR 0.134109 Loss 3.575879, Accuracy 93.190%\n",
      "Epoch 35, Batch 874, LR 0.134049 Loss 3.576382, Accuracy 93.188%\n",
      "Epoch 35, Batch 875, LR 0.133988 Loss 3.575838, Accuracy 93.189%\n",
      "Epoch 35, Batch 876, LR 0.133928 Loss 3.575556, Accuracy 93.191%\n",
      "Epoch 35, Batch 877, LR 0.133868 Loss 3.574969, Accuracy 93.193%\n",
      "Epoch 35, Batch 878, LR 0.133807 Loss 3.574910, Accuracy 93.196%\n",
      "Epoch 35, Batch 879, LR 0.133747 Loss 3.574692, Accuracy 93.195%\n",
      "Epoch 35, Batch 880, LR 0.133687 Loss 3.574059, Accuracy 93.200%\n",
      "Epoch 35, Batch 881, LR 0.133626 Loss 3.574066, Accuracy 93.198%\n",
      "Epoch 35, Batch 882, LR 0.133566 Loss 3.573730, Accuracy 93.198%\n",
      "Epoch 35, Batch 883, LR 0.133506 Loss 3.574298, Accuracy 93.197%\n",
      "Epoch 35, Batch 884, LR 0.133446 Loss 3.573866, Accuracy 93.201%\n",
      "Epoch 35, Batch 885, LR 0.133385 Loss 3.574051, Accuracy 93.198%\n",
      "Epoch 35, Batch 886, LR 0.133325 Loss 3.573597, Accuracy 93.200%\n",
      "Epoch 35, Batch 887, LR 0.133265 Loss 3.574135, Accuracy 93.198%\n",
      "Epoch 35, Batch 888, LR 0.133205 Loss 3.573992, Accuracy 93.198%\n",
      "Epoch 35, Batch 889, LR 0.133145 Loss 3.573588, Accuracy 93.200%\n",
      "Epoch 35, Batch 890, LR 0.133085 Loss 3.573643, Accuracy 93.200%\n",
      "Epoch 35, Batch 891, LR 0.133024 Loss 3.573919, Accuracy 93.199%\n",
      "Epoch 35, Batch 892, LR 0.132964 Loss 3.574500, Accuracy 93.196%\n",
      "Epoch 35, Batch 893, LR 0.132904 Loss 3.574641, Accuracy 93.198%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 894, LR 0.132844 Loss 3.574800, Accuracy 93.197%\n",
      "Epoch 35, Batch 895, LR 0.132784 Loss 3.574821, Accuracy 93.197%\n",
      "Epoch 35, Batch 896, LR 0.132724 Loss 3.574939, Accuracy 93.194%\n",
      "Epoch 35, Batch 897, LR 0.132664 Loss 3.575110, Accuracy 93.193%\n",
      "Epoch 35, Batch 898, LR 0.132604 Loss 3.575786, Accuracy 93.194%\n",
      "Epoch 35, Batch 899, LR 0.132544 Loss 3.575976, Accuracy 93.192%\n",
      "Epoch 35, Batch 900, LR 0.132484 Loss 3.576454, Accuracy 93.188%\n",
      "Epoch 35, Batch 901, LR 0.132424 Loss 3.575624, Accuracy 93.192%\n",
      "Epoch 35, Batch 902, LR 0.132364 Loss 3.575943, Accuracy 93.191%\n",
      "Epoch 35, Batch 903, LR 0.132304 Loss 3.575944, Accuracy 93.193%\n",
      "Epoch 35, Batch 904, LR 0.132244 Loss 3.576254, Accuracy 93.194%\n",
      "Epoch 35, Batch 905, LR 0.132184 Loss 3.576119, Accuracy 93.196%\n",
      "Epoch 35, Batch 906, LR 0.132124 Loss 3.576330, Accuracy 93.196%\n",
      "Epoch 35, Batch 907, LR 0.132064 Loss 3.575603, Accuracy 93.197%\n",
      "Epoch 35, Batch 908, LR 0.132004 Loss 3.575616, Accuracy 93.199%\n",
      "Epoch 35, Batch 909, LR 0.131944 Loss 3.575357, Accuracy 93.202%\n",
      "Epoch 35, Batch 910, LR 0.131884 Loss 3.576170, Accuracy 93.201%\n",
      "Epoch 35, Batch 911, LR 0.131824 Loss 3.575899, Accuracy 93.203%\n",
      "Epoch 35, Batch 912, LR 0.131765 Loss 3.575714, Accuracy 93.204%\n",
      "Epoch 35, Batch 913, LR 0.131705 Loss 3.576019, Accuracy 93.203%\n",
      "Epoch 35, Batch 914, LR 0.131645 Loss 3.576104, Accuracy 93.199%\n",
      "Epoch 35, Batch 915, LR 0.131585 Loss 3.575702, Accuracy 93.200%\n",
      "Epoch 35, Batch 916, LR 0.131525 Loss 3.575896, Accuracy 93.201%\n",
      "Epoch 35, Batch 917, LR 0.131465 Loss 3.577504, Accuracy 93.196%\n",
      "Epoch 35, Batch 918, LR 0.131406 Loss 3.577008, Accuracy 93.199%\n",
      "Epoch 35, Batch 919, LR 0.131346 Loss 3.576679, Accuracy 93.203%\n",
      "Epoch 35, Batch 920, LR 0.131286 Loss 3.577498, Accuracy 93.195%\n",
      "Epoch 35, Batch 921, LR 0.131226 Loss 3.577826, Accuracy 93.193%\n",
      "Epoch 35, Batch 922, LR 0.131167 Loss 3.577905, Accuracy 93.193%\n",
      "Epoch 35, Batch 923, LR 0.131107 Loss 3.577392, Accuracy 93.193%\n",
      "Epoch 35, Batch 924, LR 0.131047 Loss 3.577595, Accuracy 93.193%\n",
      "Epoch 35, Batch 925, LR 0.130987 Loss 3.577997, Accuracy 93.190%\n",
      "Epoch 35, Batch 926, LR 0.130928 Loss 3.578173, Accuracy 93.190%\n",
      "Epoch 35, Batch 927, LR 0.130868 Loss 3.578085, Accuracy 93.187%\n",
      "Epoch 35, Batch 928, LR 0.130808 Loss 3.578583, Accuracy 93.189%\n",
      "Epoch 35, Batch 929, LR 0.130749 Loss 3.578292, Accuracy 93.192%\n",
      "Epoch 35, Batch 930, LR 0.130689 Loss 3.578110, Accuracy 93.192%\n",
      "Epoch 35, Batch 931, LR 0.130630 Loss 3.577855, Accuracy 93.192%\n",
      "Epoch 35, Batch 932, LR 0.130570 Loss 3.577863, Accuracy 93.193%\n",
      "Epoch 35, Batch 933, LR 0.130510 Loss 3.578693, Accuracy 93.191%\n",
      "Epoch 35, Batch 934, LR 0.130451 Loss 3.578832, Accuracy 93.189%\n",
      "Epoch 35, Batch 935, LR 0.130391 Loss 3.579318, Accuracy 93.189%\n",
      "Epoch 35, Batch 936, LR 0.130332 Loss 3.579236, Accuracy 93.191%\n",
      "Epoch 35, Batch 937, LR 0.130272 Loss 3.579297, Accuracy 93.191%\n",
      "Epoch 35, Batch 938, LR 0.130213 Loss 3.579133, Accuracy 93.194%\n",
      "Epoch 35, Batch 939, LR 0.130153 Loss 3.578518, Accuracy 93.194%\n",
      "Epoch 35, Batch 940, LR 0.130094 Loss 3.579334, Accuracy 93.188%\n",
      "Epoch 35, Batch 941, LR 0.130034 Loss 3.579901, Accuracy 93.186%\n",
      "Epoch 35, Batch 942, LR 0.129975 Loss 3.579570, Accuracy 93.189%\n",
      "Epoch 35, Batch 943, LR 0.129915 Loss 3.579027, Accuracy 93.192%\n",
      "Epoch 35, Batch 944, LR 0.129856 Loss 3.579271, Accuracy 93.194%\n",
      "Epoch 35, Batch 945, LR 0.129796 Loss 3.579300, Accuracy 93.198%\n",
      "Epoch 35, Batch 946, LR 0.129737 Loss 3.579288, Accuracy 93.199%\n",
      "Epoch 35, Batch 947, LR 0.129677 Loss 3.579762, Accuracy 93.197%\n",
      "Epoch 35, Batch 948, LR 0.129618 Loss 3.579186, Accuracy 93.203%\n",
      "Epoch 35, Batch 949, LR 0.129559 Loss 3.579715, Accuracy 93.199%\n",
      "Epoch 35, Batch 950, LR 0.129499 Loss 3.579955, Accuracy 93.197%\n",
      "Epoch 35, Batch 951, LR 0.129440 Loss 3.579700, Accuracy 93.200%\n",
      "Epoch 35, Batch 952, LR 0.129380 Loss 3.580440, Accuracy 93.196%\n",
      "Epoch 35, Batch 953, LR 0.129321 Loss 3.580342, Accuracy 93.197%\n",
      "Epoch 35, Batch 954, LR 0.129262 Loss 3.580121, Accuracy 93.198%\n",
      "Epoch 35, Batch 955, LR 0.129202 Loss 3.580225, Accuracy 93.198%\n",
      "Epoch 35, Batch 956, LR 0.129143 Loss 3.579982, Accuracy 93.199%\n",
      "Epoch 35, Batch 957, LR 0.129084 Loss 3.580159, Accuracy 93.197%\n",
      "Epoch 35, Batch 958, LR 0.129025 Loss 3.579764, Accuracy 93.198%\n",
      "Epoch 35, Batch 959, LR 0.128965 Loss 3.579867, Accuracy 93.195%\n",
      "Epoch 35, Batch 960, LR 0.128906 Loss 3.579825, Accuracy 93.192%\n",
      "Epoch 35, Batch 961, LR 0.128847 Loss 3.578953, Accuracy 93.193%\n",
      "Epoch 35, Batch 962, LR 0.128788 Loss 3.579259, Accuracy 93.192%\n",
      "Epoch 35, Batch 963, LR 0.128728 Loss 3.579250, Accuracy 93.191%\n",
      "Epoch 35, Batch 964, LR 0.128669 Loss 3.579261, Accuracy 93.190%\n",
      "Epoch 35, Batch 965, LR 0.128610 Loss 3.579256, Accuracy 93.189%\n",
      "Epoch 35, Batch 966, LR 0.128551 Loss 3.580129, Accuracy 93.182%\n",
      "Epoch 35, Batch 967, LR 0.128492 Loss 3.579709, Accuracy 93.184%\n",
      "Epoch 35, Batch 968, LR 0.128433 Loss 3.580056, Accuracy 93.183%\n",
      "Epoch 35, Batch 969, LR 0.128373 Loss 3.580030, Accuracy 93.183%\n",
      "Epoch 35, Batch 970, LR 0.128314 Loss 3.579959, Accuracy 93.181%\n",
      "Epoch 35, Batch 971, LR 0.128255 Loss 3.579536, Accuracy 93.184%\n",
      "Epoch 35, Batch 972, LR 0.128196 Loss 3.579801, Accuracy 93.181%\n",
      "Epoch 35, Batch 973, LR 0.128137 Loss 3.580447, Accuracy 93.178%\n",
      "Epoch 35, Batch 974, LR 0.128078 Loss 3.581205, Accuracy 93.174%\n",
      "Epoch 35, Batch 975, LR 0.128019 Loss 3.580721, Accuracy 93.178%\n",
      "Epoch 35, Batch 976, LR 0.127960 Loss 3.580753, Accuracy 93.179%\n",
      "Epoch 35, Batch 977, LR 0.127901 Loss 3.580399, Accuracy 93.181%\n",
      "Epoch 35, Batch 978, LR 0.127842 Loss 3.579930, Accuracy 93.184%\n",
      "Epoch 35, Batch 979, LR 0.127783 Loss 3.580312, Accuracy 93.182%\n",
      "Epoch 35, Batch 980, LR 0.127724 Loss 3.580184, Accuracy 93.181%\n",
      "Epoch 35, Batch 981, LR 0.127665 Loss 3.580621, Accuracy 93.179%\n",
      "Epoch 35, Batch 982, LR 0.127606 Loss 3.580722, Accuracy 93.178%\n",
      "Epoch 35, Batch 983, LR 0.127547 Loss 3.580464, Accuracy 93.179%\n",
      "Epoch 35, Batch 984, LR 0.127488 Loss 3.580075, Accuracy 93.182%\n",
      "Epoch 35, Batch 985, LR 0.127429 Loss 3.580102, Accuracy 93.183%\n",
      "Epoch 35, Batch 986, LR 0.127370 Loss 3.579927, Accuracy 93.183%\n",
      "Epoch 35, Batch 987, LR 0.127311 Loss 3.580073, Accuracy 93.182%\n",
      "Epoch 35, Batch 988, LR 0.127252 Loss 3.579920, Accuracy 93.183%\n",
      "Epoch 35, Batch 989, LR 0.127193 Loss 3.579615, Accuracy 93.183%\n",
      "Epoch 35, Batch 990, LR 0.127135 Loss 3.579206, Accuracy 93.185%\n",
      "Epoch 35, Batch 991, LR 0.127076 Loss 3.579295, Accuracy 93.184%\n",
      "Epoch 35, Batch 992, LR 0.127017 Loss 3.579737, Accuracy 93.182%\n",
      "Epoch 35, Batch 993, LR 0.126958 Loss 3.579267, Accuracy 93.186%\n",
      "Epoch 35, Batch 994, LR 0.126899 Loss 3.579486, Accuracy 93.183%\n",
      "Epoch 35, Batch 995, LR 0.126841 Loss 3.579924, Accuracy 93.180%\n",
      "Epoch 35, Batch 996, LR 0.126782 Loss 3.580730, Accuracy 93.174%\n",
      "Epoch 35, Batch 997, LR 0.126723 Loss 3.580197, Accuracy 93.178%\n",
      "Epoch 35, Batch 998, LR 0.126664 Loss 3.580155, Accuracy 93.179%\n",
      "Epoch 35, Batch 999, LR 0.126605 Loss 3.580349, Accuracy 93.178%\n",
      "Epoch 35, Batch 1000, LR 0.126547 Loss 3.580613, Accuracy 93.173%\n",
      "Epoch 35, Batch 1001, LR 0.126488 Loss 3.580210, Accuracy 93.176%\n",
      "Epoch 35, Batch 1002, LR 0.126429 Loss 3.580242, Accuracy 93.177%\n",
      "Epoch 35, Batch 1003, LR 0.126371 Loss 3.580648, Accuracy 93.175%\n",
      "Epoch 35, Batch 1004, LR 0.126312 Loss 3.580546, Accuracy 93.176%\n",
      "Epoch 35, Batch 1005, LR 0.126253 Loss 3.580736, Accuracy 93.174%\n",
      "Epoch 35, Batch 1006, LR 0.126195 Loss 3.580145, Accuracy 93.175%\n",
      "Epoch 35, Batch 1007, LR 0.126136 Loss 3.580405, Accuracy 93.175%\n",
      "Epoch 35, Batch 1008, LR 0.126077 Loss 3.580767, Accuracy 93.174%\n",
      "Epoch 35, Batch 1009, LR 0.126019 Loss 3.580774, Accuracy 93.177%\n",
      "Epoch 35, Batch 1010, LR 0.125960 Loss 3.580596, Accuracy 93.179%\n",
      "Epoch 35, Batch 1011, LR 0.125901 Loss 3.580271, Accuracy 93.179%\n",
      "Epoch 35, Batch 1012, LR 0.125843 Loss 3.580354, Accuracy 93.180%\n",
      "Epoch 35, Batch 1013, LR 0.125784 Loss 3.580624, Accuracy 93.181%\n",
      "Epoch 35, Batch 1014, LR 0.125726 Loss 3.580956, Accuracy 93.181%\n",
      "Epoch 35, Batch 1015, LR 0.125667 Loss 3.582093, Accuracy 93.177%\n",
      "Epoch 35, Batch 1016, LR 0.125609 Loss 3.581861, Accuracy 93.179%\n",
      "Epoch 35, Batch 1017, LR 0.125550 Loss 3.581879, Accuracy 93.178%\n",
      "Epoch 35, Batch 1018, LR 0.125492 Loss 3.581933, Accuracy 93.178%\n",
      "Epoch 35, Batch 1019, LR 0.125433 Loss 3.582614, Accuracy 93.175%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 1020, LR 0.125375 Loss 3.582284, Accuracy 93.176%\n",
      "Epoch 35, Batch 1021, LR 0.125316 Loss 3.582852, Accuracy 93.176%\n",
      "Epoch 35, Batch 1022, LR 0.125258 Loss 3.582612, Accuracy 93.179%\n",
      "Epoch 35, Batch 1023, LR 0.125199 Loss 3.582599, Accuracy 93.178%\n",
      "Epoch 35, Batch 1024, LR 0.125141 Loss 3.582582, Accuracy 93.179%\n",
      "Epoch 35, Batch 1025, LR 0.125083 Loss 3.582400, Accuracy 93.179%\n",
      "Epoch 35, Batch 1026, LR 0.125024 Loss 3.582582, Accuracy 93.180%\n",
      "Epoch 35, Batch 1027, LR 0.124966 Loss 3.582580, Accuracy 93.179%\n",
      "Epoch 35, Batch 1028, LR 0.124907 Loss 3.582207, Accuracy 93.181%\n",
      "Epoch 35, Batch 1029, LR 0.124849 Loss 3.582233, Accuracy 93.183%\n",
      "Epoch 35, Batch 1030, LR 0.124791 Loss 3.582019, Accuracy 93.183%\n",
      "Epoch 35, Batch 1031, LR 0.124732 Loss 3.581974, Accuracy 93.183%\n",
      "Epoch 35, Batch 1032, LR 0.124674 Loss 3.581558, Accuracy 93.187%\n",
      "Epoch 35, Batch 1033, LR 0.124616 Loss 3.581985, Accuracy 93.187%\n",
      "Epoch 35, Batch 1034, LR 0.124557 Loss 3.581539, Accuracy 93.190%\n",
      "Epoch 35, Batch 1035, LR 0.124499 Loss 3.580847, Accuracy 93.193%\n",
      "Epoch 35, Batch 1036, LR 0.124441 Loss 3.581036, Accuracy 93.192%\n",
      "Epoch 35, Batch 1037, LR 0.124383 Loss 3.581606, Accuracy 93.189%\n",
      "Epoch 35, Batch 1038, LR 0.124324 Loss 3.581193, Accuracy 93.190%\n",
      "Epoch 35, Batch 1039, LR 0.124266 Loss 3.581044, Accuracy 93.191%\n",
      "Epoch 35, Batch 1040, LR 0.124208 Loss 3.580998, Accuracy 93.190%\n",
      "Epoch 35, Batch 1041, LR 0.124150 Loss 3.580555, Accuracy 93.192%\n",
      "Epoch 35, Batch 1042, LR 0.124092 Loss 3.580307, Accuracy 93.193%\n",
      "Epoch 35, Batch 1043, LR 0.124033 Loss 3.580652, Accuracy 93.193%\n",
      "Epoch 35, Batch 1044, LR 0.123975 Loss 3.580715, Accuracy 93.193%\n",
      "Epoch 35, Batch 1045, LR 0.123917 Loss 3.580781, Accuracy 93.194%\n",
      "Epoch 35, Batch 1046, LR 0.123859 Loss 3.580610, Accuracy 93.198%\n",
      "Epoch 35, Batch 1047, LR 0.123801 Loss 3.581222, Accuracy 93.196%\n",
      "Epoch 35, Loss (train set) 3.581222, Accuracy (train set) 93.196%\n",
      "Epoch 36, Batch 1, LR 0.123743 Loss 3.464959, Accuracy 92.969%\n",
      "Epoch 36, Batch 2, LR 0.123685 Loss 3.619210, Accuracy 92.188%\n",
      "Epoch 36, Batch 3, LR 0.123626 Loss 3.612427, Accuracy 92.188%\n",
      "Epoch 36, Batch 4, LR 0.123568 Loss 3.484578, Accuracy 92.773%\n",
      "Epoch 36, Batch 5, LR 0.123510 Loss 3.464578, Accuracy 92.812%\n",
      "Epoch 36, Batch 6, LR 0.123452 Loss 3.491007, Accuracy 93.229%\n",
      "Epoch 36, Batch 7, LR 0.123394 Loss 3.426389, Accuracy 93.415%\n",
      "Epoch 36, Batch 8, LR 0.123336 Loss 3.369199, Accuracy 93.848%\n",
      "Epoch 36, Batch 9, LR 0.123278 Loss 3.326598, Accuracy 93.837%\n",
      "Epoch 36, Batch 10, LR 0.123220 Loss 3.301363, Accuracy 93.906%\n",
      "Epoch 36, Batch 11, LR 0.123162 Loss 3.280539, Accuracy 93.963%\n",
      "Epoch 36, Batch 12, LR 0.123104 Loss 3.280513, Accuracy 93.945%\n",
      "Epoch 36, Batch 13, LR 0.123046 Loss 3.274097, Accuracy 93.990%\n",
      "Epoch 36, Batch 14, LR 0.122988 Loss 3.258613, Accuracy 94.141%\n",
      "Epoch 36, Batch 15, LR 0.122930 Loss 3.232674, Accuracy 94.219%\n",
      "Epoch 36, Batch 16, LR 0.122873 Loss 3.240995, Accuracy 94.092%\n",
      "Epoch 36, Batch 17, LR 0.122815 Loss 3.275877, Accuracy 94.072%\n",
      "Epoch 36, Batch 18, LR 0.122757 Loss 3.280277, Accuracy 94.097%\n",
      "Epoch 36, Batch 19, LR 0.122699 Loss 3.276201, Accuracy 94.120%\n",
      "Epoch 36, Batch 20, LR 0.122641 Loss 3.253216, Accuracy 94.180%\n",
      "Epoch 36, Batch 21, LR 0.122583 Loss 3.279274, Accuracy 94.122%\n",
      "Epoch 36, Batch 22, LR 0.122525 Loss 3.279774, Accuracy 94.070%\n",
      "Epoch 36, Batch 23, LR 0.122467 Loss 3.312002, Accuracy 93.988%\n",
      "Epoch 36, Batch 24, LR 0.122410 Loss 3.328665, Accuracy 93.880%\n",
      "Epoch 36, Batch 25, LR 0.122352 Loss 3.322175, Accuracy 93.906%\n",
      "Epoch 36, Batch 26, LR 0.122294 Loss 3.332293, Accuracy 93.900%\n",
      "Epoch 36, Batch 27, LR 0.122236 Loss 3.331489, Accuracy 93.837%\n",
      "Epoch 36, Batch 28, LR 0.122179 Loss 3.337989, Accuracy 93.862%\n",
      "Epoch 36, Batch 29, LR 0.122121 Loss 3.347588, Accuracy 93.777%\n",
      "Epoch 36, Batch 30, LR 0.122063 Loss 3.361955, Accuracy 93.750%\n",
      "Epoch 36, Batch 31, LR 0.122005 Loss 3.377876, Accuracy 93.800%\n",
      "Epoch 36, Batch 32, LR 0.121948 Loss 3.387459, Accuracy 93.774%\n",
      "Epoch 36, Batch 33, LR 0.121890 Loss 3.382745, Accuracy 93.750%\n",
      "Epoch 36, Batch 34, LR 0.121832 Loss 3.398273, Accuracy 93.727%\n",
      "Epoch 36, Batch 35, LR 0.121775 Loss 3.400384, Accuracy 93.705%\n",
      "Epoch 36, Batch 36, LR 0.121717 Loss 3.397685, Accuracy 93.707%\n",
      "Epoch 36, Batch 37, LR 0.121659 Loss 3.398533, Accuracy 93.687%\n",
      "Epoch 36, Batch 38, LR 0.121602 Loss 3.391218, Accuracy 93.709%\n",
      "Epoch 36, Batch 39, LR 0.121544 Loss 3.388637, Accuracy 93.710%\n",
      "Epoch 36, Batch 40, LR 0.121486 Loss 3.401804, Accuracy 93.555%\n",
      "Epoch 36, Batch 41, LR 0.121429 Loss 3.415675, Accuracy 93.483%\n",
      "Epoch 36, Batch 42, LR 0.121371 Loss 3.425092, Accuracy 93.397%\n",
      "Epoch 36, Batch 43, LR 0.121314 Loss 3.419270, Accuracy 93.441%\n",
      "Epoch 36, Batch 44, LR 0.121256 Loss 3.428299, Accuracy 93.359%\n",
      "Epoch 36, Batch 45, LR 0.121198 Loss 3.439768, Accuracy 93.299%\n",
      "Epoch 36, Batch 46, LR 0.121141 Loss 3.451046, Accuracy 93.308%\n",
      "Epoch 36, Batch 47, LR 0.121083 Loss 3.455592, Accuracy 93.318%\n",
      "Epoch 36, Batch 48, LR 0.121026 Loss 3.440820, Accuracy 93.359%\n",
      "Epoch 36, Batch 49, LR 0.120968 Loss 3.435258, Accuracy 93.383%\n",
      "Epoch 36, Batch 50, LR 0.120911 Loss 3.435218, Accuracy 93.375%\n",
      "Epoch 36, Batch 51, LR 0.120854 Loss 3.427823, Accuracy 93.398%\n",
      "Epoch 36, Batch 52, LR 0.120796 Loss 3.418621, Accuracy 93.419%\n",
      "Epoch 36, Batch 53, LR 0.120739 Loss 3.421573, Accuracy 93.426%\n",
      "Epoch 36, Batch 54, LR 0.120681 Loss 3.428583, Accuracy 93.359%\n",
      "Epoch 36, Batch 55, LR 0.120624 Loss 3.420579, Accuracy 93.395%\n",
      "Epoch 36, Batch 56, LR 0.120566 Loss 3.423862, Accuracy 93.401%\n",
      "Epoch 36, Batch 57, LR 0.120509 Loss 3.421387, Accuracy 93.435%\n",
      "Epoch 36, Batch 58, LR 0.120452 Loss 3.425109, Accuracy 93.413%\n",
      "Epoch 36, Batch 59, LR 0.120394 Loss 3.428877, Accuracy 93.392%\n",
      "Epoch 36, Batch 60, LR 0.120337 Loss 3.430729, Accuracy 93.385%\n",
      "Epoch 36, Batch 61, LR 0.120280 Loss 3.434709, Accuracy 93.404%\n",
      "Epoch 36, Batch 62, LR 0.120222 Loss 3.433264, Accuracy 93.435%\n",
      "Epoch 36, Batch 63, LR 0.120165 Loss 3.435671, Accuracy 93.415%\n",
      "Epoch 36, Batch 64, LR 0.120108 Loss 3.436753, Accuracy 93.396%\n",
      "Epoch 36, Batch 65, LR 0.120050 Loss 3.439975, Accuracy 93.413%\n",
      "Epoch 36, Batch 66, LR 0.119993 Loss 3.440639, Accuracy 93.383%\n",
      "Epoch 36, Batch 67, LR 0.119936 Loss 3.438612, Accuracy 93.389%\n",
      "Epoch 36, Batch 68, LR 0.119879 Loss 3.442496, Accuracy 93.382%\n",
      "Epoch 36, Batch 69, LR 0.119821 Loss 3.451598, Accuracy 93.331%\n",
      "Epoch 36, Batch 70, LR 0.119764 Loss 3.464087, Accuracy 93.248%\n",
      "Epoch 36, Batch 71, LR 0.119707 Loss 3.462767, Accuracy 93.277%\n",
      "Epoch 36, Batch 72, LR 0.119650 Loss 3.462612, Accuracy 93.273%\n",
      "Epoch 36, Batch 73, LR 0.119593 Loss 3.460883, Accuracy 93.290%\n",
      "Epoch 36, Batch 74, LR 0.119535 Loss 3.461288, Accuracy 93.317%\n",
      "Epoch 36, Batch 75, LR 0.119478 Loss 3.463565, Accuracy 93.292%\n",
      "Epoch 36, Batch 76, LR 0.119421 Loss 3.452000, Accuracy 93.339%\n",
      "Epoch 36, Batch 77, LR 0.119364 Loss 3.447966, Accuracy 93.385%\n",
      "Epoch 36, Batch 78, LR 0.119307 Loss 3.458897, Accuracy 93.349%\n",
      "Epoch 36, Batch 79, LR 0.119250 Loss 3.462644, Accuracy 93.345%\n",
      "Epoch 36, Batch 80, LR 0.119193 Loss 3.459787, Accuracy 93.369%\n",
      "Epoch 36, Batch 81, LR 0.119136 Loss 3.466235, Accuracy 93.355%\n",
      "Epoch 36, Batch 82, LR 0.119078 Loss 3.455586, Accuracy 93.388%\n",
      "Epoch 36, Batch 83, LR 0.119021 Loss 3.463790, Accuracy 93.326%\n",
      "Epoch 36, Batch 84, LR 0.118964 Loss 3.457759, Accuracy 93.350%\n",
      "Epoch 36, Batch 85, LR 0.118907 Loss 3.459961, Accuracy 93.355%\n",
      "Epoch 36, Batch 86, LR 0.118850 Loss 3.460009, Accuracy 93.368%\n",
      "Epoch 36, Batch 87, LR 0.118793 Loss 3.460906, Accuracy 93.328%\n",
      "Epoch 36, Batch 88, LR 0.118736 Loss 3.453319, Accuracy 93.368%\n",
      "Epoch 36, Batch 89, LR 0.118679 Loss 3.452014, Accuracy 93.364%\n",
      "Epoch 36, Batch 90, LR 0.118622 Loss 3.454626, Accuracy 93.368%\n",
      "Epoch 36, Batch 91, LR 0.118566 Loss 3.449365, Accuracy 93.364%\n",
      "Epoch 36, Batch 92, LR 0.118509 Loss 3.448869, Accuracy 93.385%\n",
      "Epoch 36, Batch 93, LR 0.118452 Loss 3.453776, Accuracy 93.380%\n",
      "Epoch 36, Batch 94, LR 0.118395 Loss 3.450456, Accuracy 93.393%\n",
      "Epoch 36, Batch 95, LR 0.118338 Loss 3.450898, Accuracy 93.396%\n",
      "Epoch 36, Batch 96, LR 0.118281 Loss 3.458012, Accuracy 93.392%\n",
      "Epoch 36, Batch 97, LR 0.118224 Loss 3.464680, Accuracy 93.347%\n",
      "Epoch 36, Batch 98, LR 0.118167 Loss 3.460096, Accuracy 93.391%\n",
      "Epoch 36, Batch 99, LR 0.118110 Loss 3.459727, Accuracy 93.348%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 100, LR 0.118054 Loss 3.467740, Accuracy 93.297%\n",
      "Epoch 36, Batch 101, LR 0.117997 Loss 3.466351, Accuracy 93.309%\n",
      "Epoch 36, Batch 102, LR 0.117940 Loss 3.470995, Accuracy 93.290%\n",
      "Epoch 36, Batch 103, LR 0.117883 Loss 3.469836, Accuracy 93.310%\n",
      "Epoch 36, Batch 104, LR 0.117826 Loss 3.469132, Accuracy 93.299%\n",
      "Epoch 36, Batch 105, LR 0.117770 Loss 3.466977, Accuracy 93.311%\n",
      "Epoch 36, Batch 106, LR 0.117713 Loss 3.469289, Accuracy 93.256%\n",
      "Epoch 36, Batch 107, LR 0.117656 Loss 3.474607, Accuracy 93.246%\n",
      "Epoch 36, Batch 108, LR 0.117599 Loss 3.476304, Accuracy 93.251%\n",
      "Epoch 36, Batch 109, LR 0.117543 Loss 3.482862, Accuracy 93.255%\n",
      "Epoch 36, Batch 110, LR 0.117486 Loss 3.476917, Accuracy 93.295%\n",
      "Epoch 36, Batch 111, LR 0.117429 Loss 3.480276, Accuracy 93.300%\n",
      "Epoch 36, Batch 112, LR 0.117373 Loss 3.480181, Accuracy 93.324%\n",
      "Epoch 36, Batch 113, LR 0.117316 Loss 3.478959, Accuracy 93.363%\n",
      "Epoch 36, Batch 114, LR 0.117259 Loss 3.474831, Accuracy 93.387%\n",
      "Epoch 36, Batch 115, LR 0.117203 Loss 3.479816, Accuracy 93.342%\n",
      "Epoch 36, Batch 116, LR 0.117146 Loss 3.478222, Accuracy 93.346%\n",
      "Epoch 36, Batch 117, LR 0.117089 Loss 3.477711, Accuracy 93.336%\n",
      "Epoch 36, Batch 118, LR 0.117033 Loss 3.481077, Accuracy 93.333%\n",
      "Epoch 36, Batch 119, LR 0.116976 Loss 3.485233, Accuracy 93.323%\n",
      "Epoch 36, Batch 120, LR 0.116920 Loss 3.484187, Accuracy 93.327%\n",
      "Epoch 36, Batch 121, LR 0.116863 Loss 3.482122, Accuracy 93.324%\n",
      "Epoch 36, Batch 122, LR 0.116807 Loss 3.480744, Accuracy 93.334%\n",
      "Epoch 36, Batch 123, LR 0.116750 Loss 3.481753, Accuracy 93.318%\n",
      "Epoch 36, Batch 124, LR 0.116694 Loss 3.484833, Accuracy 93.277%\n",
      "Epoch 36, Batch 125, LR 0.116637 Loss 3.493095, Accuracy 93.225%\n",
      "Epoch 36, Batch 126, LR 0.116581 Loss 3.498232, Accuracy 93.198%\n",
      "Epoch 36, Batch 127, LR 0.116524 Loss 3.494937, Accuracy 93.215%\n",
      "Epoch 36, Batch 128, LR 0.116468 Loss 3.495595, Accuracy 93.237%\n",
      "Epoch 36, Batch 129, LR 0.116411 Loss 3.500462, Accuracy 93.205%\n",
      "Epoch 36, Batch 130, LR 0.116355 Loss 3.503172, Accuracy 93.203%\n",
      "Epoch 36, Batch 131, LR 0.116298 Loss 3.499300, Accuracy 93.225%\n",
      "Epoch 36, Batch 132, LR 0.116242 Loss 3.501894, Accuracy 93.211%\n",
      "Epoch 36, Batch 133, LR 0.116185 Loss 3.503173, Accuracy 93.186%\n",
      "Epoch 36, Batch 134, LR 0.116129 Loss 3.503516, Accuracy 93.184%\n",
      "Epoch 36, Batch 135, LR 0.116073 Loss 3.503782, Accuracy 93.189%\n",
      "Epoch 36, Batch 136, LR 0.116016 Loss 3.506846, Accuracy 93.181%\n",
      "Epoch 36, Batch 137, LR 0.115960 Loss 3.509681, Accuracy 93.191%\n",
      "Epoch 36, Batch 138, LR 0.115904 Loss 3.510717, Accuracy 93.212%\n",
      "Epoch 36, Batch 139, LR 0.115847 Loss 3.509297, Accuracy 93.227%\n",
      "Epoch 36, Batch 140, LR 0.115791 Loss 3.513760, Accuracy 93.214%\n",
      "Epoch 36, Batch 141, LR 0.115735 Loss 3.511630, Accuracy 93.240%\n",
      "Epoch 36, Batch 142, LR 0.115678 Loss 3.516516, Accuracy 93.200%\n",
      "Epoch 36, Batch 143, LR 0.115622 Loss 3.516493, Accuracy 93.198%\n",
      "Epoch 36, Batch 144, LR 0.115566 Loss 3.518700, Accuracy 93.186%\n",
      "Epoch 36, Batch 145, LR 0.115510 Loss 3.523581, Accuracy 93.157%\n",
      "Epoch 36, Batch 146, LR 0.115453 Loss 3.520220, Accuracy 93.172%\n",
      "Epoch 36, Batch 147, LR 0.115397 Loss 3.523288, Accuracy 93.165%\n",
      "Epoch 36, Batch 148, LR 0.115341 Loss 3.521697, Accuracy 93.164%\n",
      "Epoch 36, Batch 149, LR 0.115285 Loss 3.524244, Accuracy 93.158%\n",
      "Epoch 36, Batch 150, LR 0.115229 Loss 3.523781, Accuracy 93.156%\n",
      "Epoch 36, Batch 151, LR 0.115172 Loss 3.519496, Accuracy 93.171%\n",
      "Epoch 36, Batch 152, LR 0.115116 Loss 3.525820, Accuracy 93.164%\n",
      "Epoch 36, Batch 153, LR 0.115060 Loss 3.524303, Accuracy 93.183%\n",
      "Epoch 36, Batch 154, LR 0.115004 Loss 3.522921, Accuracy 93.182%\n",
      "Epoch 36, Batch 155, LR 0.114948 Loss 3.523771, Accuracy 93.170%\n",
      "Epoch 36, Batch 156, LR 0.114892 Loss 3.522181, Accuracy 93.184%\n",
      "Epoch 36, Batch 157, LR 0.114836 Loss 3.522675, Accuracy 93.163%\n",
      "Epoch 36, Batch 158, LR 0.114780 Loss 3.522447, Accuracy 93.157%\n",
      "Epoch 36, Batch 159, LR 0.114724 Loss 3.522554, Accuracy 93.160%\n",
      "Epoch 36, Batch 160, LR 0.114668 Loss 3.521834, Accuracy 93.164%\n",
      "Epoch 36, Batch 161, LR 0.114612 Loss 3.522502, Accuracy 93.158%\n",
      "Epoch 36, Batch 162, LR 0.114556 Loss 3.521159, Accuracy 93.181%\n",
      "Epoch 36, Batch 163, LR 0.114500 Loss 3.520242, Accuracy 93.180%\n",
      "Epoch 36, Batch 164, LR 0.114444 Loss 3.517557, Accuracy 93.178%\n",
      "Epoch 36, Batch 165, LR 0.114388 Loss 3.517770, Accuracy 93.191%\n",
      "Epoch 36, Batch 166, LR 0.114332 Loss 3.514342, Accuracy 93.199%\n",
      "Epoch 36, Batch 167, LR 0.114276 Loss 3.515588, Accuracy 93.203%\n",
      "Epoch 36, Batch 168, LR 0.114220 Loss 3.516980, Accuracy 93.192%\n",
      "Epoch 36, Batch 169, LR 0.114164 Loss 3.515999, Accuracy 93.191%\n",
      "Epoch 36, Batch 170, LR 0.114108 Loss 3.514569, Accuracy 93.212%\n",
      "Epoch 36, Batch 171, LR 0.114052 Loss 3.512660, Accuracy 93.229%\n",
      "Epoch 36, Batch 172, LR 0.113996 Loss 3.515078, Accuracy 93.223%\n",
      "Epoch 36, Batch 173, LR 0.113940 Loss 3.514249, Accuracy 93.226%\n",
      "Epoch 36, Batch 174, LR 0.113884 Loss 3.514350, Accuracy 93.220%\n",
      "Epoch 36, Batch 175, LR 0.113828 Loss 3.514147, Accuracy 93.219%\n",
      "Epoch 36, Batch 176, LR 0.113773 Loss 3.515195, Accuracy 93.204%\n",
      "Epoch 36, Batch 177, LR 0.113717 Loss 3.512681, Accuracy 93.203%\n",
      "Epoch 36, Batch 178, LR 0.113661 Loss 3.511623, Accuracy 93.206%\n",
      "Epoch 36, Batch 179, LR 0.113605 Loss 3.510116, Accuracy 93.204%\n",
      "Epoch 36, Batch 180, LR 0.113549 Loss 3.511438, Accuracy 93.194%\n",
      "Epoch 36, Batch 181, LR 0.113494 Loss 3.513313, Accuracy 93.198%\n",
      "Epoch 36, Batch 182, LR 0.113438 Loss 3.511348, Accuracy 93.209%\n",
      "Epoch 36, Batch 183, LR 0.113382 Loss 3.511950, Accuracy 93.216%\n",
      "Epoch 36, Batch 184, LR 0.113326 Loss 3.515090, Accuracy 93.211%\n",
      "Epoch 36, Batch 185, LR 0.113271 Loss 3.513558, Accuracy 93.214%\n",
      "Epoch 36, Batch 186, LR 0.113215 Loss 3.516164, Accuracy 93.212%\n",
      "Epoch 36, Batch 187, LR 0.113159 Loss 3.516652, Accuracy 93.207%\n",
      "Epoch 36, Batch 188, LR 0.113104 Loss 3.520208, Accuracy 93.189%\n",
      "Epoch 36, Batch 189, LR 0.113048 Loss 3.525734, Accuracy 93.188%\n",
      "Epoch 36, Batch 190, LR 0.112992 Loss 3.526855, Accuracy 93.195%\n",
      "Epoch 36, Batch 191, LR 0.112937 Loss 3.526272, Accuracy 93.194%\n",
      "Epoch 36, Batch 192, LR 0.112881 Loss 3.525939, Accuracy 93.193%\n",
      "Epoch 36, Batch 193, LR 0.112825 Loss 3.525980, Accuracy 93.183%\n",
      "Epoch 36, Batch 194, LR 0.112770 Loss 3.524340, Accuracy 93.190%\n",
      "Epoch 36, Batch 195, LR 0.112714 Loss 3.524058, Accuracy 93.185%\n",
      "Epoch 36, Batch 196, LR 0.112659 Loss 3.526611, Accuracy 93.148%\n",
      "Epoch 36, Batch 197, LR 0.112603 Loss 3.525839, Accuracy 93.147%\n",
      "Epoch 36, Batch 198, LR 0.112547 Loss 3.526966, Accuracy 93.134%\n",
      "Epoch 36, Batch 199, LR 0.112492 Loss 3.529828, Accuracy 93.134%\n",
      "Epoch 36, Batch 200, LR 0.112436 Loss 3.529328, Accuracy 93.145%\n",
      "Epoch 36, Batch 201, LR 0.112381 Loss 3.530126, Accuracy 93.155%\n",
      "Epoch 36, Batch 202, LR 0.112325 Loss 3.530033, Accuracy 93.154%\n",
      "Epoch 36, Batch 203, LR 0.112270 Loss 3.528369, Accuracy 93.169%\n",
      "Epoch 36, Batch 204, LR 0.112214 Loss 3.529652, Accuracy 93.168%\n",
      "Epoch 36, Batch 205, LR 0.112159 Loss 3.530227, Accuracy 93.167%\n",
      "Epoch 36, Batch 206, LR 0.112103 Loss 3.532654, Accuracy 93.166%\n",
      "Epoch 36, Batch 207, LR 0.112048 Loss 3.533511, Accuracy 93.161%\n",
      "Epoch 36, Batch 208, LR 0.111993 Loss 3.536624, Accuracy 93.138%\n",
      "Epoch 36, Batch 209, LR 0.111937 Loss 3.536026, Accuracy 93.141%\n",
      "Epoch 36, Batch 210, LR 0.111882 Loss 3.535149, Accuracy 93.144%\n",
      "Epoch 36, Batch 211, LR 0.111826 Loss 3.531724, Accuracy 93.158%\n",
      "Epoch 36, Batch 212, LR 0.111771 Loss 3.531850, Accuracy 93.164%\n",
      "Epoch 36, Batch 213, LR 0.111716 Loss 3.534807, Accuracy 93.152%\n",
      "Epoch 36, Batch 214, LR 0.111660 Loss 3.535755, Accuracy 93.166%\n",
      "Epoch 36, Batch 215, LR 0.111605 Loss 3.534062, Accuracy 93.169%\n",
      "Epoch 36, Batch 216, LR 0.111550 Loss 3.535093, Accuracy 93.171%\n",
      "Epoch 36, Batch 217, LR 0.111494 Loss 3.537298, Accuracy 93.174%\n",
      "Epoch 36, Batch 218, LR 0.111439 Loss 3.534356, Accuracy 93.195%\n",
      "Epoch 36, Batch 219, LR 0.111384 Loss 3.536504, Accuracy 93.193%\n",
      "Epoch 36, Batch 220, LR 0.111329 Loss 3.536249, Accuracy 93.196%\n",
      "Epoch 36, Batch 221, LR 0.111273 Loss 3.535284, Accuracy 93.195%\n",
      "Epoch 36, Batch 222, LR 0.111218 Loss 3.537898, Accuracy 93.183%\n",
      "Epoch 36, Batch 223, LR 0.111163 Loss 3.537735, Accuracy 93.182%\n",
      "Epoch 36, Batch 224, LR 0.111108 Loss 3.537825, Accuracy 93.195%\n",
      "Epoch 36, Batch 225, LR 0.111052 Loss 3.538334, Accuracy 93.194%\n",
      "Epoch 36, Batch 226, LR 0.110997 Loss 3.539327, Accuracy 93.197%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 227, LR 0.110942 Loss 3.540351, Accuracy 93.199%\n",
      "Epoch 36, Batch 228, LR 0.110887 Loss 3.541387, Accuracy 93.195%\n",
      "Epoch 36, Batch 229, LR 0.110832 Loss 3.540866, Accuracy 93.201%\n",
      "Epoch 36, Batch 230, LR 0.110777 Loss 3.541954, Accuracy 93.190%\n",
      "Epoch 36, Batch 231, LR 0.110721 Loss 3.541667, Accuracy 93.192%\n",
      "Epoch 36, Batch 232, LR 0.110666 Loss 3.540490, Accuracy 93.198%\n",
      "Epoch 36, Batch 233, LR 0.110611 Loss 3.540789, Accuracy 93.190%\n",
      "Epoch 36, Batch 234, LR 0.110556 Loss 3.540580, Accuracy 93.189%\n",
      "Epoch 36, Batch 235, LR 0.110501 Loss 3.539911, Accuracy 93.198%\n",
      "Epoch 36, Batch 236, LR 0.110446 Loss 3.540767, Accuracy 93.207%\n",
      "Epoch 36, Batch 237, LR 0.110391 Loss 3.539612, Accuracy 93.199%\n",
      "Epoch 36, Batch 238, LR 0.110336 Loss 3.538168, Accuracy 93.205%\n",
      "Epoch 36, Batch 239, LR 0.110281 Loss 3.539821, Accuracy 93.194%\n",
      "Epoch 36, Batch 240, LR 0.110226 Loss 3.539596, Accuracy 93.206%\n",
      "Epoch 36, Batch 241, LR 0.110171 Loss 3.538786, Accuracy 93.212%\n",
      "Epoch 36, Batch 242, LR 0.110116 Loss 3.538893, Accuracy 93.211%\n",
      "Epoch 36, Batch 243, LR 0.110061 Loss 3.537498, Accuracy 93.226%\n",
      "Epoch 36, Batch 244, LR 0.110006 Loss 3.540611, Accuracy 93.218%\n",
      "Epoch 36, Batch 245, LR 0.109951 Loss 3.542031, Accuracy 93.208%\n",
      "Epoch 36, Batch 246, LR 0.109896 Loss 3.539389, Accuracy 93.223%\n",
      "Epoch 36, Batch 247, LR 0.109841 Loss 3.539591, Accuracy 93.219%\n",
      "Epoch 36, Batch 248, LR 0.109786 Loss 3.542861, Accuracy 93.218%\n",
      "Epoch 36, Batch 249, LR 0.109732 Loss 3.541678, Accuracy 93.229%\n",
      "Epoch 36, Batch 250, LR 0.109677 Loss 3.542700, Accuracy 93.228%\n",
      "Epoch 36, Batch 251, LR 0.109622 Loss 3.543504, Accuracy 93.227%\n",
      "Epoch 36, Batch 252, LR 0.109567 Loss 3.544668, Accuracy 93.229%\n",
      "Epoch 36, Batch 253, LR 0.109512 Loss 3.544720, Accuracy 93.231%\n",
      "Epoch 36, Batch 254, LR 0.109457 Loss 3.545009, Accuracy 93.230%\n",
      "Epoch 36, Batch 255, LR 0.109402 Loss 3.546445, Accuracy 93.223%\n",
      "Epoch 36, Batch 256, LR 0.109348 Loss 3.546425, Accuracy 93.228%\n",
      "Epoch 36, Batch 257, LR 0.109293 Loss 3.547565, Accuracy 93.218%\n",
      "Epoch 36, Batch 258, LR 0.109238 Loss 3.548667, Accuracy 93.214%\n",
      "Epoch 36, Batch 259, LR 0.109183 Loss 3.548594, Accuracy 93.222%\n",
      "Epoch 36, Batch 260, LR 0.109129 Loss 3.547329, Accuracy 93.236%\n",
      "Epoch 36, Batch 261, LR 0.109074 Loss 3.549428, Accuracy 93.223%\n",
      "Epoch 36, Batch 262, LR 0.109019 Loss 3.549747, Accuracy 93.216%\n",
      "Epoch 36, Batch 263, LR 0.108964 Loss 3.549867, Accuracy 93.209%\n",
      "Epoch 36, Batch 264, LR 0.108910 Loss 3.548140, Accuracy 93.208%\n",
      "Epoch 36, Batch 265, LR 0.108855 Loss 3.550897, Accuracy 93.190%\n",
      "Epoch 36, Batch 266, LR 0.108800 Loss 3.549928, Accuracy 93.192%\n",
      "Epoch 36, Batch 267, LR 0.108746 Loss 3.550476, Accuracy 93.182%\n",
      "Epoch 36, Batch 268, LR 0.108691 Loss 3.548265, Accuracy 93.187%\n",
      "Epoch 36, Batch 269, LR 0.108637 Loss 3.546877, Accuracy 93.187%\n",
      "Epoch 36, Batch 270, LR 0.108582 Loss 3.548783, Accuracy 93.183%\n",
      "Epoch 36, Batch 271, LR 0.108527 Loss 3.548632, Accuracy 93.179%\n",
      "Epoch 36, Batch 272, LR 0.108473 Loss 3.548961, Accuracy 93.181%\n",
      "Epoch 36, Batch 273, LR 0.108418 Loss 3.544542, Accuracy 93.203%\n",
      "Epoch 36, Batch 274, LR 0.108364 Loss 3.542280, Accuracy 93.217%\n",
      "Epoch 36, Batch 275, LR 0.108309 Loss 3.541511, Accuracy 93.219%\n",
      "Epoch 36, Batch 276, LR 0.108255 Loss 3.540660, Accuracy 93.221%\n",
      "Epoch 36, Batch 277, LR 0.108200 Loss 3.541038, Accuracy 93.220%\n",
      "Epoch 36, Batch 278, LR 0.108146 Loss 3.542119, Accuracy 93.227%\n",
      "Epoch 36, Batch 279, LR 0.108091 Loss 3.540799, Accuracy 93.235%\n",
      "Epoch 36, Batch 280, LR 0.108037 Loss 3.539600, Accuracy 93.239%\n",
      "Epoch 36, Batch 281, LR 0.107982 Loss 3.538140, Accuracy 93.250%\n",
      "Epoch 36, Batch 282, LR 0.107928 Loss 3.538092, Accuracy 93.254%\n",
      "Epoch 36, Batch 283, LR 0.107873 Loss 3.537775, Accuracy 93.242%\n",
      "Epoch 36, Batch 284, LR 0.107819 Loss 3.538455, Accuracy 93.233%\n",
      "Epoch 36, Batch 285, LR 0.107764 Loss 3.540726, Accuracy 93.229%\n",
      "Epoch 36, Batch 286, LR 0.107710 Loss 3.538334, Accuracy 93.242%\n",
      "Epoch 36, Batch 287, LR 0.107656 Loss 3.539198, Accuracy 93.238%\n",
      "Epoch 36, Batch 288, LR 0.107601 Loss 3.541587, Accuracy 93.229%\n",
      "Epoch 36, Batch 289, LR 0.107547 Loss 3.543444, Accuracy 93.226%\n",
      "Epoch 36, Batch 290, LR 0.107492 Loss 3.541538, Accuracy 93.238%\n",
      "Epoch 36, Batch 291, LR 0.107438 Loss 3.542878, Accuracy 93.229%\n",
      "Epoch 36, Batch 292, LR 0.107384 Loss 3.541277, Accuracy 93.231%\n",
      "Epoch 36, Batch 293, LR 0.107330 Loss 3.542056, Accuracy 93.225%\n",
      "Epoch 36, Batch 294, LR 0.107275 Loss 3.541231, Accuracy 93.227%\n",
      "Epoch 36, Batch 295, LR 0.107221 Loss 3.541560, Accuracy 93.226%\n",
      "Epoch 36, Batch 296, LR 0.107167 Loss 3.542117, Accuracy 93.225%\n",
      "Epoch 36, Batch 297, LR 0.107112 Loss 3.542175, Accuracy 93.224%\n",
      "Epoch 36, Batch 298, LR 0.107058 Loss 3.543639, Accuracy 93.220%\n",
      "Epoch 36, Batch 299, LR 0.107004 Loss 3.542744, Accuracy 93.220%\n",
      "Epoch 36, Batch 300, LR 0.106950 Loss 3.543125, Accuracy 93.224%\n",
      "Epoch 36, Batch 301, LR 0.106896 Loss 3.541488, Accuracy 93.228%\n",
      "Epoch 36, Batch 302, LR 0.106841 Loss 3.542813, Accuracy 93.217%\n",
      "Epoch 36, Batch 303, LR 0.106787 Loss 3.542399, Accuracy 93.227%\n",
      "Epoch 36, Batch 304, LR 0.106733 Loss 3.542100, Accuracy 93.228%\n",
      "Epoch 36, Batch 305, LR 0.106679 Loss 3.540146, Accuracy 93.227%\n",
      "Epoch 36, Batch 306, LR 0.106625 Loss 3.541683, Accuracy 93.222%\n",
      "Epoch 36, Batch 307, LR 0.106571 Loss 3.541109, Accuracy 93.223%\n",
      "Epoch 36, Batch 308, LR 0.106516 Loss 3.541500, Accuracy 93.220%\n",
      "Epoch 36, Batch 309, LR 0.106462 Loss 3.540943, Accuracy 93.224%\n",
      "Epoch 36, Batch 310, LR 0.106408 Loss 3.539862, Accuracy 93.231%\n",
      "Epoch 36, Batch 311, LR 0.106354 Loss 3.539884, Accuracy 93.222%\n",
      "Epoch 36, Batch 312, LR 0.106300 Loss 3.539887, Accuracy 93.224%\n",
      "Epoch 36, Batch 313, LR 0.106246 Loss 3.540771, Accuracy 93.228%\n",
      "Epoch 36, Batch 314, LR 0.106192 Loss 3.540060, Accuracy 93.237%\n",
      "Epoch 36, Batch 315, LR 0.106138 Loss 3.540555, Accuracy 93.227%\n",
      "Epoch 36, Batch 316, LR 0.106084 Loss 3.540916, Accuracy 93.228%\n",
      "Epoch 36, Batch 317, LR 0.106030 Loss 3.541360, Accuracy 93.228%\n",
      "Epoch 36, Batch 318, LR 0.105976 Loss 3.541009, Accuracy 93.234%\n",
      "Epoch 36, Batch 319, LR 0.105922 Loss 3.539044, Accuracy 93.241%\n",
      "Epoch 36, Batch 320, LR 0.105868 Loss 3.539147, Accuracy 93.245%\n",
      "Epoch 36, Batch 321, LR 0.105814 Loss 3.539543, Accuracy 93.244%\n",
      "Epoch 36, Batch 322, LR 0.105760 Loss 3.540293, Accuracy 93.228%\n",
      "Epoch 36, Batch 323, LR 0.105706 Loss 3.540286, Accuracy 93.235%\n",
      "Epoch 36, Batch 324, LR 0.105652 Loss 3.541104, Accuracy 93.220%\n",
      "Epoch 36, Batch 325, LR 0.105599 Loss 3.541991, Accuracy 93.216%\n",
      "Epoch 36, Batch 326, LR 0.105545 Loss 3.543735, Accuracy 93.218%\n",
      "Epoch 36, Batch 327, LR 0.105491 Loss 3.543437, Accuracy 93.220%\n",
      "Epoch 36, Batch 328, LR 0.105437 Loss 3.544903, Accuracy 93.205%\n",
      "Epoch 36, Batch 329, LR 0.105383 Loss 3.545228, Accuracy 93.206%\n",
      "Epoch 36, Batch 330, LR 0.105329 Loss 3.543850, Accuracy 93.215%\n",
      "Epoch 36, Batch 331, LR 0.105275 Loss 3.544305, Accuracy 93.209%\n",
      "Epoch 36, Batch 332, LR 0.105222 Loss 3.544680, Accuracy 93.206%\n",
      "Epoch 36, Batch 333, LR 0.105168 Loss 3.544246, Accuracy 93.206%\n",
      "Epoch 36, Batch 334, LR 0.105114 Loss 3.545582, Accuracy 93.196%\n",
      "Epoch 36, Batch 335, LR 0.105060 Loss 3.544677, Accuracy 93.190%\n",
      "Epoch 36, Batch 336, LR 0.105007 Loss 3.544820, Accuracy 93.192%\n",
      "Epoch 36, Batch 337, LR 0.104953 Loss 3.546338, Accuracy 93.184%\n",
      "Epoch 36, Batch 338, LR 0.104899 Loss 3.543345, Accuracy 93.195%\n",
      "Epoch 36, Batch 339, LR 0.104845 Loss 3.543356, Accuracy 93.197%\n",
      "Epoch 36, Batch 340, LR 0.104792 Loss 3.542741, Accuracy 93.201%\n",
      "Epoch 36, Batch 341, LR 0.104738 Loss 3.542526, Accuracy 93.205%\n",
      "Epoch 36, Batch 342, LR 0.104684 Loss 3.542872, Accuracy 93.209%\n",
      "Epoch 36, Batch 343, LR 0.104631 Loss 3.543947, Accuracy 93.203%\n",
      "Epoch 36, Batch 344, LR 0.104577 Loss 3.544298, Accuracy 93.203%\n",
      "Epoch 36, Batch 345, LR 0.104524 Loss 3.543008, Accuracy 93.209%\n",
      "Epoch 36, Batch 346, LR 0.104470 Loss 3.543164, Accuracy 93.208%\n",
      "Epoch 36, Batch 347, LR 0.104416 Loss 3.542303, Accuracy 93.219%\n",
      "Epoch 36, Batch 348, LR 0.104363 Loss 3.542249, Accuracy 93.220%\n",
      "Epoch 36, Batch 349, LR 0.104309 Loss 3.540067, Accuracy 93.226%\n",
      "Epoch 36, Batch 350, LR 0.104256 Loss 3.539178, Accuracy 93.228%\n",
      "Epoch 36, Batch 351, LR 0.104202 Loss 3.539390, Accuracy 93.229%\n",
      "Epoch 36, Batch 352, LR 0.104149 Loss 3.539562, Accuracy 93.224%\n",
      "Epoch 36, Batch 353, LR 0.104095 Loss 3.542136, Accuracy 93.214%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 354, LR 0.104041 Loss 3.540672, Accuracy 93.214%\n",
      "Epoch 36, Batch 355, LR 0.103988 Loss 3.542577, Accuracy 93.209%\n",
      "Epoch 36, Batch 356, LR 0.103935 Loss 3.541414, Accuracy 93.217%\n",
      "Epoch 36, Batch 357, LR 0.103881 Loss 3.540601, Accuracy 93.223%\n",
      "Epoch 36, Batch 358, LR 0.103828 Loss 3.539999, Accuracy 93.228%\n",
      "Epoch 36, Batch 359, LR 0.103774 Loss 3.540039, Accuracy 93.232%\n",
      "Epoch 36, Batch 360, LR 0.103721 Loss 3.539055, Accuracy 93.229%\n",
      "Epoch 36, Batch 361, LR 0.103667 Loss 3.539344, Accuracy 93.226%\n",
      "Epoch 36, Batch 362, LR 0.103614 Loss 3.539422, Accuracy 93.223%\n",
      "Epoch 36, Batch 363, LR 0.103561 Loss 3.538763, Accuracy 93.223%\n",
      "Epoch 36, Batch 364, LR 0.103507 Loss 3.538794, Accuracy 93.220%\n",
      "Epoch 36, Batch 365, LR 0.103454 Loss 3.539425, Accuracy 93.223%\n",
      "Epoch 36, Batch 366, LR 0.103400 Loss 3.540159, Accuracy 93.221%\n",
      "Epoch 36, Batch 367, LR 0.103347 Loss 3.538915, Accuracy 93.226%\n",
      "Epoch 36, Batch 368, LR 0.103294 Loss 3.538505, Accuracy 93.224%\n",
      "Epoch 36, Batch 369, LR 0.103240 Loss 3.538846, Accuracy 93.219%\n",
      "Epoch 36, Batch 370, LR 0.103187 Loss 3.538968, Accuracy 93.214%\n",
      "Epoch 36, Batch 371, LR 0.103134 Loss 3.539778, Accuracy 93.209%\n",
      "Epoch 36, Batch 372, LR 0.103081 Loss 3.540241, Accuracy 93.208%\n",
      "Epoch 36, Batch 373, LR 0.103027 Loss 3.540995, Accuracy 93.199%\n",
      "Epoch 36, Batch 374, LR 0.102974 Loss 3.542187, Accuracy 93.192%\n",
      "Epoch 36, Batch 375, LR 0.102921 Loss 3.540901, Accuracy 93.198%\n",
      "Epoch 36, Batch 376, LR 0.102868 Loss 3.541148, Accuracy 93.199%\n",
      "Epoch 36, Batch 377, LR 0.102814 Loss 3.541421, Accuracy 93.201%\n",
      "Epoch 36, Batch 378, LR 0.102761 Loss 3.542742, Accuracy 93.194%\n",
      "Epoch 36, Batch 379, LR 0.102708 Loss 3.543187, Accuracy 93.195%\n",
      "Epoch 36, Batch 380, LR 0.102655 Loss 3.543346, Accuracy 93.199%\n",
      "Epoch 36, Batch 381, LR 0.102602 Loss 3.545748, Accuracy 93.192%\n",
      "Epoch 36, Batch 382, LR 0.102549 Loss 3.547323, Accuracy 93.192%\n",
      "Epoch 36, Batch 383, LR 0.102495 Loss 3.548555, Accuracy 93.181%\n",
      "Epoch 36, Batch 384, LR 0.102442 Loss 3.549645, Accuracy 93.176%\n",
      "Epoch 36, Batch 385, LR 0.102389 Loss 3.548116, Accuracy 93.184%\n",
      "Epoch 36, Batch 386, LR 0.102336 Loss 3.546754, Accuracy 93.191%\n",
      "Epoch 36, Batch 387, LR 0.102283 Loss 3.548565, Accuracy 93.187%\n",
      "Epoch 36, Batch 388, LR 0.102230 Loss 3.548100, Accuracy 93.190%\n",
      "Epoch 36, Batch 389, LR 0.102177 Loss 3.547663, Accuracy 93.188%\n",
      "Epoch 36, Batch 390, LR 0.102124 Loss 3.550043, Accuracy 93.177%\n",
      "Epoch 36, Batch 391, LR 0.102071 Loss 3.550764, Accuracy 93.179%\n",
      "Epoch 36, Batch 392, LR 0.102018 Loss 3.550779, Accuracy 93.172%\n",
      "Epoch 36, Batch 393, LR 0.101965 Loss 3.551752, Accuracy 93.170%\n",
      "Epoch 36, Batch 394, LR 0.101912 Loss 3.552308, Accuracy 93.167%\n",
      "Epoch 36, Batch 395, LR 0.101859 Loss 3.552177, Accuracy 93.167%\n",
      "Epoch 36, Batch 396, LR 0.101806 Loss 3.551278, Accuracy 93.172%\n",
      "Epoch 36, Batch 397, LR 0.101753 Loss 3.550812, Accuracy 93.173%\n",
      "Epoch 36, Batch 398, LR 0.101700 Loss 3.551220, Accuracy 93.167%\n",
      "Epoch 36, Batch 399, LR 0.101647 Loss 3.549557, Accuracy 93.176%\n",
      "Epoch 36, Batch 400, LR 0.101594 Loss 3.549413, Accuracy 93.178%\n",
      "Epoch 36, Batch 401, LR 0.101541 Loss 3.549559, Accuracy 93.181%\n",
      "Epoch 36, Batch 402, LR 0.101489 Loss 3.550728, Accuracy 93.173%\n",
      "Epoch 36, Batch 403, LR 0.101436 Loss 3.551771, Accuracy 93.166%\n",
      "Epoch 36, Batch 404, LR 0.101383 Loss 3.552976, Accuracy 93.154%\n",
      "Epoch 36, Batch 405, LR 0.101330 Loss 3.554306, Accuracy 93.146%\n",
      "Epoch 36, Batch 406, LR 0.101277 Loss 3.555154, Accuracy 93.142%\n",
      "Epoch 36, Batch 407, LR 0.101224 Loss 3.555621, Accuracy 93.143%\n",
      "Epoch 36, Batch 408, LR 0.101172 Loss 3.555161, Accuracy 93.149%\n",
      "Epoch 36, Batch 409, LR 0.101119 Loss 3.554395, Accuracy 93.152%\n",
      "Epoch 36, Batch 410, LR 0.101066 Loss 3.554515, Accuracy 93.154%\n",
      "Epoch 36, Batch 411, LR 0.101013 Loss 3.554681, Accuracy 93.159%\n",
      "Epoch 36, Batch 412, LR 0.100961 Loss 3.555431, Accuracy 93.151%\n",
      "Epoch 36, Batch 413, LR 0.100908 Loss 3.553825, Accuracy 93.162%\n",
      "Epoch 36, Batch 414, LR 0.100855 Loss 3.553503, Accuracy 93.161%\n",
      "Epoch 36, Batch 415, LR 0.100802 Loss 3.553144, Accuracy 93.166%\n",
      "Epoch 36, Batch 416, LR 0.100750 Loss 3.552072, Accuracy 93.173%\n",
      "Epoch 36, Batch 417, LR 0.100697 Loss 3.552417, Accuracy 93.167%\n",
      "Epoch 36, Batch 418, LR 0.100644 Loss 3.552204, Accuracy 93.172%\n",
      "Epoch 36, Batch 419, LR 0.100592 Loss 3.551671, Accuracy 93.176%\n",
      "Epoch 36, Batch 420, LR 0.100539 Loss 3.550659, Accuracy 93.181%\n",
      "Epoch 36, Batch 421, LR 0.100486 Loss 3.550045, Accuracy 93.180%\n",
      "Epoch 36, Batch 422, LR 0.100434 Loss 3.549543, Accuracy 93.184%\n",
      "Epoch 36, Batch 423, LR 0.100381 Loss 3.549211, Accuracy 93.192%\n",
      "Epoch 36, Batch 424, LR 0.100329 Loss 3.549381, Accuracy 93.195%\n",
      "Epoch 36, Batch 425, LR 0.100276 Loss 3.549333, Accuracy 93.199%\n",
      "Epoch 36, Batch 426, LR 0.100224 Loss 3.547791, Accuracy 93.205%\n",
      "Epoch 36, Batch 427, LR 0.100171 Loss 3.548218, Accuracy 93.207%\n",
      "Epoch 36, Batch 428, LR 0.100118 Loss 3.548222, Accuracy 93.204%\n",
      "Epoch 36, Batch 429, LR 0.100066 Loss 3.547409, Accuracy 93.207%\n",
      "Epoch 36, Batch 430, LR 0.100013 Loss 3.548945, Accuracy 93.201%\n",
      "Epoch 36, Batch 431, LR 0.099961 Loss 3.548245, Accuracy 93.204%\n",
      "Epoch 36, Batch 432, LR 0.099908 Loss 3.548360, Accuracy 93.198%\n",
      "Epoch 36, Batch 433, LR 0.099856 Loss 3.548793, Accuracy 93.192%\n",
      "Epoch 36, Batch 434, LR 0.099804 Loss 3.548555, Accuracy 93.192%\n",
      "Epoch 36, Batch 435, LR 0.099751 Loss 3.547634, Accuracy 93.195%\n",
      "Epoch 36, Batch 436, LR 0.099699 Loss 3.547511, Accuracy 93.198%\n",
      "Epoch 36, Batch 437, LR 0.099646 Loss 3.547038, Accuracy 93.207%\n",
      "Epoch 36, Batch 438, LR 0.099594 Loss 3.547238, Accuracy 93.206%\n",
      "Epoch 36, Batch 439, LR 0.099542 Loss 3.546907, Accuracy 93.207%\n",
      "Epoch 36, Batch 440, LR 0.099489 Loss 3.546807, Accuracy 93.212%\n",
      "Epoch 36, Batch 441, LR 0.099437 Loss 3.546162, Accuracy 93.215%\n",
      "Epoch 36, Batch 442, LR 0.099384 Loss 3.545049, Accuracy 93.220%\n",
      "Epoch 36, Batch 443, LR 0.099332 Loss 3.544290, Accuracy 93.224%\n",
      "Epoch 36, Batch 444, LR 0.099280 Loss 3.544301, Accuracy 93.226%\n",
      "Epoch 36, Batch 445, LR 0.099227 Loss 3.544706, Accuracy 93.230%\n",
      "Epoch 36, Batch 446, LR 0.099175 Loss 3.545471, Accuracy 93.228%\n",
      "Epoch 36, Batch 447, LR 0.099123 Loss 3.543972, Accuracy 93.229%\n",
      "Epoch 36, Batch 448, LR 0.099071 Loss 3.543698, Accuracy 93.227%\n",
      "Epoch 36, Batch 449, LR 0.099018 Loss 3.543122, Accuracy 93.225%\n",
      "Epoch 36, Batch 450, LR 0.098966 Loss 3.542615, Accuracy 93.224%\n",
      "Epoch 36, Batch 451, LR 0.098914 Loss 3.542719, Accuracy 93.220%\n",
      "Epoch 36, Batch 452, LR 0.098862 Loss 3.542654, Accuracy 93.223%\n",
      "Epoch 36, Batch 453, LR 0.098809 Loss 3.542953, Accuracy 93.222%\n",
      "Epoch 36, Batch 454, LR 0.098757 Loss 3.540998, Accuracy 93.230%\n",
      "Epoch 36, Batch 455, LR 0.098705 Loss 3.540404, Accuracy 93.231%\n",
      "Epoch 36, Batch 456, LR 0.098653 Loss 3.542143, Accuracy 93.222%\n",
      "Epoch 36, Batch 457, LR 0.098601 Loss 3.543287, Accuracy 93.223%\n",
      "Epoch 36, Batch 458, LR 0.098549 Loss 3.544185, Accuracy 93.218%\n",
      "Epoch 36, Batch 459, LR 0.098497 Loss 3.544379, Accuracy 93.219%\n",
      "Epoch 36, Batch 460, LR 0.098444 Loss 3.543712, Accuracy 93.227%\n",
      "Epoch 36, Batch 461, LR 0.098392 Loss 3.544096, Accuracy 93.223%\n",
      "Epoch 36, Batch 462, LR 0.098340 Loss 3.544161, Accuracy 93.227%\n",
      "Epoch 36, Batch 463, LR 0.098288 Loss 3.544388, Accuracy 93.224%\n",
      "Epoch 36, Batch 464, LR 0.098236 Loss 3.544690, Accuracy 93.226%\n",
      "Epoch 36, Batch 465, LR 0.098184 Loss 3.543900, Accuracy 93.229%\n",
      "Epoch 36, Batch 466, LR 0.098132 Loss 3.543487, Accuracy 93.232%\n",
      "Epoch 36, Batch 467, LR 0.098080 Loss 3.542943, Accuracy 93.236%\n",
      "Epoch 36, Batch 468, LR 0.098028 Loss 3.542992, Accuracy 93.239%\n",
      "Epoch 36, Batch 469, LR 0.097976 Loss 3.543555, Accuracy 93.242%\n",
      "Epoch 36, Batch 470, LR 0.097924 Loss 3.543755, Accuracy 93.238%\n",
      "Epoch 36, Batch 471, LR 0.097872 Loss 3.544509, Accuracy 93.236%\n",
      "Epoch 36, Batch 472, LR 0.097820 Loss 3.545079, Accuracy 93.234%\n",
      "Epoch 36, Batch 473, LR 0.097768 Loss 3.545689, Accuracy 93.231%\n",
      "Epoch 36, Batch 474, LR 0.097716 Loss 3.545798, Accuracy 93.226%\n",
      "Epoch 36, Batch 475, LR 0.097664 Loss 3.545995, Accuracy 93.225%\n",
      "Epoch 36, Batch 476, LR 0.097612 Loss 3.546441, Accuracy 93.225%\n",
      "Epoch 36, Batch 477, LR 0.097561 Loss 3.547317, Accuracy 93.221%\n",
      "Epoch 36, Batch 478, LR 0.097509 Loss 3.546928, Accuracy 93.220%\n",
      "Epoch 36, Batch 479, LR 0.097457 Loss 3.547694, Accuracy 93.222%\n",
      "Epoch 36, Batch 480, LR 0.097405 Loss 3.547787, Accuracy 93.221%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 481, LR 0.097353 Loss 3.548075, Accuracy 93.211%\n",
      "Epoch 36, Batch 482, LR 0.097301 Loss 3.548424, Accuracy 93.207%\n",
      "Epoch 36, Batch 483, LR 0.097250 Loss 3.548948, Accuracy 93.207%\n",
      "Epoch 36, Batch 484, LR 0.097198 Loss 3.548521, Accuracy 93.206%\n",
      "Epoch 36, Batch 485, LR 0.097146 Loss 3.547951, Accuracy 93.207%\n",
      "Epoch 36, Batch 486, LR 0.097094 Loss 3.546970, Accuracy 93.208%\n",
      "Epoch 36, Batch 487, LR 0.097042 Loss 3.546847, Accuracy 93.209%\n",
      "Epoch 36, Batch 488, LR 0.096991 Loss 3.546239, Accuracy 93.215%\n",
      "Epoch 36, Batch 489, LR 0.096939 Loss 3.547594, Accuracy 93.205%\n",
      "Epoch 36, Batch 490, LR 0.096887 Loss 3.545674, Accuracy 93.213%\n",
      "Epoch 36, Batch 491, LR 0.096836 Loss 3.544429, Accuracy 93.219%\n",
      "Epoch 36, Batch 492, LR 0.096784 Loss 3.544466, Accuracy 93.220%\n",
      "Epoch 36, Batch 493, LR 0.096732 Loss 3.544019, Accuracy 93.222%\n",
      "Epoch 36, Batch 494, LR 0.096681 Loss 3.544714, Accuracy 93.217%\n",
      "Epoch 36, Batch 495, LR 0.096629 Loss 3.544006, Accuracy 93.218%\n",
      "Epoch 36, Batch 496, LR 0.096577 Loss 3.544300, Accuracy 93.216%\n",
      "Epoch 36, Batch 497, LR 0.096526 Loss 3.543850, Accuracy 93.219%\n",
      "Epoch 36, Batch 498, LR 0.096474 Loss 3.544363, Accuracy 93.217%\n",
      "Epoch 36, Batch 499, LR 0.096422 Loss 3.544567, Accuracy 93.211%\n",
      "Epoch 36, Batch 500, LR 0.096371 Loss 3.543454, Accuracy 93.211%\n",
      "Epoch 36, Batch 501, LR 0.096319 Loss 3.543035, Accuracy 93.212%\n",
      "Epoch 36, Batch 502, LR 0.096268 Loss 3.542973, Accuracy 93.205%\n",
      "Epoch 36, Batch 503, LR 0.096216 Loss 3.542357, Accuracy 93.206%\n",
      "Epoch 36, Batch 504, LR 0.096165 Loss 3.542586, Accuracy 93.200%\n",
      "Epoch 36, Batch 505, LR 0.096113 Loss 3.542058, Accuracy 93.202%\n",
      "Epoch 36, Batch 506, LR 0.096062 Loss 3.542164, Accuracy 93.202%\n",
      "Epoch 36, Batch 507, LR 0.096010 Loss 3.541642, Accuracy 93.208%\n",
      "Epoch 36, Batch 508, LR 0.095959 Loss 3.541368, Accuracy 93.206%\n",
      "Epoch 36, Batch 509, LR 0.095907 Loss 3.540991, Accuracy 93.208%\n",
      "Epoch 36, Batch 510, LR 0.095856 Loss 3.540576, Accuracy 93.212%\n",
      "Epoch 36, Batch 511, LR 0.095804 Loss 3.540792, Accuracy 93.213%\n",
      "Epoch 36, Batch 512, LR 0.095753 Loss 3.541554, Accuracy 93.210%\n",
      "Epoch 36, Batch 513, LR 0.095702 Loss 3.542017, Accuracy 93.208%\n",
      "Epoch 36, Batch 514, LR 0.095650 Loss 3.542952, Accuracy 93.210%\n",
      "Epoch 36, Batch 515, LR 0.095599 Loss 3.542428, Accuracy 93.213%\n",
      "Epoch 36, Batch 516, LR 0.095547 Loss 3.541489, Accuracy 93.216%\n",
      "Epoch 36, Batch 517, LR 0.095496 Loss 3.541708, Accuracy 93.217%\n",
      "Epoch 36, Batch 518, LR 0.095445 Loss 3.542267, Accuracy 93.216%\n",
      "Epoch 36, Batch 519, LR 0.095393 Loss 3.541643, Accuracy 93.223%\n",
      "Epoch 36, Batch 520, LR 0.095342 Loss 3.541021, Accuracy 93.227%\n",
      "Epoch 36, Batch 521, LR 0.095291 Loss 3.541192, Accuracy 93.227%\n",
      "Epoch 36, Batch 522, LR 0.095240 Loss 3.542834, Accuracy 93.223%\n",
      "Epoch 36, Batch 523, LR 0.095188 Loss 3.543264, Accuracy 93.218%\n",
      "Epoch 36, Batch 524, LR 0.095137 Loss 3.543527, Accuracy 93.221%\n",
      "Epoch 36, Batch 525, LR 0.095086 Loss 3.543364, Accuracy 93.222%\n",
      "Epoch 36, Batch 526, LR 0.095034 Loss 3.542682, Accuracy 93.227%\n",
      "Epoch 36, Batch 527, LR 0.094983 Loss 3.541563, Accuracy 93.236%\n",
      "Epoch 36, Batch 528, LR 0.094932 Loss 3.541480, Accuracy 93.240%\n",
      "Epoch 36, Batch 529, LR 0.094881 Loss 3.541465, Accuracy 93.238%\n",
      "Epoch 36, Batch 530, LR 0.094830 Loss 3.541404, Accuracy 93.236%\n",
      "Epoch 36, Batch 531, LR 0.094779 Loss 3.541149, Accuracy 93.237%\n",
      "Epoch 36, Batch 532, LR 0.094727 Loss 3.541019, Accuracy 93.233%\n",
      "Epoch 36, Batch 533, LR 0.094676 Loss 3.540640, Accuracy 93.231%\n",
      "Epoch 36, Batch 534, LR 0.094625 Loss 3.541697, Accuracy 93.231%\n",
      "Epoch 36, Batch 535, LR 0.094574 Loss 3.541883, Accuracy 93.224%\n",
      "Epoch 36, Batch 536, LR 0.094523 Loss 3.540923, Accuracy 93.230%\n",
      "Epoch 36, Batch 537, LR 0.094472 Loss 3.540449, Accuracy 93.231%\n",
      "Epoch 36, Batch 538, LR 0.094421 Loss 3.540073, Accuracy 93.232%\n",
      "Epoch 36, Batch 539, LR 0.094370 Loss 3.540569, Accuracy 93.230%\n",
      "Epoch 36, Batch 540, LR 0.094319 Loss 3.540512, Accuracy 93.232%\n",
      "Epoch 36, Batch 541, LR 0.094268 Loss 3.540158, Accuracy 93.233%\n",
      "Epoch 36, Batch 542, LR 0.094217 Loss 3.541209, Accuracy 93.227%\n",
      "Epoch 36, Batch 543, LR 0.094166 Loss 3.540867, Accuracy 93.229%\n",
      "Epoch 36, Batch 544, LR 0.094115 Loss 3.540641, Accuracy 93.227%\n",
      "Epoch 36, Batch 545, LR 0.094064 Loss 3.539626, Accuracy 93.233%\n",
      "Epoch 36, Batch 546, LR 0.094013 Loss 3.540110, Accuracy 93.233%\n",
      "Epoch 36, Batch 547, LR 0.093962 Loss 3.540626, Accuracy 93.234%\n",
      "Epoch 36, Batch 548, LR 0.093911 Loss 3.540952, Accuracy 93.228%\n",
      "Epoch 36, Batch 549, LR 0.093860 Loss 3.540326, Accuracy 93.233%\n",
      "Epoch 36, Batch 550, LR 0.093809 Loss 3.540501, Accuracy 93.236%\n",
      "Epoch 36, Batch 551, LR 0.093758 Loss 3.540159, Accuracy 93.238%\n",
      "Epoch 36, Batch 552, LR 0.093707 Loss 3.541032, Accuracy 93.233%\n",
      "Epoch 36, Batch 553, LR 0.093656 Loss 3.540437, Accuracy 93.234%\n",
      "Epoch 36, Batch 554, LR 0.093605 Loss 3.540160, Accuracy 93.234%\n",
      "Epoch 36, Batch 555, LR 0.093554 Loss 3.539929, Accuracy 93.233%\n",
      "Epoch 36, Batch 556, LR 0.093504 Loss 3.541079, Accuracy 93.233%\n",
      "Epoch 36, Batch 557, LR 0.093453 Loss 3.541821, Accuracy 93.228%\n",
      "Epoch 36, Batch 558, LR 0.093402 Loss 3.543228, Accuracy 93.226%\n",
      "Epoch 36, Batch 559, LR 0.093351 Loss 3.542786, Accuracy 93.230%\n",
      "Epoch 36, Batch 560, LR 0.093300 Loss 3.542097, Accuracy 93.231%\n",
      "Epoch 36, Batch 561, LR 0.093250 Loss 3.541548, Accuracy 93.229%\n",
      "Epoch 36, Batch 562, LR 0.093199 Loss 3.542236, Accuracy 93.223%\n",
      "Epoch 36, Batch 563, LR 0.093148 Loss 3.542101, Accuracy 93.220%\n",
      "Epoch 36, Batch 564, LR 0.093097 Loss 3.542183, Accuracy 93.221%\n",
      "Epoch 36, Batch 565, LR 0.093047 Loss 3.541958, Accuracy 93.222%\n",
      "Epoch 36, Batch 566, LR 0.092996 Loss 3.543558, Accuracy 93.219%\n",
      "Epoch 36, Batch 567, LR 0.092945 Loss 3.544993, Accuracy 93.206%\n",
      "Epoch 36, Batch 568, LR 0.092895 Loss 3.544898, Accuracy 93.208%\n",
      "Epoch 36, Batch 569, LR 0.092844 Loss 3.544749, Accuracy 93.206%\n",
      "Epoch 36, Batch 570, LR 0.092793 Loss 3.544220, Accuracy 93.209%\n",
      "Epoch 36, Batch 571, LR 0.092743 Loss 3.543983, Accuracy 93.208%\n",
      "Epoch 36, Batch 572, LR 0.092692 Loss 3.543481, Accuracy 93.211%\n",
      "Epoch 36, Batch 573, LR 0.092641 Loss 3.543690, Accuracy 93.213%\n",
      "Epoch 36, Batch 574, LR 0.092591 Loss 3.544688, Accuracy 93.210%\n",
      "Epoch 36, Batch 575, LR 0.092540 Loss 3.545171, Accuracy 93.213%\n",
      "Epoch 36, Batch 576, LR 0.092490 Loss 3.544545, Accuracy 93.216%\n",
      "Epoch 36, Batch 577, LR 0.092439 Loss 3.544477, Accuracy 93.215%\n",
      "Epoch 36, Batch 578, LR 0.092389 Loss 3.544539, Accuracy 93.216%\n",
      "Epoch 36, Batch 579, LR 0.092338 Loss 3.543626, Accuracy 93.218%\n",
      "Epoch 36, Batch 580, LR 0.092288 Loss 3.543573, Accuracy 93.215%\n",
      "Epoch 36, Batch 581, LR 0.092237 Loss 3.543364, Accuracy 93.213%\n",
      "Epoch 36, Batch 582, LR 0.092187 Loss 3.543321, Accuracy 93.209%\n",
      "Epoch 36, Batch 583, LR 0.092136 Loss 3.542943, Accuracy 93.207%\n",
      "Epoch 36, Batch 584, LR 0.092086 Loss 3.542361, Accuracy 93.210%\n",
      "Epoch 36, Batch 585, LR 0.092035 Loss 3.541544, Accuracy 93.214%\n",
      "Epoch 36, Batch 586, LR 0.091985 Loss 3.541847, Accuracy 93.214%\n",
      "Epoch 36, Batch 587, LR 0.091934 Loss 3.541340, Accuracy 93.211%\n",
      "Epoch 36, Batch 588, LR 0.091884 Loss 3.542295, Accuracy 93.204%\n",
      "Epoch 36, Batch 589, LR 0.091833 Loss 3.542697, Accuracy 93.202%\n",
      "Epoch 36, Batch 590, LR 0.091783 Loss 3.542232, Accuracy 93.203%\n",
      "Epoch 36, Batch 591, LR 0.091733 Loss 3.541733, Accuracy 93.205%\n",
      "Epoch 36, Batch 592, LR 0.091682 Loss 3.541960, Accuracy 93.202%\n",
      "Epoch 36, Batch 593, LR 0.091632 Loss 3.541646, Accuracy 93.199%\n",
      "Epoch 36, Batch 594, LR 0.091582 Loss 3.542358, Accuracy 93.196%\n",
      "Epoch 36, Batch 595, LR 0.091531 Loss 3.542406, Accuracy 93.193%\n",
      "Epoch 36, Batch 596, LR 0.091481 Loss 3.542570, Accuracy 93.192%\n",
      "Epoch 36, Batch 597, LR 0.091431 Loss 3.543071, Accuracy 93.187%\n",
      "Epoch 36, Batch 598, LR 0.091380 Loss 3.543471, Accuracy 93.188%\n",
      "Epoch 36, Batch 599, LR 0.091330 Loss 3.543913, Accuracy 93.190%\n",
      "Epoch 36, Batch 600, LR 0.091280 Loss 3.545201, Accuracy 93.186%\n",
      "Epoch 36, Batch 601, LR 0.091230 Loss 3.544744, Accuracy 93.186%\n",
      "Epoch 36, Batch 602, LR 0.091179 Loss 3.544377, Accuracy 93.183%\n",
      "Epoch 36, Batch 603, LR 0.091129 Loss 3.544264, Accuracy 93.185%\n",
      "Epoch 36, Batch 604, LR 0.091079 Loss 3.544501, Accuracy 93.189%\n",
      "Epoch 36, Batch 605, LR 0.091029 Loss 3.544334, Accuracy 93.191%\n",
      "Epoch 36, Batch 606, LR 0.090979 Loss 3.544995, Accuracy 93.189%\n",
      "Epoch 36, Batch 607, LR 0.090929 Loss 3.545342, Accuracy 93.185%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 608, LR 0.090878 Loss 3.545484, Accuracy 93.185%\n",
      "Epoch 36, Batch 609, LR 0.090828 Loss 3.545220, Accuracy 93.189%\n",
      "Epoch 36, Batch 610, LR 0.090778 Loss 3.544513, Accuracy 93.189%\n",
      "Epoch 36, Batch 611, LR 0.090728 Loss 3.545556, Accuracy 93.181%\n",
      "Epoch 36, Batch 612, LR 0.090678 Loss 3.545831, Accuracy 93.182%\n",
      "Epoch 36, Batch 613, LR 0.090628 Loss 3.545355, Accuracy 93.183%\n",
      "Epoch 36, Batch 614, LR 0.090578 Loss 3.546159, Accuracy 93.184%\n",
      "Epoch 36, Batch 615, LR 0.090528 Loss 3.546047, Accuracy 93.178%\n",
      "Epoch 36, Batch 616, LR 0.090478 Loss 3.546388, Accuracy 93.178%\n",
      "Epoch 36, Batch 617, LR 0.090428 Loss 3.546786, Accuracy 93.175%\n",
      "Epoch 36, Batch 618, LR 0.090378 Loss 3.547051, Accuracy 93.171%\n",
      "Epoch 36, Batch 619, LR 0.090328 Loss 3.546993, Accuracy 93.172%\n",
      "Epoch 36, Batch 620, LR 0.090278 Loss 3.547218, Accuracy 93.177%\n",
      "Epoch 36, Batch 621, LR 0.090228 Loss 3.546965, Accuracy 93.178%\n",
      "Epoch 36, Batch 622, LR 0.090178 Loss 3.547403, Accuracy 93.176%\n",
      "Epoch 36, Batch 623, LR 0.090128 Loss 3.547905, Accuracy 93.179%\n",
      "Epoch 36, Batch 624, LR 0.090078 Loss 3.547349, Accuracy 93.177%\n",
      "Epoch 36, Batch 625, LR 0.090028 Loss 3.546503, Accuracy 93.179%\n",
      "Epoch 36, Batch 626, LR 0.089978 Loss 3.546653, Accuracy 93.180%\n",
      "Epoch 36, Batch 627, LR 0.089928 Loss 3.545520, Accuracy 93.187%\n",
      "Epoch 36, Batch 628, LR 0.089878 Loss 3.545489, Accuracy 93.184%\n",
      "Epoch 36, Batch 629, LR 0.089828 Loss 3.545115, Accuracy 93.187%\n",
      "Epoch 36, Batch 630, LR 0.089779 Loss 3.545102, Accuracy 93.193%\n",
      "Epoch 36, Batch 631, LR 0.089729 Loss 3.543872, Accuracy 93.198%\n",
      "Epoch 36, Batch 632, LR 0.089679 Loss 3.544394, Accuracy 93.192%\n",
      "Epoch 36, Batch 633, LR 0.089629 Loss 3.543741, Accuracy 93.193%\n",
      "Epoch 36, Batch 634, LR 0.089579 Loss 3.543132, Accuracy 93.194%\n",
      "Epoch 36, Batch 635, LR 0.089530 Loss 3.542778, Accuracy 93.191%\n",
      "Epoch 36, Batch 636, LR 0.089480 Loss 3.541894, Accuracy 93.195%\n",
      "Epoch 36, Batch 637, LR 0.089430 Loss 3.542172, Accuracy 93.192%\n",
      "Epoch 36, Batch 638, LR 0.089380 Loss 3.541501, Accuracy 93.198%\n",
      "Epoch 36, Batch 639, LR 0.089330 Loss 3.541598, Accuracy 93.201%\n",
      "Epoch 36, Batch 640, LR 0.089281 Loss 3.542903, Accuracy 93.193%\n",
      "Epoch 36, Batch 641, LR 0.089231 Loss 3.541913, Accuracy 93.199%\n",
      "Epoch 36, Batch 642, LR 0.089181 Loss 3.542193, Accuracy 93.191%\n",
      "Epoch 36, Batch 643, LR 0.089132 Loss 3.541327, Accuracy 93.192%\n",
      "Epoch 36, Batch 644, LR 0.089082 Loss 3.541235, Accuracy 93.198%\n",
      "Epoch 36, Batch 645, LR 0.089032 Loss 3.541444, Accuracy 93.196%\n",
      "Epoch 36, Batch 646, LR 0.088983 Loss 3.541067, Accuracy 93.201%\n",
      "Epoch 36, Batch 647, LR 0.088933 Loss 3.540578, Accuracy 93.202%\n",
      "Epoch 36, Batch 648, LR 0.088884 Loss 3.540265, Accuracy 93.199%\n",
      "Epoch 36, Batch 649, LR 0.088834 Loss 3.539313, Accuracy 93.206%\n",
      "Epoch 36, Batch 650, LR 0.088784 Loss 3.539475, Accuracy 93.206%\n",
      "Epoch 36, Batch 651, LR 0.088735 Loss 3.539024, Accuracy 93.210%\n",
      "Epoch 36, Batch 652, LR 0.088685 Loss 3.539071, Accuracy 93.213%\n",
      "Epoch 36, Batch 653, LR 0.088636 Loss 3.539380, Accuracy 93.212%\n",
      "Epoch 36, Batch 654, LR 0.088586 Loss 3.539104, Accuracy 93.212%\n",
      "Epoch 36, Batch 655, LR 0.088537 Loss 3.538344, Accuracy 93.216%\n",
      "Epoch 36, Batch 656, LR 0.088487 Loss 3.538592, Accuracy 93.215%\n",
      "Epoch 36, Batch 657, LR 0.088438 Loss 3.538641, Accuracy 93.217%\n",
      "Epoch 36, Batch 658, LR 0.088388 Loss 3.538445, Accuracy 93.218%\n",
      "Epoch 36, Batch 659, LR 0.088339 Loss 3.537841, Accuracy 93.221%\n",
      "Epoch 36, Batch 660, LR 0.088289 Loss 3.537697, Accuracy 93.220%\n",
      "Epoch 36, Batch 661, LR 0.088240 Loss 3.537690, Accuracy 93.219%\n",
      "Epoch 36, Batch 662, LR 0.088190 Loss 3.537768, Accuracy 93.221%\n",
      "Epoch 36, Batch 663, LR 0.088141 Loss 3.537426, Accuracy 93.222%\n",
      "Epoch 36, Batch 664, LR 0.088092 Loss 3.537813, Accuracy 93.223%\n",
      "Epoch 36, Batch 665, LR 0.088042 Loss 3.536843, Accuracy 93.225%\n",
      "Epoch 36, Batch 666, LR 0.087993 Loss 3.536370, Accuracy 93.227%\n",
      "Epoch 36, Batch 667, LR 0.087943 Loss 3.535118, Accuracy 93.230%\n",
      "Epoch 36, Batch 668, LR 0.087894 Loss 3.534901, Accuracy 93.231%\n",
      "Epoch 36, Batch 669, LR 0.087845 Loss 3.534281, Accuracy 93.235%\n",
      "Epoch 36, Batch 670, LR 0.087795 Loss 3.534114, Accuracy 93.233%\n",
      "Epoch 36, Batch 671, LR 0.087746 Loss 3.534066, Accuracy 93.234%\n",
      "Epoch 36, Batch 672, LR 0.087697 Loss 3.533575, Accuracy 93.237%\n",
      "Epoch 36, Batch 673, LR 0.087648 Loss 3.533730, Accuracy 93.240%\n",
      "Epoch 36, Batch 674, LR 0.087598 Loss 3.534512, Accuracy 93.239%\n",
      "Epoch 36, Batch 675, LR 0.087549 Loss 3.533842, Accuracy 93.243%\n",
      "Epoch 36, Batch 676, LR 0.087500 Loss 3.534810, Accuracy 93.238%\n",
      "Epoch 36, Batch 677, LR 0.087451 Loss 3.534508, Accuracy 93.236%\n",
      "Epoch 36, Batch 678, LR 0.087401 Loss 3.534890, Accuracy 93.233%\n",
      "Epoch 36, Batch 679, LR 0.087352 Loss 3.533999, Accuracy 93.236%\n",
      "Epoch 36, Batch 680, LR 0.087303 Loss 3.533827, Accuracy 93.234%\n",
      "Epoch 36, Batch 681, LR 0.087254 Loss 3.533399, Accuracy 93.238%\n",
      "Epoch 36, Batch 682, LR 0.087205 Loss 3.534262, Accuracy 93.231%\n",
      "Epoch 36, Batch 683, LR 0.087155 Loss 3.534960, Accuracy 93.224%\n",
      "Epoch 36, Batch 684, LR 0.087106 Loss 3.535507, Accuracy 93.221%\n",
      "Epoch 36, Batch 685, LR 0.087057 Loss 3.535635, Accuracy 93.221%\n",
      "Epoch 36, Batch 686, LR 0.087008 Loss 3.534974, Accuracy 93.223%\n",
      "Epoch 36, Batch 687, LR 0.086959 Loss 3.535062, Accuracy 93.227%\n",
      "Epoch 36, Batch 688, LR 0.086910 Loss 3.536077, Accuracy 93.228%\n",
      "Epoch 36, Batch 689, LR 0.086861 Loss 3.535890, Accuracy 93.232%\n",
      "Epoch 36, Batch 690, LR 0.086812 Loss 3.535679, Accuracy 93.236%\n",
      "Epoch 36, Batch 691, LR 0.086763 Loss 3.536212, Accuracy 93.234%\n",
      "Epoch 36, Batch 692, LR 0.086714 Loss 3.536376, Accuracy 93.235%\n",
      "Epoch 36, Batch 693, LR 0.086665 Loss 3.535016, Accuracy 93.238%\n",
      "Epoch 36, Batch 694, LR 0.086616 Loss 3.534680, Accuracy 93.240%\n",
      "Epoch 36, Batch 695, LR 0.086567 Loss 3.534109, Accuracy 93.237%\n",
      "Epoch 36, Batch 696, LR 0.086518 Loss 3.534628, Accuracy 93.237%\n",
      "Epoch 36, Batch 697, LR 0.086469 Loss 3.533308, Accuracy 93.244%\n",
      "Epoch 36, Batch 698, LR 0.086420 Loss 3.534873, Accuracy 93.234%\n",
      "Epoch 36, Batch 699, LR 0.086371 Loss 3.534078, Accuracy 93.239%\n",
      "Epoch 36, Batch 700, LR 0.086322 Loss 3.534721, Accuracy 93.240%\n",
      "Epoch 36, Batch 701, LR 0.086273 Loss 3.535154, Accuracy 93.243%\n",
      "Epoch 36, Batch 702, LR 0.086224 Loss 3.534415, Accuracy 93.244%\n",
      "Epoch 36, Batch 703, LR 0.086175 Loss 3.534188, Accuracy 93.242%\n",
      "Epoch 36, Batch 704, LR 0.086126 Loss 3.534817, Accuracy 93.241%\n",
      "Epoch 36, Batch 705, LR 0.086078 Loss 3.534844, Accuracy 93.240%\n",
      "Epoch 36, Batch 706, LR 0.086029 Loss 3.535451, Accuracy 93.238%\n",
      "Epoch 36, Batch 707, LR 0.085980 Loss 3.535347, Accuracy 93.236%\n",
      "Epoch 36, Batch 708, LR 0.085931 Loss 3.535451, Accuracy 93.237%\n",
      "Epoch 36, Batch 709, LR 0.085882 Loss 3.535641, Accuracy 93.233%\n",
      "Epoch 36, Batch 710, LR 0.085834 Loss 3.535698, Accuracy 93.234%\n",
      "Epoch 36, Batch 711, LR 0.085785 Loss 3.535763, Accuracy 93.234%\n",
      "Epoch 36, Batch 712, LR 0.085736 Loss 3.535886, Accuracy 93.235%\n",
      "Epoch 36, Batch 713, LR 0.085687 Loss 3.537164, Accuracy 93.230%\n",
      "Epoch 36, Batch 714, LR 0.085639 Loss 3.537026, Accuracy 93.230%\n",
      "Epoch 36, Batch 715, LR 0.085590 Loss 3.536824, Accuracy 93.232%\n",
      "Epoch 36, Batch 716, LR 0.085541 Loss 3.536537, Accuracy 93.234%\n",
      "Epoch 36, Batch 717, LR 0.085492 Loss 3.537255, Accuracy 93.227%\n",
      "Epoch 36, Batch 718, LR 0.085444 Loss 3.537196, Accuracy 93.228%\n",
      "Epoch 36, Batch 719, LR 0.085395 Loss 3.538894, Accuracy 93.219%\n",
      "Epoch 36, Batch 720, LR 0.085346 Loss 3.538869, Accuracy 93.220%\n",
      "Epoch 36, Batch 721, LR 0.085298 Loss 3.539025, Accuracy 93.221%\n",
      "Epoch 36, Batch 722, LR 0.085249 Loss 3.537930, Accuracy 93.226%\n",
      "Epoch 36, Batch 723, LR 0.085201 Loss 3.538180, Accuracy 93.227%\n",
      "Epoch 36, Batch 724, LR 0.085152 Loss 3.538127, Accuracy 93.224%\n",
      "Epoch 36, Batch 725, LR 0.085103 Loss 3.537560, Accuracy 93.231%\n",
      "Epoch 36, Batch 726, LR 0.085055 Loss 3.537341, Accuracy 93.233%\n",
      "Epoch 36, Batch 727, LR 0.085006 Loss 3.537284, Accuracy 93.235%\n",
      "Epoch 36, Batch 728, LR 0.084958 Loss 3.537034, Accuracy 93.237%\n",
      "Epoch 36, Batch 729, LR 0.084909 Loss 3.536626, Accuracy 93.238%\n",
      "Epoch 36, Batch 730, LR 0.084861 Loss 3.536744, Accuracy 93.236%\n",
      "Epoch 36, Batch 731, LR 0.084812 Loss 3.536980, Accuracy 93.236%\n",
      "Epoch 36, Batch 732, LR 0.084764 Loss 3.536877, Accuracy 93.235%\n",
      "Epoch 36, Batch 733, LR 0.084715 Loss 3.537236, Accuracy 93.235%\n",
      "Epoch 36, Batch 734, LR 0.084667 Loss 3.537626, Accuracy 93.228%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 735, LR 0.084618 Loss 3.538531, Accuracy 93.220%\n",
      "Epoch 36, Batch 736, LR 0.084570 Loss 3.537627, Accuracy 93.228%\n",
      "Epoch 36, Batch 737, LR 0.084521 Loss 3.538283, Accuracy 93.225%\n",
      "Epoch 36, Batch 738, LR 0.084473 Loss 3.538807, Accuracy 93.224%\n",
      "Epoch 36, Batch 739, LR 0.084425 Loss 3.538397, Accuracy 93.225%\n",
      "Epoch 36, Batch 740, LR 0.084376 Loss 3.537962, Accuracy 93.224%\n",
      "Epoch 36, Batch 741, LR 0.084328 Loss 3.537879, Accuracy 93.227%\n",
      "Epoch 36, Batch 742, LR 0.084280 Loss 3.538248, Accuracy 93.229%\n",
      "Epoch 36, Batch 743, LR 0.084231 Loss 3.538159, Accuracy 93.231%\n",
      "Epoch 36, Batch 744, LR 0.084183 Loss 3.538041, Accuracy 93.229%\n",
      "Epoch 36, Batch 745, LR 0.084135 Loss 3.538015, Accuracy 93.233%\n",
      "Epoch 36, Batch 746, LR 0.084086 Loss 3.538308, Accuracy 93.230%\n",
      "Epoch 36, Batch 747, LR 0.084038 Loss 3.538054, Accuracy 93.230%\n",
      "Epoch 36, Batch 748, LR 0.083990 Loss 3.538193, Accuracy 93.226%\n",
      "Epoch 36, Batch 749, LR 0.083941 Loss 3.538877, Accuracy 93.221%\n",
      "Epoch 36, Batch 750, LR 0.083893 Loss 3.538733, Accuracy 93.218%\n",
      "Epoch 36, Batch 751, LR 0.083845 Loss 3.539454, Accuracy 93.217%\n",
      "Epoch 36, Batch 752, LR 0.083797 Loss 3.539476, Accuracy 93.218%\n",
      "Epoch 36, Batch 753, LR 0.083748 Loss 3.539087, Accuracy 93.222%\n",
      "Epoch 36, Batch 754, LR 0.083700 Loss 3.539529, Accuracy 93.221%\n",
      "Epoch 36, Batch 755, LR 0.083652 Loss 3.538712, Accuracy 93.223%\n",
      "Epoch 36, Batch 756, LR 0.083604 Loss 3.538294, Accuracy 93.223%\n",
      "Epoch 36, Batch 757, LR 0.083556 Loss 3.538368, Accuracy 93.225%\n",
      "Epoch 36, Batch 758, LR 0.083508 Loss 3.538373, Accuracy 93.223%\n",
      "Epoch 36, Batch 759, LR 0.083460 Loss 3.538195, Accuracy 93.225%\n",
      "Epoch 36, Batch 760, LR 0.083411 Loss 3.538783, Accuracy 93.225%\n",
      "Epoch 36, Batch 761, LR 0.083363 Loss 3.538569, Accuracy 93.224%\n",
      "Epoch 36, Batch 762, LR 0.083315 Loss 3.538283, Accuracy 93.224%\n",
      "Epoch 36, Batch 763, LR 0.083267 Loss 3.538575, Accuracy 93.224%\n",
      "Epoch 36, Batch 764, LR 0.083219 Loss 3.538140, Accuracy 93.223%\n",
      "Epoch 36, Batch 765, LR 0.083171 Loss 3.538201, Accuracy 93.221%\n",
      "Epoch 36, Batch 766, LR 0.083123 Loss 3.538295, Accuracy 93.223%\n",
      "Epoch 36, Batch 767, LR 0.083075 Loss 3.539759, Accuracy 93.212%\n",
      "Epoch 36, Batch 768, LR 0.083027 Loss 3.539814, Accuracy 93.211%\n",
      "Epoch 36, Batch 769, LR 0.082979 Loss 3.540061, Accuracy 93.210%\n",
      "Epoch 36, Batch 770, LR 0.082931 Loss 3.539827, Accuracy 93.210%\n",
      "Epoch 36, Batch 771, LR 0.082883 Loss 3.540675, Accuracy 93.210%\n",
      "Epoch 36, Batch 772, LR 0.082835 Loss 3.540590, Accuracy 93.210%\n",
      "Epoch 36, Batch 773, LR 0.082787 Loss 3.540586, Accuracy 93.210%\n",
      "Epoch 36, Batch 774, LR 0.082739 Loss 3.540521, Accuracy 93.208%\n",
      "Epoch 36, Batch 775, LR 0.082691 Loss 3.539841, Accuracy 93.211%\n",
      "Epoch 36, Batch 776, LR 0.082643 Loss 3.539861, Accuracy 93.211%\n",
      "Epoch 36, Batch 777, LR 0.082595 Loss 3.539922, Accuracy 93.212%\n",
      "Epoch 36, Batch 778, LR 0.082548 Loss 3.540359, Accuracy 93.214%\n",
      "Epoch 36, Batch 779, LR 0.082500 Loss 3.540144, Accuracy 93.217%\n",
      "Epoch 36, Batch 780, LR 0.082452 Loss 3.540105, Accuracy 93.220%\n",
      "Epoch 36, Batch 781, LR 0.082404 Loss 3.539002, Accuracy 93.224%\n",
      "Epoch 36, Batch 782, LR 0.082356 Loss 3.538225, Accuracy 93.223%\n",
      "Epoch 36, Batch 783, LR 0.082308 Loss 3.537708, Accuracy 93.225%\n",
      "Epoch 36, Batch 784, LR 0.082261 Loss 3.538024, Accuracy 93.223%\n",
      "Epoch 36, Batch 785, LR 0.082213 Loss 3.537342, Accuracy 93.228%\n",
      "Epoch 36, Batch 786, LR 0.082165 Loss 3.538160, Accuracy 93.224%\n",
      "Epoch 36, Batch 787, LR 0.082117 Loss 3.538169, Accuracy 93.223%\n",
      "Epoch 36, Batch 788, LR 0.082070 Loss 3.537285, Accuracy 93.226%\n",
      "Epoch 36, Batch 789, LR 0.082022 Loss 3.537557, Accuracy 93.221%\n",
      "Epoch 36, Batch 790, LR 0.081974 Loss 3.536698, Accuracy 93.226%\n",
      "Epoch 36, Batch 791, LR 0.081926 Loss 3.535974, Accuracy 93.227%\n",
      "Epoch 36, Batch 792, LR 0.081879 Loss 3.535855, Accuracy 93.229%\n",
      "Epoch 36, Batch 793, LR 0.081831 Loss 3.535730, Accuracy 93.232%\n",
      "Epoch 36, Batch 794, LR 0.081783 Loss 3.535377, Accuracy 93.232%\n",
      "Epoch 36, Batch 795, LR 0.081736 Loss 3.535417, Accuracy 93.233%\n",
      "Epoch 36, Batch 796, LR 0.081688 Loss 3.535175, Accuracy 93.237%\n",
      "Epoch 36, Batch 797, LR 0.081641 Loss 3.535753, Accuracy 93.234%\n",
      "Epoch 36, Batch 798, LR 0.081593 Loss 3.536195, Accuracy 93.231%\n",
      "Epoch 36, Batch 799, LR 0.081545 Loss 3.536792, Accuracy 93.232%\n",
      "Epoch 36, Batch 800, LR 0.081498 Loss 3.536542, Accuracy 93.234%\n",
      "Epoch 36, Batch 801, LR 0.081450 Loss 3.536649, Accuracy 93.235%\n",
      "Epoch 36, Batch 802, LR 0.081403 Loss 3.536161, Accuracy 93.238%\n",
      "Epoch 36, Batch 803, LR 0.081355 Loss 3.536163, Accuracy 93.235%\n",
      "Epoch 36, Batch 804, LR 0.081308 Loss 3.536444, Accuracy 93.234%\n",
      "Epoch 36, Batch 805, LR 0.081260 Loss 3.536888, Accuracy 93.231%\n",
      "Epoch 36, Batch 806, LR 0.081213 Loss 3.536973, Accuracy 93.231%\n",
      "Epoch 36, Batch 807, LR 0.081165 Loss 3.537522, Accuracy 93.229%\n",
      "Epoch 36, Batch 808, LR 0.081118 Loss 3.538302, Accuracy 93.224%\n",
      "Epoch 36, Batch 809, LR 0.081070 Loss 3.537320, Accuracy 93.228%\n",
      "Epoch 36, Batch 810, LR 0.081023 Loss 3.537270, Accuracy 93.227%\n",
      "Epoch 36, Batch 811, LR 0.080975 Loss 3.537141, Accuracy 93.227%\n",
      "Epoch 36, Batch 812, LR 0.080928 Loss 3.537049, Accuracy 93.228%\n",
      "Epoch 36, Batch 813, LR 0.080880 Loss 3.537128, Accuracy 93.233%\n",
      "Epoch 36, Batch 814, LR 0.080833 Loss 3.536572, Accuracy 93.234%\n",
      "Epoch 36, Batch 815, LR 0.080786 Loss 3.537149, Accuracy 93.235%\n",
      "Epoch 36, Batch 816, LR 0.080738 Loss 3.536852, Accuracy 93.236%\n",
      "Epoch 36, Batch 817, LR 0.080691 Loss 3.537200, Accuracy 93.232%\n",
      "Epoch 36, Batch 818, LR 0.080644 Loss 3.538031, Accuracy 93.226%\n",
      "Epoch 36, Batch 819, LR 0.080596 Loss 3.538376, Accuracy 93.227%\n",
      "Epoch 36, Batch 820, LR 0.080549 Loss 3.539241, Accuracy 93.219%\n",
      "Epoch 36, Batch 821, LR 0.080502 Loss 3.539050, Accuracy 93.222%\n",
      "Epoch 36, Batch 822, LR 0.080454 Loss 3.538596, Accuracy 93.224%\n",
      "Epoch 36, Batch 823, LR 0.080407 Loss 3.538817, Accuracy 93.220%\n",
      "Epoch 36, Batch 824, LR 0.080360 Loss 3.538984, Accuracy 93.216%\n",
      "Epoch 36, Batch 825, LR 0.080313 Loss 3.538805, Accuracy 93.215%\n",
      "Epoch 36, Batch 826, LR 0.080265 Loss 3.539510, Accuracy 93.214%\n",
      "Epoch 36, Batch 827, LR 0.080218 Loss 3.540105, Accuracy 93.212%\n",
      "Epoch 36, Batch 828, LR 0.080171 Loss 3.539955, Accuracy 93.210%\n",
      "Epoch 36, Batch 829, LR 0.080124 Loss 3.539797, Accuracy 93.211%\n",
      "Epoch 36, Batch 830, LR 0.080077 Loss 3.540020, Accuracy 93.212%\n",
      "Epoch 36, Batch 831, LR 0.080029 Loss 3.540255, Accuracy 93.211%\n",
      "Epoch 36, Batch 832, LR 0.079982 Loss 3.539651, Accuracy 93.215%\n",
      "Epoch 36, Batch 833, LR 0.079935 Loss 3.539175, Accuracy 93.216%\n",
      "Epoch 36, Batch 834, LR 0.079888 Loss 3.539364, Accuracy 93.216%\n",
      "Epoch 36, Batch 835, LR 0.079841 Loss 3.539198, Accuracy 93.215%\n",
      "Epoch 36, Batch 836, LR 0.079794 Loss 3.539821, Accuracy 93.207%\n",
      "Epoch 36, Batch 837, LR 0.079747 Loss 3.539327, Accuracy 93.208%\n",
      "Epoch 36, Batch 838, LR 0.079700 Loss 3.539777, Accuracy 93.203%\n",
      "Epoch 36, Batch 839, LR 0.079653 Loss 3.539042, Accuracy 93.203%\n",
      "Epoch 36, Batch 840, LR 0.079606 Loss 3.538523, Accuracy 93.204%\n",
      "Epoch 36, Batch 841, LR 0.079559 Loss 3.538329, Accuracy 93.206%\n",
      "Epoch 36, Batch 842, LR 0.079512 Loss 3.537843, Accuracy 93.211%\n",
      "Epoch 36, Batch 843, LR 0.079465 Loss 3.537395, Accuracy 93.212%\n",
      "Epoch 36, Batch 844, LR 0.079418 Loss 3.537439, Accuracy 93.213%\n",
      "Epoch 36, Batch 845, LR 0.079371 Loss 3.537565, Accuracy 93.211%\n",
      "Epoch 36, Batch 846, LR 0.079324 Loss 3.537251, Accuracy 93.211%\n",
      "Epoch 36, Batch 847, LR 0.079277 Loss 3.537481, Accuracy 93.208%\n",
      "Epoch 36, Batch 848, LR 0.079230 Loss 3.537899, Accuracy 93.204%\n",
      "Epoch 36, Batch 849, LR 0.079183 Loss 3.537580, Accuracy 93.205%\n",
      "Epoch 36, Batch 850, LR 0.079136 Loss 3.537039, Accuracy 93.207%\n",
      "Epoch 36, Batch 851, LR 0.079089 Loss 3.536961, Accuracy 93.209%\n",
      "Epoch 36, Batch 852, LR 0.079042 Loss 3.536588, Accuracy 93.213%\n",
      "Epoch 36, Batch 853, LR 0.078995 Loss 3.536679, Accuracy 93.215%\n",
      "Epoch 36, Batch 854, LR 0.078948 Loss 3.536392, Accuracy 93.218%\n",
      "Epoch 36, Batch 855, LR 0.078902 Loss 3.535841, Accuracy 93.222%\n",
      "Epoch 36, Batch 856, LR 0.078855 Loss 3.536013, Accuracy 93.219%\n",
      "Epoch 36, Batch 857, LR 0.078808 Loss 3.536055, Accuracy 93.219%\n",
      "Epoch 36, Batch 858, LR 0.078761 Loss 3.535285, Accuracy 93.223%\n",
      "Epoch 36, Batch 859, LR 0.078714 Loss 3.535510, Accuracy 93.222%\n",
      "Epoch 36, Batch 860, LR 0.078668 Loss 3.535388, Accuracy 93.222%\n",
      "Epoch 36, Batch 861, LR 0.078621 Loss 3.535174, Accuracy 93.222%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 862, LR 0.078574 Loss 3.536062, Accuracy 93.217%\n",
      "Epoch 36, Batch 863, LR 0.078527 Loss 3.535651, Accuracy 93.220%\n",
      "Epoch 36, Batch 864, LR 0.078481 Loss 3.535782, Accuracy 93.218%\n",
      "Epoch 36, Batch 865, LR 0.078434 Loss 3.535297, Accuracy 93.219%\n",
      "Epoch 36, Batch 866, LR 0.078387 Loss 3.535264, Accuracy 93.221%\n",
      "Epoch 36, Batch 867, LR 0.078341 Loss 3.534834, Accuracy 93.221%\n",
      "Epoch 36, Batch 868, LR 0.078294 Loss 3.535553, Accuracy 93.219%\n",
      "Epoch 36, Batch 869, LR 0.078247 Loss 3.535653, Accuracy 93.219%\n",
      "Epoch 36, Batch 870, LR 0.078201 Loss 3.535952, Accuracy 93.219%\n",
      "Epoch 36, Batch 871, LR 0.078154 Loss 3.535528, Accuracy 93.218%\n",
      "Epoch 36, Batch 872, LR 0.078107 Loss 3.535029, Accuracy 93.221%\n",
      "Epoch 36, Batch 873, LR 0.078061 Loss 3.535526, Accuracy 93.218%\n",
      "Epoch 36, Batch 874, LR 0.078014 Loss 3.535435, Accuracy 93.219%\n",
      "Epoch 36, Batch 875, LR 0.077968 Loss 3.535539, Accuracy 93.217%\n",
      "Epoch 36, Batch 876, LR 0.077921 Loss 3.535340, Accuracy 93.220%\n",
      "Epoch 36, Batch 877, LR 0.077874 Loss 3.535130, Accuracy 93.223%\n",
      "Epoch 36, Batch 878, LR 0.077828 Loss 3.535189, Accuracy 93.221%\n",
      "Epoch 36, Batch 879, LR 0.077781 Loss 3.535702, Accuracy 93.218%\n",
      "Epoch 36, Batch 880, LR 0.077735 Loss 3.535273, Accuracy 93.220%\n",
      "Epoch 36, Batch 881, LR 0.077688 Loss 3.535912, Accuracy 93.218%\n",
      "Epoch 36, Batch 882, LR 0.077642 Loss 3.535380, Accuracy 93.221%\n",
      "Epoch 36, Batch 883, LR 0.077595 Loss 3.535995, Accuracy 93.220%\n",
      "Epoch 36, Batch 884, LR 0.077549 Loss 3.535004, Accuracy 93.222%\n",
      "Epoch 36, Batch 885, LR 0.077503 Loss 3.535546, Accuracy 93.217%\n",
      "Epoch 36, Batch 886, LR 0.077456 Loss 3.535653, Accuracy 93.217%\n",
      "Epoch 36, Batch 887, LR 0.077410 Loss 3.536369, Accuracy 93.214%\n",
      "Epoch 36, Batch 888, LR 0.077363 Loss 3.536189, Accuracy 93.213%\n",
      "Epoch 36, Batch 889, LR 0.077317 Loss 3.536753, Accuracy 93.213%\n",
      "Epoch 36, Batch 890, LR 0.077271 Loss 3.536991, Accuracy 93.208%\n",
      "Epoch 36, Batch 891, LR 0.077224 Loss 3.537174, Accuracy 93.205%\n",
      "Epoch 36, Batch 892, LR 0.077178 Loss 3.537060, Accuracy 93.204%\n",
      "Epoch 36, Batch 893, LR 0.077132 Loss 3.536442, Accuracy 93.209%\n",
      "Epoch 36, Batch 894, LR 0.077085 Loss 3.535989, Accuracy 93.211%\n",
      "Epoch 36, Batch 895, LR 0.077039 Loss 3.535557, Accuracy 93.214%\n",
      "Epoch 36, Batch 896, LR 0.076993 Loss 3.535661, Accuracy 93.210%\n",
      "Epoch 36, Batch 897, LR 0.076946 Loss 3.535842, Accuracy 93.211%\n",
      "Epoch 36, Batch 898, LR 0.076900 Loss 3.535975, Accuracy 93.209%\n",
      "Epoch 36, Batch 899, LR 0.076854 Loss 3.536267, Accuracy 93.209%\n",
      "Epoch 36, Batch 900, LR 0.076808 Loss 3.536319, Accuracy 93.208%\n",
      "Epoch 36, Batch 901, LR 0.076761 Loss 3.536339, Accuracy 93.212%\n",
      "Epoch 36, Batch 902, LR 0.076715 Loss 3.536468, Accuracy 93.209%\n",
      "Epoch 36, Batch 903, LR 0.076669 Loss 3.536782, Accuracy 93.208%\n",
      "Epoch 36, Batch 904, LR 0.076623 Loss 3.537233, Accuracy 93.208%\n",
      "Epoch 36, Batch 905, LR 0.076577 Loss 3.537852, Accuracy 93.205%\n",
      "Epoch 36, Batch 906, LR 0.076531 Loss 3.538054, Accuracy 93.205%\n",
      "Epoch 36, Batch 907, LR 0.076484 Loss 3.538212, Accuracy 93.203%\n",
      "Epoch 36, Batch 908, LR 0.076438 Loss 3.538902, Accuracy 93.205%\n",
      "Epoch 36, Batch 909, LR 0.076392 Loss 3.538807, Accuracy 93.207%\n",
      "Epoch 36, Batch 910, LR 0.076346 Loss 3.539117, Accuracy 93.205%\n",
      "Epoch 36, Batch 911, LR 0.076300 Loss 3.538575, Accuracy 93.211%\n",
      "Epoch 36, Batch 912, LR 0.076254 Loss 3.538449, Accuracy 93.212%\n",
      "Epoch 36, Batch 913, LR 0.076208 Loss 3.538238, Accuracy 93.212%\n",
      "Epoch 36, Batch 914, LR 0.076162 Loss 3.538746, Accuracy 93.208%\n",
      "Epoch 36, Batch 915, LR 0.076116 Loss 3.538698, Accuracy 93.209%\n",
      "Epoch 36, Batch 916, LR 0.076070 Loss 3.538911, Accuracy 93.209%\n",
      "Epoch 36, Batch 917, LR 0.076024 Loss 3.539025, Accuracy 93.209%\n",
      "Epoch 36, Batch 918, LR 0.075978 Loss 3.539761, Accuracy 93.202%\n",
      "Epoch 36, Batch 919, LR 0.075932 Loss 3.540300, Accuracy 93.200%\n",
      "Epoch 36, Batch 920, LR 0.075886 Loss 3.539978, Accuracy 93.201%\n",
      "Epoch 36, Batch 921, LR 0.075840 Loss 3.540170, Accuracy 93.198%\n",
      "Epoch 36, Batch 922, LR 0.075794 Loss 3.540047, Accuracy 93.200%\n",
      "Epoch 36, Batch 923, LR 0.075748 Loss 3.539660, Accuracy 93.202%\n",
      "Epoch 36, Batch 924, LR 0.075702 Loss 3.539625, Accuracy 93.200%\n",
      "Epoch 36, Batch 925, LR 0.075656 Loss 3.539908, Accuracy 93.198%\n",
      "Epoch 36, Batch 926, LR 0.075610 Loss 3.539859, Accuracy 93.197%\n",
      "Epoch 36, Batch 927, LR 0.075564 Loss 3.540346, Accuracy 93.195%\n",
      "Epoch 36, Batch 928, LR 0.075519 Loss 3.540600, Accuracy 93.193%\n",
      "Epoch 36, Batch 929, LR 0.075473 Loss 3.540509, Accuracy 93.192%\n",
      "Epoch 36, Batch 930, LR 0.075427 Loss 3.540791, Accuracy 93.189%\n",
      "Epoch 36, Batch 931, LR 0.075381 Loss 3.540750, Accuracy 93.189%\n",
      "Epoch 36, Batch 932, LR 0.075335 Loss 3.540670, Accuracy 93.188%\n",
      "Epoch 36, Batch 933, LR 0.075289 Loss 3.540719, Accuracy 93.187%\n",
      "Epoch 36, Batch 934, LR 0.075244 Loss 3.540956, Accuracy 93.186%\n",
      "Epoch 36, Batch 935, LR 0.075198 Loss 3.540795, Accuracy 93.185%\n",
      "Epoch 36, Batch 936, LR 0.075152 Loss 3.540078, Accuracy 93.188%\n",
      "Epoch 36, Batch 937, LR 0.075106 Loss 3.539966, Accuracy 93.188%\n",
      "Epoch 36, Batch 938, LR 0.075061 Loss 3.539650, Accuracy 93.190%\n",
      "Epoch 36, Batch 939, LR 0.075015 Loss 3.540610, Accuracy 93.187%\n",
      "Epoch 36, Batch 940, LR 0.074969 Loss 3.540549, Accuracy 93.187%\n",
      "Epoch 36, Batch 941, LR 0.074924 Loss 3.540272, Accuracy 93.185%\n",
      "Epoch 36, Batch 942, LR 0.074878 Loss 3.540496, Accuracy 93.183%\n",
      "Epoch 36, Batch 943, LR 0.074832 Loss 3.540332, Accuracy 93.186%\n",
      "Epoch 36, Batch 944, LR 0.074787 Loss 3.540716, Accuracy 93.184%\n",
      "Epoch 36, Batch 945, LR 0.074741 Loss 3.540330, Accuracy 93.187%\n",
      "Epoch 36, Batch 946, LR 0.074695 Loss 3.539981, Accuracy 93.186%\n",
      "Epoch 36, Batch 947, LR 0.074650 Loss 3.539588, Accuracy 93.188%\n",
      "Epoch 36, Batch 948, LR 0.074604 Loss 3.538697, Accuracy 93.190%\n",
      "Epoch 36, Batch 949, LR 0.074559 Loss 3.538298, Accuracy 93.193%\n",
      "Epoch 36, Batch 950, LR 0.074513 Loss 3.537891, Accuracy 93.194%\n",
      "Epoch 36, Batch 951, LR 0.074468 Loss 3.537259, Accuracy 93.197%\n",
      "Epoch 36, Batch 952, LR 0.074422 Loss 3.536902, Accuracy 93.199%\n",
      "Epoch 36, Batch 953, LR 0.074377 Loss 3.536746, Accuracy 93.201%\n",
      "Epoch 36, Batch 954, LR 0.074331 Loss 3.536851, Accuracy 93.201%\n",
      "Epoch 36, Batch 955, LR 0.074286 Loss 3.536176, Accuracy 93.202%\n",
      "Epoch 36, Batch 956, LR 0.074240 Loss 3.536337, Accuracy 93.199%\n",
      "Epoch 36, Batch 957, LR 0.074195 Loss 3.535882, Accuracy 93.200%\n",
      "Epoch 36, Batch 958, LR 0.074149 Loss 3.536213, Accuracy 93.196%\n",
      "Epoch 36, Batch 959, LR 0.074104 Loss 3.535680, Accuracy 93.196%\n",
      "Epoch 36, Batch 960, LR 0.074058 Loss 3.535725, Accuracy 93.196%\n",
      "Epoch 36, Batch 961, LR 0.074013 Loss 3.535545, Accuracy 93.200%\n",
      "Epoch 36, Batch 962, LR 0.073967 Loss 3.534895, Accuracy 93.200%\n",
      "Epoch 36, Batch 963, LR 0.073922 Loss 3.534973, Accuracy 93.202%\n",
      "Epoch 36, Batch 964, LR 0.073877 Loss 3.535248, Accuracy 93.200%\n",
      "Epoch 36, Batch 965, LR 0.073831 Loss 3.535823, Accuracy 93.198%\n",
      "Epoch 36, Batch 966, LR 0.073786 Loss 3.535462, Accuracy 93.201%\n",
      "Epoch 36, Batch 967, LR 0.073741 Loss 3.535158, Accuracy 93.199%\n",
      "Epoch 36, Batch 968, LR 0.073695 Loss 3.535806, Accuracy 93.196%\n",
      "Epoch 36, Batch 969, LR 0.073650 Loss 3.536230, Accuracy 93.192%\n",
      "Epoch 36, Batch 970, LR 0.073605 Loss 3.536732, Accuracy 93.189%\n",
      "Epoch 36, Batch 971, LR 0.073559 Loss 3.536644, Accuracy 93.188%\n",
      "Epoch 36, Batch 972, LR 0.073514 Loss 3.536581, Accuracy 93.188%\n",
      "Epoch 36, Batch 973, LR 0.073469 Loss 3.537184, Accuracy 93.186%\n",
      "Epoch 36, Batch 974, LR 0.073424 Loss 3.536657, Accuracy 93.187%\n",
      "Epoch 36, Batch 975, LR 0.073378 Loss 3.536818, Accuracy 93.185%\n",
      "Epoch 36, Batch 976, LR 0.073333 Loss 3.536520, Accuracy 93.186%\n",
      "Epoch 36, Batch 977, LR 0.073288 Loss 3.536658, Accuracy 93.185%\n",
      "Epoch 36, Batch 978, LR 0.073243 Loss 3.536653, Accuracy 93.187%\n",
      "Epoch 36, Batch 979, LR 0.073198 Loss 3.536591, Accuracy 93.191%\n",
      "Epoch 36, Batch 980, LR 0.073153 Loss 3.536822, Accuracy 93.190%\n",
      "Epoch 36, Batch 981, LR 0.073107 Loss 3.537399, Accuracy 93.189%\n",
      "Epoch 36, Batch 982, LR 0.073062 Loss 3.537003, Accuracy 93.192%\n",
      "Epoch 36, Batch 983, LR 0.073017 Loss 3.537146, Accuracy 93.190%\n",
      "Epoch 36, Batch 984, LR 0.072972 Loss 3.536962, Accuracy 93.189%\n",
      "Epoch 36, Batch 985, LR 0.072927 Loss 3.537426, Accuracy 93.187%\n",
      "Epoch 36, Batch 986, LR 0.072882 Loss 3.537006, Accuracy 93.187%\n",
      "Epoch 36, Batch 987, LR 0.072837 Loss 3.536897, Accuracy 93.189%\n",
      "Epoch 36, Batch 988, LR 0.072792 Loss 3.536732, Accuracy 93.190%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 989, LR 0.072747 Loss 3.536080, Accuracy 93.190%\n",
      "Epoch 36, Batch 990, LR 0.072702 Loss 3.535334, Accuracy 93.192%\n",
      "Epoch 36, Batch 991, LR 0.072657 Loss 3.535090, Accuracy 93.193%\n",
      "Epoch 36, Batch 992, LR 0.072612 Loss 3.534773, Accuracy 93.192%\n",
      "Epoch 36, Batch 993, LR 0.072567 Loss 3.534888, Accuracy 93.189%\n",
      "Epoch 36, Batch 994, LR 0.072522 Loss 3.534223, Accuracy 93.194%\n",
      "Epoch 36, Batch 995, LR 0.072477 Loss 3.534137, Accuracy 93.194%\n",
      "Epoch 36, Batch 996, LR 0.072432 Loss 3.533991, Accuracy 93.195%\n",
      "Epoch 36, Batch 997, LR 0.072387 Loss 3.533634, Accuracy 93.198%\n",
      "Epoch 36, Batch 998, LR 0.072342 Loss 3.533497, Accuracy 93.200%\n",
      "Epoch 36, Batch 999, LR 0.072297 Loss 3.533464, Accuracy 93.204%\n",
      "Epoch 36, Batch 1000, LR 0.072252 Loss 3.532951, Accuracy 93.207%\n",
      "Epoch 36, Batch 1001, LR 0.072207 Loss 3.532734, Accuracy 93.208%\n",
      "Epoch 36, Batch 1002, LR 0.072163 Loss 3.532524, Accuracy 93.208%\n",
      "Epoch 36, Batch 1003, LR 0.072118 Loss 3.532599, Accuracy 93.208%\n",
      "Epoch 36, Batch 1004, LR 0.072073 Loss 3.532303, Accuracy 93.208%\n",
      "Epoch 36, Batch 1005, LR 0.072028 Loss 3.531955, Accuracy 93.211%\n",
      "Epoch 36, Batch 1006, LR 0.071983 Loss 3.531742, Accuracy 93.213%\n",
      "Epoch 36, Batch 1007, LR 0.071938 Loss 3.532484, Accuracy 93.207%\n",
      "Epoch 36, Batch 1008, LR 0.071894 Loss 3.531712, Accuracy 93.209%\n",
      "Epoch 36, Batch 1009, LR 0.071849 Loss 3.531488, Accuracy 93.211%\n",
      "Epoch 36, Batch 1010, LR 0.071804 Loss 3.531052, Accuracy 93.212%\n",
      "Epoch 36, Batch 1011, LR 0.071759 Loss 3.530464, Accuracy 93.212%\n",
      "Epoch 36, Batch 1012, LR 0.071715 Loss 3.530109, Accuracy 93.214%\n",
      "Epoch 36, Batch 1013, LR 0.071670 Loss 3.529888, Accuracy 93.216%\n",
      "Epoch 36, Batch 1014, LR 0.071625 Loss 3.529606, Accuracy 93.217%\n",
      "Epoch 36, Batch 1015, LR 0.071581 Loss 3.529683, Accuracy 93.217%\n",
      "Epoch 36, Batch 1016, LR 0.071536 Loss 3.530476, Accuracy 93.212%\n",
      "Epoch 36, Batch 1017, LR 0.071491 Loss 3.530061, Accuracy 93.211%\n",
      "Epoch 36, Batch 1018, LR 0.071447 Loss 3.530567, Accuracy 93.211%\n",
      "Epoch 36, Batch 1019, LR 0.071402 Loss 3.530932, Accuracy 93.210%\n",
      "Epoch 36, Batch 1020, LR 0.071357 Loss 3.530863, Accuracy 93.211%\n",
      "Epoch 36, Batch 1021, LR 0.071313 Loss 3.530731, Accuracy 93.211%\n",
      "Epoch 36, Batch 1022, LR 0.071268 Loss 3.530181, Accuracy 93.211%\n",
      "Epoch 36, Batch 1023, LR 0.071224 Loss 3.531281, Accuracy 93.209%\n",
      "Epoch 36, Batch 1024, LR 0.071179 Loss 3.531521, Accuracy 93.208%\n",
      "Epoch 36, Batch 1025, LR 0.071135 Loss 3.531285, Accuracy 93.210%\n",
      "Epoch 36, Batch 1026, LR 0.071090 Loss 3.531079, Accuracy 93.211%\n",
      "Epoch 36, Batch 1027, LR 0.071046 Loss 3.531025, Accuracy 93.208%\n",
      "Epoch 36, Batch 1028, LR 0.071001 Loss 3.531360, Accuracy 93.206%\n",
      "Epoch 36, Batch 1029, LR 0.070957 Loss 3.531747, Accuracy 93.203%\n",
      "Epoch 36, Batch 1030, LR 0.070912 Loss 3.531524, Accuracy 93.205%\n",
      "Epoch 36, Batch 1031, LR 0.070868 Loss 3.531217, Accuracy 93.207%\n",
      "Epoch 36, Batch 1032, LR 0.070823 Loss 3.531913, Accuracy 93.201%\n",
      "Epoch 36, Batch 1033, LR 0.070779 Loss 3.532668, Accuracy 93.201%\n",
      "Epoch 36, Batch 1034, LR 0.070734 Loss 3.533495, Accuracy 93.200%\n",
      "Epoch 36, Batch 1035, LR 0.070690 Loss 3.533456, Accuracy 93.199%\n",
      "Epoch 36, Batch 1036, LR 0.070645 Loss 3.533380, Accuracy 93.200%\n",
      "Epoch 36, Batch 1037, LR 0.070601 Loss 3.533267, Accuracy 93.203%\n",
      "Epoch 36, Batch 1038, LR 0.070557 Loss 3.533195, Accuracy 93.203%\n",
      "Epoch 36, Batch 1039, LR 0.070512 Loss 3.533562, Accuracy 93.202%\n",
      "Epoch 36, Batch 1040, LR 0.070468 Loss 3.533548, Accuracy 93.203%\n",
      "Epoch 36, Batch 1041, LR 0.070424 Loss 3.533318, Accuracy 93.204%\n",
      "Epoch 36, Batch 1042, LR 0.070379 Loss 3.533315, Accuracy 93.206%\n",
      "Epoch 36, Batch 1043, LR 0.070335 Loss 3.532850, Accuracy 93.208%\n",
      "Epoch 36, Batch 1044, LR 0.070291 Loss 3.533207, Accuracy 93.208%\n",
      "Epoch 36, Batch 1045, LR 0.070246 Loss 3.533204, Accuracy 93.209%\n",
      "Epoch 36, Batch 1046, LR 0.070202 Loss 3.532884, Accuracy 93.212%\n",
      "Epoch 36, Batch 1047, LR 0.070158 Loss 3.532538, Accuracy 93.211%\n",
      "Epoch 36, Loss (train set) 3.532538, Accuracy (train set) 93.211%\n",
      "Epoch 37, Batch 1, LR 0.070114 Loss 3.173484, Accuracy 94.531%\n",
      "Epoch 37, Batch 2, LR 0.070070 Loss 3.664153, Accuracy 92.188%\n",
      "Epoch 37, Batch 3, LR 0.070025 Loss 3.575803, Accuracy 93.229%\n",
      "Epoch 37, Batch 4, LR 0.069981 Loss 3.455091, Accuracy 93.164%\n",
      "Epoch 37, Batch 5, LR 0.069937 Loss 3.445626, Accuracy 92.812%\n",
      "Epoch 37, Batch 6, LR 0.069893 Loss 3.467974, Accuracy 92.969%\n",
      "Epoch 37, Batch 7, LR 0.069849 Loss 3.532879, Accuracy 92.857%\n",
      "Epoch 37, Batch 8, LR 0.069804 Loss 3.493367, Accuracy 93.066%\n",
      "Epoch 37, Batch 9, LR 0.069760 Loss 3.460411, Accuracy 93.229%\n",
      "Epoch 37, Batch 10, LR 0.069716 Loss 3.397695, Accuracy 93.516%\n",
      "Epoch 37, Batch 11, LR 0.069672 Loss 3.405485, Accuracy 93.821%\n",
      "Epoch 37, Batch 12, LR 0.069628 Loss 3.339112, Accuracy 94.076%\n",
      "Epoch 37, Batch 13, LR 0.069584 Loss 3.382783, Accuracy 93.990%\n",
      "Epoch 37, Batch 14, LR 0.069540 Loss 3.440164, Accuracy 93.471%\n",
      "Epoch 37, Batch 15, LR 0.069496 Loss 3.470877, Accuracy 93.542%\n",
      "Epoch 37, Batch 16, LR 0.069452 Loss 3.489870, Accuracy 93.408%\n",
      "Epoch 37, Batch 17, LR 0.069408 Loss 3.481507, Accuracy 93.428%\n",
      "Epoch 37, Batch 18, LR 0.069364 Loss 3.506934, Accuracy 93.229%\n",
      "Epoch 37, Batch 19, LR 0.069320 Loss 3.492818, Accuracy 93.257%\n",
      "Epoch 37, Batch 20, LR 0.069276 Loss 3.490898, Accuracy 93.281%\n",
      "Epoch 37, Batch 21, LR 0.069232 Loss 3.486941, Accuracy 93.378%\n",
      "Epoch 37, Batch 22, LR 0.069188 Loss 3.473791, Accuracy 93.359%\n",
      "Epoch 37, Batch 23, LR 0.069144 Loss 3.498663, Accuracy 93.240%\n",
      "Epoch 37, Batch 24, LR 0.069100 Loss 3.505046, Accuracy 93.229%\n",
      "Epoch 37, Batch 25, LR 0.069056 Loss 3.517343, Accuracy 93.312%\n",
      "Epoch 37, Batch 26, LR 0.069012 Loss 3.487860, Accuracy 93.419%\n",
      "Epoch 37, Batch 27, LR 0.068968 Loss 3.491316, Accuracy 93.519%\n",
      "Epoch 37, Batch 28, LR 0.068925 Loss 3.480535, Accuracy 93.583%\n",
      "Epoch 37, Batch 29, LR 0.068881 Loss 3.467476, Accuracy 93.615%\n",
      "Epoch 37, Batch 30, LR 0.068837 Loss 3.460244, Accuracy 93.646%\n",
      "Epoch 37, Batch 31, LR 0.068793 Loss 3.451120, Accuracy 93.725%\n",
      "Epoch 37, Batch 32, LR 0.068749 Loss 3.444988, Accuracy 93.750%\n",
      "Epoch 37, Batch 33, LR 0.068705 Loss 3.430418, Accuracy 93.845%\n",
      "Epoch 37, Batch 34, LR 0.068662 Loss 3.427238, Accuracy 93.842%\n",
      "Epoch 37, Batch 35, LR 0.068618 Loss 3.419021, Accuracy 93.817%\n",
      "Epoch 37, Batch 36, LR 0.068574 Loss 3.424431, Accuracy 93.793%\n",
      "Epoch 37, Batch 37, LR 0.068530 Loss 3.446913, Accuracy 93.729%\n",
      "Epoch 37, Batch 38, LR 0.068487 Loss 3.446924, Accuracy 93.709%\n",
      "Epoch 37, Batch 39, LR 0.068443 Loss 3.461936, Accuracy 93.550%\n",
      "Epoch 37, Batch 40, LR 0.068399 Loss 3.454163, Accuracy 93.496%\n",
      "Epoch 37, Batch 41, LR 0.068355 Loss 3.476869, Accuracy 93.350%\n",
      "Epoch 37, Batch 42, LR 0.068312 Loss 3.467389, Accuracy 93.415%\n",
      "Epoch 37, Batch 43, LR 0.068268 Loss 3.470381, Accuracy 93.441%\n",
      "Epoch 37, Batch 44, LR 0.068224 Loss 3.470863, Accuracy 93.448%\n",
      "Epoch 37, Batch 45, LR 0.068181 Loss 3.479813, Accuracy 93.420%\n",
      "Epoch 37, Batch 46, LR 0.068137 Loss 3.479606, Accuracy 93.393%\n",
      "Epoch 37, Batch 47, LR 0.068094 Loss 3.467712, Accuracy 93.467%\n",
      "Epoch 37, Batch 48, LR 0.068050 Loss 3.473687, Accuracy 93.408%\n",
      "Epoch 37, Batch 49, LR 0.068006 Loss 3.465744, Accuracy 93.463%\n",
      "Epoch 37, Batch 50, LR 0.067963 Loss 3.459525, Accuracy 93.484%\n",
      "Epoch 37, Batch 51, LR 0.067919 Loss 3.464611, Accuracy 93.474%\n",
      "Epoch 37, Batch 52, LR 0.067876 Loss 3.455081, Accuracy 93.525%\n",
      "Epoch 37, Batch 53, LR 0.067832 Loss 3.449566, Accuracy 93.573%\n",
      "Epoch 37, Batch 54, LR 0.067789 Loss 3.448572, Accuracy 93.576%\n",
      "Epoch 37, Batch 55, LR 0.067745 Loss 3.443015, Accuracy 93.565%\n",
      "Epoch 37, Batch 56, LR 0.067702 Loss 3.441674, Accuracy 93.569%\n",
      "Epoch 37, Batch 57, LR 0.067658 Loss 3.448689, Accuracy 93.531%\n",
      "Epoch 37, Batch 58, LR 0.067615 Loss 3.452892, Accuracy 93.561%\n",
      "Epoch 37, Batch 59, LR 0.067571 Loss 3.447226, Accuracy 93.578%\n",
      "Epoch 37, Batch 60, LR 0.067528 Loss 3.451820, Accuracy 93.529%\n",
      "Epoch 37, Batch 61, LR 0.067484 Loss 3.449610, Accuracy 93.532%\n",
      "Epoch 37, Batch 62, LR 0.067441 Loss 3.453585, Accuracy 93.511%\n",
      "Epoch 37, Batch 63, LR 0.067398 Loss 3.448737, Accuracy 93.552%\n",
      "Epoch 37, Batch 64, LR 0.067354 Loss 3.442859, Accuracy 93.616%\n",
      "Epoch 37, Batch 65, LR 0.067311 Loss 3.441335, Accuracy 93.594%\n",
      "Epoch 37, Batch 66, LR 0.067268 Loss 3.448090, Accuracy 93.549%\n",
      "Epoch 37, Batch 67, LR 0.067224 Loss 3.446659, Accuracy 93.552%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 68, LR 0.067181 Loss 3.437977, Accuracy 93.624%\n",
      "Epoch 37, Batch 69, LR 0.067138 Loss 3.431654, Accuracy 93.648%\n",
      "Epoch 37, Batch 70, LR 0.067094 Loss 3.434190, Accuracy 93.616%\n",
      "Epoch 37, Batch 71, LR 0.067051 Loss 3.429534, Accuracy 93.629%\n",
      "Epoch 37, Batch 72, LR 0.067008 Loss 3.434674, Accuracy 93.620%\n",
      "Epoch 37, Batch 73, LR 0.066964 Loss 3.431292, Accuracy 93.611%\n",
      "Epoch 37, Batch 74, LR 0.066921 Loss 3.426039, Accuracy 93.623%\n",
      "Epoch 37, Batch 75, LR 0.066878 Loss 3.432493, Accuracy 93.542%\n",
      "Epoch 37, Batch 76, LR 0.066835 Loss 3.432582, Accuracy 93.524%\n",
      "Epoch 37, Batch 77, LR 0.066792 Loss 3.424368, Accuracy 93.567%\n",
      "Epoch 37, Batch 78, LR 0.066748 Loss 3.428024, Accuracy 93.590%\n",
      "Epoch 37, Batch 79, LR 0.066705 Loss 3.427172, Accuracy 93.621%\n",
      "Epoch 37, Batch 80, LR 0.066662 Loss 3.423688, Accuracy 93.643%\n",
      "Epoch 37, Batch 81, LR 0.066619 Loss 3.421173, Accuracy 93.654%\n",
      "Epoch 37, Batch 82, LR 0.066576 Loss 3.431789, Accuracy 93.588%\n",
      "Epoch 37, Batch 83, LR 0.066533 Loss 3.430067, Accuracy 93.637%\n",
      "Epoch 37, Batch 84, LR 0.066490 Loss 3.426638, Accuracy 93.676%\n",
      "Epoch 37, Batch 85, LR 0.066446 Loss 3.432414, Accuracy 93.686%\n",
      "Epoch 37, Batch 86, LR 0.066403 Loss 3.442586, Accuracy 93.632%\n",
      "Epoch 37, Batch 87, LR 0.066360 Loss 3.441094, Accuracy 93.642%\n",
      "Epoch 37, Batch 88, LR 0.066317 Loss 3.444256, Accuracy 93.643%\n",
      "Epoch 37, Batch 89, LR 0.066274 Loss 3.439827, Accuracy 93.671%\n",
      "Epoch 37, Batch 90, LR 0.066231 Loss 3.444037, Accuracy 93.663%\n",
      "Epoch 37, Batch 91, LR 0.066188 Loss 3.444737, Accuracy 93.664%\n",
      "Epoch 37, Batch 92, LR 0.066145 Loss 3.444401, Accuracy 93.657%\n",
      "Epoch 37, Batch 93, LR 0.066102 Loss 3.447154, Accuracy 93.624%\n",
      "Epoch 37, Batch 94, LR 0.066059 Loss 3.446611, Accuracy 93.600%\n",
      "Epoch 37, Batch 95, LR 0.066016 Loss 3.452799, Accuracy 93.561%\n",
      "Epoch 37, Batch 96, LR 0.065973 Loss 3.455194, Accuracy 93.538%\n",
      "Epoch 37, Batch 97, LR 0.065930 Loss 3.451430, Accuracy 93.557%\n",
      "Epoch 37, Batch 98, LR 0.065887 Loss 3.453393, Accuracy 93.551%\n",
      "Epoch 37, Batch 99, LR 0.065845 Loss 3.447198, Accuracy 93.584%\n",
      "Epoch 37, Batch 100, LR 0.065802 Loss 3.441164, Accuracy 93.594%\n",
      "Epoch 37, Batch 101, LR 0.065759 Loss 3.449185, Accuracy 93.541%\n",
      "Epoch 37, Batch 102, LR 0.065716 Loss 3.448537, Accuracy 93.551%\n",
      "Epoch 37, Batch 103, LR 0.065673 Loss 3.444983, Accuracy 93.538%\n",
      "Epoch 37, Batch 104, LR 0.065630 Loss 3.443589, Accuracy 93.517%\n",
      "Epoch 37, Batch 105, LR 0.065587 Loss 3.447403, Accuracy 93.504%\n",
      "Epoch 37, Batch 106, LR 0.065545 Loss 3.449109, Accuracy 93.499%\n",
      "Epoch 37, Batch 107, LR 0.065502 Loss 3.444460, Accuracy 93.524%\n",
      "Epoch 37, Batch 108, LR 0.065459 Loss 3.446219, Accuracy 93.511%\n",
      "Epoch 37, Batch 109, LR 0.065416 Loss 3.445800, Accuracy 93.506%\n",
      "Epoch 37, Batch 110, LR 0.065373 Loss 3.448869, Accuracy 93.480%\n",
      "Epoch 37, Batch 111, LR 0.065331 Loss 3.448345, Accuracy 93.504%\n",
      "Epoch 37, Batch 112, LR 0.065288 Loss 3.453511, Accuracy 93.471%\n",
      "Epoch 37, Batch 113, LR 0.065245 Loss 3.447830, Accuracy 93.487%\n",
      "Epoch 37, Batch 114, LR 0.065203 Loss 3.450210, Accuracy 93.476%\n",
      "Epoch 37, Batch 115, LR 0.065160 Loss 3.448910, Accuracy 93.478%\n",
      "Epoch 37, Batch 116, LR 0.065117 Loss 3.450088, Accuracy 93.467%\n",
      "Epoch 37, Batch 117, LR 0.065075 Loss 3.449847, Accuracy 93.470%\n",
      "Epoch 37, Batch 118, LR 0.065032 Loss 3.455208, Accuracy 93.432%\n",
      "Epoch 37, Batch 119, LR 0.064989 Loss 3.457079, Accuracy 93.402%\n",
      "Epoch 37, Batch 120, LR 0.064947 Loss 3.459550, Accuracy 93.379%\n",
      "Epoch 37, Batch 121, LR 0.064904 Loss 3.458804, Accuracy 93.376%\n",
      "Epoch 37, Batch 122, LR 0.064861 Loss 3.460979, Accuracy 93.379%\n",
      "Epoch 37, Batch 123, LR 0.064819 Loss 3.460539, Accuracy 93.356%\n",
      "Epoch 37, Batch 124, LR 0.064776 Loss 3.465005, Accuracy 93.340%\n",
      "Epoch 37, Batch 125, LR 0.064734 Loss 3.463522, Accuracy 93.338%\n",
      "Epoch 37, Batch 126, LR 0.064691 Loss 3.459408, Accuracy 93.359%\n",
      "Epoch 37, Batch 127, LR 0.064649 Loss 3.459972, Accuracy 93.319%\n",
      "Epoch 37, Batch 128, LR 0.064606 Loss 3.460855, Accuracy 93.311%\n",
      "Epoch 37, Batch 129, LR 0.064564 Loss 3.466308, Accuracy 93.278%\n",
      "Epoch 37, Batch 130, LR 0.064521 Loss 3.465159, Accuracy 93.281%\n",
      "Epoch 37, Batch 131, LR 0.064479 Loss 3.472562, Accuracy 93.261%\n",
      "Epoch 37, Batch 132, LR 0.064436 Loss 3.475067, Accuracy 93.277%\n",
      "Epoch 37, Batch 133, LR 0.064394 Loss 3.472177, Accuracy 93.298%\n",
      "Epoch 37, Batch 134, LR 0.064351 Loss 3.470271, Accuracy 93.319%\n",
      "Epoch 37, Batch 135, LR 0.064309 Loss 3.470377, Accuracy 93.328%\n",
      "Epoch 37, Batch 136, LR 0.064267 Loss 3.471166, Accuracy 93.336%\n",
      "Epoch 37, Batch 137, LR 0.064224 Loss 3.470223, Accuracy 93.334%\n",
      "Epoch 37, Batch 138, LR 0.064182 Loss 3.471785, Accuracy 93.320%\n",
      "Epoch 37, Batch 139, LR 0.064139 Loss 3.465624, Accuracy 93.345%\n",
      "Epoch 37, Batch 140, LR 0.064097 Loss 3.466894, Accuracy 93.337%\n",
      "Epoch 37, Batch 141, LR 0.064055 Loss 3.472544, Accuracy 93.296%\n",
      "Epoch 37, Batch 142, LR 0.064012 Loss 3.473511, Accuracy 93.293%\n",
      "Epoch 37, Batch 143, LR 0.063970 Loss 3.475100, Accuracy 93.275%\n",
      "Epoch 37, Batch 144, LR 0.063928 Loss 3.476245, Accuracy 93.278%\n",
      "Epoch 37, Batch 145, LR 0.063886 Loss 3.473609, Accuracy 93.287%\n",
      "Epoch 37, Batch 146, LR 0.063843 Loss 3.471561, Accuracy 93.306%\n",
      "Epoch 37, Batch 147, LR 0.063801 Loss 3.475063, Accuracy 93.293%\n",
      "Epoch 37, Batch 148, LR 0.063759 Loss 3.474200, Accuracy 93.296%\n",
      "Epoch 37, Batch 149, LR 0.063717 Loss 3.474995, Accuracy 93.273%\n",
      "Epoch 37, Batch 150, LR 0.063674 Loss 3.471773, Accuracy 93.271%\n",
      "Epoch 37, Batch 151, LR 0.063632 Loss 3.466548, Accuracy 93.290%\n",
      "Epoch 37, Batch 152, LR 0.063590 Loss 3.467604, Accuracy 93.287%\n",
      "Epoch 37, Batch 153, LR 0.063548 Loss 3.467960, Accuracy 93.280%\n",
      "Epoch 37, Batch 154, LR 0.063506 Loss 3.474227, Accuracy 93.253%\n",
      "Epoch 37, Batch 155, LR 0.063464 Loss 3.478342, Accuracy 93.251%\n",
      "Epoch 37, Batch 156, LR 0.063421 Loss 3.478737, Accuracy 93.249%\n",
      "Epoch 37, Batch 157, LR 0.063379 Loss 3.479116, Accuracy 93.223%\n",
      "Epoch 37, Batch 158, LR 0.063337 Loss 3.481972, Accuracy 93.226%\n",
      "Epoch 37, Batch 159, LR 0.063295 Loss 3.480586, Accuracy 93.229%\n",
      "Epoch 37, Batch 160, LR 0.063253 Loss 3.480476, Accuracy 93.228%\n",
      "Epoch 37, Batch 161, LR 0.063211 Loss 3.478855, Accuracy 93.236%\n",
      "Epoch 37, Batch 162, LR 0.063169 Loss 3.479692, Accuracy 93.244%\n",
      "Epoch 37, Batch 163, LR 0.063127 Loss 3.478352, Accuracy 93.266%\n",
      "Epoch 37, Batch 164, LR 0.063085 Loss 3.480401, Accuracy 93.259%\n",
      "Epoch 37, Batch 165, LR 0.063043 Loss 3.477418, Accuracy 93.272%\n",
      "Epoch 37, Batch 166, LR 0.063001 Loss 3.480846, Accuracy 93.261%\n",
      "Epoch 37, Batch 167, LR 0.062959 Loss 3.478564, Accuracy 93.263%\n",
      "Epoch 37, Batch 168, LR 0.062917 Loss 3.479969, Accuracy 93.271%\n",
      "Epoch 37, Batch 169, LR 0.062875 Loss 3.476810, Accuracy 93.274%\n",
      "Epoch 37, Batch 170, LR 0.062833 Loss 3.474150, Accuracy 93.286%\n",
      "Epoch 37, Batch 171, LR 0.062791 Loss 3.473680, Accuracy 93.302%\n",
      "Epoch 37, Batch 172, LR 0.062749 Loss 3.471337, Accuracy 93.323%\n",
      "Epoch 37, Batch 173, LR 0.062707 Loss 3.473538, Accuracy 93.307%\n",
      "Epoch 37, Batch 174, LR 0.062665 Loss 3.472846, Accuracy 93.323%\n",
      "Epoch 37, Batch 175, LR 0.062624 Loss 3.471410, Accuracy 93.335%\n",
      "Epoch 37, Batch 176, LR 0.062582 Loss 3.469637, Accuracy 93.342%\n",
      "Epoch 37, Batch 177, LR 0.062540 Loss 3.466966, Accuracy 93.344%\n",
      "Epoch 37, Batch 178, LR 0.062498 Loss 3.465464, Accuracy 93.342%\n",
      "Epoch 37, Batch 179, LR 0.062456 Loss 3.468161, Accuracy 93.335%\n",
      "Epoch 37, Batch 180, LR 0.062414 Loss 3.466678, Accuracy 93.346%\n",
      "Epoch 37, Batch 181, LR 0.062373 Loss 3.465962, Accuracy 93.353%\n",
      "Epoch 37, Batch 182, LR 0.062331 Loss 3.464117, Accuracy 93.364%\n",
      "Epoch 37, Batch 183, LR 0.062289 Loss 3.466884, Accuracy 93.353%\n",
      "Epoch 37, Batch 184, LR 0.062247 Loss 3.468025, Accuracy 93.342%\n",
      "Epoch 37, Batch 185, LR 0.062206 Loss 3.470933, Accuracy 93.328%\n",
      "Epoch 37, Batch 186, LR 0.062164 Loss 3.470171, Accuracy 93.330%\n",
      "Epoch 37, Batch 187, LR 0.062122 Loss 3.467878, Accuracy 93.349%\n",
      "Epoch 37, Batch 188, LR 0.062081 Loss 3.472092, Accuracy 93.343%\n",
      "Epoch 37, Batch 189, LR 0.062039 Loss 3.471004, Accuracy 93.361%\n",
      "Epoch 37, Batch 190, LR 0.061997 Loss 3.473178, Accuracy 93.359%\n",
      "Epoch 37, Batch 191, LR 0.061956 Loss 3.472848, Accuracy 93.357%\n",
      "Epoch 37, Batch 192, LR 0.061914 Loss 3.476934, Accuracy 93.355%\n",
      "Epoch 37, Batch 193, LR 0.061872 Loss 3.477913, Accuracy 93.341%\n",
      "Epoch 37, Batch 194, LR 0.061831 Loss 3.476428, Accuracy 93.339%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 195, LR 0.061789 Loss 3.473436, Accuracy 93.361%\n",
      "Epoch 37, Batch 196, LR 0.061747 Loss 3.473158, Accuracy 93.355%\n",
      "Epoch 37, Batch 197, LR 0.061706 Loss 3.475680, Accuracy 93.353%\n",
      "Epoch 37, Batch 198, LR 0.061664 Loss 3.477157, Accuracy 93.355%\n",
      "Epoch 37, Batch 199, LR 0.061623 Loss 3.474459, Accuracy 93.357%\n",
      "Epoch 37, Batch 200, LR 0.061581 Loss 3.474443, Accuracy 93.340%\n",
      "Epoch 37, Batch 201, LR 0.061540 Loss 3.473233, Accuracy 93.346%\n",
      "Epoch 37, Batch 202, LR 0.061498 Loss 3.475221, Accuracy 93.352%\n",
      "Epoch 37, Batch 203, LR 0.061457 Loss 3.475120, Accuracy 93.350%\n",
      "Epoch 37, Batch 204, LR 0.061415 Loss 3.475294, Accuracy 93.348%\n",
      "Epoch 37, Batch 205, LR 0.061374 Loss 3.473878, Accuracy 93.365%\n",
      "Epoch 37, Batch 206, LR 0.061332 Loss 3.473636, Accuracy 93.363%\n",
      "Epoch 37, Batch 207, LR 0.061291 Loss 3.472944, Accuracy 93.357%\n",
      "Epoch 37, Batch 208, LR 0.061250 Loss 3.471309, Accuracy 93.356%\n",
      "Epoch 37, Batch 209, LR 0.061208 Loss 3.468885, Accuracy 93.369%\n",
      "Epoch 37, Batch 210, LR 0.061167 Loss 3.469297, Accuracy 93.367%\n",
      "Epoch 37, Batch 211, LR 0.061125 Loss 3.468328, Accuracy 93.365%\n",
      "Epoch 37, Batch 212, LR 0.061084 Loss 3.469148, Accuracy 93.370%\n",
      "Epoch 37, Batch 213, LR 0.061043 Loss 3.470247, Accuracy 93.361%\n",
      "Epoch 37, Batch 214, LR 0.061001 Loss 3.468580, Accuracy 93.381%\n",
      "Epoch 37, Batch 215, LR 0.060960 Loss 3.465428, Accuracy 93.394%\n",
      "Epoch 37, Batch 216, LR 0.060919 Loss 3.464254, Accuracy 93.410%\n",
      "Epoch 37, Batch 217, LR 0.060877 Loss 3.463138, Accuracy 93.422%\n",
      "Epoch 37, Batch 218, LR 0.060836 Loss 3.462651, Accuracy 93.420%\n",
      "Epoch 37, Batch 219, LR 0.060795 Loss 3.462600, Accuracy 93.418%\n",
      "Epoch 37, Batch 220, LR 0.060754 Loss 3.463482, Accuracy 93.413%\n",
      "Epoch 37, Batch 221, LR 0.060712 Loss 3.465802, Accuracy 93.404%\n",
      "Epoch 37, Batch 222, LR 0.060671 Loss 3.467633, Accuracy 93.384%\n",
      "Epoch 37, Batch 223, LR 0.060630 Loss 3.468576, Accuracy 93.386%\n",
      "Epoch 37, Batch 224, LR 0.060589 Loss 3.468933, Accuracy 93.384%\n",
      "Epoch 37, Batch 225, LR 0.060547 Loss 3.467644, Accuracy 93.396%\n",
      "Epoch 37, Batch 226, LR 0.060506 Loss 3.465866, Accuracy 93.408%\n",
      "Epoch 37, Batch 227, LR 0.060465 Loss 3.465668, Accuracy 93.406%\n",
      "Epoch 37, Batch 228, LR 0.060424 Loss 3.468321, Accuracy 93.411%\n",
      "Epoch 37, Batch 229, LR 0.060383 Loss 3.469875, Accuracy 93.405%\n",
      "Epoch 37, Batch 230, LR 0.060342 Loss 3.469384, Accuracy 93.407%\n",
      "Epoch 37, Batch 231, LR 0.060301 Loss 3.469683, Accuracy 93.402%\n",
      "Epoch 37, Batch 232, LR 0.060260 Loss 3.467167, Accuracy 93.410%\n",
      "Epoch 37, Batch 233, LR 0.060218 Loss 3.468223, Accuracy 93.411%\n",
      "Epoch 37, Batch 234, LR 0.060177 Loss 3.466787, Accuracy 93.416%\n",
      "Epoch 37, Batch 235, LR 0.060136 Loss 3.465966, Accuracy 93.424%\n",
      "Epoch 37, Batch 236, LR 0.060095 Loss 3.465661, Accuracy 93.436%\n",
      "Epoch 37, Batch 237, LR 0.060054 Loss 3.468867, Accuracy 93.437%\n",
      "Epoch 37, Batch 238, LR 0.060013 Loss 3.468295, Accuracy 93.441%\n",
      "Epoch 37, Batch 239, LR 0.059972 Loss 3.468704, Accuracy 93.433%\n",
      "Epoch 37, Batch 240, LR 0.059931 Loss 3.469002, Accuracy 93.438%\n",
      "Epoch 37, Batch 241, LR 0.059890 Loss 3.468171, Accuracy 93.436%\n",
      "Epoch 37, Batch 242, LR 0.059849 Loss 3.469220, Accuracy 93.440%\n",
      "Epoch 37, Batch 243, LR 0.059808 Loss 3.470675, Accuracy 93.445%\n",
      "Epoch 37, Batch 244, LR 0.059767 Loss 3.471422, Accuracy 93.439%\n",
      "Epoch 37, Batch 245, LR 0.059727 Loss 3.473485, Accuracy 93.438%\n",
      "Epoch 37, Batch 246, LR 0.059686 Loss 3.473566, Accuracy 93.436%\n",
      "Epoch 37, Batch 247, LR 0.059645 Loss 3.473728, Accuracy 93.431%\n",
      "Epoch 37, Batch 248, LR 0.059604 Loss 3.474446, Accuracy 93.426%\n",
      "Epoch 37, Batch 249, LR 0.059563 Loss 3.474497, Accuracy 93.414%\n",
      "Epoch 37, Batch 250, LR 0.059522 Loss 3.477751, Accuracy 93.397%\n",
      "Epoch 37, Batch 251, LR 0.059481 Loss 3.478621, Accuracy 93.401%\n",
      "Epoch 37, Batch 252, LR 0.059441 Loss 3.478638, Accuracy 93.393%\n",
      "Epoch 37, Batch 253, LR 0.059400 Loss 3.477253, Accuracy 93.407%\n",
      "Epoch 37, Batch 254, LR 0.059359 Loss 3.475823, Accuracy 93.412%\n",
      "Epoch 37, Batch 255, LR 0.059318 Loss 3.477630, Accuracy 93.404%\n",
      "Epoch 37, Batch 256, LR 0.059277 Loss 3.478989, Accuracy 93.402%\n",
      "Epoch 37, Batch 257, LR 0.059237 Loss 3.480433, Accuracy 93.406%\n",
      "Epoch 37, Batch 258, LR 0.059196 Loss 3.478962, Accuracy 93.414%\n",
      "Epoch 37, Batch 259, LR 0.059155 Loss 3.481108, Accuracy 93.415%\n",
      "Epoch 37, Batch 260, LR 0.059115 Loss 3.479470, Accuracy 93.425%\n",
      "Epoch 37, Batch 261, LR 0.059074 Loss 3.479044, Accuracy 93.424%\n",
      "Epoch 37, Batch 262, LR 0.059033 Loss 3.478781, Accuracy 93.425%\n",
      "Epoch 37, Batch 263, LR 0.058992 Loss 3.481550, Accuracy 93.414%\n",
      "Epoch 37, Batch 264, LR 0.058952 Loss 3.481316, Accuracy 93.419%\n",
      "Epoch 37, Batch 265, LR 0.058911 Loss 3.481184, Accuracy 93.417%\n",
      "Epoch 37, Batch 266, LR 0.058871 Loss 3.479943, Accuracy 93.418%\n",
      "Epoch 37, Batch 267, LR 0.058830 Loss 3.481798, Accuracy 93.414%\n",
      "Epoch 37, Batch 268, LR 0.058789 Loss 3.484234, Accuracy 93.403%\n",
      "Epoch 37, Batch 269, LR 0.058749 Loss 3.485995, Accuracy 93.399%\n",
      "Epoch 37, Batch 270, LR 0.058708 Loss 3.485439, Accuracy 93.391%\n",
      "Epoch 37, Batch 271, LR 0.058668 Loss 3.485471, Accuracy 93.390%\n",
      "Epoch 37, Batch 272, LR 0.058627 Loss 3.486499, Accuracy 93.385%\n",
      "Epoch 37, Batch 273, LR 0.058587 Loss 3.485286, Accuracy 93.387%\n",
      "Epoch 37, Batch 274, LR 0.058546 Loss 3.482374, Accuracy 93.396%\n",
      "Epoch 37, Batch 275, LR 0.058506 Loss 3.483567, Accuracy 93.375%\n",
      "Epoch 37, Batch 276, LR 0.058465 Loss 3.484102, Accuracy 93.379%\n",
      "Epoch 37, Batch 277, LR 0.058425 Loss 3.485226, Accuracy 93.386%\n",
      "Epoch 37, Batch 278, LR 0.058384 Loss 3.483865, Accuracy 93.390%\n",
      "Epoch 37, Batch 279, LR 0.058344 Loss 3.484864, Accuracy 93.389%\n",
      "Epoch 37, Batch 280, LR 0.058303 Loss 3.489271, Accuracy 93.368%\n",
      "Epoch 37, Batch 281, LR 0.058263 Loss 3.492252, Accuracy 93.364%\n",
      "Epoch 37, Batch 282, LR 0.058222 Loss 3.490419, Accuracy 93.382%\n",
      "Epoch 37, Batch 283, LR 0.058182 Loss 3.491667, Accuracy 93.386%\n",
      "Epoch 37, Batch 284, LR 0.058142 Loss 3.490706, Accuracy 93.384%\n",
      "Epoch 37, Batch 285, LR 0.058101 Loss 3.491185, Accuracy 93.383%\n",
      "Epoch 37, Batch 286, LR 0.058061 Loss 3.491191, Accuracy 93.381%\n",
      "Epoch 37, Batch 287, LR 0.058021 Loss 3.492439, Accuracy 93.377%\n",
      "Epoch 37, Batch 288, LR 0.057980 Loss 3.493208, Accuracy 93.378%\n",
      "Epoch 37, Batch 289, LR 0.057940 Loss 3.496192, Accuracy 93.366%\n",
      "Epoch 37, Batch 290, LR 0.057900 Loss 3.495312, Accuracy 93.370%\n",
      "Epoch 37, Batch 291, LR 0.057859 Loss 3.496030, Accuracy 93.363%\n",
      "Epoch 37, Batch 292, LR 0.057819 Loss 3.498688, Accuracy 93.367%\n",
      "Epoch 37, Batch 293, LR 0.057779 Loss 3.498369, Accuracy 93.369%\n",
      "Epoch 37, Batch 294, LR 0.057739 Loss 3.498140, Accuracy 93.373%\n",
      "Epoch 37, Batch 295, LR 0.057698 Loss 3.498022, Accuracy 93.377%\n",
      "Epoch 37, Batch 296, LR 0.057658 Loss 3.498220, Accuracy 93.375%\n",
      "Epoch 37, Batch 297, LR 0.057618 Loss 3.497446, Accuracy 93.371%\n",
      "Epoch 37, Batch 298, LR 0.057578 Loss 3.496544, Accuracy 93.372%\n",
      "Epoch 37, Batch 299, LR 0.057538 Loss 3.500603, Accuracy 93.361%\n",
      "Epoch 37, Batch 300, LR 0.057497 Loss 3.499539, Accuracy 93.365%\n",
      "Epoch 37, Batch 301, LR 0.057457 Loss 3.500654, Accuracy 93.366%\n",
      "Epoch 37, Batch 302, LR 0.057417 Loss 3.501562, Accuracy 93.362%\n",
      "Epoch 37, Batch 303, LR 0.057377 Loss 3.502073, Accuracy 93.356%\n",
      "Epoch 37, Batch 304, LR 0.057337 Loss 3.503256, Accuracy 93.349%\n",
      "Epoch 37, Batch 305, LR 0.057297 Loss 3.501600, Accuracy 93.361%\n",
      "Epoch 37, Batch 306, LR 0.057257 Loss 3.500654, Accuracy 93.357%\n",
      "Epoch 37, Batch 307, LR 0.057217 Loss 3.500977, Accuracy 93.358%\n",
      "Epoch 37, Batch 308, LR 0.057177 Loss 3.499825, Accuracy 93.362%\n",
      "Epoch 37, Batch 309, LR 0.057137 Loss 3.499284, Accuracy 93.366%\n",
      "Epoch 37, Batch 310, LR 0.057096 Loss 3.499276, Accuracy 93.377%\n",
      "Epoch 37, Batch 311, LR 0.057056 Loss 3.500329, Accuracy 93.366%\n",
      "Epoch 37, Batch 312, LR 0.057016 Loss 3.497153, Accuracy 93.382%\n",
      "Epoch 37, Batch 313, LR 0.056977 Loss 3.498537, Accuracy 93.371%\n",
      "Epoch 37, Batch 314, LR 0.056937 Loss 3.499020, Accuracy 93.372%\n",
      "Epoch 37, Batch 315, LR 0.056897 Loss 3.497603, Accuracy 93.373%\n",
      "Epoch 37, Batch 316, LR 0.056857 Loss 3.497218, Accuracy 93.374%\n",
      "Epoch 37, Batch 317, LR 0.056817 Loss 3.497291, Accuracy 93.363%\n",
      "Epoch 37, Batch 318, LR 0.056777 Loss 3.499333, Accuracy 93.350%\n",
      "Epoch 37, Batch 319, LR 0.056737 Loss 3.500327, Accuracy 93.348%\n",
      "Epoch 37, Batch 320, LR 0.056697 Loss 3.499472, Accuracy 93.347%\n",
      "Epoch 37, Batch 321, LR 0.056657 Loss 3.501055, Accuracy 93.329%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 322, LR 0.056617 Loss 3.499094, Accuracy 93.340%\n",
      "Epoch 37, Batch 323, LR 0.056577 Loss 3.498959, Accuracy 93.341%\n",
      "Epoch 37, Batch 324, LR 0.056538 Loss 3.497982, Accuracy 93.335%\n",
      "Epoch 37, Batch 325, LR 0.056498 Loss 3.497940, Accuracy 93.334%\n",
      "Epoch 37, Batch 326, LR 0.056458 Loss 3.498057, Accuracy 93.343%\n",
      "Epoch 37, Batch 327, LR 0.056418 Loss 3.498335, Accuracy 93.341%\n",
      "Epoch 37, Batch 328, LR 0.056378 Loss 3.496440, Accuracy 93.350%\n",
      "Epoch 37, Batch 329, LR 0.056339 Loss 3.495331, Accuracy 93.353%\n",
      "Epoch 37, Batch 330, LR 0.056299 Loss 3.495982, Accuracy 93.350%\n",
      "Epoch 37, Batch 331, LR 0.056259 Loss 3.496127, Accuracy 93.351%\n",
      "Epoch 37, Batch 332, LR 0.056219 Loss 3.497330, Accuracy 93.350%\n",
      "Epoch 37, Batch 333, LR 0.056180 Loss 3.496248, Accuracy 93.346%\n",
      "Epoch 37, Batch 334, LR 0.056140 Loss 3.497174, Accuracy 93.341%\n",
      "Epoch 37, Batch 335, LR 0.056100 Loss 3.496100, Accuracy 93.342%\n",
      "Epoch 37, Batch 336, LR 0.056061 Loss 3.495360, Accuracy 93.343%\n",
      "Epoch 37, Batch 337, LR 0.056021 Loss 3.496304, Accuracy 93.340%\n",
      "Epoch 37, Batch 338, LR 0.055981 Loss 3.496185, Accuracy 93.343%\n",
      "Epoch 37, Batch 339, LR 0.055942 Loss 3.494424, Accuracy 93.358%\n",
      "Epoch 37, Batch 340, LR 0.055902 Loss 3.494605, Accuracy 93.362%\n",
      "Epoch 37, Batch 341, LR 0.055863 Loss 3.493873, Accuracy 93.370%\n",
      "Epoch 37, Batch 342, LR 0.055823 Loss 3.492533, Accuracy 93.380%\n",
      "Epoch 37, Batch 343, LR 0.055783 Loss 3.491888, Accuracy 93.383%\n",
      "Epoch 37, Batch 344, LR 0.055744 Loss 3.490757, Accuracy 93.384%\n",
      "Epoch 37, Batch 345, LR 0.055704 Loss 3.490547, Accuracy 93.381%\n",
      "Epoch 37, Batch 346, LR 0.055665 Loss 3.490961, Accuracy 93.380%\n",
      "Epoch 37, Batch 347, LR 0.055625 Loss 3.491296, Accuracy 93.376%\n",
      "Epoch 37, Batch 348, LR 0.055586 Loss 3.491029, Accuracy 93.377%\n",
      "Epoch 37, Batch 349, LR 0.055546 Loss 3.490226, Accuracy 93.385%\n",
      "Epoch 37, Batch 350, LR 0.055507 Loss 3.490486, Accuracy 93.375%\n",
      "Epoch 37, Batch 351, LR 0.055467 Loss 3.489903, Accuracy 93.383%\n",
      "Epoch 37, Batch 352, LR 0.055428 Loss 3.490375, Accuracy 93.386%\n",
      "Epoch 37, Batch 353, LR 0.055388 Loss 3.489403, Accuracy 93.394%\n",
      "Epoch 37, Batch 354, LR 0.055349 Loss 3.488035, Accuracy 93.397%\n",
      "Epoch 37, Batch 355, LR 0.055310 Loss 3.486756, Accuracy 93.400%\n",
      "Epoch 37, Batch 356, LR 0.055270 Loss 3.486996, Accuracy 93.399%\n",
      "Epoch 37, Batch 357, LR 0.055231 Loss 3.485963, Accuracy 93.393%\n",
      "Epoch 37, Batch 358, LR 0.055191 Loss 3.485655, Accuracy 93.396%\n",
      "Epoch 37, Batch 359, LR 0.055152 Loss 3.484831, Accuracy 93.406%\n",
      "Epoch 37, Batch 360, LR 0.055113 Loss 3.485497, Accuracy 93.398%\n",
      "Epoch 37, Batch 361, LR 0.055073 Loss 3.486258, Accuracy 93.386%\n",
      "Epoch 37, Batch 362, LR 0.055034 Loss 3.486635, Accuracy 93.385%\n",
      "Epoch 37, Batch 363, LR 0.054995 Loss 3.484369, Accuracy 93.391%\n",
      "Epoch 37, Batch 364, LR 0.054956 Loss 3.484425, Accuracy 93.400%\n",
      "Epoch 37, Batch 365, LR 0.054916 Loss 3.484311, Accuracy 93.395%\n",
      "Epoch 37, Batch 366, LR 0.054877 Loss 3.483484, Accuracy 93.396%\n",
      "Epoch 37, Batch 367, LR 0.054838 Loss 3.482941, Accuracy 93.394%\n",
      "Epoch 37, Batch 368, LR 0.054799 Loss 3.482717, Accuracy 93.393%\n",
      "Epoch 37, Batch 369, LR 0.054759 Loss 3.482170, Accuracy 93.390%\n",
      "Epoch 37, Batch 370, LR 0.054720 Loss 3.483422, Accuracy 93.391%\n",
      "Epoch 37, Batch 371, LR 0.054681 Loss 3.484509, Accuracy 93.390%\n",
      "Epoch 37, Batch 372, LR 0.054642 Loss 3.485523, Accuracy 93.385%\n",
      "Epoch 37, Batch 373, LR 0.054603 Loss 3.485662, Accuracy 93.390%\n",
      "Epoch 37, Batch 374, LR 0.054563 Loss 3.484393, Accuracy 93.397%\n",
      "Epoch 37, Batch 375, LR 0.054524 Loss 3.482863, Accuracy 93.400%\n",
      "Epoch 37, Batch 376, LR 0.054485 Loss 3.481873, Accuracy 93.407%\n",
      "Epoch 37, Batch 377, LR 0.054446 Loss 3.483368, Accuracy 93.400%\n",
      "Epoch 37, Batch 378, LR 0.054407 Loss 3.483118, Accuracy 93.397%\n",
      "Epoch 37, Batch 379, LR 0.054368 Loss 3.483694, Accuracy 93.398%\n",
      "Epoch 37, Batch 380, LR 0.054329 Loss 3.483410, Accuracy 93.405%\n",
      "Epoch 37, Batch 381, LR 0.054290 Loss 3.483747, Accuracy 93.403%\n",
      "Epoch 37, Batch 382, LR 0.054251 Loss 3.482826, Accuracy 93.406%\n",
      "Epoch 37, Batch 383, LR 0.054212 Loss 3.484890, Accuracy 93.391%\n",
      "Epoch 37, Batch 384, LR 0.054173 Loss 3.484463, Accuracy 93.394%\n",
      "Epoch 37, Batch 385, LR 0.054134 Loss 3.484995, Accuracy 93.397%\n",
      "Epoch 37, Batch 386, LR 0.054095 Loss 3.484582, Accuracy 93.402%\n",
      "Epoch 37, Batch 387, LR 0.054056 Loss 3.483335, Accuracy 93.403%\n",
      "Epoch 37, Batch 388, LR 0.054017 Loss 3.483162, Accuracy 93.408%\n",
      "Epoch 37, Batch 389, LR 0.053978 Loss 3.482583, Accuracy 93.407%\n",
      "Epoch 37, Batch 390, LR 0.053939 Loss 3.482494, Accuracy 93.405%\n",
      "Epoch 37, Batch 391, LR 0.053900 Loss 3.484213, Accuracy 93.398%\n",
      "Epoch 37, Batch 392, LR 0.053861 Loss 3.485207, Accuracy 93.399%\n",
      "Epoch 37, Batch 393, LR 0.053822 Loss 3.484403, Accuracy 93.402%\n",
      "Epoch 37, Batch 394, LR 0.053783 Loss 3.484443, Accuracy 93.401%\n",
      "Epoch 37, Batch 395, LR 0.053744 Loss 3.484677, Accuracy 93.398%\n",
      "Epoch 37, Batch 396, LR 0.053706 Loss 3.485245, Accuracy 93.395%\n",
      "Epoch 37, Batch 397, LR 0.053667 Loss 3.486009, Accuracy 93.394%\n",
      "Epoch 37, Batch 398, LR 0.053628 Loss 3.485034, Accuracy 93.397%\n",
      "Epoch 37, Batch 399, LR 0.053589 Loss 3.484529, Accuracy 93.398%\n",
      "Epoch 37, Batch 400, LR 0.053550 Loss 3.485473, Accuracy 93.391%\n",
      "Epoch 37, Batch 401, LR 0.053512 Loss 3.486209, Accuracy 93.399%\n",
      "Epoch 37, Batch 402, LR 0.053473 Loss 3.487441, Accuracy 93.400%\n",
      "Epoch 37, Batch 403, LR 0.053434 Loss 3.485717, Accuracy 93.409%\n",
      "Epoch 37, Batch 404, LR 0.053395 Loss 3.485826, Accuracy 93.410%\n",
      "Epoch 37, Batch 405, LR 0.053357 Loss 3.486141, Accuracy 93.407%\n",
      "Epoch 37, Batch 406, LR 0.053318 Loss 3.486610, Accuracy 93.404%\n",
      "Epoch 37, Batch 407, LR 0.053279 Loss 3.488282, Accuracy 93.389%\n",
      "Epoch 37, Batch 408, LR 0.053241 Loss 3.489264, Accuracy 93.380%\n",
      "Epoch 37, Batch 409, LR 0.053202 Loss 3.489055, Accuracy 93.385%\n",
      "Epoch 37, Batch 410, LR 0.053163 Loss 3.488692, Accuracy 93.380%\n",
      "Epoch 37, Batch 411, LR 0.053125 Loss 3.487899, Accuracy 93.379%\n",
      "Epoch 37, Batch 412, LR 0.053086 Loss 3.487039, Accuracy 93.386%\n",
      "Epoch 37, Batch 413, LR 0.053047 Loss 3.487467, Accuracy 93.385%\n",
      "Epoch 37, Batch 414, LR 0.053009 Loss 3.488100, Accuracy 93.378%\n",
      "Epoch 37, Batch 415, LR 0.052970 Loss 3.486686, Accuracy 93.381%\n",
      "Epoch 37, Batch 416, LR 0.052932 Loss 3.485972, Accuracy 93.384%\n",
      "Epoch 37, Batch 417, LR 0.052893 Loss 3.486815, Accuracy 93.383%\n",
      "Epoch 37, Batch 418, LR 0.052855 Loss 3.484975, Accuracy 93.393%\n",
      "Epoch 37, Batch 419, LR 0.052816 Loss 3.485761, Accuracy 93.398%\n",
      "Epoch 37, Batch 420, LR 0.052777 Loss 3.485446, Accuracy 93.397%\n",
      "Epoch 37, Batch 421, LR 0.052739 Loss 3.485907, Accuracy 93.394%\n",
      "Epoch 37, Batch 422, LR 0.052701 Loss 3.485774, Accuracy 93.391%\n",
      "Epoch 37, Batch 423, LR 0.052662 Loss 3.484200, Accuracy 93.401%\n",
      "Epoch 37, Batch 424, LR 0.052624 Loss 3.484679, Accuracy 93.393%\n",
      "Epoch 37, Batch 425, LR 0.052585 Loss 3.483968, Accuracy 93.393%\n",
      "Epoch 37, Batch 426, LR 0.052547 Loss 3.484553, Accuracy 93.381%\n",
      "Epoch 37, Batch 427, LR 0.052508 Loss 3.483877, Accuracy 93.386%\n",
      "Epoch 37, Batch 428, LR 0.052470 Loss 3.483431, Accuracy 93.387%\n",
      "Epoch 37, Batch 429, LR 0.052431 Loss 3.483333, Accuracy 93.388%\n",
      "Epoch 37, Batch 430, LR 0.052393 Loss 3.482558, Accuracy 93.388%\n",
      "Epoch 37, Batch 431, LR 0.052355 Loss 3.484388, Accuracy 93.377%\n",
      "Epoch 37, Batch 432, LR 0.052316 Loss 3.483730, Accuracy 93.377%\n",
      "Epoch 37, Batch 433, LR 0.052278 Loss 3.483066, Accuracy 93.382%\n",
      "Epoch 37, Batch 434, LR 0.052240 Loss 3.483566, Accuracy 93.381%\n",
      "Epoch 37, Batch 435, LR 0.052201 Loss 3.483933, Accuracy 93.382%\n",
      "Epoch 37, Batch 436, LR 0.052163 Loss 3.483128, Accuracy 93.388%\n",
      "Epoch 37, Batch 437, LR 0.052125 Loss 3.484009, Accuracy 93.376%\n",
      "Epoch 37, Batch 438, LR 0.052087 Loss 3.484368, Accuracy 93.374%\n",
      "Epoch 37, Batch 439, LR 0.052048 Loss 3.486765, Accuracy 93.371%\n",
      "Epoch 37, Batch 440, LR 0.052010 Loss 3.487395, Accuracy 93.370%\n",
      "Epoch 37, Batch 441, LR 0.051972 Loss 3.489743, Accuracy 93.357%\n",
      "Epoch 37, Batch 442, LR 0.051934 Loss 3.489583, Accuracy 93.358%\n",
      "Epoch 37, Batch 443, LR 0.051895 Loss 3.489471, Accuracy 93.360%\n",
      "Epoch 37, Batch 444, LR 0.051857 Loss 3.490040, Accuracy 93.363%\n",
      "Epoch 37, Batch 445, LR 0.051819 Loss 3.490925, Accuracy 93.362%\n",
      "Epoch 37, Batch 446, LR 0.051781 Loss 3.490485, Accuracy 93.370%\n",
      "Epoch 37, Batch 447, LR 0.051743 Loss 3.490853, Accuracy 93.371%\n",
      "Epoch 37, Batch 448, LR 0.051705 Loss 3.491031, Accuracy 93.363%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 449, LR 0.051667 Loss 3.491407, Accuracy 93.360%\n",
      "Epoch 37, Batch 450, LR 0.051628 Loss 3.490639, Accuracy 93.368%\n",
      "Epoch 37, Batch 451, LR 0.051590 Loss 3.488643, Accuracy 93.374%\n",
      "Epoch 37, Batch 452, LR 0.051552 Loss 3.488035, Accuracy 93.378%\n",
      "Epoch 37, Batch 453, LR 0.051514 Loss 3.487476, Accuracy 93.383%\n",
      "Epoch 37, Batch 454, LR 0.051476 Loss 3.487327, Accuracy 93.380%\n",
      "Epoch 37, Batch 455, LR 0.051438 Loss 3.488063, Accuracy 93.379%\n",
      "Epoch 37, Batch 456, LR 0.051400 Loss 3.487376, Accuracy 93.385%\n",
      "Epoch 37, Batch 457, LR 0.051362 Loss 3.487721, Accuracy 93.386%\n",
      "Epoch 37, Batch 458, LR 0.051324 Loss 3.488326, Accuracy 93.385%\n",
      "Epoch 37, Batch 459, LR 0.051286 Loss 3.489190, Accuracy 93.386%\n",
      "Epoch 37, Batch 460, LR 0.051248 Loss 3.489533, Accuracy 93.387%\n",
      "Epoch 37, Batch 461, LR 0.051210 Loss 3.489466, Accuracy 93.377%\n",
      "Epoch 37, Batch 462, LR 0.051172 Loss 3.490328, Accuracy 93.381%\n",
      "Epoch 37, Batch 463, LR 0.051134 Loss 3.489471, Accuracy 93.386%\n",
      "Epoch 37, Batch 464, LR 0.051096 Loss 3.489857, Accuracy 93.381%\n",
      "Epoch 37, Batch 465, LR 0.051059 Loss 3.489320, Accuracy 93.380%\n",
      "Epoch 37, Batch 466, LR 0.051021 Loss 3.490072, Accuracy 93.371%\n",
      "Epoch 37, Batch 467, LR 0.050983 Loss 3.490043, Accuracy 93.369%\n",
      "Epoch 37, Batch 468, LR 0.050945 Loss 3.489357, Accuracy 93.369%\n",
      "Epoch 37, Batch 469, LR 0.050907 Loss 3.488710, Accuracy 93.377%\n",
      "Epoch 37, Batch 470, LR 0.050869 Loss 3.488508, Accuracy 93.374%\n",
      "Epoch 37, Batch 471, LR 0.050831 Loss 3.487914, Accuracy 93.375%\n",
      "Epoch 37, Batch 472, LR 0.050794 Loss 3.487237, Accuracy 93.378%\n",
      "Epoch 37, Batch 473, LR 0.050756 Loss 3.488065, Accuracy 93.377%\n",
      "Epoch 37, Batch 474, LR 0.050718 Loss 3.488045, Accuracy 93.382%\n",
      "Epoch 37, Batch 475, LR 0.050680 Loss 3.488603, Accuracy 93.380%\n",
      "Epoch 37, Batch 476, LR 0.050643 Loss 3.487875, Accuracy 93.384%\n",
      "Epoch 37, Batch 477, LR 0.050605 Loss 3.488194, Accuracy 93.380%\n",
      "Epoch 37, Batch 478, LR 0.050567 Loss 3.488496, Accuracy 93.382%\n",
      "Epoch 37, Batch 479, LR 0.050529 Loss 3.488149, Accuracy 93.385%\n",
      "Epoch 37, Batch 480, LR 0.050492 Loss 3.487432, Accuracy 93.384%\n",
      "Epoch 37, Batch 481, LR 0.050454 Loss 3.487747, Accuracy 93.383%\n",
      "Epoch 37, Batch 482, LR 0.050416 Loss 3.488575, Accuracy 93.380%\n",
      "Epoch 37, Batch 483, LR 0.050379 Loss 3.486640, Accuracy 93.389%\n",
      "Epoch 37, Batch 484, LR 0.050341 Loss 3.486417, Accuracy 93.393%\n",
      "Epoch 37, Batch 485, LR 0.050304 Loss 3.486928, Accuracy 93.392%\n",
      "Epoch 37, Batch 486, LR 0.050266 Loss 3.487392, Accuracy 93.393%\n",
      "Epoch 37, Batch 487, LR 0.050228 Loss 3.487187, Accuracy 93.387%\n",
      "Epoch 37, Batch 488, LR 0.050191 Loss 3.488547, Accuracy 93.379%\n",
      "Epoch 37, Batch 489, LR 0.050153 Loss 3.489129, Accuracy 93.375%\n",
      "Epoch 37, Batch 490, LR 0.050116 Loss 3.488556, Accuracy 93.380%\n",
      "Epoch 37, Batch 491, LR 0.050078 Loss 3.489596, Accuracy 93.378%\n",
      "Epoch 37, Batch 492, LR 0.050041 Loss 3.489460, Accuracy 93.383%\n",
      "Epoch 37, Batch 493, LR 0.050003 Loss 3.489084, Accuracy 93.381%\n",
      "Epoch 37, Batch 494, LR 0.049966 Loss 3.489536, Accuracy 93.386%\n",
      "Epoch 37, Batch 495, LR 0.049928 Loss 3.489192, Accuracy 93.390%\n",
      "Epoch 37, Batch 496, LR 0.049891 Loss 3.489948, Accuracy 93.385%\n",
      "Epoch 37, Batch 497, LR 0.049853 Loss 3.489055, Accuracy 93.387%\n",
      "Epoch 37, Batch 498, LR 0.049816 Loss 3.488249, Accuracy 93.389%\n",
      "Epoch 37, Batch 499, LR 0.049778 Loss 3.488412, Accuracy 93.390%\n",
      "Epoch 37, Batch 500, LR 0.049741 Loss 3.487487, Accuracy 93.395%\n",
      "Epoch 37, Batch 501, LR 0.049703 Loss 3.488059, Accuracy 93.398%\n",
      "Epoch 37, Batch 502, LR 0.049666 Loss 3.489202, Accuracy 93.395%\n",
      "Epoch 37, Batch 503, LR 0.049629 Loss 3.490682, Accuracy 93.388%\n",
      "Epoch 37, Batch 504, LR 0.049591 Loss 3.490083, Accuracy 93.392%\n",
      "Epoch 37, Batch 505, LR 0.049554 Loss 3.489771, Accuracy 93.397%\n",
      "Epoch 37, Batch 506, LR 0.049517 Loss 3.489958, Accuracy 93.395%\n",
      "Epoch 37, Batch 507, LR 0.049479 Loss 3.490535, Accuracy 93.402%\n",
      "Epoch 37, Batch 508, LR 0.049442 Loss 3.492095, Accuracy 93.402%\n",
      "Epoch 37, Batch 509, LR 0.049405 Loss 3.492346, Accuracy 93.403%\n",
      "Epoch 37, Batch 510, LR 0.049367 Loss 3.492575, Accuracy 93.402%\n",
      "Epoch 37, Batch 511, LR 0.049330 Loss 3.492760, Accuracy 93.404%\n",
      "Epoch 37, Batch 512, LR 0.049293 Loss 3.492807, Accuracy 93.399%\n",
      "Epoch 37, Batch 513, LR 0.049256 Loss 3.493000, Accuracy 93.398%\n",
      "Epoch 37, Batch 514, LR 0.049219 Loss 3.493260, Accuracy 93.396%\n",
      "Epoch 37, Batch 515, LR 0.049181 Loss 3.494248, Accuracy 93.395%\n",
      "Epoch 37, Batch 516, LR 0.049144 Loss 3.494295, Accuracy 93.391%\n",
      "Epoch 37, Batch 517, LR 0.049107 Loss 3.495430, Accuracy 93.384%\n",
      "Epoch 37, Batch 518, LR 0.049070 Loss 3.494585, Accuracy 93.393%\n",
      "Epoch 37, Batch 519, LR 0.049033 Loss 3.495555, Accuracy 93.389%\n",
      "Epoch 37, Batch 520, LR 0.048995 Loss 3.495959, Accuracy 93.388%\n",
      "Epoch 37, Batch 521, LR 0.048958 Loss 3.496999, Accuracy 93.384%\n",
      "Epoch 37, Batch 522, LR 0.048921 Loss 3.497967, Accuracy 93.377%\n",
      "Epoch 37, Batch 523, LR 0.048884 Loss 3.496823, Accuracy 93.383%\n",
      "Epoch 37, Batch 524, LR 0.048847 Loss 3.496468, Accuracy 93.389%\n",
      "Epoch 37, Batch 525, LR 0.048810 Loss 3.496196, Accuracy 93.388%\n",
      "Epoch 37, Batch 526, LR 0.048773 Loss 3.496639, Accuracy 93.383%\n",
      "Epoch 37, Batch 527, LR 0.048736 Loss 3.497379, Accuracy 93.375%\n",
      "Epoch 37, Batch 528, LR 0.048699 Loss 3.496897, Accuracy 93.382%\n",
      "Epoch 37, Batch 529, LR 0.048662 Loss 3.497438, Accuracy 93.378%\n",
      "Epoch 37, Batch 530, LR 0.048625 Loss 3.496160, Accuracy 93.384%\n",
      "Epoch 37, Batch 531, LR 0.048588 Loss 3.495304, Accuracy 93.394%\n",
      "Epoch 37, Batch 532, LR 0.048551 Loss 3.494603, Accuracy 93.396%\n",
      "Epoch 37, Batch 533, LR 0.048514 Loss 3.494776, Accuracy 93.386%\n",
      "Epoch 37, Batch 534, LR 0.048477 Loss 3.494534, Accuracy 93.383%\n",
      "Epoch 37, Batch 535, LR 0.048440 Loss 3.494272, Accuracy 93.383%\n",
      "Epoch 37, Batch 536, LR 0.048403 Loss 3.494173, Accuracy 93.389%\n",
      "Epoch 37, Batch 537, LR 0.048366 Loss 3.493457, Accuracy 93.394%\n",
      "Epoch 37, Batch 538, LR 0.048329 Loss 3.492199, Accuracy 93.399%\n",
      "Epoch 37, Batch 539, LR 0.048292 Loss 3.491718, Accuracy 93.402%\n",
      "Epoch 37, Batch 540, LR 0.048256 Loss 3.491645, Accuracy 93.398%\n",
      "Epoch 37, Batch 541, LR 0.048219 Loss 3.491009, Accuracy 93.401%\n",
      "Epoch 37, Batch 542, LR 0.048182 Loss 3.491816, Accuracy 93.398%\n",
      "Epoch 37, Batch 543, LR 0.048145 Loss 3.490415, Accuracy 93.405%\n",
      "Epoch 37, Batch 544, LR 0.048108 Loss 3.489947, Accuracy 93.405%\n",
      "Epoch 37, Batch 545, LR 0.048072 Loss 3.489681, Accuracy 93.405%\n",
      "Epoch 37, Batch 546, LR 0.048035 Loss 3.489992, Accuracy 93.402%\n",
      "Epoch 37, Batch 547, LR 0.047998 Loss 3.489494, Accuracy 93.406%\n",
      "Epoch 37, Batch 548, LR 0.047961 Loss 3.490397, Accuracy 93.398%\n",
      "Epoch 37, Batch 549, LR 0.047924 Loss 3.489640, Accuracy 93.397%\n",
      "Epoch 37, Batch 550, LR 0.047888 Loss 3.489578, Accuracy 93.395%\n",
      "Epoch 37, Batch 551, LR 0.047851 Loss 3.490172, Accuracy 93.390%\n",
      "Epoch 37, Batch 552, LR 0.047814 Loss 3.490112, Accuracy 93.391%\n",
      "Epoch 37, Batch 553, LR 0.047778 Loss 3.490123, Accuracy 93.387%\n",
      "Epoch 37, Batch 554, LR 0.047741 Loss 3.489890, Accuracy 93.389%\n",
      "Epoch 37, Batch 555, LR 0.047704 Loss 3.489984, Accuracy 93.390%\n",
      "Epoch 37, Batch 556, LR 0.047668 Loss 3.489418, Accuracy 93.392%\n",
      "Epoch 37, Batch 557, LR 0.047631 Loss 3.489725, Accuracy 93.394%\n",
      "Epoch 37, Batch 558, LR 0.047594 Loss 3.489452, Accuracy 93.396%\n",
      "Epoch 37, Batch 559, LR 0.047558 Loss 3.488855, Accuracy 93.405%\n",
      "Epoch 37, Batch 560, LR 0.047521 Loss 3.489396, Accuracy 93.401%\n",
      "Epoch 37, Batch 561, LR 0.047485 Loss 3.490494, Accuracy 93.398%\n",
      "Epoch 37, Batch 562, LR 0.047448 Loss 3.490038, Accuracy 93.402%\n",
      "Epoch 37, Batch 563, LR 0.047412 Loss 3.489957, Accuracy 93.411%\n",
      "Epoch 37, Batch 564, LR 0.047375 Loss 3.489286, Accuracy 93.416%\n",
      "Epoch 37, Batch 565, LR 0.047339 Loss 3.489661, Accuracy 93.415%\n",
      "Epoch 37, Batch 566, LR 0.047302 Loss 3.489004, Accuracy 93.420%\n",
      "Epoch 37, Batch 567, LR 0.047266 Loss 3.489872, Accuracy 93.417%\n",
      "Epoch 37, Batch 568, LR 0.047229 Loss 3.489490, Accuracy 93.416%\n",
      "Epoch 37, Batch 569, LR 0.047193 Loss 3.489494, Accuracy 93.418%\n",
      "Epoch 37, Batch 570, LR 0.047156 Loss 3.489493, Accuracy 93.421%\n",
      "Epoch 37, Batch 571, LR 0.047120 Loss 3.488782, Accuracy 93.423%\n",
      "Epoch 37, Batch 572, LR 0.047083 Loss 3.488405, Accuracy 93.422%\n",
      "Epoch 37, Batch 573, LR 0.047047 Loss 3.488422, Accuracy 93.424%\n",
      "Epoch 37, Batch 574, LR 0.047011 Loss 3.486923, Accuracy 93.425%\n",
      "Epoch 37, Batch 575, LR 0.046974 Loss 3.486474, Accuracy 93.423%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 576, LR 0.046938 Loss 3.485773, Accuracy 93.426%\n",
      "Epoch 37, Batch 577, LR 0.046901 Loss 3.485437, Accuracy 93.428%\n",
      "Epoch 37, Batch 578, LR 0.046865 Loss 3.485235, Accuracy 93.427%\n",
      "Epoch 37, Batch 579, LR 0.046829 Loss 3.485124, Accuracy 93.425%\n",
      "Epoch 37, Batch 580, LR 0.046792 Loss 3.484737, Accuracy 93.427%\n",
      "Epoch 37, Batch 581, LR 0.046756 Loss 3.484716, Accuracy 93.429%\n",
      "Epoch 37, Batch 582, LR 0.046720 Loss 3.485134, Accuracy 93.425%\n",
      "Epoch 37, Batch 583, LR 0.046684 Loss 3.484709, Accuracy 93.426%\n",
      "Epoch 37, Batch 584, LR 0.046647 Loss 3.484776, Accuracy 93.428%\n",
      "Epoch 37, Batch 585, LR 0.046611 Loss 3.485012, Accuracy 93.429%\n",
      "Epoch 37, Batch 586, LR 0.046575 Loss 3.485337, Accuracy 93.421%\n",
      "Epoch 37, Batch 587, LR 0.046539 Loss 3.485611, Accuracy 93.420%\n",
      "Epoch 37, Batch 588, LR 0.046502 Loss 3.485228, Accuracy 93.422%\n",
      "Epoch 37, Batch 589, LR 0.046466 Loss 3.484741, Accuracy 93.424%\n",
      "Epoch 37, Batch 590, LR 0.046430 Loss 3.484931, Accuracy 93.424%\n",
      "Epoch 37, Batch 591, LR 0.046394 Loss 3.485042, Accuracy 93.425%\n",
      "Epoch 37, Batch 592, LR 0.046358 Loss 3.485072, Accuracy 93.424%\n",
      "Epoch 37, Batch 593, LR 0.046322 Loss 3.486625, Accuracy 93.421%\n",
      "Epoch 37, Batch 594, LR 0.046286 Loss 3.486169, Accuracy 93.424%\n",
      "Epoch 37, Batch 595, LR 0.046249 Loss 3.486893, Accuracy 93.419%\n",
      "Epoch 37, Batch 596, LR 0.046213 Loss 3.486823, Accuracy 93.416%\n",
      "Epoch 37, Batch 597, LR 0.046177 Loss 3.486661, Accuracy 93.415%\n",
      "Epoch 37, Batch 598, LR 0.046141 Loss 3.486704, Accuracy 93.414%\n",
      "Epoch 37, Batch 599, LR 0.046105 Loss 3.486577, Accuracy 93.420%\n",
      "Epoch 37, Batch 600, LR 0.046069 Loss 3.486630, Accuracy 93.419%\n",
      "Epoch 37, Batch 601, LR 0.046033 Loss 3.486931, Accuracy 93.417%\n",
      "Epoch 37, Batch 602, LR 0.045997 Loss 3.486694, Accuracy 93.422%\n",
      "Epoch 37, Batch 603, LR 0.045961 Loss 3.486151, Accuracy 93.422%\n",
      "Epoch 37, Batch 604, LR 0.045925 Loss 3.486575, Accuracy 93.420%\n",
      "Epoch 37, Batch 605, LR 0.045889 Loss 3.486921, Accuracy 93.416%\n",
      "Epoch 37, Batch 606, LR 0.045853 Loss 3.487355, Accuracy 93.416%\n",
      "Epoch 37, Batch 607, LR 0.045817 Loss 3.487204, Accuracy 93.418%\n",
      "Epoch 37, Batch 608, LR 0.045781 Loss 3.486768, Accuracy 93.416%\n",
      "Epoch 37, Batch 609, LR 0.045745 Loss 3.486857, Accuracy 93.418%\n",
      "Epoch 37, Batch 610, LR 0.045710 Loss 3.486521, Accuracy 93.421%\n",
      "Epoch 37, Batch 611, LR 0.045674 Loss 3.486030, Accuracy 93.421%\n",
      "Epoch 37, Batch 612, LR 0.045638 Loss 3.486381, Accuracy 93.422%\n",
      "Epoch 37, Batch 613, LR 0.045602 Loss 3.486360, Accuracy 93.419%\n",
      "Epoch 37, Batch 614, LR 0.045566 Loss 3.486484, Accuracy 93.419%\n",
      "Epoch 37, Batch 615, LR 0.045530 Loss 3.485507, Accuracy 93.422%\n",
      "Epoch 37, Batch 616, LR 0.045494 Loss 3.486266, Accuracy 93.416%\n",
      "Epoch 37, Batch 617, LR 0.045459 Loss 3.487530, Accuracy 93.412%\n",
      "Epoch 37, Batch 618, LR 0.045423 Loss 3.487197, Accuracy 93.412%\n",
      "Epoch 37, Batch 619, LR 0.045387 Loss 3.486976, Accuracy 93.414%\n",
      "Epoch 37, Batch 620, LR 0.045351 Loss 3.486707, Accuracy 93.419%\n",
      "Epoch 37, Batch 621, LR 0.045316 Loss 3.486922, Accuracy 93.417%\n",
      "Epoch 37, Batch 622, LR 0.045280 Loss 3.485809, Accuracy 93.421%\n",
      "Epoch 37, Batch 623, LR 0.045244 Loss 3.485826, Accuracy 93.420%\n",
      "Epoch 37, Batch 624, LR 0.045208 Loss 3.485735, Accuracy 93.422%\n",
      "Epoch 37, Batch 625, LR 0.045173 Loss 3.484752, Accuracy 93.426%\n",
      "Epoch 37, Batch 626, LR 0.045137 Loss 3.484183, Accuracy 93.433%\n",
      "Epoch 37, Batch 627, LR 0.045101 Loss 3.483638, Accuracy 93.435%\n",
      "Epoch 37, Batch 628, LR 0.045066 Loss 3.484455, Accuracy 93.434%\n",
      "Epoch 37, Batch 629, LR 0.045030 Loss 3.486017, Accuracy 93.428%\n",
      "Epoch 37, Batch 630, LR 0.044995 Loss 3.485104, Accuracy 93.434%\n",
      "Epoch 37, Batch 631, LR 0.044959 Loss 3.484918, Accuracy 93.432%\n",
      "Epoch 37, Batch 632, LR 0.044923 Loss 3.484848, Accuracy 93.437%\n",
      "Epoch 37, Batch 633, LR 0.044888 Loss 3.484015, Accuracy 93.439%\n",
      "Epoch 37, Batch 634, LR 0.044852 Loss 3.483860, Accuracy 93.439%\n",
      "Epoch 37, Batch 635, LR 0.044817 Loss 3.483078, Accuracy 93.441%\n",
      "Epoch 37, Batch 636, LR 0.044781 Loss 3.482445, Accuracy 93.444%\n",
      "Epoch 37, Batch 637, LR 0.044746 Loss 3.482338, Accuracy 93.446%\n",
      "Epoch 37, Batch 638, LR 0.044710 Loss 3.482469, Accuracy 93.444%\n",
      "Epoch 37, Batch 639, LR 0.044675 Loss 3.482049, Accuracy 93.444%\n",
      "Epoch 37, Batch 640, LR 0.044639 Loss 3.481986, Accuracy 93.450%\n",
      "Epoch 37, Batch 641, LR 0.044604 Loss 3.481765, Accuracy 93.449%\n",
      "Epoch 37, Batch 642, LR 0.044568 Loss 3.481810, Accuracy 93.448%\n",
      "Epoch 37, Batch 643, LR 0.044533 Loss 3.481741, Accuracy 93.447%\n",
      "Epoch 37, Batch 644, LR 0.044497 Loss 3.482553, Accuracy 93.447%\n",
      "Epoch 37, Batch 645, LR 0.044462 Loss 3.482239, Accuracy 93.447%\n",
      "Epoch 37, Batch 646, LR 0.044427 Loss 3.482167, Accuracy 93.448%\n",
      "Epoch 37, Batch 647, LR 0.044391 Loss 3.481923, Accuracy 93.451%\n",
      "Epoch 37, Batch 648, LR 0.044356 Loss 3.481654, Accuracy 93.458%\n",
      "Epoch 37, Batch 649, LR 0.044320 Loss 3.481896, Accuracy 93.460%\n",
      "Epoch 37, Batch 650, LR 0.044285 Loss 3.480846, Accuracy 93.465%\n",
      "Epoch 37, Batch 651, LR 0.044250 Loss 3.481715, Accuracy 93.464%\n",
      "Epoch 37, Batch 652, LR 0.044214 Loss 3.481606, Accuracy 93.464%\n",
      "Epoch 37, Batch 653, LR 0.044179 Loss 3.481534, Accuracy 93.463%\n",
      "Epoch 37, Batch 654, LR 0.044144 Loss 3.481457, Accuracy 93.461%\n",
      "Epoch 37, Batch 655, LR 0.044109 Loss 3.482052, Accuracy 93.459%\n",
      "Epoch 37, Batch 656, LR 0.044073 Loss 3.482160, Accuracy 93.456%\n",
      "Epoch 37, Batch 657, LR 0.044038 Loss 3.481379, Accuracy 93.459%\n",
      "Epoch 37, Batch 658, LR 0.044003 Loss 3.482188, Accuracy 93.453%\n",
      "Epoch 37, Batch 659, LR 0.043968 Loss 3.482873, Accuracy 93.454%\n",
      "Epoch 37, Batch 660, LR 0.043932 Loss 3.483786, Accuracy 93.453%\n",
      "Epoch 37, Batch 661, LR 0.043897 Loss 3.483954, Accuracy 93.449%\n",
      "Epoch 37, Batch 662, LR 0.043862 Loss 3.483165, Accuracy 93.453%\n",
      "Epoch 37, Batch 663, LR 0.043827 Loss 3.483956, Accuracy 93.450%\n",
      "Epoch 37, Batch 664, LR 0.043792 Loss 3.484141, Accuracy 93.446%\n",
      "Epoch 37, Batch 665, LR 0.043757 Loss 3.483342, Accuracy 93.448%\n",
      "Epoch 37, Batch 666, LR 0.043722 Loss 3.483806, Accuracy 93.446%\n",
      "Epoch 37, Batch 667, LR 0.043686 Loss 3.483360, Accuracy 93.445%\n",
      "Epoch 37, Batch 668, LR 0.043651 Loss 3.484330, Accuracy 93.438%\n",
      "Epoch 37, Batch 669, LR 0.043616 Loss 3.485031, Accuracy 93.438%\n",
      "Epoch 37, Batch 670, LR 0.043581 Loss 3.484231, Accuracy 93.440%\n",
      "Epoch 37, Batch 671, LR 0.043546 Loss 3.484789, Accuracy 93.437%\n",
      "Epoch 37, Batch 672, LR 0.043511 Loss 3.483969, Accuracy 93.438%\n",
      "Epoch 37, Batch 673, LR 0.043476 Loss 3.484181, Accuracy 93.439%\n",
      "Epoch 37, Batch 674, LR 0.043441 Loss 3.482966, Accuracy 93.445%\n",
      "Epoch 37, Batch 675, LR 0.043406 Loss 3.482642, Accuracy 93.448%\n",
      "Epoch 37, Batch 676, LR 0.043371 Loss 3.483472, Accuracy 93.444%\n",
      "Epoch 37, Batch 677, LR 0.043336 Loss 3.483323, Accuracy 93.443%\n",
      "Epoch 37, Batch 678, LR 0.043301 Loss 3.483868, Accuracy 93.440%\n",
      "Epoch 37, Batch 679, LR 0.043266 Loss 3.484514, Accuracy 93.438%\n",
      "Epoch 37, Batch 680, LR 0.043231 Loss 3.484325, Accuracy 93.439%\n",
      "Epoch 37, Batch 681, LR 0.043196 Loss 3.484177, Accuracy 93.435%\n",
      "Epoch 37, Batch 682, LR 0.043161 Loss 3.483857, Accuracy 93.437%\n",
      "Epoch 37, Batch 683, LR 0.043127 Loss 3.484166, Accuracy 93.435%\n",
      "Epoch 37, Batch 684, LR 0.043092 Loss 3.483856, Accuracy 93.436%\n",
      "Epoch 37, Batch 685, LR 0.043057 Loss 3.483526, Accuracy 93.441%\n",
      "Epoch 37, Batch 686, LR 0.043022 Loss 3.482917, Accuracy 93.445%\n",
      "Epoch 37, Batch 687, LR 0.042987 Loss 3.483532, Accuracy 93.437%\n",
      "Epoch 37, Batch 688, LR 0.042952 Loss 3.483323, Accuracy 93.434%\n",
      "Epoch 37, Batch 689, LR 0.042918 Loss 3.483085, Accuracy 93.435%\n",
      "Epoch 37, Batch 690, LR 0.042883 Loss 3.483793, Accuracy 93.434%\n",
      "Epoch 37, Batch 691, LR 0.042848 Loss 3.483204, Accuracy 93.436%\n",
      "Epoch 37, Batch 692, LR 0.042813 Loss 3.482949, Accuracy 93.436%\n",
      "Epoch 37, Batch 693, LR 0.042779 Loss 3.483882, Accuracy 93.431%\n",
      "Epoch 37, Batch 694, LR 0.042744 Loss 3.484122, Accuracy 93.431%\n",
      "Epoch 37, Batch 695, LR 0.042709 Loss 3.484360, Accuracy 93.431%\n",
      "Epoch 37, Batch 696, LR 0.042674 Loss 3.484972, Accuracy 93.428%\n",
      "Epoch 37, Batch 697, LR 0.042640 Loss 3.484694, Accuracy 93.428%\n",
      "Epoch 37, Batch 698, LR 0.042605 Loss 3.484862, Accuracy 93.428%\n",
      "Epoch 37, Batch 699, LR 0.042570 Loss 3.484866, Accuracy 93.428%\n",
      "Epoch 37, Batch 700, LR 0.042536 Loss 3.485103, Accuracy 93.429%\n",
      "Epoch 37, Batch 701, LR 0.042501 Loss 3.485158, Accuracy 93.428%\n",
      "Epoch 37, Batch 702, LR 0.042466 Loss 3.484792, Accuracy 93.429%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 703, LR 0.042432 Loss 3.484498, Accuracy 93.427%\n",
      "Epoch 37, Batch 704, LR 0.042397 Loss 3.484087, Accuracy 93.428%\n",
      "Epoch 37, Batch 705, LR 0.042363 Loss 3.482611, Accuracy 93.433%\n",
      "Epoch 37, Batch 706, LR 0.042328 Loss 3.482701, Accuracy 93.431%\n",
      "Epoch 37, Batch 707, LR 0.042294 Loss 3.483324, Accuracy 93.428%\n",
      "Epoch 37, Batch 708, LR 0.042259 Loss 3.484594, Accuracy 93.421%\n",
      "Epoch 37, Batch 709, LR 0.042224 Loss 3.484929, Accuracy 93.416%\n",
      "Epoch 37, Batch 710, LR 0.042190 Loss 3.485091, Accuracy 93.418%\n",
      "Epoch 37, Batch 711, LR 0.042155 Loss 3.485289, Accuracy 93.419%\n",
      "Epoch 37, Batch 712, LR 0.042121 Loss 3.484559, Accuracy 93.421%\n",
      "Epoch 37, Batch 713, LR 0.042087 Loss 3.484674, Accuracy 93.419%\n",
      "Epoch 37, Batch 714, LR 0.042052 Loss 3.485625, Accuracy 93.416%\n",
      "Epoch 37, Batch 715, LR 0.042018 Loss 3.486192, Accuracy 93.413%\n",
      "Epoch 37, Batch 716, LR 0.041983 Loss 3.485543, Accuracy 93.417%\n",
      "Epoch 37, Batch 717, LR 0.041949 Loss 3.485510, Accuracy 93.418%\n",
      "Epoch 37, Batch 718, LR 0.041914 Loss 3.486082, Accuracy 93.415%\n",
      "Epoch 37, Batch 719, LR 0.041880 Loss 3.485794, Accuracy 93.419%\n",
      "Epoch 37, Batch 720, LR 0.041846 Loss 3.486290, Accuracy 93.417%\n",
      "Epoch 37, Batch 721, LR 0.041811 Loss 3.486396, Accuracy 93.415%\n",
      "Epoch 37, Batch 722, LR 0.041777 Loss 3.486014, Accuracy 93.419%\n",
      "Epoch 37, Batch 723, LR 0.041743 Loss 3.485963, Accuracy 93.416%\n",
      "Epoch 37, Batch 724, LR 0.041708 Loss 3.485960, Accuracy 93.417%\n",
      "Epoch 37, Batch 725, LR 0.041674 Loss 3.485861, Accuracy 93.419%\n",
      "Epoch 37, Batch 726, LR 0.041640 Loss 3.484990, Accuracy 93.422%\n",
      "Epoch 37, Batch 727, LR 0.041605 Loss 3.485532, Accuracy 93.420%\n",
      "Epoch 37, Batch 728, LR 0.041571 Loss 3.486258, Accuracy 93.415%\n",
      "Epoch 37, Batch 729, LR 0.041537 Loss 3.485830, Accuracy 93.419%\n",
      "Epoch 37, Batch 730, LR 0.041503 Loss 3.486504, Accuracy 93.418%\n",
      "Epoch 37, Batch 731, LR 0.041468 Loss 3.485606, Accuracy 93.418%\n",
      "Epoch 37, Batch 732, LR 0.041434 Loss 3.485582, Accuracy 93.416%\n",
      "Epoch 37, Batch 733, LR 0.041400 Loss 3.486696, Accuracy 93.411%\n",
      "Epoch 37, Batch 734, LR 0.041366 Loss 3.487262, Accuracy 93.409%\n",
      "Epoch 37, Batch 735, LR 0.041332 Loss 3.487223, Accuracy 93.412%\n",
      "Epoch 37, Batch 736, LR 0.041298 Loss 3.487759, Accuracy 93.408%\n",
      "Epoch 37, Batch 737, LR 0.041263 Loss 3.487410, Accuracy 93.405%\n",
      "Epoch 37, Batch 738, LR 0.041229 Loss 3.487664, Accuracy 93.405%\n",
      "Epoch 37, Batch 739, LR 0.041195 Loss 3.487260, Accuracy 93.403%\n",
      "Epoch 37, Batch 740, LR 0.041161 Loss 3.486856, Accuracy 93.408%\n",
      "Epoch 37, Batch 741, LR 0.041127 Loss 3.486592, Accuracy 93.411%\n",
      "Epoch 37, Batch 742, LR 0.041093 Loss 3.487175, Accuracy 93.407%\n",
      "Epoch 37, Batch 743, LR 0.041059 Loss 3.487502, Accuracy 93.411%\n",
      "Epoch 37, Batch 744, LR 0.041025 Loss 3.486889, Accuracy 93.416%\n",
      "Epoch 37, Batch 745, LR 0.040991 Loss 3.486822, Accuracy 93.418%\n",
      "Epoch 37, Batch 746, LR 0.040957 Loss 3.486332, Accuracy 93.419%\n",
      "Epoch 37, Batch 747, LR 0.040923 Loss 3.486008, Accuracy 93.420%\n",
      "Epoch 37, Batch 748, LR 0.040889 Loss 3.485953, Accuracy 93.422%\n",
      "Epoch 37, Batch 749, LR 0.040855 Loss 3.485427, Accuracy 93.427%\n",
      "Epoch 37, Batch 750, LR 0.040821 Loss 3.484917, Accuracy 93.432%\n",
      "Epoch 37, Batch 751, LR 0.040787 Loss 3.485155, Accuracy 93.432%\n",
      "Epoch 37, Batch 752, LR 0.040753 Loss 3.484742, Accuracy 93.433%\n",
      "Epoch 37, Batch 753, LR 0.040719 Loss 3.484595, Accuracy 93.436%\n",
      "Epoch 37, Batch 754, LR 0.040685 Loss 3.484932, Accuracy 93.433%\n",
      "Epoch 37, Batch 755, LR 0.040651 Loss 3.484208, Accuracy 93.436%\n",
      "Epoch 37, Batch 756, LR 0.040618 Loss 3.483958, Accuracy 93.437%\n",
      "Epoch 37, Batch 757, LR 0.040584 Loss 3.484252, Accuracy 93.436%\n",
      "Epoch 37, Batch 758, LR 0.040550 Loss 3.484562, Accuracy 93.434%\n",
      "Epoch 37, Batch 759, LR 0.040516 Loss 3.485789, Accuracy 93.426%\n",
      "Epoch 37, Batch 760, LR 0.040482 Loss 3.485371, Accuracy 93.425%\n",
      "Epoch 37, Batch 761, LR 0.040448 Loss 3.484783, Accuracy 93.430%\n",
      "Epoch 37, Batch 762, LR 0.040415 Loss 3.484026, Accuracy 93.433%\n",
      "Epoch 37, Batch 763, LR 0.040381 Loss 3.484581, Accuracy 93.430%\n",
      "Epoch 37, Batch 764, LR 0.040347 Loss 3.484558, Accuracy 93.429%\n",
      "Epoch 37, Batch 765, LR 0.040313 Loss 3.484305, Accuracy 93.429%\n",
      "Epoch 37, Batch 766, LR 0.040280 Loss 3.483611, Accuracy 93.430%\n",
      "Epoch 37, Batch 767, LR 0.040246 Loss 3.482261, Accuracy 93.434%\n",
      "Epoch 37, Batch 768, LR 0.040212 Loss 3.483093, Accuracy 93.431%\n",
      "Epoch 37, Batch 769, LR 0.040178 Loss 3.483662, Accuracy 93.427%\n",
      "Epoch 37, Batch 770, LR 0.040145 Loss 3.484362, Accuracy 93.423%\n",
      "Epoch 37, Batch 771, LR 0.040111 Loss 3.483860, Accuracy 93.423%\n",
      "Epoch 37, Batch 772, LR 0.040077 Loss 3.483991, Accuracy 93.419%\n",
      "Epoch 37, Batch 773, LR 0.040044 Loss 3.484179, Accuracy 93.417%\n",
      "Epoch 37, Batch 774, LR 0.040010 Loss 3.483884, Accuracy 93.420%\n",
      "Epoch 37, Batch 775, LR 0.039977 Loss 3.484335, Accuracy 93.419%\n",
      "Epoch 37, Batch 776, LR 0.039943 Loss 3.484555, Accuracy 93.420%\n",
      "Epoch 37, Batch 777, LR 0.039909 Loss 3.484574, Accuracy 93.420%\n",
      "Epoch 37, Batch 778, LR 0.039876 Loss 3.485157, Accuracy 93.413%\n",
      "Epoch 37, Batch 779, LR 0.039842 Loss 3.485084, Accuracy 93.414%\n",
      "Epoch 37, Batch 780, LR 0.039809 Loss 3.485288, Accuracy 93.415%\n",
      "Epoch 37, Batch 781, LR 0.039775 Loss 3.484990, Accuracy 93.418%\n",
      "Epoch 37, Batch 782, LR 0.039742 Loss 3.484717, Accuracy 93.419%\n",
      "Epoch 37, Batch 783, LR 0.039708 Loss 3.484982, Accuracy 93.419%\n",
      "Epoch 37, Batch 784, LR 0.039675 Loss 3.483771, Accuracy 93.421%\n",
      "Epoch 37, Batch 785, LR 0.039641 Loss 3.483961, Accuracy 93.422%\n",
      "Epoch 37, Batch 786, LR 0.039608 Loss 3.484029, Accuracy 93.422%\n",
      "Epoch 37, Batch 787, LR 0.039574 Loss 3.484192, Accuracy 93.421%\n",
      "Epoch 37, Batch 788, LR 0.039541 Loss 3.485027, Accuracy 93.419%\n",
      "Epoch 37, Batch 789, LR 0.039508 Loss 3.485159, Accuracy 93.418%\n",
      "Epoch 37, Batch 790, LR 0.039474 Loss 3.485174, Accuracy 93.419%\n",
      "Epoch 37, Batch 791, LR 0.039441 Loss 3.485212, Accuracy 93.419%\n",
      "Epoch 37, Batch 792, LR 0.039407 Loss 3.485568, Accuracy 93.420%\n",
      "Epoch 37, Batch 793, LR 0.039374 Loss 3.486858, Accuracy 93.413%\n",
      "Epoch 37, Batch 794, LR 0.039341 Loss 3.487190, Accuracy 93.413%\n",
      "Epoch 37, Batch 795, LR 0.039307 Loss 3.486956, Accuracy 93.413%\n",
      "Epoch 37, Batch 796, LR 0.039274 Loss 3.487629, Accuracy 93.409%\n",
      "Epoch 37, Batch 797, LR 0.039241 Loss 3.486685, Accuracy 93.412%\n",
      "Epoch 37, Batch 798, LR 0.039207 Loss 3.487256, Accuracy 93.407%\n",
      "Epoch 37, Batch 799, LR 0.039174 Loss 3.487540, Accuracy 93.407%\n",
      "Epoch 37, Batch 800, LR 0.039141 Loss 3.488025, Accuracy 93.404%\n",
      "Epoch 37, Batch 801, LR 0.039108 Loss 3.487991, Accuracy 93.405%\n",
      "Epoch 37, Batch 802, LR 0.039074 Loss 3.488026, Accuracy 93.404%\n",
      "Epoch 37, Batch 803, LR 0.039041 Loss 3.487631, Accuracy 93.405%\n",
      "Epoch 37, Batch 804, LR 0.039008 Loss 3.487742, Accuracy 93.406%\n",
      "Epoch 37, Batch 805, LR 0.038975 Loss 3.488533, Accuracy 93.405%\n",
      "Epoch 37, Batch 806, LR 0.038942 Loss 3.488137, Accuracy 93.405%\n",
      "Epoch 37, Batch 807, LR 0.038908 Loss 3.488771, Accuracy 93.402%\n",
      "Epoch 37, Batch 808, LR 0.038875 Loss 3.488418, Accuracy 93.402%\n",
      "Epoch 37, Batch 809, LR 0.038842 Loss 3.487703, Accuracy 93.408%\n",
      "Epoch 37, Batch 810, LR 0.038809 Loss 3.487378, Accuracy 93.408%\n",
      "Epoch 37, Batch 811, LR 0.038776 Loss 3.487603, Accuracy 93.409%\n",
      "Epoch 37, Batch 812, LR 0.038743 Loss 3.486895, Accuracy 93.411%\n",
      "Epoch 37, Batch 813, LR 0.038710 Loss 3.486626, Accuracy 93.413%\n",
      "Epoch 37, Batch 814, LR 0.038677 Loss 3.487254, Accuracy 93.408%\n",
      "Epoch 37, Batch 815, LR 0.038644 Loss 3.487304, Accuracy 93.410%\n",
      "Epoch 37, Batch 816, LR 0.038611 Loss 3.487145, Accuracy 93.410%\n",
      "Epoch 37, Batch 817, LR 0.038578 Loss 3.486471, Accuracy 93.413%\n",
      "Epoch 37, Batch 818, LR 0.038545 Loss 3.486160, Accuracy 93.415%\n",
      "Epoch 37, Batch 819, LR 0.038512 Loss 3.486182, Accuracy 93.419%\n",
      "Epoch 37, Batch 820, LR 0.038479 Loss 3.486462, Accuracy 93.418%\n",
      "Epoch 37, Batch 821, LR 0.038446 Loss 3.486355, Accuracy 93.419%\n",
      "Epoch 37, Batch 822, LR 0.038413 Loss 3.486311, Accuracy 93.419%\n",
      "Epoch 37, Batch 823, LR 0.038380 Loss 3.486875, Accuracy 93.418%\n",
      "Epoch 37, Batch 824, LR 0.038347 Loss 3.487953, Accuracy 93.412%\n",
      "Epoch 37, Batch 825, LR 0.038314 Loss 3.487781, Accuracy 93.412%\n",
      "Epoch 37, Batch 826, LR 0.038281 Loss 3.488161, Accuracy 93.409%\n",
      "Epoch 37, Batch 827, LR 0.038248 Loss 3.487579, Accuracy 93.410%\n",
      "Epoch 37, Batch 828, LR 0.038215 Loss 3.487832, Accuracy 93.410%\n",
      "Epoch 37, Batch 829, LR 0.038182 Loss 3.487992, Accuracy 93.411%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 830, LR 0.038150 Loss 3.488006, Accuracy 93.409%\n",
      "Epoch 37, Batch 831, LR 0.038117 Loss 3.488443, Accuracy 93.408%\n",
      "Epoch 37, Batch 832, LR 0.038084 Loss 3.488045, Accuracy 93.406%\n",
      "Epoch 37, Batch 833, LR 0.038051 Loss 3.488460, Accuracy 93.404%\n",
      "Epoch 37, Batch 834, LR 0.038018 Loss 3.487629, Accuracy 93.409%\n",
      "Epoch 37, Batch 835, LR 0.037986 Loss 3.487516, Accuracy 93.409%\n",
      "Epoch 37, Batch 836, LR 0.037953 Loss 3.487672, Accuracy 93.411%\n",
      "Epoch 37, Batch 837, LR 0.037920 Loss 3.487746, Accuracy 93.407%\n",
      "Epoch 37, Batch 838, LR 0.037887 Loss 3.487913, Accuracy 93.408%\n",
      "Epoch 37, Batch 839, LR 0.037855 Loss 3.488419, Accuracy 93.408%\n",
      "Epoch 37, Batch 840, LR 0.037822 Loss 3.487885, Accuracy 93.409%\n",
      "Epoch 37, Batch 841, LR 0.037789 Loss 3.486943, Accuracy 93.414%\n",
      "Epoch 37, Batch 842, LR 0.037757 Loss 3.486308, Accuracy 93.413%\n",
      "Epoch 37, Batch 843, LR 0.037724 Loss 3.486573, Accuracy 93.415%\n",
      "Epoch 37, Batch 844, LR 0.037691 Loss 3.487163, Accuracy 93.410%\n",
      "Epoch 37, Batch 845, LR 0.037659 Loss 3.487591, Accuracy 93.408%\n",
      "Epoch 37, Batch 846, LR 0.037626 Loss 3.487165, Accuracy 93.411%\n",
      "Epoch 37, Batch 847, LR 0.037593 Loss 3.487563, Accuracy 93.406%\n",
      "Epoch 37, Batch 848, LR 0.037561 Loss 3.487294, Accuracy 93.411%\n",
      "Epoch 37, Batch 849, LR 0.037528 Loss 3.487696, Accuracy 93.411%\n",
      "Epoch 37, Batch 850, LR 0.037496 Loss 3.487831, Accuracy 93.412%\n",
      "Epoch 37, Batch 851, LR 0.037463 Loss 3.488653, Accuracy 93.408%\n",
      "Epoch 37, Batch 852, LR 0.037431 Loss 3.489246, Accuracy 93.403%\n",
      "Epoch 37, Batch 853, LR 0.037398 Loss 3.488905, Accuracy 93.401%\n",
      "Epoch 37, Batch 854, LR 0.037366 Loss 3.489000, Accuracy 93.399%\n",
      "Epoch 37, Batch 855, LR 0.037333 Loss 3.489274, Accuracy 93.397%\n",
      "Epoch 37, Batch 856, LR 0.037301 Loss 3.489373, Accuracy 93.396%\n",
      "Epoch 37, Batch 857, LR 0.037268 Loss 3.489517, Accuracy 93.394%\n",
      "Epoch 37, Batch 858, LR 0.037236 Loss 3.489453, Accuracy 93.392%\n",
      "Epoch 37, Batch 859, LR 0.037203 Loss 3.489871, Accuracy 93.390%\n",
      "Epoch 37, Batch 860, LR 0.037171 Loss 3.489892, Accuracy 93.390%\n",
      "Epoch 37, Batch 861, LR 0.037138 Loss 3.490209, Accuracy 93.385%\n",
      "Epoch 37, Batch 862, LR 0.037106 Loss 3.489671, Accuracy 93.386%\n",
      "Epoch 37, Batch 863, LR 0.037074 Loss 3.489702, Accuracy 93.386%\n",
      "Epoch 37, Batch 864, LR 0.037041 Loss 3.489166, Accuracy 93.387%\n",
      "Epoch 37, Batch 865, LR 0.037009 Loss 3.489545, Accuracy 93.384%\n",
      "Epoch 37, Batch 866, LR 0.036976 Loss 3.489336, Accuracy 93.383%\n",
      "Epoch 37, Batch 867, LR 0.036944 Loss 3.490025, Accuracy 93.379%\n",
      "Epoch 37, Batch 868, LR 0.036912 Loss 3.489890, Accuracy 93.377%\n",
      "Epoch 37, Batch 869, LR 0.036880 Loss 3.489332, Accuracy 93.381%\n",
      "Epoch 37, Batch 870, LR 0.036847 Loss 3.488217, Accuracy 93.388%\n",
      "Epoch 37, Batch 871, LR 0.036815 Loss 3.488585, Accuracy 93.384%\n",
      "Epoch 37, Batch 872, LR 0.036783 Loss 3.488454, Accuracy 93.384%\n",
      "Epoch 37, Batch 873, LR 0.036750 Loss 3.488532, Accuracy 93.383%\n",
      "Epoch 37, Batch 874, LR 0.036718 Loss 3.488952, Accuracy 93.380%\n",
      "Epoch 37, Batch 875, LR 0.036686 Loss 3.488648, Accuracy 93.385%\n",
      "Epoch 37, Batch 876, LR 0.036654 Loss 3.488828, Accuracy 93.386%\n",
      "Epoch 37, Batch 877, LR 0.036622 Loss 3.488824, Accuracy 93.384%\n",
      "Epoch 37, Batch 878, LR 0.036589 Loss 3.489184, Accuracy 93.385%\n",
      "Epoch 37, Batch 879, LR 0.036557 Loss 3.489071, Accuracy 93.386%\n",
      "Epoch 37, Batch 880, LR 0.036525 Loss 3.489026, Accuracy 93.382%\n",
      "Epoch 37, Batch 881, LR 0.036493 Loss 3.489052, Accuracy 93.381%\n",
      "Epoch 37, Batch 882, LR 0.036461 Loss 3.488976, Accuracy 93.380%\n",
      "Epoch 37, Batch 883, LR 0.036429 Loss 3.489065, Accuracy 93.377%\n",
      "Epoch 37, Batch 884, LR 0.036397 Loss 3.489087, Accuracy 93.376%\n",
      "Epoch 37, Batch 885, LR 0.036365 Loss 3.489064, Accuracy 93.377%\n",
      "Epoch 37, Batch 886, LR 0.036333 Loss 3.489570, Accuracy 93.373%\n",
      "Epoch 37, Batch 887, LR 0.036301 Loss 3.488776, Accuracy 93.374%\n",
      "Epoch 37, Batch 888, LR 0.036268 Loss 3.488777, Accuracy 93.372%\n",
      "Epoch 37, Batch 889, LR 0.036236 Loss 3.488619, Accuracy 93.375%\n",
      "Epoch 37, Batch 890, LR 0.036204 Loss 3.488410, Accuracy 93.377%\n",
      "Epoch 37, Batch 891, LR 0.036172 Loss 3.487893, Accuracy 93.380%\n",
      "Epoch 37, Batch 892, LR 0.036140 Loss 3.487194, Accuracy 93.380%\n",
      "Epoch 37, Batch 893, LR 0.036109 Loss 3.486238, Accuracy 93.382%\n",
      "Epoch 37, Batch 894, LR 0.036077 Loss 3.485577, Accuracy 93.382%\n",
      "Epoch 37, Batch 895, LR 0.036045 Loss 3.486136, Accuracy 93.377%\n",
      "Epoch 37, Batch 896, LR 0.036013 Loss 3.486353, Accuracy 93.379%\n",
      "Epoch 37, Batch 897, LR 0.035981 Loss 3.485831, Accuracy 93.382%\n",
      "Epoch 37, Batch 898, LR 0.035949 Loss 3.485836, Accuracy 93.381%\n",
      "Epoch 37, Batch 899, LR 0.035917 Loss 3.485839, Accuracy 93.379%\n",
      "Epoch 37, Batch 900, LR 0.035885 Loss 3.485693, Accuracy 93.378%\n",
      "Epoch 37, Batch 901, LR 0.035853 Loss 3.485625, Accuracy 93.380%\n",
      "Epoch 37, Batch 902, LR 0.035821 Loss 3.485489, Accuracy 93.381%\n",
      "Epoch 37, Batch 903, LR 0.035790 Loss 3.485708, Accuracy 93.379%\n",
      "Epoch 37, Batch 904, LR 0.035758 Loss 3.485129, Accuracy 93.382%\n",
      "Epoch 37, Batch 905, LR 0.035726 Loss 3.484115, Accuracy 93.386%\n",
      "Epoch 37, Batch 906, LR 0.035694 Loss 3.483950, Accuracy 93.386%\n",
      "Epoch 37, Batch 907, LR 0.035662 Loss 3.484419, Accuracy 93.382%\n",
      "Epoch 37, Batch 908, LR 0.035631 Loss 3.484276, Accuracy 93.379%\n",
      "Epoch 37, Batch 909, LR 0.035599 Loss 3.484475, Accuracy 93.378%\n",
      "Epoch 37, Batch 910, LR 0.035567 Loss 3.483468, Accuracy 93.382%\n",
      "Epoch 37, Batch 911, LR 0.035536 Loss 3.484070, Accuracy 93.380%\n",
      "Epoch 37, Batch 912, LR 0.035504 Loss 3.483985, Accuracy 93.379%\n",
      "Epoch 37, Batch 913, LR 0.035472 Loss 3.483404, Accuracy 93.382%\n",
      "Epoch 37, Batch 914, LR 0.035440 Loss 3.483247, Accuracy 93.382%\n",
      "Epoch 37, Batch 915, LR 0.035409 Loss 3.483535, Accuracy 93.377%\n",
      "Epoch 37, Batch 916, LR 0.035377 Loss 3.483630, Accuracy 93.377%\n",
      "Epoch 37, Batch 917, LR 0.035346 Loss 3.483633, Accuracy 93.376%\n",
      "Epoch 37, Batch 918, LR 0.035314 Loss 3.484175, Accuracy 93.375%\n",
      "Epoch 37, Batch 919, LR 0.035282 Loss 3.484635, Accuracy 93.373%\n",
      "Epoch 37, Batch 920, LR 0.035251 Loss 3.484773, Accuracy 93.372%\n",
      "Epoch 37, Batch 921, LR 0.035219 Loss 3.485104, Accuracy 93.369%\n",
      "Epoch 37, Batch 922, LR 0.035188 Loss 3.486618, Accuracy 93.362%\n",
      "Epoch 37, Batch 923, LR 0.035156 Loss 3.486704, Accuracy 93.363%\n",
      "Epoch 37, Batch 924, LR 0.035125 Loss 3.486197, Accuracy 93.364%\n",
      "Epoch 37, Batch 925, LR 0.035093 Loss 3.486116, Accuracy 93.365%\n",
      "Epoch 37, Batch 926, LR 0.035061 Loss 3.486449, Accuracy 93.365%\n",
      "Epoch 37, Batch 927, LR 0.035030 Loss 3.487100, Accuracy 93.362%\n",
      "Epoch 37, Batch 928, LR 0.034999 Loss 3.486910, Accuracy 93.364%\n",
      "Epoch 37, Batch 929, LR 0.034967 Loss 3.486092, Accuracy 93.367%\n",
      "Epoch 37, Batch 930, LR 0.034936 Loss 3.486260, Accuracy 93.369%\n",
      "Epoch 37, Batch 931, LR 0.034904 Loss 3.486086, Accuracy 93.370%\n",
      "Epoch 37, Batch 932, LR 0.034873 Loss 3.485915, Accuracy 93.371%\n",
      "Epoch 37, Batch 933, LR 0.034841 Loss 3.485677, Accuracy 93.372%\n",
      "Epoch 37, Batch 934, LR 0.034810 Loss 3.485768, Accuracy 93.371%\n",
      "Epoch 37, Batch 935, LR 0.034779 Loss 3.485526, Accuracy 93.371%\n",
      "Epoch 37, Batch 936, LR 0.034747 Loss 3.485606, Accuracy 93.373%\n",
      "Epoch 37, Batch 937, LR 0.034716 Loss 3.485990, Accuracy 93.371%\n",
      "Epoch 37, Batch 938, LR 0.034685 Loss 3.486525, Accuracy 93.371%\n",
      "Epoch 37, Batch 939, LR 0.034653 Loss 3.486131, Accuracy 93.374%\n",
      "Epoch 37, Batch 940, LR 0.034622 Loss 3.485485, Accuracy 93.376%\n",
      "Epoch 37, Batch 941, LR 0.034591 Loss 3.485957, Accuracy 93.373%\n",
      "Epoch 37, Batch 942, LR 0.034559 Loss 3.485795, Accuracy 93.374%\n",
      "Epoch 37, Batch 943, LR 0.034528 Loss 3.485412, Accuracy 93.376%\n",
      "Epoch 37, Batch 944, LR 0.034497 Loss 3.485758, Accuracy 93.375%\n",
      "Epoch 37, Batch 945, LR 0.034466 Loss 3.485555, Accuracy 93.376%\n",
      "Epoch 37, Batch 946, LR 0.034434 Loss 3.485915, Accuracy 93.375%\n",
      "Epoch 37, Batch 947, LR 0.034403 Loss 3.485306, Accuracy 93.377%\n",
      "Epoch 37, Batch 948, LR 0.034372 Loss 3.485881, Accuracy 93.375%\n",
      "Epoch 37, Batch 949, LR 0.034341 Loss 3.485533, Accuracy 93.375%\n",
      "Epoch 37, Batch 950, LR 0.034310 Loss 3.485365, Accuracy 93.376%\n",
      "Epoch 37, Batch 951, LR 0.034278 Loss 3.485422, Accuracy 93.376%\n",
      "Epoch 37, Batch 952, LR 0.034247 Loss 3.485679, Accuracy 93.377%\n",
      "Epoch 37, Batch 953, LR 0.034216 Loss 3.486442, Accuracy 93.374%\n",
      "Epoch 37, Batch 954, LR 0.034185 Loss 3.486034, Accuracy 93.375%\n",
      "Epoch 37, Batch 955, LR 0.034154 Loss 3.486384, Accuracy 93.375%\n",
      "Epoch 37, Batch 956, LR 0.034123 Loss 3.486101, Accuracy 93.378%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 957, LR 0.034092 Loss 3.486846, Accuracy 93.374%\n",
      "Epoch 37, Batch 958, LR 0.034061 Loss 3.487838, Accuracy 93.369%\n",
      "Epoch 37, Batch 959, LR 0.034030 Loss 3.487694, Accuracy 93.372%\n",
      "Epoch 37, Batch 960, LR 0.033999 Loss 3.488374, Accuracy 93.368%\n",
      "Epoch 37, Batch 961, LR 0.033968 Loss 3.488479, Accuracy 93.368%\n",
      "Epoch 37, Batch 962, LR 0.033937 Loss 3.488370, Accuracy 93.368%\n",
      "Epoch 37, Batch 963, LR 0.033906 Loss 3.488819, Accuracy 93.365%\n",
      "Epoch 37, Batch 964, LR 0.033875 Loss 3.489464, Accuracy 93.362%\n",
      "Epoch 37, Batch 965, LR 0.033844 Loss 3.488927, Accuracy 93.361%\n",
      "Epoch 37, Batch 966, LR 0.033813 Loss 3.489020, Accuracy 93.363%\n",
      "Epoch 37, Batch 967, LR 0.033782 Loss 3.489286, Accuracy 93.361%\n",
      "Epoch 37, Batch 968, LR 0.033751 Loss 3.489161, Accuracy 93.360%\n",
      "Epoch 37, Batch 969, LR 0.033720 Loss 3.490238, Accuracy 93.357%\n",
      "Epoch 37, Batch 970, LR 0.033689 Loss 3.489917, Accuracy 93.359%\n",
      "Epoch 37, Batch 971, LR 0.033658 Loss 3.490318, Accuracy 93.359%\n",
      "Epoch 37, Batch 972, LR 0.033627 Loss 3.490209, Accuracy 93.358%\n",
      "Epoch 37, Batch 973, LR 0.033596 Loss 3.490135, Accuracy 93.358%\n",
      "Epoch 37, Batch 974, LR 0.033566 Loss 3.490237, Accuracy 93.358%\n",
      "Epoch 37, Batch 975, LR 0.033535 Loss 3.490143, Accuracy 93.360%\n",
      "Epoch 37, Batch 976, LR 0.033504 Loss 3.489578, Accuracy 93.362%\n",
      "Epoch 37, Batch 977, LR 0.033473 Loss 3.488901, Accuracy 93.364%\n",
      "Epoch 37, Batch 978, LR 0.033442 Loss 3.488606, Accuracy 93.360%\n",
      "Epoch 37, Batch 979, LR 0.033412 Loss 3.488612, Accuracy 93.359%\n",
      "Epoch 37, Batch 980, LR 0.033381 Loss 3.488433, Accuracy 93.360%\n",
      "Epoch 37, Batch 981, LR 0.033350 Loss 3.488537, Accuracy 93.360%\n",
      "Epoch 37, Batch 982, LR 0.033319 Loss 3.488726, Accuracy 93.359%\n",
      "Epoch 37, Batch 983, LR 0.033289 Loss 3.489736, Accuracy 93.356%\n",
      "Epoch 37, Batch 984, LR 0.033258 Loss 3.489329, Accuracy 93.355%\n",
      "Epoch 37, Batch 985, LR 0.033227 Loss 3.488692, Accuracy 93.359%\n",
      "Epoch 37, Batch 986, LR 0.033197 Loss 3.488399, Accuracy 93.361%\n",
      "Epoch 37, Batch 987, LR 0.033166 Loss 3.487871, Accuracy 93.363%\n",
      "Epoch 37, Batch 988, LR 0.033135 Loss 3.487752, Accuracy 93.363%\n",
      "Epoch 37, Batch 989, LR 0.033105 Loss 3.487868, Accuracy 93.363%\n",
      "Epoch 37, Batch 990, LR 0.033074 Loss 3.488633, Accuracy 93.361%\n",
      "Epoch 37, Batch 991, LR 0.033044 Loss 3.488101, Accuracy 93.363%\n",
      "Epoch 37, Batch 992, LR 0.033013 Loss 3.488552, Accuracy 93.359%\n",
      "Epoch 37, Batch 993, LR 0.032982 Loss 3.488229, Accuracy 93.361%\n",
      "Epoch 37, Batch 994, LR 0.032952 Loss 3.487837, Accuracy 93.365%\n",
      "Epoch 37, Batch 995, LR 0.032921 Loss 3.488099, Accuracy 93.367%\n",
      "Epoch 37, Batch 996, LR 0.032891 Loss 3.488157, Accuracy 93.366%\n",
      "Epoch 37, Batch 997, LR 0.032860 Loss 3.487690, Accuracy 93.368%\n",
      "Epoch 37, Batch 998, LR 0.032830 Loss 3.487939, Accuracy 93.367%\n",
      "Epoch 37, Batch 999, LR 0.032799 Loss 3.488262, Accuracy 93.368%\n",
      "Epoch 37, Batch 1000, LR 0.032769 Loss 3.488962, Accuracy 93.363%\n",
      "Epoch 37, Batch 1001, LR 0.032738 Loss 3.489072, Accuracy 93.361%\n",
      "Epoch 37, Batch 1002, LR 0.032708 Loss 3.489446, Accuracy 93.356%\n",
      "Epoch 37, Batch 1003, LR 0.032678 Loss 3.488721, Accuracy 93.358%\n",
      "Epoch 37, Batch 1004, LR 0.032647 Loss 3.488734, Accuracy 93.359%\n",
      "Epoch 37, Batch 1005, LR 0.032617 Loss 3.488701, Accuracy 93.357%\n",
      "Epoch 37, Batch 1006, LR 0.032586 Loss 3.489432, Accuracy 93.355%\n",
      "Epoch 37, Batch 1007, LR 0.032556 Loss 3.489933, Accuracy 93.353%\n",
      "Epoch 37, Batch 1008, LR 0.032526 Loss 3.489790, Accuracy 93.353%\n",
      "Epoch 37, Batch 1009, LR 0.032495 Loss 3.489223, Accuracy 93.354%\n",
      "Epoch 37, Batch 1010, LR 0.032465 Loss 3.489347, Accuracy 93.352%\n",
      "Epoch 37, Batch 1011, LR 0.032435 Loss 3.490131, Accuracy 93.347%\n",
      "Epoch 37, Batch 1012, LR 0.032404 Loss 3.489706, Accuracy 93.348%\n",
      "Epoch 37, Batch 1013, LR 0.032374 Loss 3.489817, Accuracy 93.349%\n",
      "Epoch 37, Batch 1014, LR 0.032344 Loss 3.489683, Accuracy 93.349%\n",
      "Epoch 37, Batch 1015, LR 0.032313 Loss 3.489543, Accuracy 93.349%\n",
      "Epoch 37, Batch 1016, LR 0.032283 Loss 3.489574, Accuracy 93.348%\n",
      "Epoch 37, Batch 1017, LR 0.032253 Loss 3.488877, Accuracy 93.351%\n",
      "Epoch 37, Batch 1018, LR 0.032223 Loss 3.488389, Accuracy 93.349%\n",
      "Epoch 37, Batch 1019, LR 0.032193 Loss 3.488427, Accuracy 93.350%\n",
      "Epoch 37, Batch 1020, LR 0.032162 Loss 3.488881, Accuracy 93.349%\n",
      "Epoch 37, Batch 1021, LR 0.032132 Loss 3.488307, Accuracy 93.354%\n",
      "Epoch 37, Batch 1022, LR 0.032102 Loss 3.487863, Accuracy 93.352%\n",
      "Epoch 37, Batch 1023, LR 0.032072 Loss 3.487544, Accuracy 93.355%\n",
      "Epoch 37, Batch 1024, LR 0.032042 Loss 3.486951, Accuracy 93.359%\n",
      "Epoch 37, Batch 1025, LR 0.032012 Loss 3.486561, Accuracy 93.360%\n",
      "Epoch 37, Batch 1026, LR 0.031981 Loss 3.486462, Accuracy 93.361%\n",
      "Epoch 37, Batch 1027, LR 0.031951 Loss 3.486330, Accuracy 93.360%\n",
      "Epoch 37, Batch 1028, LR 0.031921 Loss 3.486393, Accuracy 93.359%\n",
      "Epoch 37, Batch 1029, LR 0.031891 Loss 3.486315, Accuracy 93.361%\n",
      "Epoch 37, Batch 1030, LR 0.031861 Loss 3.486143, Accuracy 93.362%\n",
      "Epoch 37, Batch 1031, LR 0.031831 Loss 3.486329, Accuracy 93.361%\n",
      "Epoch 37, Batch 1032, LR 0.031801 Loss 3.486282, Accuracy 93.363%\n",
      "Epoch 37, Batch 1033, LR 0.031771 Loss 3.486167, Accuracy 93.364%\n",
      "Epoch 37, Batch 1034, LR 0.031741 Loss 3.486033, Accuracy 93.365%\n",
      "Epoch 37, Batch 1035, LR 0.031711 Loss 3.485736, Accuracy 93.367%\n",
      "Epoch 37, Batch 1036, LR 0.031681 Loss 3.485799, Accuracy 93.368%\n",
      "Epoch 37, Batch 1037, LR 0.031651 Loss 3.485692, Accuracy 93.371%\n",
      "Epoch 37, Batch 1038, LR 0.031621 Loss 3.485531, Accuracy 93.372%\n",
      "Epoch 37, Batch 1039, LR 0.031591 Loss 3.485413, Accuracy 93.373%\n",
      "Epoch 37, Batch 1040, LR 0.031561 Loss 3.485209, Accuracy 93.374%\n",
      "Epoch 37, Batch 1041, LR 0.031532 Loss 3.485346, Accuracy 93.375%\n",
      "Epoch 37, Batch 1042, LR 0.031502 Loss 3.485942, Accuracy 93.369%\n",
      "Epoch 37, Batch 1043, LR 0.031472 Loss 3.486242, Accuracy 93.369%\n",
      "Epoch 37, Batch 1044, LR 0.031442 Loss 3.486626, Accuracy 93.368%\n",
      "Epoch 37, Batch 1045, LR 0.031412 Loss 3.486365, Accuracy 93.369%\n",
      "Epoch 37, Batch 1046, LR 0.031382 Loss 3.485954, Accuracy 93.370%\n",
      "Epoch 37, Batch 1047, LR 0.031352 Loss 3.485160, Accuracy 93.373%\n",
      "Epoch 37, Loss (train set) 3.485160, Accuracy (train set) 93.373%\n",
      "Epoch 38, Batch 1, LR 0.031323 Loss 3.201301, Accuracy 95.312%\n",
      "Epoch 38, Batch 2, LR 0.031293 Loss 3.126791, Accuracy 95.312%\n",
      "Epoch 38, Batch 3, LR 0.031263 Loss 3.187159, Accuracy 94.792%\n",
      "Epoch 38, Batch 4, LR 0.031233 Loss 3.291034, Accuracy 95.117%\n",
      "Epoch 38, Batch 5, LR 0.031204 Loss 3.374417, Accuracy 94.219%\n",
      "Epoch 38, Batch 6, LR 0.031174 Loss 3.429614, Accuracy 93.880%\n",
      "Epoch 38, Batch 7, LR 0.031144 Loss 3.496223, Accuracy 93.750%\n",
      "Epoch 38, Batch 8, LR 0.031114 Loss 3.506883, Accuracy 93.457%\n",
      "Epoch 38, Batch 9, LR 0.031085 Loss 3.499963, Accuracy 93.229%\n",
      "Epoch 38, Batch 10, LR 0.031055 Loss 3.523231, Accuracy 92.969%\n",
      "Epoch 38, Batch 11, LR 0.031025 Loss 3.492163, Accuracy 92.756%\n",
      "Epoch 38, Batch 12, LR 0.030996 Loss 3.483429, Accuracy 92.773%\n",
      "Epoch 38, Batch 13, LR 0.030966 Loss 3.516219, Accuracy 92.728%\n",
      "Epoch 38, Batch 14, LR 0.030937 Loss 3.506371, Accuracy 92.746%\n",
      "Epoch 38, Batch 15, LR 0.030907 Loss 3.523281, Accuracy 92.656%\n",
      "Epoch 38, Batch 16, LR 0.030877 Loss 3.513192, Accuracy 92.676%\n",
      "Epoch 38, Batch 17, LR 0.030848 Loss 3.521256, Accuracy 92.509%\n",
      "Epoch 38, Batch 18, LR 0.030818 Loss 3.512829, Accuracy 92.665%\n",
      "Epoch 38, Batch 19, LR 0.030789 Loss 3.514559, Accuracy 92.763%\n",
      "Epoch 38, Batch 20, LR 0.030759 Loss 3.553294, Accuracy 92.578%\n",
      "Epoch 38, Batch 21, LR 0.030730 Loss 3.575970, Accuracy 92.597%\n",
      "Epoch 38, Batch 22, LR 0.030700 Loss 3.557741, Accuracy 92.720%\n",
      "Epoch 38, Batch 23, LR 0.030671 Loss 3.547053, Accuracy 92.731%\n",
      "Epoch 38, Batch 24, LR 0.030641 Loss 3.530841, Accuracy 92.741%\n",
      "Epoch 38, Batch 25, LR 0.030612 Loss 3.527012, Accuracy 92.812%\n",
      "Epoch 38, Batch 26, LR 0.030582 Loss 3.522129, Accuracy 92.698%\n",
      "Epoch 38, Batch 27, LR 0.030553 Loss 3.520153, Accuracy 92.795%\n",
      "Epoch 38, Batch 28, LR 0.030523 Loss 3.537309, Accuracy 92.746%\n",
      "Epoch 38, Batch 29, LR 0.030494 Loss 3.534850, Accuracy 92.753%\n",
      "Epoch 38, Batch 30, LR 0.030465 Loss 3.510153, Accuracy 92.839%\n",
      "Epoch 38, Batch 31, LR 0.030435 Loss 3.519105, Accuracy 92.767%\n",
      "Epoch 38, Batch 32, LR 0.030406 Loss 3.535019, Accuracy 92.676%\n",
      "Epoch 38, Batch 33, LR 0.030376 Loss 3.544162, Accuracy 92.661%\n",
      "Epoch 38, Batch 34, LR 0.030347 Loss 3.539574, Accuracy 92.716%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 35, LR 0.030318 Loss 3.531088, Accuracy 92.790%\n",
      "Epoch 38, Batch 36, LR 0.030288 Loss 3.534179, Accuracy 92.687%\n",
      "Epoch 38, Batch 37, LR 0.030259 Loss 3.534531, Accuracy 92.673%\n",
      "Epoch 38, Batch 38, LR 0.030230 Loss 3.532741, Accuracy 92.701%\n",
      "Epoch 38, Batch 39, LR 0.030201 Loss 3.520325, Accuracy 92.748%\n",
      "Epoch 38, Batch 40, LR 0.030171 Loss 3.524268, Accuracy 92.676%\n",
      "Epoch 38, Batch 41, LR 0.030142 Loss 3.518699, Accuracy 92.683%\n",
      "Epoch 38, Batch 42, LR 0.030113 Loss 3.515005, Accuracy 92.727%\n",
      "Epoch 38, Batch 43, LR 0.030084 Loss 3.499648, Accuracy 92.842%\n",
      "Epoch 38, Batch 44, LR 0.030054 Loss 3.499139, Accuracy 92.844%\n",
      "Epoch 38, Batch 45, LR 0.030025 Loss 3.506582, Accuracy 92.830%\n",
      "Epoch 38, Batch 46, LR 0.029996 Loss 3.506089, Accuracy 92.867%\n",
      "Epoch 38, Batch 47, LR 0.029967 Loss 3.514729, Accuracy 92.819%\n",
      "Epoch 38, Batch 48, LR 0.029938 Loss 3.512285, Accuracy 92.887%\n",
      "Epoch 38, Batch 49, LR 0.029909 Loss 3.520296, Accuracy 92.841%\n",
      "Epoch 38, Batch 50, LR 0.029880 Loss 3.523495, Accuracy 92.797%\n",
      "Epoch 38, Batch 51, LR 0.029850 Loss 3.517324, Accuracy 92.877%\n",
      "Epoch 38, Batch 52, LR 0.029821 Loss 3.518964, Accuracy 92.819%\n",
      "Epoch 38, Batch 53, LR 0.029792 Loss 3.510701, Accuracy 92.807%\n",
      "Epoch 38, Batch 54, LR 0.029763 Loss 3.508612, Accuracy 92.839%\n",
      "Epoch 38, Batch 55, LR 0.029734 Loss 3.512370, Accuracy 92.841%\n",
      "Epoch 38, Batch 56, LR 0.029705 Loss 3.512583, Accuracy 92.899%\n",
      "Epoch 38, Batch 57, LR 0.029676 Loss 3.517057, Accuracy 92.955%\n",
      "Epoch 38, Batch 58, LR 0.029647 Loss 3.513834, Accuracy 92.901%\n",
      "Epoch 38, Batch 59, LR 0.029618 Loss 3.519927, Accuracy 92.876%\n",
      "Epoch 38, Batch 60, LR 0.029589 Loss 3.517363, Accuracy 92.891%\n",
      "Epoch 38, Batch 61, LR 0.029560 Loss 3.515546, Accuracy 92.892%\n",
      "Epoch 38, Batch 62, LR 0.029531 Loss 3.510563, Accuracy 92.893%\n",
      "Epoch 38, Batch 63, LR 0.029502 Loss 3.502959, Accuracy 92.956%\n",
      "Epoch 38, Batch 64, LR 0.029473 Loss 3.510441, Accuracy 92.883%\n",
      "Epoch 38, Batch 65, LR 0.029445 Loss 3.512772, Accuracy 92.885%\n",
      "Epoch 38, Batch 66, LR 0.029416 Loss 3.513477, Accuracy 92.862%\n",
      "Epoch 38, Batch 67, LR 0.029387 Loss 3.519558, Accuracy 92.852%\n",
      "Epoch 38, Batch 68, LR 0.029358 Loss 3.521223, Accuracy 92.831%\n",
      "Epoch 38, Batch 69, LR 0.029329 Loss 3.522057, Accuracy 92.799%\n",
      "Epoch 38, Batch 70, LR 0.029300 Loss 3.513890, Accuracy 92.857%\n",
      "Epoch 38, Batch 71, LR 0.029271 Loss 3.507383, Accuracy 92.914%\n",
      "Epoch 38, Batch 72, LR 0.029243 Loss 3.507818, Accuracy 92.936%\n",
      "Epoch 38, Batch 73, LR 0.029214 Loss 3.507911, Accuracy 92.937%\n",
      "Epoch 38, Batch 74, LR 0.029185 Loss 3.511139, Accuracy 92.937%\n",
      "Epoch 38, Batch 75, LR 0.029156 Loss 3.500934, Accuracy 92.979%\n",
      "Epoch 38, Batch 76, LR 0.029128 Loss 3.491876, Accuracy 93.010%\n",
      "Epoch 38, Batch 77, LR 0.029099 Loss 3.497409, Accuracy 93.009%\n",
      "Epoch 38, Batch 78, LR 0.029070 Loss 3.502673, Accuracy 92.979%\n",
      "Epoch 38, Batch 79, LR 0.029041 Loss 3.507372, Accuracy 92.998%\n",
      "Epoch 38, Batch 80, LR 0.029013 Loss 3.499819, Accuracy 93.047%\n",
      "Epoch 38, Batch 81, LR 0.028984 Loss 3.508210, Accuracy 92.998%\n",
      "Epoch 38, Batch 82, LR 0.028955 Loss 3.503117, Accuracy 93.007%\n",
      "Epoch 38, Batch 83, LR 0.028927 Loss 3.506464, Accuracy 92.997%\n",
      "Epoch 38, Batch 84, LR 0.028898 Loss 3.497635, Accuracy 93.025%\n",
      "Epoch 38, Batch 85, LR 0.028869 Loss 3.489467, Accuracy 93.024%\n",
      "Epoch 38, Batch 86, LR 0.028841 Loss 3.483100, Accuracy 93.069%\n",
      "Epoch 38, Batch 87, LR 0.028812 Loss 3.482948, Accuracy 93.059%\n",
      "Epoch 38, Batch 88, LR 0.028784 Loss 3.478787, Accuracy 93.084%\n",
      "Epoch 38, Batch 89, LR 0.028755 Loss 3.477726, Accuracy 93.092%\n",
      "Epoch 38, Batch 90, LR 0.028727 Loss 3.477748, Accuracy 93.056%\n",
      "Epoch 38, Batch 91, LR 0.028698 Loss 3.479248, Accuracy 93.029%\n",
      "Epoch 38, Batch 92, LR 0.028669 Loss 3.483565, Accuracy 93.028%\n",
      "Epoch 38, Batch 93, LR 0.028641 Loss 3.486177, Accuracy 93.019%\n",
      "Epoch 38, Batch 94, LR 0.028612 Loss 3.490327, Accuracy 92.985%\n",
      "Epoch 38, Batch 95, LR 0.028584 Loss 3.492676, Accuracy 92.985%\n",
      "Epoch 38, Batch 96, LR 0.028555 Loss 3.489178, Accuracy 92.969%\n",
      "Epoch 38, Batch 97, LR 0.028527 Loss 3.488024, Accuracy 92.977%\n",
      "Epoch 38, Batch 98, LR 0.028499 Loss 3.488721, Accuracy 92.977%\n",
      "Epoch 38, Batch 99, LR 0.028470 Loss 3.489282, Accuracy 92.977%\n",
      "Epoch 38, Batch 100, LR 0.028442 Loss 3.484318, Accuracy 92.984%\n",
      "Epoch 38, Batch 101, LR 0.028413 Loss 3.482796, Accuracy 93.007%\n",
      "Epoch 38, Batch 102, LR 0.028385 Loss 3.482315, Accuracy 93.015%\n",
      "Epoch 38, Batch 103, LR 0.028357 Loss 3.485418, Accuracy 92.976%\n",
      "Epoch 38, Batch 104, LR 0.028328 Loss 3.479261, Accuracy 93.006%\n",
      "Epoch 38, Batch 105, LR 0.028300 Loss 3.477486, Accuracy 93.013%\n",
      "Epoch 38, Batch 106, LR 0.028272 Loss 3.478375, Accuracy 93.006%\n",
      "Epoch 38, Batch 107, LR 0.028243 Loss 3.487288, Accuracy 92.932%\n",
      "Epoch 38, Batch 108, LR 0.028215 Loss 3.482098, Accuracy 92.954%\n",
      "Epoch 38, Batch 109, LR 0.028187 Loss 3.482947, Accuracy 92.969%\n",
      "Epoch 38, Batch 110, LR 0.028158 Loss 3.487666, Accuracy 92.969%\n",
      "Epoch 38, Batch 111, LR 0.028130 Loss 3.483560, Accuracy 92.990%\n",
      "Epoch 38, Batch 112, LR 0.028102 Loss 3.480443, Accuracy 92.997%\n",
      "Epoch 38, Batch 113, LR 0.028074 Loss 3.478066, Accuracy 93.003%\n",
      "Epoch 38, Batch 114, LR 0.028045 Loss 3.479250, Accuracy 93.010%\n",
      "Epoch 38, Batch 115, LR 0.028017 Loss 3.478942, Accuracy 93.010%\n",
      "Epoch 38, Batch 116, LR 0.027989 Loss 3.473533, Accuracy 93.029%\n",
      "Epoch 38, Batch 117, LR 0.027961 Loss 3.478389, Accuracy 93.009%\n",
      "Epoch 38, Batch 118, LR 0.027933 Loss 3.481080, Accuracy 93.002%\n",
      "Epoch 38, Batch 119, LR 0.027905 Loss 3.478369, Accuracy 93.021%\n",
      "Epoch 38, Batch 120, LR 0.027876 Loss 3.478141, Accuracy 93.034%\n",
      "Epoch 38, Batch 121, LR 0.027848 Loss 3.477935, Accuracy 93.033%\n",
      "Epoch 38, Batch 122, LR 0.027820 Loss 3.474575, Accuracy 93.046%\n",
      "Epoch 38, Batch 123, LR 0.027792 Loss 3.474653, Accuracy 93.070%\n",
      "Epoch 38, Batch 124, LR 0.027764 Loss 3.477715, Accuracy 93.051%\n",
      "Epoch 38, Batch 125, LR 0.027736 Loss 3.478751, Accuracy 93.025%\n",
      "Epoch 38, Batch 126, LR 0.027708 Loss 3.484491, Accuracy 93.006%\n",
      "Epoch 38, Batch 127, LR 0.027680 Loss 3.483503, Accuracy 93.024%\n",
      "Epoch 38, Batch 128, LR 0.027652 Loss 3.487276, Accuracy 93.011%\n",
      "Epoch 38, Batch 129, LR 0.027624 Loss 3.484597, Accuracy 93.023%\n",
      "Epoch 38, Batch 130, LR 0.027596 Loss 3.483485, Accuracy 93.041%\n",
      "Epoch 38, Batch 131, LR 0.027568 Loss 3.482873, Accuracy 93.034%\n",
      "Epoch 38, Batch 132, LR 0.027540 Loss 3.484502, Accuracy 93.028%\n",
      "Epoch 38, Batch 133, LR 0.027512 Loss 3.490799, Accuracy 93.010%\n",
      "Epoch 38, Batch 134, LR 0.027484 Loss 3.488489, Accuracy 93.010%\n",
      "Epoch 38, Batch 135, LR 0.027456 Loss 3.488316, Accuracy 93.015%\n",
      "Epoch 38, Batch 136, LR 0.027428 Loss 3.486559, Accuracy 93.015%\n",
      "Epoch 38, Batch 137, LR 0.027400 Loss 3.482733, Accuracy 93.031%\n",
      "Epoch 38, Batch 138, LR 0.027372 Loss 3.488640, Accuracy 93.037%\n",
      "Epoch 38, Batch 139, LR 0.027345 Loss 3.484589, Accuracy 93.059%\n",
      "Epoch 38, Batch 140, LR 0.027317 Loss 3.485137, Accuracy 93.058%\n",
      "Epoch 38, Batch 141, LR 0.027289 Loss 3.491802, Accuracy 93.046%\n",
      "Epoch 38, Batch 142, LR 0.027261 Loss 3.489442, Accuracy 93.057%\n",
      "Epoch 38, Batch 143, LR 0.027233 Loss 3.487091, Accuracy 93.062%\n",
      "Epoch 38, Batch 144, LR 0.027205 Loss 3.486931, Accuracy 93.045%\n",
      "Epoch 38, Batch 145, LR 0.027178 Loss 3.487659, Accuracy 93.055%\n",
      "Epoch 38, Batch 146, LR 0.027150 Loss 3.493309, Accuracy 93.017%\n",
      "Epoch 38, Batch 147, LR 0.027122 Loss 3.489963, Accuracy 93.033%\n",
      "Epoch 38, Batch 148, LR 0.027094 Loss 3.487949, Accuracy 93.032%\n",
      "Epoch 38, Batch 149, LR 0.027067 Loss 3.482929, Accuracy 93.063%\n",
      "Epoch 38, Batch 150, LR 0.027039 Loss 3.486681, Accuracy 93.031%\n",
      "Epoch 38, Batch 151, LR 0.027011 Loss 3.489599, Accuracy 93.031%\n",
      "Epoch 38, Batch 152, LR 0.026984 Loss 3.489584, Accuracy 93.036%\n",
      "Epoch 38, Batch 153, LR 0.026956 Loss 3.492733, Accuracy 93.020%\n",
      "Epoch 38, Batch 154, LR 0.026928 Loss 3.491154, Accuracy 93.025%\n",
      "Epoch 38, Batch 155, LR 0.026901 Loss 3.490487, Accuracy 93.039%\n",
      "Epoch 38, Batch 156, LR 0.026873 Loss 3.492552, Accuracy 93.034%\n",
      "Epoch 38, Batch 157, LR 0.026845 Loss 3.492835, Accuracy 93.038%\n",
      "Epoch 38, Batch 158, LR 0.026818 Loss 3.494161, Accuracy 93.038%\n",
      "Epoch 38, Batch 159, LR 0.026790 Loss 3.497866, Accuracy 93.018%\n",
      "Epoch 38, Batch 160, LR 0.026763 Loss 3.496720, Accuracy 93.042%\n",
      "Epoch 38, Batch 161, LR 0.026735 Loss 3.498065, Accuracy 93.042%\n",
      "Epoch 38, Batch 162, LR 0.026708 Loss 3.496315, Accuracy 93.046%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 163, LR 0.026680 Loss 3.499467, Accuracy 93.036%\n",
      "Epoch 38, Batch 164, LR 0.026652 Loss 3.496585, Accuracy 93.050%\n",
      "Epoch 38, Batch 165, LR 0.026625 Loss 3.495845, Accuracy 93.045%\n",
      "Epoch 38, Batch 166, LR 0.026598 Loss 3.496325, Accuracy 93.053%\n",
      "Epoch 38, Batch 167, LR 0.026570 Loss 3.499607, Accuracy 93.039%\n",
      "Epoch 38, Batch 168, LR 0.026543 Loss 3.501177, Accuracy 93.025%\n",
      "Epoch 38, Batch 169, LR 0.026515 Loss 3.499272, Accuracy 93.029%\n",
      "Epoch 38, Batch 170, LR 0.026488 Loss 3.497144, Accuracy 93.047%\n",
      "Epoch 38, Batch 171, LR 0.026460 Loss 3.497580, Accuracy 93.051%\n",
      "Epoch 38, Batch 172, LR 0.026433 Loss 3.496503, Accuracy 93.055%\n",
      "Epoch 38, Batch 173, LR 0.026405 Loss 3.490828, Accuracy 93.073%\n",
      "Epoch 38, Batch 174, LR 0.026378 Loss 3.489106, Accuracy 93.077%\n",
      "Epoch 38, Batch 175, LR 0.026351 Loss 3.489152, Accuracy 93.089%\n",
      "Epoch 38, Batch 176, LR 0.026323 Loss 3.487806, Accuracy 93.115%\n",
      "Epoch 38, Batch 177, LR 0.026296 Loss 3.487759, Accuracy 93.114%\n",
      "Epoch 38, Batch 178, LR 0.026269 Loss 3.488805, Accuracy 93.109%\n",
      "Epoch 38, Batch 179, LR 0.026241 Loss 3.492456, Accuracy 93.104%\n",
      "Epoch 38, Batch 180, LR 0.026214 Loss 3.492523, Accuracy 93.077%\n",
      "Epoch 38, Batch 181, LR 0.026187 Loss 3.490143, Accuracy 93.081%\n",
      "Epoch 38, Batch 182, LR 0.026160 Loss 3.488357, Accuracy 93.093%\n",
      "Epoch 38, Batch 183, LR 0.026132 Loss 3.484614, Accuracy 93.114%\n",
      "Epoch 38, Batch 184, LR 0.026105 Loss 3.483522, Accuracy 93.122%\n",
      "Epoch 38, Batch 185, LR 0.026078 Loss 3.486549, Accuracy 93.129%\n",
      "Epoch 38, Batch 186, LR 0.026051 Loss 3.486683, Accuracy 93.133%\n",
      "Epoch 38, Batch 187, LR 0.026024 Loss 3.490027, Accuracy 93.123%\n",
      "Epoch 38, Batch 188, LR 0.025996 Loss 3.494713, Accuracy 93.098%\n",
      "Epoch 38, Batch 189, LR 0.025969 Loss 3.495246, Accuracy 93.105%\n",
      "Epoch 38, Batch 190, LR 0.025942 Loss 3.494446, Accuracy 93.113%\n",
      "Epoch 38, Batch 191, LR 0.025915 Loss 3.492080, Accuracy 93.120%\n",
      "Epoch 38, Batch 192, LR 0.025888 Loss 3.491345, Accuracy 93.132%\n",
      "Epoch 38, Batch 193, LR 0.025861 Loss 3.493930, Accuracy 93.119%\n",
      "Epoch 38, Batch 194, LR 0.025834 Loss 3.490646, Accuracy 93.134%\n",
      "Epoch 38, Batch 195, LR 0.025807 Loss 3.486666, Accuracy 93.149%\n",
      "Epoch 38, Batch 196, LR 0.025779 Loss 3.486006, Accuracy 93.148%\n",
      "Epoch 38, Batch 197, LR 0.025752 Loss 3.487012, Accuracy 93.139%\n",
      "Epoch 38, Batch 198, LR 0.025725 Loss 3.485925, Accuracy 93.134%\n",
      "Epoch 38, Batch 199, LR 0.025698 Loss 3.483730, Accuracy 93.130%\n",
      "Epoch 38, Batch 200, LR 0.025671 Loss 3.485216, Accuracy 93.117%\n",
      "Epoch 38, Batch 201, LR 0.025644 Loss 3.486594, Accuracy 93.120%\n",
      "Epoch 38, Batch 202, LR 0.025617 Loss 3.486803, Accuracy 93.116%\n",
      "Epoch 38, Batch 203, LR 0.025590 Loss 3.487028, Accuracy 93.127%\n",
      "Epoch 38, Batch 204, LR 0.025563 Loss 3.486651, Accuracy 93.118%\n",
      "Epoch 38, Batch 205, LR 0.025536 Loss 3.485210, Accuracy 93.129%\n",
      "Epoch 38, Batch 206, LR 0.025510 Loss 3.483182, Accuracy 93.139%\n",
      "Epoch 38, Batch 207, LR 0.025483 Loss 3.484836, Accuracy 93.135%\n",
      "Epoch 38, Batch 208, LR 0.025456 Loss 3.482589, Accuracy 93.149%\n",
      "Epoch 38, Batch 209, LR 0.025429 Loss 3.484034, Accuracy 93.133%\n",
      "Epoch 38, Batch 210, LR 0.025402 Loss 3.486185, Accuracy 93.125%\n",
      "Epoch 38, Batch 211, LR 0.025375 Loss 3.487798, Accuracy 93.113%\n",
      "Epoch 38, Batch 212, LR 0.025348 Loss 3.487960, Accuracy 93.109%\n",
      "Epoch 38, Batch 213, LR 0.025321 Loss 3.487943, Accuracy 93.112%\n",
      "Epoch 38, Batch 214, LR 0.025295 Loss 3.489797, Accuracy 93.111%\n",
      "Epoch 38, Batch 215, LR 0.025268 Loss 3.489099, Accuracy 93.110%\n",
      "Epoch 38, Batch 216, LR 0.025241 Loss 3.490375, Accuracy 93.106%\n",
      "Epoch 38, Batch 217, LR 0.025214 Loss 3.490041, Accuracy 93.109%\n",
      "Epoch 38, Batch 218, LR 0.025188 Loss 3.487267, Accuracy 93.119%\n",
      "Epoch 38, Batch 219, LR 0.025161 Loss 3.487074, Accuracy 93.111%\n",
      "Epoch 38, Batch 220, LR 0.025134 Loss 3.490042, Accuracy 93.093%\n",
      "Epoch 38, Batch 221, LR 0.025107 Loss 3.489430, Accuracy 93.107%\n",
      "Epoch 38, Batch 222, LR 0.025081 Loss 3.489462, Accuracy 93.113%\n",
      "Epoch 38, Batch 223, LR 0.025054 Loss 3.491921, Accuracy 93.098%\n",
      "Epoch 38, Batch 224, LR 0.025027 Loss 3.489753, Accuracy 93.105%\n",
      "Epoch 38, Batch 225, LR 0.025001 Loss 3.488551, Accuracy 93.111%\n",
      "Epoch 38, Batch 226, LR 0.024974 Loss 3.489703, Accuracy 93.104%\n",
      "Epoch 38, Batch 227, LR 0.024947 Loss 3.486394, Accuracy 93.110%\n",
      "Epoch 38, Batch 228, LR 0.024921 Loss 3.487218, Accuracy 93.113%\n",
      "Epoch 38, Batch 229, LR 0.024894 Loss 3.485347, Accuracy 93.126%\n",
      "Epoch 38, Batch 230, LR 0.024868 Loss 3.484909, Accuracy 93.139%\n",
      "Epoch 38, Batch 231, LR 0.024841 Loss 3.486229, Accuracy 93.134%\n",
      "Epoch 38, Batch 232, LR 0.024814 Loss 3.485790, Accuracy 93.147%\n",
      "Epoch 38, Batch 233, LR 0.024788 Loss 3.489886, Accuracy 93.123%\n",
      "Epoch 38, Batch 234, LR 0.024761 Loss 3.488350, Accuracy 93.132%\n",
      "Epoch 38, Batch 235, LR 0.024735 Loss 3.488107, Accuracy 93.128%\n",
      "Epoch 38, Batch 236, LR 0.024708 Loss 3.489319, Accuracy 93.124%\n",
      "Epoch 38, Batch 237, LR 0.024682 Loss 3.488341, Accuracy 93.130%\n",
      "Epoch 38, Batch 238, LR 0.024655 Loss 3.487345, Accuracy 93.130%\n",
      "Epoch 38, Batch 239, LR 0.024629 Loss 3.487271, Accuracy 93.129%\n",
      "Epoch 38, Batch 240, LR 0.024603 Loss 3.491834, Accuracy 93.118%\n",
      "Epoch 38, Batch 241, LR 0.024576 Loss 3.492792, Accuracy 93.118%\n",
      "Epoch 38, Batch 242, LR 0.024550 Loss 3.493731, Accuracy 93.111%\n",
      "Epoch 38, Batch 243, LR 0.024523 Loss 3.492922, Accuracy 93.123%\n",
      "Epoch 38, Batch 244, LR 0.024497 Loss 3.493256, Accuracy 93.122%\n",
      "Epoch 38, Batch 245, LR 0.024470 Loss 3.489928, Accuracy 93.135%\n",
      "Epoch 38, Batch 246, LR 0.024444 Loss 3.489753, Accuracy 93.140%\n",
      "Epoch 38, Batch 247, LR 0.024418 Loss 3.492169, Accuracy 93.133%\n",
      "Epoch 38, Batch 248, LR 0.024391 Loss 3.491961, Accuracy 93.148%\n",
      "Epoch 38, Batch 249, LR 0.024365 Loss 3.494468, Accuracy 93.141%\n",
      "Epoch 38, Batch 250, LR 0.024339 Loss 3.495995, Accuracy 93.131%\n",
      "Epoch 38, Batch 251, LR 0.024313 Loss 3.495833, Accuracy 93.121%\n",
      "Epoch 38, Batch 252, LR 0.024286 Loss 3.495432, Accuracy 93.121%\n",
      "Epoch 38, Batch 253, LR 0.024260 Loss 3.497780, Accuracy 93.120%\n",
      "Epoch 38, Batch 254, LR 0.024234 Loss 3.498119, Accuracy 93.116%\n",
      "Epoch 38, Batch 255, LR 0.024207 Loss 3.499638, Accuracy 93.104%\n",
      "Epoch 38, Batch 256, LR 0.024181 Loss 3.502192, Accuracy 93.100%\n",
      "Epoch 38, Batch 257, LR 0.024155 Loss 3.502106, Accuracy 93.106%\n",
      "Epoch 38, Batch 258, LR 0.024129 Loss 3.501757, Accuracy 93.108%\n",
      "Epoch 38, Batch 259, LR 0.024103 Loss 3.502692, Accuracy 93.108%\n",
      "Epoch 38, Batch 260, LR 0.024077 Loss 3.504624, Accuracy 93.104%\n",
      "Epoch 38, Batch 261, LR 0.024050 Loss 3.502836, Accuracy 93.115%\n",
      "Epoch 38, Batch 262, LR 0.024024 Loss 3.502452, Accuracy 93.115%\n",
      "Epoch 38, Batch 263, LR 0.023998 Loss 3.501747, Accuracy 93.114%\n",
      "Epoch 38, Batch 264, LR 0.023972 Loss 3.500590, Accuracy 93.123%\n",
      "Epoch 38, Batch 265, LR 0.023946 Loss 3.502057, Accuracy 93.110%\n",
      "Epoch 38, Batch 266, LR 0.023920 Loss 3.500667, Accuracy 93.127%\n",
      "Epoch 38, Batch 267, LR 0.023894 Loss 3.499610, Accuracy 93.136%\n",
      "Epoch 38, Batch 268, LR 0.023868 Loss 3.499586, Accuracy 93.132%\n",
      "Epoch 38, Batch 269, LR 0.023842 Loss 3.500537, Accuracy 93.123%\n",
      "Epoch 38, Batch 270, LR 0.023816 Loss 3.502332, Accuracy 93.111%\n",
      "Epoch 38, Batch 271, LR 0.023790 Loss 3.504175, Accuracy 93.098%\n",
      "Epoch 38, Batch 272, LR 0.023764 Loss 3.504073, Accuracy 93.107%\n",
      "Epoch 38, Batch 273, LR 0.023738 Loss 3.503690, Accuracy 93.103%\n",
      "Epoch 38, Batch 274, LR 0.023712 Loss 3.504803, Accuracy 93.091%\n",
      "Epoch 38, Batch 275, LR 0.023686 Loss 3.505445, Accuracy 93.094%\n",
      "Epoch 38, Batch 276, LR 0.023660 Loss 3.505531, Accuracy 93.099%\n",
      "Epoch 38, Batch 277, LR 0.023634 Loss 3.504802, Accuracy 93.104%\n",
      "Epoch 38, Batch 278, LR 0.023608 Loss 3.505254, Accuracy 93.106%\n",
      "Epoch 38, Batch 279, LR 0.023582 Loss 3.507952, Accuracy 93.100%\n",
      "Epoch 38, Batch 280, LR 0.023556 Loss 3.509290, Accuracy 93.103%\n",
      "Epoch 38, Batch 281, LR 0.023530 Loss 3.510446, Accuracy 93.108%\n",
      "Epoch 38, Batch 282, LR 0.023504 Loss 3.510087, Accuracy 93.113%\n",
      "Epoch 38, Batch 283, LR 0.023479 Loss 3.511114, Accuracy 93.118%\n",
      "Epoch 38, Batch 284, LR 0.023453 Loss 3.511757, Accuracy 93.098%\n",
      "Epoch 38, Batch 285, LR 0.023427 Loss 3.510234, Accuracy 93.100%\n",
      "Epoch 38, Batch 286, LR 0.023401 Loss 3.508925, Accuracy 93.108%\n",
      "Epoch 38, Batch 287, LR 0.023375 Loss 3.508542, Accuracy 93.108%\n",
      "Epoch 38, Batch 288, LR 0.023350 Loss 3.510172, Accuracy 93.088%\n",
      "Epoch 38, Batch 289, LR 0.023324 Loss 3.512099, Accuracy 93.088%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 290, LR 0.023298 Loss 3.510790, Accuracy 93.090%\n",
      "Epoch 38, Batch 291, LR 0.023272 Loss 3.513451, Accuracy 93.071%\n",
      "Epoch 38, Batch 292, LR 0.023247 Loss 3.514140, Accuracy 93.078%\n",
      "Epoch 38, Batch 293, LR 0.023221 Loss 3.514841, Accuracy 93.075%\n",
      "Epoch 38, Batch 294, LR 0.023195 Loss 3.514233, Accuracy 93.088%\n",
      "Epoch 38, Batch 295, LR 0.023170 Loss 3.512121, Accuracy 93.099%\n",
      "Epoch 38, Batch 296, LR 0.023144 Loss 3.511667, Accuracy 93.098%\n",
      "Epoch 38, Batch 297, LR 0.023118 Loss 3.511978, Accuracy 93.098%\n",
      "Epoch 38, Batch 298, LR 0.023093 Loss 3.509438, Accuracy 93.110%\n",
      "Epoch 38, Batch 299, LR 0.023067 Loss 3.509759, Accuracy 93.102%\n",
      "Epoch 38, Batch 300, LR 0.023041 Loss 3.509643, Accuracy 93.109%\n",
      "Epoch 38, Batch 301, LR 0.023016 Loss 3.509241, Accuracy 93.106%\n",
      "Epoch 38, Batch 302, LR 0.022990 Loss 3.510479, Accuracy 93.108%\n",
      "Epoch 38, Batch 303, LR 0.022965 Loss 3.511012, Accuracy 93.113%\n",
      "Epoch 38, Batch 304, LR 0.022939 Loss 3.510890, Accuracy 93.113%\n",
      "Epoch 38, Batch 305, LR 0.022914 Loss 3.512235, Accuracy 93.107%\n",
      "Epoch 38, Batch 306, LR 0.022888 Loss 3.512879, Accuracy 93.102%\n",
      "Epoch 38, Batch 307, LR 0.022863 Loss 3.514536, Accuracy 93.088%\n",
      "Epoch 38, Batch 308, LR 0.022837 Loss 3.515190, Accuracy 93.093%\n",
      "Epoch 38, Batch 309, LR 0.022812 Loss 3.516550, Accuracy 93.083%\n",
      "Epoch 38, Batch 310, LR 0.022786 Loss 3.516492, Accuracy 93.080%\n",
      "Epoch 38, Batch 311, LR 0.022761 Loss 3.515655, Accuracy 93.079%\n",
      "Epoch 38, Batch 312, LR 0.022735 Loss 3.515523, Accuracy 93.076%\n",
      "Epoch 38, Batch 313, LR 0.022710 Loss 3.517449, Accuracy 93.061%\n",
      "Epoch 38, Batch 314, LR 0.022685 Loss 3.518635, Accuracy 93.048%\n",
      "Epoch 38, Batch 315, LR 0.022659 Loss 3.521243, Accuracy 93.046%\n",
      "Epoch 38, Batch 316, LR 0.022634 Loss 3.521022, Accuracy 93.050%\n",
      "Epoch 38, Batch 317, LR 0.022608 Loss 3.522141, Accuracy 93.045%\n",
      "Epoch 38, Batch 318, LR 0.022583 Loss 3.521688, Accuracy 93.040%\n",
      "Epoch 38, Batch 319, LR 0.022558 Loss 3.523379, Accuracy 93.023%\n",
      "Epoch 38, Batch 320, LR 0.022532 Loss 3.523331, Accuracy 93.020%\n",
      "Epoch 38, Batch 321, LR 0.022507 Loss 3.524915, Accuracy 93.020%\n",
      "Epoch 38, Batch 322, LR 0.022482 Loss 3.524084, Accuracy 93.027%\n",
      "Epoch 38, Batch 323, LR 0.022457 Loss 3.521859, Accuracy 93.039%\n",
      "Epoch 38, Batch 324, LR 0.022431 Loss 3.522009, Accuracy 93.041%\n",
      "Epoch 38, Batch 325, LR 0.022406 Loss 3.519873, Accuracy 93.046%\n",
      "Epoch 38, Batch 326, LR 0.022381 Loss 3.519578, Accuracy 93.041%\n",
      "Epoch 38, Batch 327, LR 0.022356 Loss 3.519414, Accuracy 93.045%\n",
      "Epoch 38, Batch 328, LR 0.022330 Loss 3.518678, Accuracy 93.052%\n",
      "Epoch 38, Batch 329, LR 0.022305 Loss 3.518541, Accuracy 93.042%\n",
      "Epoch 38, Batch 330, LR 0.022280 Loss 3.519529, Accuracy 93.037%\n",
      "Epoch 38, Batch 331, LR 0.022255 Loss 3.518651, Accuracy 93.042%\n",
      "Epoch 38, Batch 332, LR 0.022230 Loss 3.519959, Accuracy 93.037%\n",
      "Epoch 38, Batch 333, LR 0.022205 Loss 3.520152, Accuracy 93.037%\n",
      "Epoch 38, Batch 334, LR 0.022179 Loss 3.520247, Accuracy 93.037%\n",
      "Epoch 38, Batch 335, LR 0.022154 Loss 3.519935, Accuracy 93.036%\n",
      "Epoch 38, Batch 336, LR 0.022129 Loss 3.518962, Accuracy 93.036%\n",
      "Epoch 38, Batch 337, LR 0.022104 Loss 3.518569, Accuracy 93.038%\n",
      "Epoch 38, Batch 338, LR 0.022079 Loss 3.518163, Accuracy 93.045%\n",
      "Epoch 38, Batch 339, LR 0.022054 Loss 3.516283, Accuracy 93.056%\n",
      "Epoch 38, Batch 340, LR 0.022029 Loss 3.517106, Accuracy 93.056%\n",
      "Epoch 38, Batch 341, LR 0.022004 Loss 3.517622, Accuracy 93.049%\n",
      "Epoch 38, Batch 342, LR 0.021979 Loss 3.519068, Accuracy 93.049%\n",
      "Epoch 38, Batch 343, LR 0.021954 Loss 3.519730, Accuracy 93.048%\n",
      "Epoch 38, Batch 344, LR 0.021929 Loss 3.521664, Accuracy 93.044%\n",
      "Epoch 38, Batch 345, LR 0.021904 Loss 3.522337, Accuracy 93.037%\n",
      "Epoch 38, Batch 346, LR 0.021879 Loss 3.521734, Accuracy 93.039%\n",
      "Epoch 38, Batch 347, LR 0.021854 Loss 3.521682, Accuracy 93.043%\n",
      "Epoch 38, Batch 348, LR 0.021829 Loss 3.521074, Accuracy 93.050%\n",
      "Epoch 38, Batch 349, LR 0.021804 Loss 3.522251, Accuracy 93.049%\n",
      "Epoch 38, Batch 350, LR 0.021779 Loss 3.521430, Accuracy 93.054%\n",
      "Epoch 38, Batch 351, LR 0.021755 Loss 3.522067, Accuracy 93.051%\n",
      "Epoch 38, Batch 352, LR 0.021730 Loss 3.519601, Accuracy 93.064%\n",
      "Epoch 38, Batch 353, LR 0.021705 Loss 3.517496, Accuracy 93.071%\n",
      "Epoch 38, Batch 354, LR 0.021680 Loss 3.517108, Accuracy 93.072%\n",
      "Epoch 38, Batch 355, LR 0.021655 Loss 3.516102, Accuracy 93.068%\n",
      "Epoch 38, Batch 356, LR 0.021630 Loss 3.515073, Accuracy 93.074%\n",
      "Epoch 38, Batch 357, LR 0.021606 Loss 3.515236, Accuracy 93.074%\n",
      "Epoch 38, Batch 358, LR 0.021581 Loss 3.514397, Accuracy 93.080%\n",
      "Epoch 38, Batch 359, LR 0.021556 Loss 3.513312, Accuracy 93.086%\n",
      "Epoch 38, Batch 360, LR 0.021531 Loss 3.513562, Accuracy 93.090%\n",
      "Epoch 38, Batch 361, LR 0.021506 Loss 3.512553, Accuracy 93.101%\n",
      "Epoch 38, Batch 362, LR 0.021482 Loss 3.511373, Accuracy 93.094%\n",
      "Epoch 38, Batch 363, LR 0.021457 Loss 3.511499, Accuracy 93.091%\n",
      "Epoch 38, Batch 364, LR 0.021432 Loss 3.511001, Accuracy 93.093%\n",
      "Epoch 38, Batch 365, LR 0.021408 Loss 3.512993, Accuracy 93.086%\n",
      "Epoch 38, Batch 366, LR 0.021383 Loss 3.513637, Accuracy 93.080%\n",
      "Epoch 38, Batch 367, LR 0.021358 Loss 3.513361, Accuracy 93.082%\n",
      "Epoch 38, Batch 368, LR 0.021334 Loss 3.512114, Accuracy 93.083%\n",
      "Epoch 38, Batch 369, LR 0.021309 Loss 3.510681, Accuracy 93.083%\n",
      "Epoch 38, Batch 370, LR 0.021284 Loss 3.509672, Accuracy 93.093%\n",
      "Epoch 38, Batch 371, LR 0.021260 Loss 3.507843, Accuracy 93.099%\n",
      "Epoch 38, Batch 372, LR 0.021235 Loss 3.506641, Accuracy 93.105%\n",
      "Epoch 38, Batch 373, LR 0.021211 Loss 3.506869, Accuracy 93.109%\n",
      "Epoch 38, Batch 374, LR 0.021186 Loss 3.506326, Accuracy 93.121%\n",
      "Epoch 38, Batch 375, LR 0.021162 Loss 3.505730, Accuracy 93.125%\n",
      "Epoch 38, Batch 376, LR 0.021137 Loss 3.504995, Accuracy 93.129%\n",
      "Epoch 38, Batch 377, LR 0.021113 Loss 3.504848, Accuracy 93.126%\n",
      "Epoch 38, Batch 378, LR 0.021088 Loss 3.505228, Accuracy 93.124%\n",
      "Epoch 38, Batch 379, LR 0.021064 Loss 3.504143, Accuracy 93.130%\n",
      "Epoch 38, Batch 380, LR 0.021039 Loss 3.504394, Accuracy 93.125%\n",
      "Epoch 38, Batch 381, LR 0.021015 Loss 3.502865, Accuracy 93.127%\n",
      "Epoch 38, Batch 382, LR 0.020990 Loss 3.503783, Accuracy 93.124%\n",
      "Epoch 38, Batch 383, LR 0.020966 Loss 3.503237, Accuracy 93.126%\n",
      "Epoch 38, Batch 384, LR 0.020941 Loss 3.502764, Accuracy 93.127%\n",
      "Epoch 38, Batch 385, LR 0.020917 Loss 3.506016, Accuracy 93.117%\n",
      "Epoch 38, Batch 386, LR 0.020893 Loss 3.506387, Accuracy 93.110%\n",
      "Epoch 38, Batch 387, LR 0.020868 Loss 3.505859, Accuracy 93.104%\n",
      "Epoch 38, Batch 388, LR 0.020844 Loss 3.505925, Accuracy 93.112%\n",
      "Epoch 38, Batch 389, LR 0.020819 Loss 3.506477, Accuracy 93.105%\n",
      "Epoch 38, Batch 390, LR 0.020795 Loss 3.505992, Accuracy 93.115%\n",
      "Epoch 38, Batch 391, LR 0.020771 Loss 3.507295, Accuracy 93.105%\n",
      "Epoch 38, Batch 392, LR 0.020746 Loss 3.506255, Accuracy 93.112%\n",
      "Epoch 38, Batch 393, LR 0.020722 Loss 3.506422, Accuracy 93.114%\n",
      "Epoch 38, Batch 394, LR 0.020698 Loss 3.505216, Accuracy 93.125%\n",
      "Epoch 38, Batch 395, LR 0.020674 Loss 3.504501, Accuracy 93.133%\n",
      "Epoch 38, Batch 396, LR 0.020649 Loss 3.504836, Accuracy 93.134%\n",
      "Epoch 38, Batch 397, LR 0.020625 Loss 3.504375, Accuracy 93.138%\n",
      "Epoch 38, Batch 398, LR 0.020601 Loss 3.504700, Accuracy 93.138%\n",
      "Epoch 38, Batch 399, LR 0.020577 Loss 3.503739, Accuracy 93.141%\n",
      "Epoch 38, Batch 400, LR 0.020553 Loss 3.504205, Accuracy 93.139%\n",
      "Epoch 38, Batch 401, LR 0.020528 Loss 3.504769, Accuracy 93.142%\n",
      "Epoch 38, Batch 402, LR 0.020504 Loss 3.504870, Accuracy 93.148%\n",
      "Epoch 38, Batch 403, LR 0.020480 Loss 3.505103, Accuracy 93.147%\n",
      "Epoch 38, Batch 404, LR 0.020456 Loss 3.505471, Accuracy 93.149%\n",
      "Epoch 38, Batch 405, LR 0.020432 Loss 3.504789, Accuracy 93.158%\n",
      "Epoch 38, Batch 406, LR 0.020408 Loss 3.504397, Accuracy 93.157%\n",
      "Epoch 38, Batch 407, LR 0.020384 Loss 3.504432, Accuracy 93.155%\n",
      "Epoch 38, Batch 408, LR 0.020360 Loss 3.503299, Accuracy 93.156%\n",
      "Epoch 38, Batch 409, LR 0.020335 Loss 3.504869, Accuracy 93.148%\n",
      "Epoch 38, Batch 410, LR 0.020311 Loss 3.504094, Accuracy 93.146%\n",
      "Epoch 38, Batch 411, LR 0.020287 Loss 3.504503, Accuracy 93.142%\n",
      "Epoch 38, Batch 412, LR 0.020263 Loss 3.504833, Accuracy 93.149%\n",
      "Epoch 38, Batch 413, LR 0.020239 Loss 3.505129, Accuracy 93.148%\n",
      "Epoch 38, Batch 414, LR 0.020215 Loss 3.504414, Accuracy 93.154%\n",
      "Epoch 38, Batch 415, LR 0.020191 Loss 3.503309, Accuracy 93.157%\n",
      "Epoch 38, Batch 416, LR 0.020167 Loss 3.504778, Accuracy 93.147%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 417, LR 0.020143 Loss 3.505825, Accuracy 93.132%\n",
      "Epoch 38, Batch 418, LR 0.020120 Loss 3.504982, Accuracy 93.135%\n",
      "Epoch 38, Batch 419, LR 0.020096 Loss 3.506381, Accuracy 93.129%\n",
      "Epoch 38, Batch 420, LR 0.020072 Loss 3.506946, Accuracy 93.131%\n",
      "Epoch 38, Batch 421, LR 0.020048 Loss 3.506476, Accuracy 93.128%\n",
      "Epoch 38, Batch 422, LR 0.020024 Loss 3.505305, Accuracy 93.130%\n",
      "Epoch 38, Batch 423, LR 0.020000 Loss 3.504497, Accuracy 93.126%\n",
      "Epoch 38, Batch 424, LR 0.019976 Loss 3.504046, Accuracy 93.127%\n",
      "Epoch 38, Batch 425, LR 0.019952 Loss 3.505489, Accuracy 93.118%\n",
      "Epoch 38, Batch 426, LR 0.019929 Loss 3.505108, Accuracy 93.117%\n",
      "Epoch 38, Batch 427, LR 0.019905 Loss 3.504882, Accuracy 93.117%\n",
      "Epoch 38, Batch 428, LR 0.019881 Loss 3.505861, Accuracy 93.113%\n",
      "Epoch 38, Batch 429, LR 0.019857 Loss 3.506117, Accuracy 93.107%\n",
      "Epoch 38, Batch 430, LR 0.019833 Loss 3.507091, Accuracy 93.105%\n",
      "Epoch 38, Batch 431, LR 0.019810 Loss 3.505365, Accuracy 93.108%\n",
      "Epoch 38, Batch 432, LR 0.019786 Loss 3.503892, Accuracy 93.115%\n",
      "Epoch 38, Batch 433, LR 0.019762 Loss 3.503057, Accuracy 93.120%\n",
      "Epoch 38, Batch 434, LR 0.019738 Loss 3.501858, Accuracy 93.122%\n",
      "Epoch 38, Batch 435, LR 0.019715 Loss 3.503102, Accuracy 93.120%\n",
      "Epoch 38, Batch 436, LR 0.019691 Loss 3.501932, Accuracy 93.128%\n",
      "Epoch 38, Batch 437, LR 0.019667 Loss 3.502332, Accuracy 93.119%\n",
      "Epoch 38, Batch 438, LR 0.019644 Loss 3.501764, Accuracy 93.122%\n",
      "Epoch 38, Batch 439, LR 0.019620 Loss 3.503133, Accuracy 93.120%\n",
      "Epoch 38, Batch 440, LR 0.019597 Loss 3.503520, Accuracy 93.114%\n",
      "Epoch 38, Batch 441, LR 0.019573 Loss 3.504631, Accuracy 93.116%\n",
      "Epoch 38, Batch 442, LR 0.019549 Loss 3.504436, Accuracy 93.110%\n",
      "Epoch 38, Batch 443, LR 0.019526 Loss 3.503803, Accuracy 93.108%\n",
      "Epoch 38, Batch 444, LR 0.019502 Loss 3.504662, Accuracy 93.101%\n",
      "Epoch 38, Batch 445, LR 0.019479 Loss 3.504329, Accuracy 93.100%\n",
      "Epoch 38, Batch 446, LR 0.019455 Loss 3.503930, Accuracy 93.107%\n",
      "Epoch 38, Batch 447, LR 0.019432 Loss 3.503123, Accuracy 93.112%\n",
      "Epoch 38, Batch 448, LR 0.019408 Loss 3.503086, Accuracy 93.113%\n",
      "Epoch 38, Batch 449, LR 0.019385 Loss 3.504021, Accuracy 93.111%\n",
      "Epoch 38, Batch 450, LR 0.019361 Loss 3.504926, Accuracy 93.109%\n",
      "Epoch 38, Batch 451, LR 0.019338 Loss 3.505088, Accuracy 93.106%\n",
      "Epoch 38, Batch 452, LR 0.019314 Loss 3.505042, Accuracy 93.102%\n",
      "Epoch 38, Batch 453, LR 0.019291 Loss 3.505338, Accuracy 93.098%\n",
      "Epoch 38, Batch 454, LR 0.019267 Loss 3.505038, Accuracy 93.100%\n",
      "Epoch 38, Batch 455, LR 0.019244 Loss 3.504989, Accuracy 93.099%\n",
      "Epoch 38, Batch 456, LR 0.019220 Loss 3.505038, Accuracy 93.106%\n",
      "Epoch 38, Batch 457, LR 0.019197 Loss 3.504149, Accuracy 93.106%\n",
      "Epoch 38, Batch 458, LR 0.019174 Loss 3.503413, Accuracy 93.109%\n",
      "Epoch 38, Batch 459, LR 0.019150 Loss 3.503636, Accuracy 93.113%\n",
      "Epoch 38, Batch 460, LR 0.019127 Loss 3.502531, Accuracy 93.118%\n",
      "Epoch 38, Batch 461, LR 0.019104 Loss 3.502684, Accuracy 93.121%\n",
      "Epoch 38, Batch 462, LR 0.019080 Loss 3.504608, Accuracy 93.111%\n",
      "Epoch 38, Batch 463, LR 0.019057 Loss 3.503154, Accuracy 93.114%\n",
      "Epoch 38, Batch 464, LR 0.019034 Loss 3.503619, Accuracy 93.105%\n",
      "Epoch 38, Batch 465, LR 0.019010 Loss 3.504017, Accuracy 93.105%\n",
      "Epoch 38, Batch 466, LR 0.018987 Loss 3.504373, Accuracy 93.103%\n",
      "Epoch 38, Batch 467, LR 0.018964 Loss 3.504206, Accuracy 93.104%\n",
      "Epoch 38, Batch 468, LR 0.018941 Loss 3.503757, Accuracy 93.109%\n",
      "Epoch 38, Batch 469, LR 0.018918 Loss 3.505067, Accuracy 93.109%\n",
      "Epoch 38, Batch 470, LR 0.018894 Loss 3.505428, Accuracy 93.108%\n",
      "Epoch 38, Batch 471, LR 0.018871 Loss 3.504608, Accuracy 93.115%\n",
      "Epoch 38, Batch 472, LR 0.018848 Loss 3.504234, Accuracy 93.116%\n",
      "Epoch 38, Batch 473, LR 0.018825 Loss 3.504464, Accuracy 93.112%\n",
      "Epoch 38, Batch 474, LR 0.018802 Loss 3.504617, Accuracy 93.110%\n",
      "Epoch 38, Batch 475, LR 0.018779 Loss 3.505270, Accuracy 93.112%\n",
      "Epoch 38, Batch 476, LR 0.018755 Loss 3.505520, Accuracy 93.110%\n",
      "Epoch 38, Batch 477, LR 0.018732 Loss 3.505327, Accuracy 93.111%\n",
      "Epoch 38, Batch 478, LR 0.018709 Loss 3.504576, Accuracy 93.117%\n",
      "Epoch 38, Batch 479, LR 0.018686 Loss 3.502994, Accuracy 93.122%\n",
      "Epoch 38, Batch 480, LR 0.018663 Loss 3.501519, Accuracy 93.128%\n",
      "Epoch 38, Batch 481, LR 0.018640 Loss 3.502945, Accuracy 93.120%\n",
      "Epoch 38, Batch 482, LR 0.018617 Loss 3.503461, Accuracy 93.124%\n",
      "Epoch 38, Batch 483, LR 0.018594 Loss 3.502129, Accuracy 93.126%\n",
      "Epoch 38, Batch 484, LR 0.018571 Loss 3.502311, Accuracy 93.129%\n",
      "Epoch 38, Batch 485, LR 0.018548 Loss 3.501281, Accuracy 93.131%\n",
      "Epoch 38, Batch 486, LR 0.018525 Loss 3.501293, Accuracy 93.131%\n",
      "Epoch 38, Batch 487, LR 0.018502 Loss 3.501209, Accuracy 93.131%\n",
      "Epoch 38, Batch 488, LR 0.018479 Loss 3.500442, Accuracy 93.130%\n",
      "Epoch 38, Batch 489, LR 0.018456 Loss 3.500189, Accuracy 93.132%\n",
      "Epoch 38, Batch 490, LR 0.018433 Loss 3.500762, Accuracy 93.135%\n",
      "Epoch 38, Batch 491, LR 0.018410 Loss 3.500412, Accuracy 93.134%\n",
      "Epoch 38, Batch 492, LR 0.018387 Loss 3.500509, Accuracy 93.135%\n",
      "Epoch 38, Batch 493, LR 0.018365 Loss 3.501310, Accuracy 93.135%\n",
      "Epoch 38, Batch 494, LR 0.018342 Loss 3.501099, Accuracy 93.135%\n",
      "Epoch 38, Batch 495, LR 0.018319 Loss 3.500513, Accuracy 93.138%\n",
      "Epoch 38, Batch 496, LR 0.018296 Loss 3.500305, Accuracy 93.139%\n",
      "Epoch 38, Batch 497, LR 0.018273 Loss 3.499830, Accuracy 93.145%\n",
      "Epoch 38, Batch 498, LR 0.018250 Loss 3.500229, Accuracy 93.143%\n",
      "Epoch 38, Batch 499, LR 0.018228 Loss 3.500752, Accuracy 93.133%\n",
      "Epoch 38, Batch 500, LR 0.018205 Loss 3.500281, Accuracy 93.130%\n",
      "Epoch 38, Batch 501, LR 0.018182 Loss 3.499413, Accuracy 93.136%\n",
      "Epoch 38, Batch 502, LR 0.018159 Loss 3.499383, Accuracy 93.140%\n",
      "Epoch 38, Batch 503, LR 0.018137 Loss 3.498944, Accuracy 93.141%\n",
      "Epoch 38, Batch 504, LR 0.018114 Loss 3.499175, Accuracy 93.138%\n",
      "Epoch 38, Batch 505, LR 0.018091 Loss 3.497890, Accuracy 93.144%\n",
      "Epoch 38, Batch 506, LR 0.018068 Loss 3.498319, Accuracy 93.145%\n",
      "Epoch 38, Batch 507, LR 0.018046 Loss 3.498364, Accuracy 93.144%\n",
      "Epoch 38, Batch 508, LR 0.018023 Loss 3.497667, Accuracy 93.147%\n",
      "Epoch 38, Batch 509, LR 0.018000 Loss 3.497021, Accuracy 93.154%\n",
      "Epoch 38, Batch 510, LR 0.017978 Loss 3.496382, Accuracy 93.160%\n",
      "Epoch 38, Batch 511, LR 0.017955 Loss 3.495248, Accuracy 93.163%\n",
      "Epoch 38, Batch 512, LR 0.017933 Loss 3.494950, Accuracy 93.164%\n",
      "Epoch 38, Batch 513, LR 0.017910 Loss 3.494679, Accuracy 93.165%\n",
      "Epoch 38, Batch 514, LR 0.017887 Loss 3.495406, Accuracy 93.157%\n",
      "Epoch 38, Batch 515, LR 0.017865 Loss 3.494057, Accuracy 93.164%\n",
      "Epoch 38, Batch 516, LR 0.017842 Loss 3.493107, Accuracy 93.167%\n",
      "Epoch 38, Batch 517, LR 0.017820 Loss 3.493160, Accuracy 93.171%\n",
      "Epoch 38, Batch 518, LR 0.017797 Loss 3.492155, Accuracy 93.174%\n",
      "Epoch 38, Batch 519, LR 0.017775 Loss 3.493399, Accuracy 93.167%\n",
      "Epoch 38, Batch 520, LR 0.017752 Loss 3.494591, Accuracy 93.167%\n",
      "Epoch 38, Batch 521, LR 0.017730 Loss 3.495830, Accuracy 93.162%\n",
      "Epoch 38, Batch 522, LR 0.017707 Loss 3.495289, Accuracy 93.162%\n",
      "Epoch 38, Batch 523, LR 0.017685 Loss 3.495273, Accuracy 93.161%\n",
      "Epoch 38, Batch 524, LR 0.017662 Loss 3.496045, Accuracy 93.158%\n",
      "Epoch 38, Batch 525, LR 0.017640 Loss 3.496313, Accuracy 93.150%\n",
      "Epoch 38, Batch 526, LR 0.017618 Loss 3.498005, Accuracy 93.145%\n",
      "Epoch 38, Batch 527, LR 0.017595 Loss 3.497812, Accuracy 93.147%\n",
      "Epoch 38, Batch 528, LR 0.017573 Loss 3.497643, Accuracy 93.149%\n",
      "Epoch 38, Batch 529, LR 0.017550 Loss 3.498229, Accuracy 93.150%\n",
      "Epoch 38, Batch 530, LR 0.017528 Loss 3.498895, Accuracy 93.147%\n",
      "Epoch 38, Batch 531, LR 0.017506 Loss 3.499296, Accuracy 93.150%\n",
      "Epoch 38, Batch 532, LR 0.017483 Loss 3.499148, Accuracy 93.151%\n",
      "Epoch 38, Batch 533, LR 0.017461 Loss 3.499606, Accuracy 93.148%\n",
      "Epoch 38, Batch 534, LR 0.017439 Loss 3.499251, Accuracy 93.146%\n",
      "Epoch 38, Batch 535, LR 0.017417 Loss 3.497540, Accuracy 93.154%\n",
      "Epoch 38, Batch 536, LR 0.017394 Loss 3.496622, Accuracy 93.158%\n",
      "Epoch 38, Batch 537, LR 0.017372 Loss 3.496042, Accuracy 93.162%\n",
      "Epoch 38, Batch 538, LR 0.017350 Loss 3.496564, Accuracy 93.160%\n",
      "Epoch 38, Batch 539, LR 0.017328 Loss 3.496354, Accuracy 93.163%\n",
      "Epoch 38, Batch 540, LR 0.017305 Loss 3.496397, Accuracy 93.161%\n",
      "Epoch 38, Batch 541, LR 0.017283 Loss 3.495847, Accuracy 93.158%\n",
      "Epoch 38, Batch 542, LR 0.017261 Loss 3.495516, Accuracy 93.156%\n",
      "Epoch 38, Batch 543, LR 0.017239 Loss 3.495920, Accuracy 93.156%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 544, LR 0.017217 Loss 3.496975, Accuracy 93.154%\n",
      "Epoch 38, Batch 545, LR 0.017194 Loss 3.496666, Accuracy 93.157%\n",
      "Epoch 38, Batch 546, LR 0.017172 Loss 3.496254, Accuracy 93.155%\n",
      "Epoch 38, Batch 547, LR 0.017150 Loss 3.496531, Accuracy 93.156%\n",
      "Epoch 38, Batch 548, LR 0.017128 Loss 3.495408, Accuracy 93.156%\n",
      "Epoch 38, Batch 549, LR 0.017106 Loss 3.495409, Accuracy 93.154%\n",
      "Epoch 38, Batch 550, LR 0.017084 Loss 3.495787, Accuracy 93.152%\n",
      "Epoch 38, Batch 551, LR 0.017062 Loss 3.495123, Accuracy 93.152%\n",
      "Epoch 38, Batch 552, LR 0.017040 Loss 3.495487, Accuracy 93.153%\n",
      "Epoch 38, Batch 553, LR 0.017018 Loss 3.494371, Accuracy 93.159%\n",
      "Epoch 38, Batch 554, LR 0.016996 Loss 3.495463, Accuracy 93.158%\n",
      "Epoch 38, Batch 555, LR 0.016974 Loss 3.494915, Accuracy 93.159%\n",
      "Epoch 38, Batch 556, LR 0.016952 Loss 3.494142, Accuracy 93.164%\n",
      "Epoch 38, Batch 557, LR 0.016930 Loss 3.493262, Accuracy 93.168%\n",
      "Epoch 38, Batch 558, LR 0.016908 Loss 3.493204, Accuracy 93.170%\n",
      "Epoch 38, Batch 559, LR 0.016886 Loss 3.494048, Accuracy 93.166%\n",
      "Epoch 38, Batch 560, LR 0.016864 Loss 3.494839, Accuracy 93.161%\n",
      "Epoch 38, Batch 561, LR 0.016842 Loss 3.494011, Accuracy 93.166%\n",
      "Epoch 38, Batch 562, LR 0.016820 Loss 3.495101, Accuracy 93.165%\n",
      "Epoch 38, Batch 563, LR 0.016798 Loss 3.495450, Accuracy 93.166%\n",
      "Epoch 38, Batch 564, LR 0.016777 Loss 3.496797, Accuracy 93.159%\n",
      "Epoch 38, Batch 565, LR 0.016755 Loss 3.496985, Accuracy 93.161%\n",
      "Epoch 38, Batch 566, LR 0.016733 Loss 3.497082, Accuracy 93.162%\n",
      "Epoch 38, Batch 567, LR 0.016711 Loss 3.497860, Accuracy 93.163%\n",
      "Epoch 38, Batch 568, LR 0.016689 Loss 3.497308, Accuracy 93.171%\n",
      "Epoch 38, Batch 569, LR 0.016667 Loss 3.495185, Accuracy 93.182%\n",
      "Epoch 38, Batch 570, LR 0.016646 Loss 3.495541, Accuracy 93.176%\n",
      "Epoch 38, Batch 571, LR 0.016624 Loss 3.494832, Accuracy 93.184%\n",
      "Epoch 38, Batch 572, LR 0.016602 Loss 3.495199, Accuracy 93.186%\n",
      "Epoch 38, Batch 573, LR 0.016580 Loss 3.494372, Accuracy 93.187%\n",
      "Epoch 38, Batch 574, LR 0.016559 Loss 3.494663, Accuracy 93.181%\n",
      "Epoch 38, Batch 575, LR 0.016537 Loss 3.494517, Accuracy 93.182%\n",
      "Epoch 38, Batch 576, LR 0.016515 Loss 3.494250, Accuracy 93.187%\n",
      "Epoch 38, Batch 577, LR 0.016493 Loss 3.495353, Accuracy 93.183%\n",
      "Epoch 38, Batch 578, LR 0.016472 Loss 3.495702, Accuracy 93.182%\n",
      "Epoch 38, Batch 579, LR 0.016450 Loss 3.495688, Accuracy 93.179%\n",
      "Epoch 38, Batch 580, LR 0.016428 Loss 3.495549, Accuracy 93.176%\n",
      "Epoch 38, Batch 581, LR 0.016407 Loss 3.496426, Accuracy 93.170%\n",
      "Epoch 38, Batch 582, LR 0.016385 Loss 3.495888, Accuracy 93.173%\n",
      "Epoch 38, Batch 583, LR 0.016364 Loss 3.496898, Accuracy 93.170%\n",
      "Epoch 38, Batch 584, LR 0.016342 Loss 3.497166, Accuracy 93.168%\n",
      "Epoch 38, Batch 585, LR 0.016320 Loss 3.496834, Accuracy 93.170%\n",
      "Epoch 38, Batch 586, LR 0.016299 Loss 3.496975, Accuracy 93.169%\n",
      "Epoch 38, Batch 587, LR 0.016277 Loss 3.497411, Accuracy 93.166%\n",
      "Epoch 38, Batch 588, LR 0.016256 Loss 3.496853, Accuracy 93.168%\n",
      "Epoch 38, Batch 589, LR 0.016234 Loss 3.498616, Accuracy 93.168%\n",
      "Epoch 38, Batch 590, LR 0.016213 Loss 3.498731, Accuracy 93.173%\n",
      "Epoch 38, Batch 591, LR 0.016191 Loss 3.497345, Accuracy 93.180%\n",
      "Epoch 38, Batch 592, LR 0.016170 Loss 3.497270, Accuracy 93.179%\n",
      "Epoch 38, Batch 593, LR 0.016148 Loss 3.497509, Accuracy 93.176%\n",
      "Epoch 38, Batch 594, LR 0.016127 Loss 3.497297, Accuracy 93.177%\n",
      "Epoch 38, Batch 595, LR 0.016106 Loss 3.497522, Accuracy 93.171%\n",
      "Epoch 38, Batch 596, LR 0.016084 Loss 3.498119, Accuracy 93.167%\n",
      "Epoch 38, Batch 597, LR 0.016063 Loss 3.498379, Accuracy 93.165%\n",
      "Epoch 38, Batch 598, LR 0.016041 Loss 3.499009, Accuracy 93.161%\n",
      "Epoch 38, Batch 599, LR 0.016020 Loss 3.498405, Accuracy 93.166%\n",
      "Epoch 38, Batch 600, LR 0.015999 Loss 3.497849, Accuracy 93.168%\n",
      "Epoch 38, Batch 601, LR 0.015977 Loss 3.497511, Accuracy 93.170%\n",
      "Epoch 38, Batch 602, LR 0.015956 Loss 3.497780, Accuracy 93.171%\n",
      "Epoch 38, Batch 603, LR 0.015935 Loss 3.498135, Accuracy 93.170%\n",
      "Epoch 38, Batch 604, LR 0.015913 Loss 3.498042, Accuracy 93.173%\n",
      "Epoch 38, Batch 605, LR 0.015892 Loss 3.498163, Accuracy 93.169%\n",
      "Epoch 38, Batch 606, LR 0.015871 Loss 3.497750, Accuracy 93.174%\n",
      "Epoch 38, Batch 607, LR 0.015849 Loss 3.497034, Accuracy 93.175%\n",
      "Epoch 38, Batch 608, LR 0.015828 Loss 3.496381, Accuracy 93.178%\n",
      "Epoch 38, Batch 609, LR 0.015807 Loss 3.496319, Accuracy 93.180%\n",
      "Epoch 38, Batch 610, LR 0.015786 Loss 3.495444, Accuracy 93.186%\n",
      "Epoch 38, Batch 611, LR 0.015765 Loss 3.495208, Accuracy 93.189%\n",
      "Epoch 38, Batch 612, LR 0.015743 Loss 3.494256, Accuracy 93.191%\n",
      "Epoch 38, Batch 613, LR 0.015722 Loss 3.493937, Accuracy 93.193%\n",
      "Epoch 38, Batch 614, LR 0.015701 Loss 3.493947, Accuracy 93.188%\n",
      "Epoch 38, Batch 615, LR 0.015680 Loss 3.495040, Accuracy 93.185%\n",
      "Epoch 38, Batch 616, LR 0.015659 Loss 3.495238, Accuracy 93.187%\n",
      "Epoch 38, Batch 617, LR 0.015638 Loss 3.494457, Accuracy 93.192%\n",
      "Epoch 38, Batch 618, LR 0.015616 Loss 3.495256, Accuracy 93.190%\n",
      "Epoch 38, Batch 619, LR 0.015595 Loss 3.494972, Accuracy 93.188%\n",
      "Epoch 38, Batch 620, LR 0.015574 Loss 3.495185, Accuracy 93.188%\n",
      "Epoch 38, Batch 621, LR 0.015553 Loss 3.495144, Accuracy 93.185%\n",
      "Epoch 38, Batch 622, LR 0.015532 Loss 3.494464, Accuracy 93.187%\n",
      "Epoch 38, Batch 623, LR 0.015511 Loss 3.494908, Accuracy 93.183%\n",
      "Epoch 38, Batch 624, LR 0.015490 Loss 3.495094, Accuracy 93.187%\n",
      "Epoch 38, Batch 625, LR 0.015469 Loss 3.495809, Accuracy 93.185%\n",
      "Epoch 38, Batch 626, LR 0.015448 Loss 3.495099, Accuracy 93.190%\n",
      "Epoch 38, Batch 627, LR 0.015427 Loss 3.493926, Accuracy 93.194%\n",
      "Epoch 38, Batch 628, LR 0.015406 Loss 3.493180, Accuracy 93.201%\n",
      "Epoch 38, Batch 629, LR 0.015385 Loss 3.494443, Accuracy 93.197%\n",
      "Epoch 38, Batch 630, LR 0.015364 Loss 3.495177, Accuracy 93.196%\n",
      "Epoch 38, Batch 631, LR 0.015343 Loss 3.494888, Accuracy 93.195%\n",
      "Epoch 38, Batch 632, LR 0.015322 Loss 3.495593, Accuracy 93.192%\n",
      "Epoch 38, Batch 633, LR 0.015302 Loss 3.495828, Accuracy 93.191%\n",
      "Epoch 38, Batch 634, LR 0.015281 Loss 3.496049, Accuracy 93.193%\n",
      "Epoch 38, Batch 635, LR 0.015260 Loss 3.495316, Accuracy 93.196%\n",
      "Epoch 38, Batch 636, LR 0.015239 Loss 3.496852, Accuracy 93.189%\n",
      "Epoch 38, Batch 637, LR 0.015218 Loss 3.496851, Accuracy 93.192%\n",
      "Epoch 38, Batch 638, LR 0.015197 Loss 3.496862, Accuracy 93.193%\n",
      "Epoch 38, Batch 639, LR 0.015177 Loss 3.496709, Accuracy 93.194%\n",
      "Epoch 38, Batch 640, LR 0.015156 Loss 3.497377, Accuracy 93.192%\n",
      "Epoch 38, Batch 641, LR 0.015135 Loss 3.497609, Accuracy 93.187%\n",
      "Epoch 38, Batch 642, LR 0.015114 Loss 3.499471, Accuracy 93.177%\n",
      "Epoch 38, Batch 643, LR 0.015093 Loss 3.500008, Accuracy 93.170%\n",
      "Epoch 38, Batch 644, LR 0.015073 Loss 3.498871, Accuracy 93.175%\n",
      "Epoch 38, Batch 645, LR 0.015052 Loss 3.498660, Accuracy 93.176%\n",
      "Epoch 38, Batch 646, LR 0.015031 Loss 3.499241, Accuracy 93.174%\n",
      "Epoch 38, Batch 647, LR 0.015011 Loss 3.499083, Accuracy 93.176%\n",
      "Epoch 38, Batch 648, LR 0.014990 Loss 3.499043, Accuracy 93.177%\n",
      "Epoch 38, Batch 649, LR 0.014969 Loss 3.498417, Accuracy 93.182%\n",
      "Epoch 38, Batch 650, LR 0.014949 Loss 3.498845, Accuracy 93.181%\n",
      "Epoch 38, Batch 651, LR 0.014928 Loss 3.498846, Accuracy 93.184%\n",
      "Epoch 38, Batch 652, LR 0.014907 Loss 3.499194, Accuracy 93.186%\n",
      "Epoch 38, Batch 653, LR 0.014887 Loss 3.499204, Accuracy 93.186%\n",
      "Epoch 38, Batch 654, LR 0.014866 Loss 3.499563, Accuracy 93.187%\n",
      "Epoch 38, Batch 655, LR 0.014845 Loss 3.499328, Accuracy 93.189%\n",
      "Epoch 38, Batch 656, LR 0.014825 Loss 3.499364, Accuracy 93.188%\n",
      "Epoch 38, Batch 657, LR 0.014804 Loss 3.498219, Accuracy 93.191%\n",
      "Epoch 38, Batch 658, LR 0.014784 Loss 3.497837, Accuracy 93.190%\n",
      "Epoch 38, Batch 659, LR 0.014763 Loss 3.497597, Accuracy 93.189%\n",
      "Epoch 38, Batch 660, LR 0.014743 Loss 3.498003, Accuracy 93.188%\n",
      "Epoch 38, Batch 661, LR 0.014722 Loss 3.498642, Accuracy 93.180%\n",
      "Epoch 38, Batch 662, LR 0.014702 Loss 3.498879, Accuracy 93.180%\n",
      "Epoch 38, Batch 663, LR 0.014681 Loss 3.498506, Accuracy 93.181%\n",
      "Epoch 38, Batch 664, LR 0.014661 Loss 3.499364, Accuracy 93.172%\n",
      "Epoch 38, Batch 665, LR 0.014640 Loss 3.499512, Accuracy 93.168%\n",
      "Epoch 38, Batch 666, LR 0.014620 Loss 3.499832, Accuracy 93.168%\n",
      "Epoch 38, Batch 667, LR 0.014600 Loss 3.501051, Accuracy 93.164%\n",
      "Epoch 38, Batch 668, LR 0.014579 Loss 3.500924, Accuracy 93.169%\n",
      "Epoch 38, Batch 669, LR 0.014559 Loss 3.500545, Accuracy 93.171%\n",
      "Epoch 38, Batch 670, LR 0.014538 Loss 3.500902, Accuracy 93.169%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 671, LR 0.014518 Loss 3.501838, Accuracy 93.167%\n",
      "Epoch 38, Batch 672, LR 0.014498 Loss 3.501771, Accuracy 93.169%\n",
      "Epoch 38, Batch 673, LR 0.014477 Loss 3.500965, Accuracy 93.167%\n",
      "Epoch 38, Batch 674, LR 0.014457 Loss 3.500943, Accuracy 93.167%\n",
      "Epoch 38, Batch 675, LR 0.014437 Loss 3.501118, Accuracy 93.168%\n",
      "Epoch 38, Batch 676, LR 0.014417 Loss 3.500147, Accuracy 93.171%\n",
      "Epoch 38, Batch 677, LR 0.014396 Loss 3.499521, Accuracy 93.170%\n",
      "Epoch 38, Batch 678, LR 0.014376 Loss 3.499215, Accuracy 93.173%\n",
      "Epoch 38, Batch 679, LR 0.014356 Loss 3.498821, Accuracy 93.174%\n",
      "Epoch 38, Batch 680, LR 0.014336 Loss 3.498637, Accuracy 93.176%\n",
      "Epoch 38, Batch 681, LR 0.014315 Loss 3.498446, Accuracy 93.180%\n",
      "Epoch 38, Batch 682, LR 0.014295 Loss 3.498390, Accuracy 93.181%\n",
      "Epoch 38, Batch 683, LR 0.014275 Loss 3.498649, Accuracy 93.179%\n",
      "Epoch 38, Batch 684, LR 0.014255 Loss 3.498391, Accuracy 93.179%\n",
      "Epoch 38, Batch 685, LR 0.014235 Loss 3.498741, Accuracy 93.177%\n",
      "Epoch 38, Batch 686, LR 0.014214 Loss 3.500150, Accuracy 93.170%\n",
      "Epoch 38, Batch 687, LR 0.014194 Loss 3.500323, Accuracy 93.170%\n",
      "Epoch 38, Batch 688, LR 0.014174 Loss 3.499591, Accuracy 93.174%\n",
      "Epoch 38, Batch 689, LR 0.014154 Loss 3.500134, Accuracy 93.172%\n",
      "Epoch 38, Batch 690, LR 0.014134 Loss 3.500238, Accuracy 93.171%\n",
      "Epoch 38, Batch 691, LR 0.014114 Loss 3.499736, Accuracy 93.178%\n",
      "Epoch 38, Batch 692, LR 0.014094 Loss 3.499273, Accuracy 93.181%\n",
      "Epoch 38, Batch 693, LR 0.014074 Loss 3.498986, Accuracy 93.182%\n",
      "Epoch 38, Batch 694, LR 0.014054 Loss 3.498312, Accuracy 93.185%\n",
      "Epoch 38, Batch 695, LR 0.014034 Loss 3.498949, Accuracy 93.181%\n",
      "Epoch 38, Batch 696, LR 0.014014 Loss 3.499138, Accuracy 93.179%\n",
      "Epoch 38, Batch 697, LR 0.013994 Loss 3.499210, Accuracy 93.181%\n",
      "Epoch 38, Batch 698, LR 0.013974 Loss 3.499218, Accuracy 93.180%\n",
      "Epoch 38, Batch 699, LR 0.013954 Loss 3.498931, Accuracy 93.182%\n",
      "Epoch 38, Batch 700, LR 0.013934 Loss 3.499561, Accuracy 93.180%\n",
      "Epoch 38, Batch 701, LR 0.013914 Loss 3.499943, Accuracy 93.178%\n",
      "Epoch 38, Batch 702, LR 0.013894 Loss 3.500188, Accuracy 93.180%\n",
      "Epoch 38, Batch 703, LR 0.013874 Loss 3.500198, Accuracy 93.182%\n",
      "Epoch 38, Batch 704, LR 0.013854 Loss 3.500214, Accuracy 93.180%\n",
      "Epoch 38, Batch 705, LR 0.013834 Loss 3.500474, Accuracy 93.173%\n",
      "Epoch 38, Batch 706, LR 0.013815 Loss 3.500647, Accuracy 93.168%\n",
      "Epoch 38, Batch 707, LR 0.013795 Loss 3.501034, Accuracy 93.168%\n",
      "Epoch 38, Batch 708, LR 0.013775 Loss 3.501380, Accuracy 93.166%\n",
      "Epoch 38, Batch 709, LR 0.013755 Loss 3.501109, Accuracy 93.168%\n",
      "Epoch 38, Batch 710, LR 0.013735 Loss 3.501700, Accuracy 93.167%\n",
      "Epoch 38, Batch 711, LR 0.013716 Loss 3.500444, Accuracy 93.173%\n",
      "Epoch 38, Batch 712, LR 0.013696 Loss 3.499851, Accuracy 93.178%\n",
      "Epoch 38, Batch 713, LR 0.013676 Loss 3.499372, Accuracy 93.181%\n",
      "Epoch 38, Batch 714, LR 0.013656 Loss 3.498815, Accuracy 93.183%\n",
      "Epoch 38, Batch 715, LR 0.013637 Loss 3.498571, Accuracy 93.186%\n",
      "Epoch 38, Batch 716, LR 0.013617 Loss 3.497903, Accuracy 93.190%\n",
      "Epoch 38, Batch 717, LR 0.013597 Loss 3.497457, Accuracy 93.192%\n",
      "Epoch 38, Batch 718, LR 0.013577 Loss 3.496482, Accuracy 93.193%\n",
      "Epoch 38, Batch 719, LR 0.013558 Loss 3.495683, Accuracy 93.194%\n",
      "Epoch 38, Batch 720, LR 0.013538 Loss 3.495294, Accuracy 93.198%\n",
      "Epoch 38, Batch 721, LR 0.013518 Loss 3.494798, Accuracy 93.202%\n",
      "Epoch 38, Batch 722, LR 0.013499 Loss 3.494752, Accuracy 93.204%\n",
      "Epoch 38, Batch 723, LR 0.013479 Loss 3.495028, Accuracy 93.202%\n",
      "Epoch 38, Batch 724, LR 0.013460 Loss 3.494814, Accuracy 93.204%\n",
      "Epoch 38, Batch 725, LR 0.013440 Loss 3.495430, Accuracy 93.200%\n",
      "Epoch 38, Batch 726, LR 0.013420 Loss 3.495030, Accuracy 93.203%\n",
      "Epoch 38, Batch 727, LR 0.013401 Loss 3.496214, Accuracy 93.195%\n",
      "Epoch 38, Batch 728, LR 0.013381 Loss 3.496102, Accuracy 93.194%\n",
      "Epoch 38, Batch 729, LR 0.013362 Loss 3.495513, Accuracy 93.196%\n",
      "Epoch 38, Batch 730, LR 0.013342 Loss 3.495412, Accuracy 93.197%\n",
      "Epoch 38, Batch 731, LR 0.013323 Loss 3.495708, Accuracy 93.199%\n",
      "Epoch 38, Batch 732, LR 0.013303 Loss 3.495115, Accuracy 93.200%\n",
      "Epoch 38, Batch 733, LR 0.013284 Loss 3.495377, Accuracy 93.198%\n",
      "Epoch 38, Batch 734, LR 0.013264 Loss 3.494868, Accuracy 93.199%\n",
      "Epoch 38, Batch 735, LR 0.013245 Loss 3.494983, Accuracy 93.196%\n",
      "Epoch 38, Batch 736, LR 0.013225 Loss 3.494983, Accuracy 93.197%\n",
      "Epoch 38, Batch 737, LR 0.013206 Loss 3.495884, Accuracy 93.192%\n",
      "Epoch 38, Batch 738, LR 0.013187 Loss 3.495728, Accuracy 93.193%\n",
      "Epoch 38, Batch 739, LR 0.013167 Loss 3.495781, Accuracy 93.192%\n",
      "Epoch 38, Batch 740, LR 0.013148 Loss 3.495709, Accuracy 93.194%\n",
      "Epoch 38, Batch 741, LR 0.013129 Loss 3.495574, Accuracy 93.196%\n",
      "Epoch 38, Batch 742, LR 0.013109 Loss 3.496358, Accuracy 93.196%\n",
      "Epoch 38, Batch 743, LR 0.013090 Loss 3.496661, Accuracy 93.193%\n",
      "Epoch 38, Batch 744, LR 0.013071 Loss 3.496713, Accuracy 93.192%\n",
      "Epoch 38, Batch 745, LR 0.013051 Loss 3.496308, Accuracy 93.195%\n",
      "Epoch 38, Batch 746, LR 0.013032 Loss 3.496627, Accuracy 93.192%\n",
      "Epoch 38, Batch 747, LR 0.013013 Loss 3.496735, Accuracy 93.192%\n",
      "Epoch 38, Batch 748, LR 0.012993 Loss 3.496778, Accuracy 93.193%\n",
      "Epoch 38, Batch 749, LR 0.012974 Loss 3.496964, Accuracy 93.192%\n",
      "Epoch 38, Batch 750, LR 0.012955 Loss 3.496838, Accuracy 93.192%\n",
      "Epoch 38, Batch 751, LR 0.012936 Loss 3.497958, Accuracy 93.185%\n",
      "Epoch 38, Batch 752, LR 0.012917 Loss 3.497625, Accuracy 93.185%\n",
      "Epoch 38, Batch 753, LR 0.012897 Loss 3.497961, Accuracy 93.180%\n",
      "Epoch 38, Batch 754, LR 0.012878 Loss 3.498328, Accuracy 93.178%\n",
      "Epoch 38, Batch 755, LR 0.012859 Loss 3.498546, Accuracy 93.177%\n",
      "Epoch 38, Batch 756, LR 0.012840 Loss 3.498969, Accuracy 93.177%\n",
      "Epoch 38, Batch 757, LR 0.012821 Loss 3.499558, Accuracy 93.180%\n",
      "Epoch 38, Batch 758, LR 0.012802 Loss 3.500225, Accuracy 93.180%\n",
      "Epoch 38, Batch 759, LR 0.012782 Loss 3.500674, Accuracy 93.180%\n",
      "Epoch 38, Batch 760, LR 0.012763 Loss 3.501005, Accuracy 93.176%\n",
      "Epoch 38, Batch 761, LR 0.012744 Loss 3.501095, Accuracy 93.178%\n",
      "Epoch 38, Batch 762, LR 0.012725 Loss 3.500789, Accuracy 93.179%\n",
      "Epoch 38, Batch 763, LR 0.012706 Loss 3.500128, Accuracy 93.182%\n",
      "Epoch 38, Batch 764, LR 0.012687 Loss 3.499631, Accuracy 93.185%\n",
      "Epoch 38, Batch 765, LR 0.012668 Loss 3.500137, Accuracy 93.181%\n",
      "Epoch 38, Batch 766, LR 0.012649 Loss 3.501126, Accuracy 93.180%\n",
      "Epoch 38, Batch 767, LR 0.012630 Loss 3.501350, Accuracy 93.177%\n",
      "Epoch 38, Batch 768, LR 0.012611 Loss 3.501355, Accuracy 93.179%\n",
      "Epoch 38, Batch 769, LR 0.012592 Loss 3.501611, Accuracy 93.180%\n",
      "Epoch 38, Batch 770, LR 0.012573 Loss 3.501516, Accuracy 93.180%\n",
      "Epoch 38, Batch 771, LR 0.012554 Loss 3.500729, Accuracy 93.184%\n",
      "Epoch 38, Batch 772, LR 0.012535 Loss 3.499845, Accuracy 93.186%\n",
      "Epoch 38, Batch 773, LR 0.012516 Loss 3.500065, Accuracy 93.185%\n",
      "Epoch 38, Batch 774, LR 0.012498 Loss 3.499955, Accuracy 93.186%\n",
      "Epoch 38, Batch 775, LR 0.012479 Loss 3.500809, Accuracy 93.183%\n",
      "Epoch 38, Batch 776, LR 0.012460 Loss 3.500923, Accuracy 93.182%\n",
      "Epoch 38, Batch 777, LR 0.012441 Loss 3.500718, Accuracy 93.183%\n",
      "Epoch 38, Batch 778, LR 0.012422 Loss 3.500466, Accuracy 93.183%\n",
      "Epoch 38, Batch 779, LR 0.012403 Loss 3.500753, Accuracy 93.181%\n",
      "Epoch 38, Batch 780, LR 0.012385 Loss 3.501212, Accuracy 93.175%\n",
      "Epoch 38, Batch 781, LR 0.012366 Loss 3.501199, Accuracy 93.174%\n",
      "Epoch 38, Batch 782, LR 0.012347 Loss 3.501010, Accuracy 93.176%\n",
      "Epoch 38, Batch 783, LR 0.012328 Loss 3.499622, Accuracy 93.180%\n",
      "Epoch 38, Batch 784, LR 0.012309 Loss 3.499687, Accuracy 93.180%\n",
      "Epoch 38, Batch 785, LR 0.012291 Loss 3.499524, Accuracy 93.180%\n",
      "Epoch 38, Batch 786, LR 0.012272 Loss 3.499889, Accuracy 93.185%\n",
      "Epoch 38, Batch 787, LR 0.012253 Loss 3.499491, Accuracy 93.187%\n",
      "Epoch 38, Batch 788, LR 0.012235 Loss 3.499688, Accuracy 93.190%\n",
      "Epoch 38, Batch 789, LR 0.012216 Loss 3.499563, Accuracy 93.195%\n",
      "Epoch 38, Batch 790, LR 0.012197 Loss 3.498775, Accuracy 93.200%\n",
      "Epoch 38, Batch 791, LR 0.012179 Loss 3.498637, Accuracy 93.202%\n",
      "Epoch 38, Batch 792, LR 0.012160 Loss 3.498974, Accuracy 93.198%\n",
      "Epoch 38, Batch 793, LR 0.012141 Loss 3.498606, Accuracy 93.198%\n",
      "Epoch 38, Batch 794, LR 0.012123 Loss 3.498840, Accuracy 93.198%\n",
      "Epoch 38, Batch 795, LR 0.012104 Loss 3.498818, Accuracy 93.198%\n",
      "Epoch 38, Batch 796, LR 0.012086 Loss 3.498836, Accuracy 93.194%\n",
      "Epoch 38, Batch 797, LR 0.012067 Loss 3.499060, Accuracy 93.192%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 798, LR 0.012048 Loss 3.499024, Accuracy 93.194%\n",
      "Epoch 38, Batch 799, LR 0.012030 Loss 3.499196, Accuracy 93.193%\n",
      "Epoch 38, Batch 800, LR 0.012011 Loss 3.499283, Accuracy 93.190%\n",
      "Epoch 38, Batch 801, LR 0.011993 Loss 3.499675, Accuracy 93.189%\n",
      "Epoch 38, Batch 802, LR 0.011974 Loss 3.498588, Accuracy 93.196%\n",
      "Epoch 38, Batch 803, LR 0.011956 Loss 3.498813, Accuracy 93.196%\n",
      "Epoch 38, Batch 804, LR 0.011937 Loss 3.499067, Accuracy 93.195%\n",
      "Epoch 38, Batch 805, LR 0.011919 Loss 3.498200, Accuracy 93.197%\n",
      "Epoch 38, Batch 806, LR 0.011901 Loss 3.498833, Accuracy 93.194%\n",
      "Epoch 38, Batch 807, LR 0.011882 Loss 3.499298, Accuracy 93.192%\n",
      "Epoch 38, Batch 808, LR 0.011864 Loss 3.498656, Accuracy 93.195%\n",
      "Epoch 38, Batch 809, LR 0.011845 Loss 3.498315, Accuracy 93.199%\n",
      "Epoch 38, Batch 810, LR 0.011827 Loss 3.497823, Accuracy 93.201%\n",
      "Epoch 38, Batch 811, LR 0.011809 Loss 3.497986, Accuracy 93.201%\n",
      "Epoch 38, Batch 812, LR 0.011790 Loss 3.498518, Accuracy 93.201%\n",
      "Epoch 38, Batch 813, LR 0.011772 Loss 3.498074, Accuracy 93.203%\n",
      "Epoch 38, Batch 814, LR 0.011754 Loss 3.497496, Accuracy 93.204%\n",
      "Epoch 38, Batch 815, LR 0.011735 Loss 3.496575, Accuracy 93.207%\n",
      "Epoch 38, Batch 816, LR 0.011717 Loss 3.497174, Accuracy 93.205%\n",
      "Epoch 38, Batch 817, LR 0.011699 Loss 3.497220, Accuracy 93.202%\n",
      "Epoch 38, Batch 818, LR 0.011680 Loss 3.496576, Accuracy 93.205%\n",
      "Epoch 38, Batch 819, LR 0.011662 Loss 3.496003, Accuracy 93.201%\n",
      "Epoch 38, Batch 820, LR 0.011644 Loss 3.496670, Accuracy 93.199%\n",
      "Epoch 38, Batch 821, LR 0.011626 Loss 3.497259, Accuracy 93.200%\n",
      "Epoch 38, Batch 822, LR 0.011607 Loss 3.496786, Accuracy 93.201%\n",
      "Epoch 38, Batch 823, LR 0.011589 Loss 3.496421, Accuracy 93.202%\n",
      "Epoch 38, Batch 824, LR 0.011571 Loss 3.497544, Accuracy 93.201%\n",
      "Epoch 38, Batch 825, LR 0.011553 Loss 3.497592, Accuracy 93.198%\n",
      "Epoch 38, Batch 826, LR 0.011535 Loss 3.497583, Accuracy 93.199%\n",
      "Epoch 38, Batch 827, LR 0.011517 Loss 3.497725, Accuracy 93.196%\n",
      "Epoch 38, Batch 828, LR 0.011499 Loss 3.498444, Accuracy 93.193%\n",
      "Epoch 38, Batch 829, LR 0.011480 Loss 3.498305, Accuracy 93.193%\n",
      "Epoch 38, Batch 830, LR 0.011462 Loss 3.498073, Accuracy 93.192%\n",
      "Epoch 38, Batch 831, LR 0.011444 Loss 3.498311, Accuracy 93.192%\n",
      "Epoch 38, Batch 832, LR 0.011426 Loss 3.498543, Accuracy 93.193%\n",
      "Epoch 38, Batch 833, LR 0.011408 Loss 3.498732, Accuracy 93.194%\n",
      "Epoch 38, Batch 834, LR 0.011390 Loss 3.498664, Accuracy 93.193%\n",
      "Epoch 38, Batch 835, LR 0.011372 Loss 3.498458, Accuracy 93.192%\n",
      "Epoch 38, Batch 836, LR 0.011354 Loss 3.498288, Accuracy 93.195%\n",
      "Epoch 38, Batch 837, LR 0.011336 Loss 3.498964, Accuracy 93.191%\n",
      "Epoch 38, Batch 838, LR 0.011318 Loss 3.499215, Accuracy 93.189%\n",
      "Epoch 38, Batch 839, LR 0.011300 Loss 3.498985, Accuracy 93.189%\n",
      "Epoch 38, Batch 840, LR 0.011282 Loss 3.498871, Accuracy 93.190%\n",
      "Epoch 38, Batch 841, LR 0.011264 Loss 3.499198, Accuracy 93.188%\n",
      "Epoch 38, Batch 842, LR 0.011246 Loss 3.499598, Accuracy 93.187%\n",
      "Epoch 38, Batch 843, LR 0.011228 Loss 3.498798, Accuracy 93.188%\n",
      "Epoch 38, Batch 844, LR 0.011210 Loss 3.498923, Accuracy 93.188%\n",
      "Epoch 38, Batch 845, LR 0.011193 Loss 3.498794, Accuracy 93.190%\n",
      "Epoch 38, Batch 846, LR 0.011175 Loss 3.498771, Accuracy 93.192%\n",
      "Epoch 38, Batch 847, LR 0.011157 Loss 3.498609, Accuracy 93.193%\n",
      "Epoch 38, Batch 848, LR 0.011139 Loss 3.498850, Accuracy 93.195%\n",
      "Epoch 38, Batch 849, LR 0.011121 Loss 3.499267, Accuracy 93.192%\n",
      "Epoch 38, Batch 850, LR 0.011103 Loss 3.499970, Accuracy 93.183%\n",
      "Epoch 38, Batch 851, LR 0.011086 Loss 3.499373, Accuracy 93.184%\n",
      "Epoch 38, Batch 852, LR 0.011068 Loss 3.499120, Accuracy 93.184%\n",
      "Epoch 38, Batch 853, LR 0.011050 Loss 3.499273, Accuracy 93.184%\n",
      "Epoch 38, Batch 854, LR 0.011032 Loss 3.499110, Accuracy 93.183%\n",
      "Epoch 38, Batch 855, LR 0.011015 Loss 3.499375, Accuracy 93.185%\n",
      "Epoch 38, Batch 856, LR 0.010997 Loss 3.499111, Accuracy 93.187%\n",
      "Epoch 38, Batch 857, LR 0.010979 Loss 3.499258, Accuracy 93.183%\n",
      "Epoch 38, Batch 858, LR 0.010961 Loss 3.499500, Accuracy 93.184%\n",
      "Epoch 38, Batch 859, LR 0.010944 Loss 3.499443, Accuracy 93.180%\n",
      "Epoch 38, Batch 860, LR 0.010926 Loss 3.500257, Accuracy 93.180%\n",
      "Epoch 38, Batch 861, LR 0.010908 Loss 3.500006, Accuracy 93.182%\n",
      "Epoch 38, Batch 862, LR 0.010891 Loss 3.498895, Accuracy 93.186%\n",
      "Epoch 38, Batch 863, LR 0.010873 Loss 3.499094, Accuracy 93.186%\n",
      "Epoch 38, Batch 864, LR 0.010856 Loss 3.499476, Accuracy 93.184%\n",
      "Epoch 38, Batch 865, LR 0.010838 Loss 3.499865, Accuracy 93.185%\n",
      "Epoch 38, Batch 866, LR 0.010820 Loss 3.499251, Accuracy 93.187%\n",
      "Epoch 38, Batch 867, LR 0.010803 Loss 3.499531, Accuracy 93.186%\n",
      "Epoch 38, Batch 868, LR 0.010785 Loss 3.498781, Accuracy 93.191%\n",
      "Epoch 38, Batch 869, LR 0.010768 Loss 3.498508, Accuracy 93.195%\n",
      "Epoch 38, Batch 870, LR 0.010750 Loss 3.498941, Accuracy 93.193%\n",
      "Epoch 38, Batch 871, LR 0.010733 Loss 3.498901, Accuracy 93.194%\n",
      "Epoch 38, Batch 872, LR 0.010715 Loss 3.499142, Accuracy 93.195%\n",
      "Epoch 38, Batch 873, LR 0.010698 Loss 3.498500, Accuracy 93.195%\n",
      "Epoch 38, Batch 874, LR 0.010680 Loss 3.498703, Accuracy 93.198%\n",
      "Epoch 38, Batch 875, LR 0.010663 Loss 3.498685, Accuracy 93.201%\n",
      "Epoch 38, Batch 876, LR 0.010645 Loss 3.498889, Accuracy 93.202%\n",
      "Epoch 38, Batch 877, LR 0.010628 Loss 3.498555, Accuracy 93.202%\n",
      "Epoch 38, Batch 878, LR 0.010610 Loss 3.499181, Accuracy 93.202%\n",
      "Epoch 38, Batch 879, LR 0.010593 Loss 3.498652, Accuracy 93.205%\n",
      "Epoch 38, Batch 880, LR 0.010576 Loss 3.498352, Accuracy 93.202%\n",
      "Epoch 38, Batch 881, LR 0.010558 Loss 3.498135, Accuracy 93.200%\n",
      "Epoch 38, Batch 882, LR 0.010541 Loss 3.497436, Accuracy 93.203%\n",
      "Epoch 38, Batch 883, LR 0.010524 Loss 3.496981, Accuracy 93.205%\n",
      "Epoch 38, Batch 884, LR 0.010506 Loss 3.496505, Accuracy 93.210%\n",
      "Epoch 38, Batch 885, LR 0.010489 Loss 3.496211, Accuracy 93.212%\n",
      "Epoch 38, Batch 886, LR 0.010472 Loss 3.496441, Accuracy 93.211%\n",
      "Epoch 38, Batch 887, LR 0.010454 Loss 3.496096, Accuracy 93.212%\n",
      "Epoch 38, Batch 888, LR 0.010437 Loss 3.496323, Accuracy 93.211%\n",
      "Epoch 38, Batch 889, LR 0.010420 Loss 3.495988, Accuracy 93.210%\n",
      "Epoch 38, Batch 890, LR 0.010403 Loss 3.496113, Accuracy 93.213%\n",
      "Epoch 38, Batch 891, LR 0.010385 Loss 3.495442, Accuracy 93.219%\n",
      "Epoch 38, Batch 892, LR 0.010368 Loss 3.495399, Accuracy 93.221%\n",
      "Epoch 38, Batch 893, LR 0.010351 Loss 3.495509, Accuracy 93.222%\n",
      "Epoch 38, Batch 894, LR 0.010334 Loss 3.495836, Accuracy 93.219%\n",
      "Epoch 38, Batch 895, LR 0.010317 Loss 3.496500, Accuracy 93.218%\n",
      "Epoch 38, Batch 896, LR 0.010299 Loss 3.496536, Accuracy 93.218%\n",
      "Epoch 38, Batch 897, LR 0.010282 Loss 3.496607, Accuracy 93.218%\n",
      "Epoch 38, Batch 898, LR 0.010265 Loss 3.496228, Accuracy 93.215%\n",
      "Epoch 38, Batch 899, LR 0.010248 Loss 3.496864, Accuracy 93.212%\n",
      "Epoch 38, Batch 900, LR 0.010231 Loss 3.497592, Accuracy 93.207%\n",
      "Epoch 38, Batch 901, LR 0.010214 Loss 3.497371, Accuracy 93.210%\n",
      "Epoch 38, Batch 902, LR 0.010197 Loss 3.497806, Accuracy 93.215%\n",
      "Epoch 38, Batch 903, LR 0.010180 Loss 3.497864, Accuracy 93.213%\n",
      "Epoch 38, Batch 904, LR 0.010163 Loss 3.497822, Accuracy 93.211%\n",
      "Epoch 38, Batch 905, LR 0.010146 Loss 3.497417, Accuracy 93.213%\n",
      "Epoch 38, Batch 906, LR 0.010129 Loss 3.497328, Accuracy 93.210%\n",
      "Epoch 38, Batch 907, LR 0.010112 Loss 3.497251, Accuracy 93.210%\n",
      "Epoch 38, Batch 908, LR 0.010095 Loss 3.496783, Accuracy 93.212%\n",
      "Epoch 38, Batch 909, LR 0.010078 Loss 3.496885, Accuracy 93.211%\n",
      "Epoch 38, Batch 910, LR 0.010061 Loss 3.496915, Accuracy 93.208%\n",
      "Epoch 38, Batch 911, LR 0.010044 Loss 3.496921, Accuracy 93.209%\n",
      "Epoch 38, Batch 912, LR 0.010027 Loss 3.497308, Accuracy 93.211%\n",
      "Epoch 38, Batch 913, LR 0.010010 Loss 3.497462, Accuracy 93.210%\n",
      "Epoch 38, Batch 914, LR 0.009993 Loss 3.496759, Accuracy 93.215%\n",
      "Epoch 38, Batch 915, LR 0.009976 Loss 3.496252, Accuracy 93.219%\n",
      "Epoch 38, Batch 916, LR 0.009959 Loss 3.495990, Accuracy 93.220%\n",
      "Epoch 38, Batch 917, LR 0.009942 Loss 3.495361, Accuracy 93.222%\n",
      "Epoch 38, Batch 918, LR 0.009926 Loss 3.495020, Accuracy 93.224%\n",
      "Epoch 38, Batch 919, LR 0.009909 Loss 3.495019, Accuracy 93.223%\n",
      "Epoch 38, Batch 920, LR 0.009892 Loss 3.493925, Accuracy 93.226%\n",
      "Epoch 38, Batch 921, LR 0.009875 Loss 3.493954, Accuracy 93.228%\n",
      "Epoch 38, Batch 922, LR 0.009858 Loss 3.493828, Accuracy 93.228%\n",
      "Epoch 38, Batch 923, LR 0.009842 Loss 3.493869, Accuracy 93.229%\n",
      "Epoch 38, Batch 924, LR 0.009825 Loss 3.493158, Accuracy 93.231%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 925, LR 0.009808 Loss 3.493209, Accuracy 93.231%\n",
      "Epoch 38, Batch 926, LR 0.009791 Loss 3.493262, Accuracy 93.232%\n",
      "Epoch 38, Batch 927, LR 0.009775 Loss 3.493288, Accuracy 93.233%\n",
      "Epoch 38, Batch 928, LR 0.009758 Loss 3.493004, Accuracy 93.233%\n",
      "Epoch 38, Batch 929, LR 0.009741 Loss 3.493077, Accuracy 93.234%\n",
      "Epoch 38, Batch 930, LR 0.009725 Loss 3.493595, Accuracy 93.227%\n",
      "Epoch 38, Batch 931, LR 0.009708 Loss 3.493167, Accuracy 93.229%\n",
      "Epoch 38, Batch 932, LR 0.009691 Loss 3.492891, Accuracy 93.230%\n",
      "Epoch 38, Batch 933, LR 0.009675 Loss 3.492927, Accuracy 93.227%\n",
      "Epoch 38, Batch 934, LR 0.009658 Loss 3.492249, Accuracy 93.228%\n",
      "Epoch 38, Batch 935, LR 0.009641 Loss 3.491897, Accuracy 93.228%\n",
      "Epoch 38, Batch 936, LR 0.009625 Loss 3.491645, Accuracy 93.230%\n",
      "Epoch 38, Batch 937, LR 0.009608 Loss 3.490946, Accuracy 93.235%\n",
      "Epoch 38, Batch 938, LR 0.009592 Loss 3.490317, Accuracy 93.238%\n",
      "Epoch 38, Batch 939, LR 0.009575 Loss 3.490676, Accuracy 93.236%\n",
      "Epoch 38, Batch 940, LR 0.009559 Loss 3.491243, Accuracy 93.234%\n",
      "Epoch 38, Batch 941, LR 0.009542 Loss 3.491088, Accuracy 93.234%\n",
      "Epoch 38, Batch 942, LR 0.009526 Loss 3.490340, Accuracy 93.238%\n",
      "Epoch 38, Batch 943, LR 0.009509 Loss 3.490414, Accuracy 93.240%\n",
      "Epoch 38, Batch 944, LR 0.009493 Loss 3.490659, Accuracy 93.239%\n",
      "Epoch 38, Batch 945, LR 0.009476 Loss 3.490271, Accuracy 93.244%\n",
      "Epoch 38, Batch 946, LR 0.009460 Loss 3.490089, Accuracy 93.244%\n",
      "Epoch 38, Batch 947, LR 0.009443 Loss 3.489793, Accuracy 93.245%\n",
      "Epoch 38, Batch 948, LR 0.009427 Loss 3.489789, Accuracy 93.245%\n",
      "Epoch 38, Batch 949, LR 0.009410 Loss 3.489508, Accuracy 93.245%\n",
      "Epoch 38, Batch 950, LR 0.009394 Loss 3.490005, Accuracy 93.243%\n",
      "Epoch 38, Batch 951, LR 0.009378 Loss 3.489639, Accuracy 93.245%\n",
      "Epoch 38, Batch 952, LR 0.009361 Loss 3.489450, Accuracy 93.247%\n",
      "Epoch 38, Batch 953, LR 0.009345 Loss 3.488832, Accuracy 93.249%\n",
      "Epoch 38, Batch 954, LR 0.009329 Loss 3.488851, Accuracy 93.250%\n",
      "Epoch 38, Batch 955, LR 0.009312 Loss 3.488756, Accuracy 93.253%\n",
      "Epoch 38, Batch 956, LR 0.009296 Loss 3.488383, Accuracy 93.255%\n",
      "Epoch 38, Batch 957, LR 0.009280 Loss 3.488279, Accuracy 93.254%\n",
      "Epoch 38, Batch 958, LR 0.009263 Loss 3.487995, Accuracy 93.256%\n",
      "Epoch 38, Batch 959, LR 0.009247 Loss 3.488020, Accuracy 93.256%\n",
      "Epoch 38, Batch 960, LR 0.009231 Loss 3.487797, Accuracy 93.260%\n",
      "Epoch 38, Batch 961, LR 0.009215 Loss 3.487939, Accuracy 93.260%\n",
      "Epoch 38, Batch 962, LR 0.009199 Loss 3.487773, Accuracy 93.259%\n",
      "Epoch 38, Batch 963, LR 0.009182 Loss 3.487643, Accuracy 93.262%\n",
      "Epoch 38, Batch 964, LR 0.009166 Loss 3.487985, Accuracy 93.260%\n",
      "Epoch 38, Batch 965, LR 0.009150 Loss 3.487704, Accuracy 93.259%\n",
      "Epoch 38, Batch 966, LR 0.009134 Loss 3.487454, Accuracy 93.260%\n",
      "Epoch 38, Batch 967, LR 0.009118 Loss 3.487403, Accuracy 93.258%\n",
      "Epoch 38, Batch 968, LR 0.009102 Loss 3.487328, Accuracy 93.258%\n",
      "Epoch 38, Batch 969, LR 0.009085 Loss 3.486906, Accuracy 93.261%\n",
      "Epoch 38, Batch 970, LR 0.009069 Loss 3.487502, Accuracy 93.258%\n",
      "Epoch 38, Batch 971, LR 0.009053 Loss 3.488280, Accuracy 93.253%\n",
      "Epoch 38, Batch 972, LR 0.009037 Loss 3.488027, Accuracy 93.256%\n",
      "Epoch 38, Batch 973, LR 0.009021 Loss 3.488056, Accuracy 93.255%\n",
      "Epoch 38, Batch 974, LR 0.009005 Loss 3.488042, Accuracy 93.257%\n",
      "Epoch 38, Batch 975, LR 0.008989 Loss 3.488836, Accuracy 93.253%\n",
      "Epoch 38, Batch 976, LR 0.008973 Loss 3.488966, Accuracy 93.253%\n",
      "Epoch 38, Batch 977, LR 0.008957 Loss 3.489427, Accuracy 93.253%\n",
      "Epoch 38, Batch 978, LR 0.008941 Loss 3.489584, Accuracy 93.254%\n",
      "Epoch 38, Batch 979, LR 0.008925 Loss 3.489393, Accuracy 93.254%\n",
      "Epoch 38, Batch 980, LR 0.008909 Loss 3.489043, Accuracy 93.257%\n",
      "Epoch 38, Batch 981, LR 0.008893 Loss 3.488797, Accuracy 93.256%\n",
      "Epoch 38, Batch 982, LR 0.008877 Loss 3.488431, Accuracy 93.258%\n",
      "Epoch 38, Batch 983, LR 0.008861 Loss 3.488170, Accuracy 93.260%\n",
      "Epoch 38, Batch 984, LR 0.008845 Loss 3.487489, Accuracy 93.263%\n",
      "Epoch 38, Batch 985, LR 0.008829 Loss 3.487819, Accuracy 93.261%\n",
      "Epoch 38, Batch 986, LR 0.008814 Loss 3.487673, Accuracy 93.262%\n",
      "Epoch 38, Batch 987, LR 0.008798 Loss 3.487589, Accuracy 93.262%\n",
      "Epoch 38, Batch 988, LR 0.008782 Loss 3.487229, Accuracy 93.262%\n",
      "Epoch 38, Batch 989, LR 0.008766 Loss 3.486865, Accuracy 93.267%\n",
      "Epoch 38, Batch 990, LR 0.008750 Loss 3.486760, Accuracy 93.269%\n",
      "Epoch 38, Batch 991, LR 0.008734 Loss 3.486581, Accuracy 93.271%\n",
      "Epoch 38, Batch 992, LR 0.008719 Loss 3.486757, Accuracy 93.274%\n",
      "Epoch 38, Batch 993, LR 0.008703 Loss 3.485753, Accuracy 93.277%\n",
      "Epoch 38, Batch 994, LR 0.008687 Loss 3.486079, Accuracy 93.277%\n",
      "Epoch 38, Batch 995, LR 0.008671 Loss 3.485540, Accuracy 93.280%\n",
      "Epoch 38, Batch 996, LR 0.008656 Loss 3.485521, Accuracy 93.279%\n",
      "Epoch 38, Batch 997, LR 0.008640 Loss 3.485820, Accuracy 93.277%\n",
      "Epoch 38, Batch 998, LR 0.008624 Loss 3.485919, Accuracy 93.274%\n",
      "Epoch 38, Batch 999, LR 0.008609 Loss 3.485669, Accuracy 93.275%\n",
      "Epoch 38, Batch 1000, LR 0.008593 Loss 3.485301, Accuracy 93.276%\n",
      "Epoch 38, Batch 1001, LR 0.008577 Loss 3.485131, Accuracy 93.279%\n",
      "Epoch 38, Batch 1002, LR 0.008562 Loss 3.484920, Accuracy 93.281%\n",
      "Epoch 38, Batch 1003, LR 0.008546 Loss 3.485730, Accuracy 93.277%\n",
      "Epoch 38, Batch 1004, LR 0.008530 Loss 3.485502, Accuracy 93.278%\n",
      "Epoch 38, Batch 1005, LR 0.008515 Loss 3.485657, Accuracy 93.278%\n",
      "Epoch 38, Batch 1006, LR 0.008499 Loss 3.485299, Accuracy 93.279%\n",
      "Epoch 38, Batch 1007, LR 0.008484 Loss 3.485567, Accuracy 93.278%\n",
      "Epoch 38, Batch 1008, LR 0.008468 Loss 3.485229, Accuracy 93.280%\n",
      "Epoch 38, Batch 1009, LR 0.008452 Loss 3.485324, Accuracy 93.278%\n",
      "Epoch 38, Batch 1010, LR 0.008437 Loss 3.485156, Accuracy 93.278%\n",
      "Epoch 38, Batch 1011, LR 0.008421 Loss 3.485023, Accuracy 93.276%\n",
      "Epoch 38, Batch 1012, LR 0.008406 Loss 3.484937, Accuracy 93.276%\n",
      "Epoch 38, Batch 1013, LR 0.008390 Loss 3.485164, Accuracy 93.276%\n",
      "Epoch 38, Batch 1014, LR 0.008375 Loss 3.485160, Accuracy 93.274%\n",
      "Epoch 38, Batch 1015, LR 0.008359 Loss 3.484737, Accuracy 93.277%\n",
      "Epoch 38, Batch 1016, LR 0.008344 Loss 3.484753, Accuracy 93.277%\n",
      "Epoch 38, Batch 1017, LR 0.008329 Loss 3.484841, Accuracy 93.276%\n",
      "Epoch 38, Batch 1018, LR 0.008313 Loss 3.484885, Accuracy 93.275%\n",
      "Epoch 38, Batch 1019, LR 0.008298 Loss 3.484652, Accuracy 93.278%\n",
      "Epoch 38, Batch 1020, LR 0.008282 Loss 3.484462, Accuracy 93.279%\n",
      "Epoch 38, Batch 1021, LR 0.008267 Loss 3.484718, Accuracy 93.281%\n",
      "Epoch 38, Batch 1022, LR 0.008252 Loss 3.484594, Accuracy 93.279%\n",
      "Epoch 38, Batch 1023, LR 0.008236 Loss 3.485119, Accuracy 93.280%\n",
      "Epoch 38, Batch 1024, LR 0.008221 Loss 3.485503, Accuracy 93.279%\n",
      "Epoch 38, Batch 1025, LR 0.008206 Loss 3.485873, Accuracy 93.276%\n",
      "Epoch 38, Batch 1026, LR 0.008190 Loss 3.485814, Accuracy 93.275%\n",
      "Epoch 38, Batch 1027, LR 0.008175 Loss 3.485639, Accuracy 93.275%\n",
      "Epoch 38, Batch 1028, LR 0.008160 Loss 3.485861, Accuracy 93.275%\n",
      "Epoch 38, Batch 1029, LR 0.008144 Loss 3.486139, Accuracy 93.273%\n",
      "Epoch 38, Batch 1030, LR 0.008129 Loss 3.486214, Accuracy 93.271%\n",
      "Epoch 38, Batch 1031, LR 0.008114 Loss 3.486753, Accuracy 93.269%\n",
      "Epoch 38, Batch 1032, LR 0.008099 Loss 3.486711, Accuracy 93.269%\n",
      "Epoch 38, Batch 1033, LR 0.008084 Loss 3.486847, Accuracy 93.268%\n",
      "Epoch 38, Batch 1034, LR 0.008068 Loss 3.486357, Accuracy 93.271%\n",
      "Epoch 38, Batch 1035, LR 0.008053 Loss 3.486558, Accuracy 93.270%\n",
      "Epoch 38, Batch 1036, LR 0.008038 Loss 3.487270, Accuracy 93.270%\n",
      "Epoch 38, Batch 1037, LR 0.008023 Loss 3.487057, Accuracy 93.268%\n",
      "Epoch 38, Batch 1038, LR 0.008008 Loss 3.487395, Accuracy 93.269%\n",
      "Epoch 38, Batch 1039, LR 0.007993 Loss 3.486980, Accuracy 93.271%\n",
      "Epoch 38, Batch 1040, LR 0.007978 Loss 3.487216, Accuracy 93.270%\n",
      "Epoch 38, Batch 1041, LR 0.007962 Loss 3.486669, Accuracy 93.271%\n",
      "Epoch 38, Batch 1042, LR 0.007947 Loss 3.486471, Accuracy 93.272%\n",
      "Epoch 38, Batch 1043, LR 0.007932 Loss 3.486684, Accuracy 93.271%\n",
      "Epoch 38, Batch 1044, LR 0.007917 Loss 3.486676, Accuracy 93.270%\n",
      "Epoch 38, Batch 1045, LR 0.007902 Loss 3.486436, Accuracy 93.270%\n",
      "Epoch 38, Batch 1046, LR 0.007887 Loss 3.486243, Accuracy 93.269%\n",
      "Epoch 38, Batch 1047, LR 0.007872 Loss 3.485974, Accuracy 93.269%\n",
      "Epoch 38, Loss (train set) 3.485974, Accuracy (train set) 93.269%\n",
      "Epoch 39, Batch 1, LR 0.007857 Loss 3.048418, Accuracy 96.094%\n",
      "Epoch 39, Batch 2, LR 0.007842 Loss 3.650783, Accuracy 92.188%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 3, LR 0.007827 Loss 3.474355, Accuracy 93.229%\n",
      "Epoch 39, Batch 4, LR 0.007812 Loss 3.428459, Accuracy 93.750%\n",
      "Epoch 39, Batch 5, LR 0.007797 Loss 3.505848, Accuracy 93.594%\n",
      "Epoch 39, Batch 6, LR 0.007782 Loss 3.556008, Accuracy 93.099%\n",
      "Epoch 39, Batch 7, LR 0.007768 Loss 3.537831, Accuracy 92.969%\n",
      "Epoch 39, Batch 8, LR 0.007753 Loss 3.475421, Accuracy 93.164%\n",
      "Epoch 39, Batch 9, LR 0.007738 Loss 3.496101, Accuracy 92.708%\n",
      "Epoch 39, Batch 10, LR 0.007723 Loss 3.460519, Accuracy 93.125%\n",
      "Epoch 39, Batch 11, LR 0.007708 Loss 3.478030, Accuracy 93.182%\n",
      "Epoch 39, Batch 12, LR 0.007693 Loss 3.497398, Accuracy 93.164%\n",
      "Epoch 39, Batch 13, LR 0.007678 Loss 3.460400, Accuracy 93.450%\n",
      "Epoch 39, Batch 14, LR 0.007664 Loss 3.439978, Accuracy 93.471%\n",
      "Epoch 39, Batch 15, LR 0.007649 Loss 3.492650, Accuracy 93.333%\n",
      "Epoch 39, Batch 16, LR 0.007634 Loss 3.497736, Accuracy 93.311%\n",
      "Epoch 39, Batch 17, LR 0.007619 Loss 3.527206, Accuracy 93.199%\n",
      "Epoch 39, Batch 18, LR 0.007605 Loss 3.536165, Accuracy 93.056%\n",
      "Epoch 39, Batch 19, LR 0.007590 Loss 3.561881, Accuracy 93.174%\n",
      "Epoch 39, Batch 20, LR 0.007575 Loss 3.562885, Accuracy 93.047%\n",
      "Epoch 39, Batch 21, LR 0.007560 Loss 3.573138, Accuracy 92.857%\n",
      "Epoch 39, Batch 22, LR 0.007546 Loss 3.540368, Accuracy 93.004%\n",
      "Epoch 39, Batch 23, LR 0.007531 Loss 3.511538, Accuracy 93.207%\n",
      "Epoch 39, Batch 24, LR 0.007516 Loss 3.528660, Accuracy 93.164%\n",
      "Epoch 39, Batch 25, LR 0.007502 Loss 3.516318, Accuracy 93.219%\n",
      "Epoch 39, Batch 26, LR 0.007487 Loss 3.517813, Accuracy 93.149%\n",
      "Epoch 39, Batch 27, LR 0.007472 Loss 3.526825, Accuracy 93.113%\n",
      "Epoch 39, Batch 28, LR 0.007458 Loss 3.503376, Accuracy 93.220%\n",
      "Epoch 39, Batch 29, LR 0.007443 Loss 3.520192, Accuracy 93.103%\n",
      "Epoch 39, Batch 30, LR 0.007429 Loss 3.506691, Accuracy 93.125%\n",
      "Epoch 39, Batch 31, LR 0.007414 Loss 3.498381, Accuracy 93.246%\n",
      "Epoch 39, Batch 32, LR 0.007400 Loss 3.478415, Accuracy 93.335%\n",
      "Epoch 39, Batch 33, LR 0.007385 Loss 3.493953, Accuracy 93.277%\n",
      "Epoch 39, Batch 34, LR 0.007371 Loss 3.508375, Accuracy 93.222%\n",
      "Epoch 39, Batch 35, LR 0.007356 Loss 3.508138, Accuracy 93.237%\n",
      "Epoch 39, Batch 36, LR 0.007342 Loss 3.509573, Accuracy 93.229%\n",
      "Epoch 39, Batch 37, LR 0.007327 Loss 3.513507, Accuracy 93.222%\n",
      "Epoch 39, Batch 38, LR 0.007313 Loss 3.507137, Accuracy 93.277%\n",
      "Epoch 39, Batch 39, LR 0.007298 Loss 3.503029, Accuracy 93.249%\n",
      "Epoch 39, Batch 40, LR 0.007284 Loss 3.517252, Accuracy 93.184%\n",
      "Epoch 39, Batch 41, LR 0.007269 Loss 3.511187, Accuracy 93.178%\n",
      "Epoch 39, Batch 42, LR 0.007255 Loss 3.526897, Accuracy 93.192%\n",
      "Epoch 39, Batch 43, LR 0.007240 Loss 3.503505, Accuracy 93.296%\n",
      "Epoch 39, Batch 44, LR 0.007226 Loss 3.497162, Accuracy 93.306%\n",
      "Epoch 39, Batch 45, LR 0.007212 Loss 3.493823, Accuracy 93.299%\n",
      "Epoch 39, Batch 46, LR 0.007197 Loss 3.489909, Accuracy 93.342%\n",
      "Epoch 39, Batch 47, LR 0.007183 Loss 3.481286, Accuracy 93.401%\n",
      "Epoch 39, Batch 48, LR 0.007169 Loss 3.487331, Accuracy 93.376%\n",
      "Epoch 39, Batch 49, LR 0.007154 Loss 3.500432, Accuracy 93.335%\n",
      "Epoch 39, Batch 50, LR 0.007140 Loss 3.489369, Accuracy 93.359%\n",
      "Epoch 39, Batch 51, LR 0.007126 Loss 3.494298, Accuracy 93.352%\n",
      "Epoch 39, Batch 52, LR 0.007112 Loss 3.497794, Accuracy 93.299%\n",
      "Epoch 39, Batch 53, LR 0.007097 Loss 3.494475, Accuracy 93.308%\n",
      "Epoch 39, Batch 54, LR 0.007083 Loss 3.491523, Accuracy 93.330%\n",
      "Epoch 39, Batch 55, LR 0.007069 Loss 3.492876, Accuracy 93.324%\n",
      "Epoch 39, Batch 56, LR 0.007055 Loss 3.496400, Accuracy 93.331%\n",
      "Epoch 39, Batch 57, LR 0.007040 Loss 3.495713, Accuracy 93.298%\n",
      "Epoch 39, Batch 58, LR 0.007026 Loss 3.493931, Accuracy 93.346%\n",
      "Epoch 39, Batch 59, LR 0.007012 Loss 3.492273, Accuracy 93.353%\n",
      "Epoch 39, Batch 60, LR 0.006998 Loss 3.491828, Accuracy 93.333%\n",
      "Epoch 39, Batch 61, LR 0.006984 Loss 3.492805, Accuracy 93.353%\n",
      "Epoch 39, Batch 62, LR 0.006970 Loss 3.492711, Accuracy 93.347%\n",
      "Epoch 39, Batch 63, LR 0.006956 Loss 3.491005, Accuracy 93.366%\n",
      "Epoch 39, Batch 64, LR 0.006942 Loss 3.495229, Accuracy 93.347%\n",
      "Epoch 39, Batch 65, LR 0.006927 Loss 3.493374, Accuracy 93.353%\n",
      "Epoch 39, Batch 66, LR 0.006913 Loss 3.494939, Accuracy 93.371%\n",
      "Epoch 39, Batch 67, LR 0.006899 Loss 3.489457, Accuracy 93.389%\n",
      "Epoch 39, Batch 68, LR 0.006885 Loss 3.485127, Accuracy 93.405%\n",
      "Epoch 39, Batch 69, LR 0.006871 Loss 3.497577, Accuracy 93.354%\n",
      "Epoch 39, Batch 70, LR 0.006857 Loss 3.488502, Accuracy 93.359%\n",
      "Epoch 39, Batch 71, LR 0.006843 Loss 3.498207, Accuracy 93.354%\n",
      "Epoch 39, Batch 72, LR 0.006829 Loss 3.492546, Accuracy 93.403%\n",
      "Epoch 39, Batch 73, LR 0.006815 Loss 3.495316, Accuracy 93.365%\n",
      "Epoch 39, Batch 74, LR 0.006801 Loss 3.495133, Accuracy 93.317%\n",
      "Epoch 39, Batch 75, LR 0.006787 Loss 3.494397, Accuracy 93.302%\n",
      "Epoch 39, Batch 76, LR 0.006774 Loss 3.494093, Accuracy 93.287%\n",
      "Epoch 39, Batch 77, LR 0.006760 Loss 3.488009, Accuracy 93.293%\n",
      "Epoch 39, Batch 78, LR 0.006746 Loss 3.490952, Accuracy 93.319%\n",
      "Epoch 39, Batch 79, LR 0.006732 Loss 3.497850, Accuracy 93.265%\n",
      "Epoch 39, Batch 80, LR 0.006718 Loss 3.502232, Accuracy 93.281%\n",
      "Epoch 39, Batch 81, LR 0.006704 Loss 3.503266, Accuracy 93.239%\n",
      "Epoch 39, Batch 82, LR 0.006690 Loss 3.493412, Accuracy 93.255%\n",
      "Epoch 39, Batch 83, LR 0.006677 Loss 3.499599, Accuracy 93.242%\n",
      "Epoch 39, Batch 84, LR 0.006663 Loss 3.500621, Accuracy 93.238%\n",
      "Epoch 39, Batch 85, LR 0.006649 Loss 3.503597, Accuracy 93.208%\n",
      "Epoch 39, Batch 86, LR 0.006635 Loss 3.499080, Accuracy 93.214%\n",
      "Epoch 39, Batch 87, LR 0.006621 Loss 3.502652, Accuracy 93.184%\n",
      "Epoch 39, Batch 88, LR 0.006608 Loss 3.514401, Accuracy 93.120%\n",
      "Epoch 39, Batch 89, LR 0.006594 Loss 3.516012, Accuracy 93.100%\n",
      "Epoch 39, Batch 90, LR 0.006580 Loss 3.515138, Accuracy 93.108%\n",
      "Epoch 39, Batch 91, LR 0.006566 Loss 3.518540, Accuracy 93.089%\n",
      "Epoch 39, Batch 92, LR 0.006553 Loss 3.518714, Accuracy 93.105%\n",
      "Epoch 39, Batch 93, LR 0.006539 Loss 3.516143, Accuracy 93.120%\n",
      "Epoch 39, Batch 94, LR 0.006525 Loss 3.522391, Accuracy 93.127%\n",
      "Epoch 39, Batch 95, LR 0.006512 Loss 3.521684, Accuracy 93.133%\n",
      "Epoch 39, Batch 96, LR 0.006498 Loss 3.535574, Accuracy 93.075%\n",
      "Epoch 39, Batch 97, LR 0.006485 Loss 3.532592, Accuracy 93.057%\n",
      "Epoch 39, Batch 98, LR 0.006471 Loss 3.532964, Accuracy 93.017%\n",
      "Epoch 39, Batch 99, LR 0.006457 Loss 3.528983, Accuracy 93.040%\n",
      "Epoch 39, Batch 100, LR 0.006444 Loss 3.528518, Accuracy 93.031%\n",
      "Epoch 39, Batch 101, LR 0.006430 Loss 3.532664, Accuracy 93.038%\n",
      "Epoch 39, Batch 102, LR 0.006417 Loss 3.533630, Accuracy 93.022%\n",
      "Epoch 39, Batch 103, LR 0.006403 Loss 3.529511, Accuracy 93.067%\n",
      "Epoch 39, Batch 104, LR 0.006390 Loss 3.531989, Accuracy 93.051%\n",
      "Epoch 39, Batch 105, LR 0.006376 Loss 3.534938, Accuracy 93.051%\n",
      "Epoch 39, Batch 106, LR 0.006363 Loss 3.531725, Accuracy 93.087%\n",
      "Epoch 39, Batch 107, LR 0.006349 Loss 3.527961, Accuracy 93.100%\n",
      "Epoch 39, Batch 108, LR 0.006336 Loss 3.530034, Accuracy 93.113%\n",
      "Epoch 39, Batch 109, LR 0.006322 Loss 3.535744, Accuracy 93.055%\n",
      "Epoch 39, Batch 110, LR 0.006309 Loss 3.538495, Accuracy 93.026%\n",
      "Epoch 39, Batch 111, LR 0.006295 Loss 3.535322, Accuracy 93.046%\n",
      "Epoch 39, Batch 112, LR 0.006282 Loss 3.534724, Accuracy 93.052%\n",
      "Epoch 39, Batch 113, LR 0.006269 Loss 3.532924, Accuracy 93.079%\n",
      "Epoch 39, Batch 114, LR 0.006255 Loss 3.535599, Accuracy 93.078%\n",
      "Epoch 39, Batch 115, LR 0.006242 Loss 3.535329, Accuracy 93.098%\n",
      "Epoch 39, Batch 116, LR 0.006228 Loss 3.527701, Accuracy 93.110%\n",
      "Epoch 39, Batch 117, LR 0.006215 Loss 3.532000, Accuracy 93.109%\n",
      "Epoch 39, Batch 118, LR 0.006202 Loss 3.526192, Accuracy 93.141%\n",
      "Epoch 39, Batch 119, LR 0.006188 Loss 3.528707, Accuracy 93.113%\n",
      "Epoch 39, Batch 120, LR 0.006175 Loss 3.525303, Accuracy 93.125%\n",
      "Epoch 39, Batch 121, LR 0.006162 Loss 3.523848, Accuracy 93.130%\n",
      "Epoch 39, Batch 122, LR 0.006149 Loss 3.524658, Accuracy 93.122%\n",
      "Epoch 39, Batch 123, LR 0.006135 Loss 3.526147, Accuracy 93.108%\n",
      "Epoch 39, Batch 124, LR 0.006122 Loss 3.524470, Accuracy 93.114%\n",
      "Epoch 39, Batch 125, LR 0.006109 Loss 3.514519, Accuracy 93.156%\n",
      "Epoch 39, Batch 126, LR 0.006096 Loss 3.510189, Accuracy 93.167%\n",
      "Epoch 39, Batch 127, LR 0.006083 Loss 3.512640, Accuracy 93.159%\n",
      "Epoch 39, Batch 128, LR 0.006069 Loss 3.506783, Accuracy 93.195%\n",
      "Epoch 39, Batch 129, LR 0.006056 Loss 3.502921, Accuracy 93.205%\n",
      "Epoch 39, Batch 130, LR 0.006043 Loss 3.505592, Accuracy 93.179%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 131, LR 0.006030 Loss 3.506893, Accuracy 93.166%\n",
      "Epoch 39, Batch 132, LR 0.006017 Loss 3.503266, Accuracy 93.170%\n",
      "Epoch 39, Batch 133, LR 0.006004 Loss 3.503934, Accuracy 93.151%\n",
      "Epoch 39, Batch 134, LR 0.005991 Loss 3.504677, Accuracy 93.132%\n",
      "Epoch 39, Batch 135, LR 0.005978 Loss 3.500444, Accuracy 93.148%\n",
      "Epoch 39, Batch 136, LR 0.005964 Loss 3.502150, Accuracy 93.147%\n",
      "Epoch 39, Batch 137, LR 0.005951 Loss 3.497232, Accuracy 93.157%\n",
      "Epoch 39, Batch 138, LR 0.005938 Loss 3.492863, Accuracy 93.195%\n",
      "Epoch 39, Batch 139, LR 0.005925 Loss 3.493818, Accuracy 93.210%\n",
      "Epoch 39, Batch 140, LR 0.005912 Loss 3.493162, Accuracy 93.220%\n",
      "Epoch 39, Batch 141, LR 0.005899 Loss 3.491718, Accuracy 93.229%\n",
      "Epoch 39, Batch 142, LR 0.005886 Loss 3.490051, Accuracy 93.249%\n",
      "Epoch 39, Batch 143, LR 0.005873 Loss 3.489632, Accuracy 93.231%\n",
      "Epoch 39, Batch 144, LR 0.005860 Loss 3.489373, Accuracy 93.218%\n",
      "Epoch 39, Batch 145, LR 0.005848 Loss 3.493623, Accuracy 93.200%\n",
      "Epoch 39, Batch 146, LR 0.005835 Loss 3.493966, Accuracy 93.204%\n",
      "Epoch 39, Batch 147, LR 0.005822 Loss 3.493913, Accuracy 93.203%\n",
      "Epoch 39, Batch 148, LR 0.005809 Loss 3.494187, Accuracy 93.217%\n",
      "Epoch 39, Batch 149, LR 0.005796 Loss 3.494246, Accuracy 93.236%\n",
      "Epoch 39, Batch 150, LR 0.005783 Loss 3.492543, Accuracy 93.234%\n",
      "Epoch 39, Batch 151, LR 0.005770 Loss 3.496564, Accuracy 93.227%\n",
      "Epoch 39, Batch 152, LR 0.005757 Loss 3.500291, Accuracy 93.236%\n",
      "Epoch 39, Batch 153, LR 0.005745 Loss 3.503592, Accuracy 93.219%\n",
      "Epoch 39, Batch 154, LR 0.005732 Loss 3.502582, Accuracy 93.212%\n",
      "Epoch 39, Batch 155, LR 0.005719 Loss 3.500513, Accuracy 93.241%\n",
      "Epoch 39, Batch 156, LR 0.005706 Loss 3.498673, Accuracy 93.249%\n",
      "Epoch 39, Batch 157, LR 0.005693 Loss 3.495863, Accuracy 93.252%\n",
      "Epoch 39, Batch 158, LR 0.005681 Loss 3.491846, Accuracy 93.260%\n",
      "Epoch 39, Batch 159, LR 0.005668 Loss 3.491766, Accuracy 93.268%\n",
      "Epoch 39, Batch 160, LR 0.005655 Loss 3.496363, Accuracy 93.242%\n",
      "Epoch 39, Batch 161, LR 0.005643 Loss 3.501361, Accuracy 93.211%\n",
      "Epoch 39, Batch 162, LR 0.005630 Loss 3.501767, Accuracy 93.210%\n",
      "Epoch 39, Batch 163, LR 0.005617 Loss 3.504121, Accuracy 93.189%\n",
      "Epoch 39, Batch 164, LR 0.005604 Loss 3.505023, Accuracy 93.159%\n",
      "Epoch 39, Batch 165, LR 0.005592 Loss 3.505402, Accuracy 93.172%\n",
      "Epoch 39, Batch 166, LR 0.005579 Loss 3.500832, Accuracy 93.190%\n",
      "Epoch 39, Batch 167, LR 0.005567 Loss 3.496837, Accuracy 93.207%\n",
      "Epoch 39, Batch 168, LR 0.005554 Loss 3.497774, Accuracy 93.187%\n",
      "Epoch 39, Batch 169, LR 0.005541 Loss 3.494895, Accuracy 93.195%\n",
      "Epoch 39, Batch 170, LR 0.005529 Loss 3.494865, Accuracy 93.203%\n",
      "Epoch 39, Batch 171, LR 0.005516 Loss 3.499726, Accuracy 93.183%\n",
      "Epoch 39, Batch 172, LR 0.005504 Loss 3.497262, Accuracy 93.187%\n",
      "Epoch 39, Batch 173, LR 0.005491 Loss 3.503153, Accuracy 93.167%\n",
      "Epoch 39, Batch 174, LR 0.005479 Loss 3.502014, Accuracy 93.166%\n",
      "Epoch 39, Batch 175, LR 0.005466 Loss 3.503193, Accuracy 93.161%\n",
      "Epoch 39, Batch 176, LR 0.005454 Loss 3.499045, Accuracy 93.177%\n",
      "Epoch 39, Batch 177, LR 0.005441 Loss 3.501026, Accuracy 93.167%\n",
      "Epoch 39, Batch 178, LR 0.005429 Loss 3.499711, Accuracy 93.157%\n",
      "Epoch 39, Batch 179, LR 0.005416 Loss 3.496992, Accuracy 93.161%\n",
      "Epoch 39, Batch 180, LR 0.005404 Loss 3.496320, Accuracy 93.160%\n",
      "Epoch 39, Batch 181, LR 0.005391 Loss 3.496175, Accuracy 93.176%\n",
      "Epoch 39, Batch 182, LR 0.005379 Loss 3.493510, Accuracy 93.188%\n",
      "Epoch 39, Batch 183, LR 0.005367 Loss 3.494863, Accuracy 93.161%\n",
      "Epoch 39, Batch 184, LR 0.005354 Loss 3.494303, Accuracy 93.156%\n",
      "Epoch 39, Batch 185, LR 0.005342 Loss 3.493471, Accuracy 93.150%\n",
      "Epoch 39, Batch 186, LR 0.005329 Loss 3.489676, Accuracy 93.166%\n",
      "Epoch 39, Batch 187, LR 0.005317 Loss 3.488336, Accuracy 93.165%\n",
      "Epoch 39, Batch 188, LR 0.005305 Loss 3.489869, Accuracy 93.160%\n",
      "Epoch 39, Batch 189, LR 0.005293 Loss 3.492354, Accuracy 93.146%\n",
      "Epoch 39, Batch 190, LR 0.005280 Loss 3.489858, Accuracy 93.162%\n",
      "Epoch 39, Batch 191, LR 0.005268 Loss 3.491223, Accuracy 93.145%\n",
      "Epoch 39, Batch 192, LR 0.005256 Loss 3.488545, Accuracy 93.152%\n",
      "Epoch 39, Batch 193, LR 0.005243 Loss 3.484604, Accuracy 93.163%\n",
      "Epoch 39, Batch 194, LR 0.005231 Loss 3.483423, Accuracy 93.170%\n",
      "Epoch 39, Batch 195, LR 0.005219 Loss 3.485473, Accuracy 93.173%\n",
      "Epoch 39, Batch 196, LR 0.005207 Loss 3.485777, Accuracy 93.176%\n",
      "Epoch 39, Batch 197, LR 0.005195 Loss 3.488140, Accuracy 93.183%\n",
      "Epoch 39, Batch 198, LR 0.005182 Loss 3.487403, Accuracy 93.178%\n",
      "Epoch 39, Batch 199, LR 0.005170 Loss 3.486878, Accuracy 93.185%\n",
      "Epoch 39, Batch 200, LR 0.005158 Loss 3.484454, Accuracy 93.191%\n",
      "Epoch 39, Batch 201, LR 0.005146 Loss 3.485544, Accuracy 93.190%\n",
      "Epoch 39, Batch 202, LR 0.005134 Loss 3.485332, Accuracy 93.181%\n",
      "Epoch 39, Batch 203, LR 0.005122 Loss 3.484109, Accuracy 93.180%\n",
      "Epoch 39, Batch 204, LR 0.005110 Loss 3.488335, Accuracy 93.172%\n",
      "Epoch 39, Batch 205, LR 0.005098 Loss 3.487830, Accuracy 93.163%\n",
      "Epoch 39, Batch 206, LR 0.005086 Loss 3.486839, Accuracy 93.162%\n",
      "Epoch 39, Batch 207, LR 0.005073 Loss 3.486949, Accuracy 93.161%\n",
      "Epoch 39, Batch 208, LR 0.005061 Loss 3.485154, Accuracy 93.172%\n",
      "Epoch 39, Batch 209, LR 0.005049 Loss 3.484093, Accuracy 93.186%\n",
      "Epoch 39, Batch 210, LR 0.005037 Loss 3.483380, Accuracy 93.173%\n",
      "Epoch 39, Batch 211, LR 0.005025 Loss 3.480945, Accuracy 93.184%\n",
      "Epoch 39, Batch 212, LR 0.005013 Loss 3.480560, Accuracy 93.179%\n",
      "Epoch 39, Batch 213, LR 0.005001 Loss 3.480537, Accuracy 93.181%\n",
      "Epoch 39, Batch 214, LR 0.004990 Loss 3.481467, Accuracy 93.166%\n",
      "Epoch 39, Batch 215, LR 0.004978 Loss 3.481591, Accuracy 93.172%\n",
      "Epoch 39, Batch 216, LR 0.004966 Loss 3.480023, Accuracy 93.168%\n",
      "Epoch 39, Batch 217, LR 0.004954 Loss 3.479748, Accuracy 93.174%\n",
      "Epoch 39, Batch 218, LR 0.004942 Loss 3.480185, Accuracy 93.173%\n",
      "Epoch 39, Batch 219, LR 0.004930 Loss 3.481173, Accuracy 93.176%\n",
      "Epoch 39, Batch 220, LR 0.004918 Loss 3.479589, Accuracy 93.185%\n",
      "Epoch 39, Batch 221, LR 0.004906 Loss 3.479886, Accuracy 93.184%\n",
      "Epoch 39, Batch 222, LR 0.004894 Loss 3.479154, Accuracy 93.187%\n",
      "Epoch 39, Batch 223, LR 0.004883 Loss 3.479919, Accuracy 93.189%\n",
      "Epoch 39, Batch 224, LR 0.004871 Loss 3.481072, Accuracy 93.185%\n",
      "Epoch 39, Batch 225, LR 0.004859 Loss 3.480381, Accuracy 93.181%\n",
      "Epoch 39, Batch 226, LR 0.004847 Loss 3.479841, Accuracy 93.176%\n",
      "Epoch 39, Batch 227, LR 0.004835 Loss 3.479398, Accuracy 93.179%\n",
      "Epoch 39, Batch 228, LR 0.004824 Loss 3.479468, Accuracy 93.185%\n",
      "Epoch 39, Batch 229, LR 0.004812 Loss 3.479693, Accuracy 93.180%\n",
      "Epoch 39, Batch 230, LR 0.004800 Loss 3.478757, Accuracy 93.176%\n",
      "Epoch 39, Batch 231, LR 0.004789 Loss 3.480004, Accuracy 93.182%\n",
      "Epoch 39, Batch 232, LR 0.004777 Loss 3.478260, Accuracy 93.194%\n",
      "Epoch 39, Batch 233, LR 0.004765 Loss 3.477657, Accuracy 93.203%\n",
      "Epoch 39, Batch 234, LR 0.004754 Loss 3.477035, Accuracy 93.209%\n",
      "Epoch 39, Batch 235, LR 0.004742 Loss 3.478324, Accuracy 93.221%\n",
      "Epoch 39, Batch 236, LR 0.004730 Loss 3.477512, Accuracy 93.237%\n",
      "Epoch 39, Batch 237, LR 0.004719 Loss 3.478863, Accuracy 93.232%\n",
      "Epoch 39, Batch 238, LR 0.004707 Loss 3.477849, Accuracy 93.241%\n",
      "Epoch 39, Batch 239, LR 0.004695 Loss 3.477365, Accuracy 93.237%\n",
      "Epoch 39, Batch 240, LR 0.004684 Loss 3.481427, Accuracy 93.213%\n",
      "Epoch 39, Batch 241, LR 0.004672 Loss 3.480721, Accuracy 93.225%\n",
      "Epoch 39, Batch 242, LR 0.004661 Loss 3.479814, Accuracy 93.221%\n",
      "Epoch 39, Batch 243, LR 0.004649 Loss 3.477706, Accuracy 93.239%\n",
      "Epoch 39, Batch 244, LR 0.004638 Loss 3.478132, Accuracy 93.241%\n",
      "Epoch 39, Batch 245, LR 0.004626 Loss 3.480495, Accuracy 93.246%\n",
      "Epoch 39, Batch 246, LR 0.004615 Loss 3.481952, Accuracy 93.245%\n",
      "Epoch 39, Batch 247, LR 0.004603 Loss 3.483501, Accuracy 93.231%\n",
      "Epoch 39, Batch 248, LR 0.004592 Loss 3.483639, Accuracy 93.237%\n",
      "Epoch 39, Batch 249, LR 0.004580 Loss 3.484675, Accuracy 93.235%\n",
      "Epoch 39, Batch 250, LR 0.004569 Loss 3.484608, Accuracy 93.231%\n",
      "Epoch 39, Batch 251, LR 0.004557 Loss 3.486505, Accuracy 93.224%\n",
      "Epoch 39, Batch 252, LR 0.004546 Loss 3.486843, Accuracy 93.217%\n",
      "Epoch 39, Batch 253, LR 0.004535 Loss 3.486542, Accuracy 93.219%\n",
      "Epoch 39, Batch 254, LR 0.004523 Loss 3.487124, Accuracy 93.215%\n",
      "Epoch 39, Batch 255, LR 0.004512 Loss 3.486313, Accuracy 93.214%\n",
      "Epoch 39, Batch 256, LR 0.004501 Loss 3.490521, Accuracy 93.198%\n",
      "Epoch 39, Batch 257, LR 0.004489 Loss 3.492151, Accuracy 93.203%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 258, LR 0.004478 Loss 3.492151, Accuracy 93.199%\n",
      "Epoch 39, Batch 259, LR 0.004467 Loss 3.490413, Accuracy 93.204%\n",
      "Epoch 39, Batch 260, LR 0.004455 Loss 3.490094, Accuracy 93.212%\n",
      "Epoch 39, Batch 261, LR 0.004444 Loss 3.489528, Accuracy 93.217%\n",
      "Epoch 39, Batch 262, LR 0.004433 Loss 3.489084, Accuracy 93.228%\n",
      "Epoch 39, Batch 263, LR 0.004422 Loss 3.488757, Accuracy 93.224%\n",
      "Epoch 39, Batch 264, LR 0.004410 Loss 3.490315, Accuracy 93.214%\n",
      "Epoch 39, Batch 265, LR 0.004399 Loss 3.490321, Accuracy 93.213%\n",
      "Epoch 39, Batch 266, LR 0.004388 Loss 3.491335, Accuracy 93.204%\n",
      "Epoch 39, Batch 267, LR 0.004377 Loss 3.493123, Accuracy 93.200%\n",
      "Epoch 39, Batch 268, LR 0.004366 Loss 3.493743, Accuracy 93.196%\n",
      "Epoch 39, Batch 269, LR 0.004354 Loss 3.494836, Accuracy 93.198%\n",
      "Epoch 39, Batch 270, LR 0.004343 Loss 3.494532, Accuracy 93.203%\n",
      "Epoch 39, Batch 271, LR 0.004332 Loss 3.494875, Accuracy 93.208%\n",
      "Epoch 39, Batch 272, LR 0.004321 Loss 3.493356, Accuracy 93.213%\n",
      "Epoch 39, Batch 273, LR 0.004310 Loss 3.491529, Accuracy 93.223%\n",
      "Epoch 39, Batch 274, LR 0.004299 Loss 3.492259, Accuracy 93.225%\n",
      "Epoch 39, Batch 275, LR 0.004288 Loss 3.493243, Accuracy 93.216%\n",
      "Epoch 39, Batch 276, LR 0.004277 Loss 3.493116, Accuracy 93.218%\n",
      "Epoch 39, Batch 277, LR 0.004266 Loss 3.493916, Accuracy 93.223%\n",
      "Epoch 39, Batch 278, LR 0.004255 Loss 3.493631, Accuracy 93.227%\n",
      "Epoch 39, Batch 279, LR 0.004244 Loss 3.491496, Accuracy 93.235%\n",
      "Epoch 39, Batch 280, LR 0.004233 Loss 3.491822, Accuracy 93.239%\n",
      "Epoch 39, Batch 281, LR 0.004222 Loss 3.494338, Accuracy 93.236%\n",
      "Epoch 39, Batch 282, LR 0.004211 Loss 3.494495, Accuracy 93.235%\n",
      "Epoch 39, Batch 283, LR 0.004200 Loss 3.492523, Accuracy 93.242%\n",
      "Epoch 39, Batch 284, LR 0.004189 Loss 3.492191, Accuracy 93.247%\n",
      "Epoch 39, Batch 285, LR 0.004178 Loss 3.492811, Accuracy 93.248%\n",
      "Epoch 39, Batch 286, LR 0.004167 Loss 3.492693, Accuracy 93.247%\n",
      "Epoch 39, Batch 287, LR 0.004156 Loss 3.492183, Accuracy 93.252%\n",
      "Epoch 39, Batch 288, LR 0.004145 Loss 3.492349, Accuracy 93.248%\n",
      "Epoch 39, Batch 289, LR 0.004134 Loss 3.493216, Accuracy 93.247%\n",
      "Epoch 39, Batch 290, LR 0.004123 Loss 3.495546, Accuracy 93.235%\n",
      "Epoch 39, Batch 291, LR 0.004112 Loss 3.498434, Accuracy 93.213%\n",
      "Epoch 39, Batch 292, LR 0.004102 Loss 3.498315, Accuracy 93.215%\n",
      "Epoch 39, Batch 293, LR 0.004091 Loss 3.498928, Accuracy 93.209%\n",
      "Epoch 39, Batch 294, LR 0.004080 Loss 3.498658, Accuracy 93.208%\n",
      "Epoch 39, Batch 295, LR 0.004069 Loss 3.498629, Accuracy 93.212%\n",
      "Epoch 39, Batch 296, LR 0.004058 Loss 3.499933, Accuracy 93.206%\n",
      "Epoch 39, Batch 297, LR 0.004048 Loss 3.498232, Accuracy 93.224%\n",
      "Epoch 39, Batch 298, LR 0.004037 Loss 3.497606, Accuracy 93.223%\n",
      "Epoch 39, Batch 299, LR 0.004026 Loss 3.497096, Accuracy 93.227%\n",
      "Epoch 39, Batch 300, LR 0.004015 Loss 3.496694, Accuracy 93.229%\n",
      "Epoch 39, Batch 301, LR 0.004005 Loss 3.494158, Accuracy 93.241%\n",
      "Epoch 39, Batch 302, LR 0.003994 Loss 3.494927, Accuracy 93.235%\n",
      "Epoch 39, Batch 303, LR 0.003983 Loss 3.494332, Accuracy 93.245%\n",
      "Epoch 39, Batch 304, LR 0.003973 Loss 3.492700, Accuracy 93.257%\n",
      "Epoch 39, Batch 305, LR 0.003962 Loss 3.493706, Accuracy 93.248%\n",
      "Epoch 39, Batch 306, LR 0.003951 Loss 3.494180, Accuracy 93.250%\n",
      "Epoch 39, Batch 307, LR 0.003941 Loss 3.494709, Accuracy 93.246%\n",
      "Epoch 39, Batch 308, LR 0.003930 Loss 3.495311, Accuracy 93.245%\n",
      "Epoch 39, Batch 309, LR 0.003920 Loss 3.492811, Accuracy 93.252%\n",
      "Epoch 39, Batch 310, LR 0.003909 Loss 3.492919, Accuracy 93.251%\n",
      "Epoch 39, Batch 311, LR 0.003898 Loss 3.493362, Accuracy 93.258%\n",
      "Epoch 39, Batch 312, LR 0.003888 Loss 3.493061, Accuracy 93.252%\n",
      "Epoch 39, Batch 313, LR 0.003877 Loss 3.494895, Accuracy 93.251%\n",
      "Epoch 39, Batch 314, LR 0.003867 Loss 3.495191, Accuracy 93.252%\n",
      "Epoch 39, Batch 315, LR 0.003856 Loss 3.496441, Accuracy 93.254%\n",
      "Epoch 39, Batch 316, LR 0.003846 Loss 3.496501, Accuracy 93.253%\n",
      "Epoch 39, Batch 317, LR 0.003835 Loss 3.496538, Accuracy 93.257%\n",
      "Epoch 39, Batch 318, LR 0.003825 Loss 3.496069, Accuracy 93.256%\n",
      "Epoch 39, Batch 319, LR 0.003814 Loss 3.495872, Accuracy 93.258%\n",
      "Epoch 39, Batch 320, LR 0.003804 Loss 3.495037, Accuracy 93.247%\n",
      "Epoch 39, Batch 321, LR 0.003794 Loss 3.494828, Accuracy 93.246%\n",
      "Epoch 39, Batch 322, LR 0.003783 Loss 3.495421, Accuracy 93.243%\n",
      "Epoch 39, Batch 323, LR 0.003773 Loss 3.495060, Accuracy 93.249%\n",
      "Epoch 39, Batch 324, LR 0.003762 Loss 3.494341, Accuracy 93.251%\n",
      "Epoch 39, Batch 325, LR 0.003752 Loss 3.495092, Accuracy 93.248%\n",
      "Epoch 39, Batch 326, LR 0.003742 Loss 3.493565, Accuracy 93.249%\n",
      "Epoch 39, Batch 327, LR 0.003731 Loss 3.492781, Accuracy 93.258%\n",
      "Epoch 39, Batch 328, LR 0.003721 Loss 3.492642, Accuracy 93.255%\n",
      "Epoch 39, Batch 329, LR 0.003711 Loss 3.493250, Accuracy 93.247%\n",
      "Epoch 39, Batch 330, LR 0.003701 Loss 3.492883, Accuracy 93.241%\n",
      "Epoch 39, Batch 331, LR 0.003690 Loss 3.493026, Accuracy 93.240%\n",
      "Epoch 39, Batch 332, LR 0.003680 Loss 3.492966, Accuracy 93.244%\n",
      "Epoch 39, Batch 333, LR 0.003670 Loss 3.491798, Accuracy 93.243%\n",
      "Epoch 39, Batch 334, LR 0.003660 Loss 3.490635, Accuracy 93.242%\n",
      "Epoch 39, Batch 335, LR 0.003649 Loss 3.490246, Accuracy 93.242%\n",
      "Epoch 39, Batch 336, LR 0.003639 Loss 3.492238, Accuracy 93.234%\n",
      "Epoch 39, Batch 337, LR 0.003629 Loss 3.491613, Accuracy 93.240%\n",
      "Epoch 39, Batch 338, LR 0.003619 Loss 3.490847, Accuracy 93.244%\n",
      "Epoch 39, Batch 339, LR 0.003609 Loss 3.492604, Accuracy 93.243%\n",
      "Epoch 39, Batch 340, LR 0.003598 Loss 3.494120, Accuracy 93.240%\n",
      "Epoch 39, Batch 341, LR 0.003588 Loss 3.494876, Accuracy 93.232%\n",
      "Epoch 39, Batch 342, LR 0.003578 Loss 3.495431, Accuracy 93.227%\n",
      "Epoch 39, Batch 343, LR 0.003568 Loss 3.494976, Accuracy 93.226%\n",
      "Epoch 39, Batch 344, LR 0.003558 Loss 3.495270, Accuracy 93.216%\n",
      "Epoch 39, Batch 345, LR 0.003548 Loss 3.495814, Accuracy 93.218%\n",
      "Epoch 39, Batch 346, LR 0.003538 Loss 3.496990, Accuracy 93.213%\n",
      "Epoch 39, Batch 347, LR 0.003528 Loss 3.497181, Accuracy 93.214%\n",
      "Epoch 39, Batch 348, LR 0.003518 Loss 3.497207, Accuracy 93.211%\n",
      "Epoch 39, Batch 349, LR 0.003508 Loss 3.498383, Accuracy 93.208%\n",
      "Epoch 39, Batch 350, LR 0.003498 Loss 3.496925, Accuracy 93.210%\n",
      "Epoch 39, Batch 351, LR 0.003488 Loss 3.496769, Accuracy 93.200%\n",
      "Epoch 39, Batch 352, LR 0.003478 Loss 3.497907, Accuracy 93.202%\n",
      "Epoch 39, Batch 353, LR 0.003468 Loss 3.495496, Accuracy 93.203%\n",
      "Epoch 39, Batch 354, LR 0.003458 Loss 3.494892, Accuracy 93.198%\n",
      "Epoch 39, Batch 355, LR 0.003448 Loss 3.495231, Accuracy 93.195%\n",
      "Epoch 39, Batch 356, LR 0.003438 Loss 3.496423, Accuracy 93.199%\n",
      "Epoch 39, Batch 357, LR 0.003428 Loss 3.496523, Accuracy 93.199%\n",
      "Epoch 39, Batch 358, LR 0.003418 Loss 3.496200, Accuracy 93.198%\n",
      "Epoch 39, Batch 359, LR 0.003408 Loss 3.497303, Accuracy 93.193%\n",
      "Epoch 39, Batch 360, LR 0.003398 Loss 3.496671, Accuracy 93.192%\n",
      "Epoch 39, Batch 361, LR 0.003389 Loss 3.497973, Accuracy 93.187%\n",
      "Epoch 39, Batch 362, LR 0.003379 Loss 3.498390, Accuracy 93.187%\n",
      "Epoch 39, Batch 363, LR 0.003369 Loss 3.498253, Accuracy 93.186%\n",
      "Epoch 39, Batch 364, LR 0.003359 Loss 3.498742, Accuracy 93.183%\n",
      "Epoch 39, Batch 365, LR 0.003349 Loss 3.499571, Accuracy 93.181%\n",
      "Epoch 39, Batch 366, LR 0.003340 Loss 3.500378, Accuracy 93.178%\n",
      "Epoch 39, Batch 367, LR 0.003330 Loss 3.499272, Accuracy 93.182%\n",
      "Epoch 39, Batch 368, LR 0.003320 Loss 3.499298, Accuracy 93.183%\n",
      "Epoch 39, Batch 369, LR 0.003310 Loss 3.499052, Accuracy 93.183%\n",
      "Epoch 39, Batch 370, LR 0.003301 Loss 3.499489, Accuracy 93.182%\n",
      "Epoch 39, Batch 371, LR 0.003291 Loss 3.499799, Accuracy 93.190%\n",
      "Epoch 39, Batch 372, LR 0.003281 Loss 3.499680, Accuracy 93.183%\n",
      "Epoch 39, Batch 373, LR 0.003272 Loss 3.500877, Accuracy 93.178%\n",
      "Epoch 39, Batch 374, LR 0.003262 Loss 3.501376, Accuracy 93.180%\n",
      "Epoch 39, Batch 375, LR 0.003252 Loss 3.501097, Accuracy 93.183%\n",
      "Epoch 39, Batch 376, LR 0.003243 Loss 3.501287, Accuracy 93.191%\n",
      "Epoch 39, Batch 377, LR 0.003233 Loss 3.502337, Accuracy 93.184%\n",
      "Epoch 39, Batch 378, LR 0.003223 Loss 3.501241, Accuracy 93.192%\n",
      "Epoch 39, Batch 379, LR 0.003214 Loss 3.500968, Accuracy 93.193%\n",
      "Epoch 39, Batch 380, LR 0.003204 Loss 3.502345, Accuracy 93.185%\n",
      "Epoch 39, Batch 381, LR 0.003195 Loss 3.500309, Accuracy 93.190%\n",
      "Epoch 39, Batch 382, LR 0.003185 Loss 3.499023, Accuracy 93.198%\n",
      "Epoch 39, Batch 383, LR 0.003176 Loss 3.499146, Accuracy 93.203%\n",
      "Epoch 39, Batch 384, LR 0.003166 Loss 3.500071, Accuracy 93.209%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 385, LR 0.003157 Loss 3.499506, Accuracy 93.216%\n",
      "Epoch 39, Batch 386, LR 0.003147 Loss 3.500083, Accuracy 93.212%\n",
      "Epoch 39, Batch 387, LR 0.003138 Loss 3.498796, Accuracy 93.215%\n",
      "Epoch 39, Batch 388, LR 0.003128 Loss 3.496687, Accuracy 93.233%\n",
      "Epoch 39, Batch 389, LR 0.003119 Loss 3.497395, Accuracy 93.230%\n",
      "Epoch 39, Batch 390, LR 0.003109 Loss 3.496863, Accuracy 93.229%\n",
      "Epoch 39, Batch 391, LR 0.003100 Loss 3.498352, Accuracy 93.221%\n",
      "Epoch 39, Batch 392, LR 0.003091 Loss 3.499770, Accuracy 93.220%\n",
      "Epoch 39, Batch 393, LR 0.003081 Loss 3.500460, Accuracy 93.213%\n",
      "Epoch 39, Batch 394, LR 0.003072 Loss 3.499805, Accuracy 93.213%\n",
      "Epoch 39, Batch 395, LR 0.003062 Loss 3.500930, Accuracy 93.204%\n",
      "Epoch 39, Batch 396, LR 0.003053 Loss 3.502093, Accuracy 93.198%\n",
      "Epoch 39, Batch 397, LR 0.003044 Loss 3.500761, Accuracy 93.205%\n",
      "Epoch 39, Batch 398, LR 0.003034 Loss 3.500146, Accuracy 93.210%\n",
      "Epoch 39, Batch 399, LR 0.003025 Loss 3.501580, Accuracy 93.202%\n",
      "Epoch 39, Batch 400, LR 0.003016 Loss 3.502315, Accuracy 93.201%\n",
      "Epoch 39, Batch 401, LR 0.003007 Loss 3.502061, Accuracy 93.204%\n",
      "Epoch 39, Batch 402, LR 0.002997 Loss 3.501985, Accuracy 93.204%\n",
      "Epoch 39, Batch 403, LR 0.002988 Loss 3.503241, Accuracy 93.199%\n",
      "Epoch 39, Batch 404, LR 0.002979 Loss 3.504370, Accuracy 93.199%\n",
      "Epoch 39, Batch 405, LR 0.002970 Loss 3.503578, Accuracy 93.204%\n",
      "Epoch 39, Batch 406, LR 0.002960 Loss 3.504164, Accuracy 93.202%\n",
      "Epoch 39, Batch 407, LR 0.002951 Loss 3.501374, Accuracy 93.214%\n",
      "Epoch 39, Batch 408, LR 0.002942 Loss 3.501156, Accuracy 93.223%\n",
      "Epoch 39, Batch 409, LR 0.002933 Loss 3.502856, Accuracy 93.215%\n",
      "Epoch 39, Batch 410, LR 0.002924 Loss 3.501864, Accuracy 93.215%\n",
      "Epoch 39, Batch 411, LR 0.002915 Loss 3.502111, Accuracy 93.216%\n",
      "Epoch 39, Batch 412, LR 0.002905 Loss 3.501145, Accuracy 93.223%\n",
      "Epoch 39, Batch 413, LR 0.002896 Loss 3.501059, Accuracy 93.220%\n",
      "Epoch 39, Batch 414, LR 0.002887 Loss 3.500239, Accuracy 93.216%\n",
      "Epoch 39, Batch 415, LR 0.002878 Loss 3.501381, Accuracy 93.206%\n",
      "Epoch 39, Batch 416, LR 0.002869 Loss 3.501640, Accuracy 93.204%\n",
      "Epoch 39, Batch 417, LR 0.002860 Loss 3.501469, Accuracy 93.203%\n",
      "Epoch 39, Batch 418, LR 0.002851 Loss 3.499536, Accuracy 93.208%\n",
      "Epoch 39, Batch 419, LR 0.002842 Loss 3.498965, Accuracy 93.207%\n",
      "Epoch 39, Batch 420, LR 0.002833 Loss 3.499460, Accuracy 93.199%\n",
      "Epoch 39, Batch 421, LR 0.002824 Loss 3.499374, Accuracy 93.203%\n",
      "Epoch 39, Batch 422, LR 0.002815 Loss 3.497652, Accuracy 93.209%\n",
      "Epoch 39, Batch 423, LR 0.002806 Loss 3.497675, Accuracy 93.211%\n",
      "Epoch 39, Batch 424, LR 0.002797 Loss 3.496683, Accuracy 93.210%\n",
      "Epoch 39, Batch 425, LR 0.002788 Loss 3.496119, Accuracy 93.211%\n",
      "Epoch 39, Batch 426, LR 0.002779 Loss 3.495885, Accuracy 93.214%\n",
      "Epoch 39, Batch 427, LR 0.002770 Loss 3.495414, Accuracy 93.210%\n",
      "Epoch 39, Batch 428, LR 0.002762 Loss 3.494387, Accuracy 93.215%\n",
      "Epoch 39, Batch 429, LR 0.002753 Loss 3.495472, Accuracy 93.209%\n",
      "Epoch 39, Batch 430, LR 0.002744 Loss 3.494900, Accuracy 93.209%\n",
      "Epoch 39, Batch 431, LR 0.002735 Loss 3.496106, Accuracy 93.203%\n",
      "Epoch 39, Batch 432, LR 0.002726 Loss 3.497631, Accuracy 93.193%\n",
      "Epoch 39, Batch 433, LR 0.002717 Loss 3.496758, Accuracy 93.200%\n",
      "Epoch 39, Batch 434, LR 0.002709 Loss 3.497873, Accuracy 93.192%\n",
      "Epoch 39, Batch 435, LR 0.002700 Loss 3.496378, Accuracy 93.197%\n",
      "Epoch 39, Batch 436, LR 0.002691 Loss 3.497063, Accuracy 93.195%\n",
      "Epoch 39, Batch 437, LR 0.002682 Loss 3.497921, Accuracy 93.192%\n",
      "Epoch 39, Batch 438, LR 0.002674 Loss 3.497489, Accuracy 93.197%\n",
      "Epoch 39, Batch 439, LR 0.002665 Loss 3.496915, Accuracy 93.198%\n",
      "Epoch 39, Batch 440, LR 0.002656 Loss 3.498054, Accuracy 93.196%\n",
      "Epoch 39, Batch 441, LR 0.002647 Loss 3.498707, Accuracy 93.199%\n",
      "Epoch 39, Batch 442, LR 0.002639 Loss 3.499345, Accuracy 93.197%\n",
      "Epoch 39, Batch 443, LR 0.002630 Loss 3.499015, Accuracy 93.198%\n",
      "Epoch 39, Batch 444, LR 0.002621 Loss 3.499356, Accuracy 93.192%\n",
      "Epoch 39, Batch 445, LR 0.002613 Loss 3.499554, Accuracy 93.186%\n",
      "Epoch 39, Batch 446, LR 0.002604 Loss 3.499667, Accuracy 93.189%\n",
      "Epoch 39, Batch 447, LR 0.002595 Loss 3.499099, Accuracy 93.194%\n",
      "Epoch 39, Batch 448, LR 0.002587 Loss 3.499308, Accuracy 93.192%\n",
      "Epoch 39, Batch 449, LR 0.002578 Loss 3.499131, Accuracy 93.188%\n",
      "Epoch 39, Batch 450, LR 0.002570 Loss 3.500178, Accuracy 93.181%\n",
      "Epoch 39, Batch 451, LR 0.002561 Loss 3.499987, Accuracy 93.178%\n",
      "Epoch 39, Batch 452, LR 0.002553 Loss 3.498661, Accuracy 93.181%\n",
      "Epoch 39, Batch 453, LR 0.002544 Loss 3.497928, Accuracy 93.186%\n",
      "Epoch 39, Batch 454, LR 0.002536 Loss 3.497780, Accuracy 93.189%\n",
      "Epoch 39, Batch 455, LR 0.002527 Loss 3.498540, Accuracy 93.187%\n",
      "Epoch 39, Batch 456, LR 0.002519 Loss 3.498011, Accuracy 93.188%\n",
      "Epoch 39, Batch 457, LR 0.002510 Loss 3.498137, Accuracy 93.189%\n",
      "Epoch 39, Batch 458, LR 0.002502 Loss 3.496918, Accuracy 93.194%\n",
      "Epoch 39, Batch 459, LR 0.002493 Loss 3.497320, Accuracy 93.190%\n",
      "Epoch 39, Batch 460, LR 0.002485 Loss 3.498662, Accuracy 93.184%\n",
      "Epoch 39, Batch 461, LR 0.002476 Loss 3.498569, Accuracy 93.184%\n",
      "Epoch 39, Batch 462, LR 0.002468 Loss 3.497061, Accuracy 93.192%\n",
      "Epoch 39, Batch 463, LR 0.002460 Loss 3.496625, Accuracy 93.195%\n",
      "Epoch 39, Batch 464, LR 0.002451 Loss 3.496585, Accuracy 93.194%\n",
      "Epoch 39, Batch 465, LR 0.002443 Loss 3.495687, Accuracy 93.192%\n",
      "Epoch 39, Batch 466, LR 0.002435 Loss 3.496572, Accuracy 93.190%\n",
      "Epoch 39, Batch 467, LR 0.002426 Loss 3.496658, Accuracy 93.196%\n",
      "Epoch 39, Batch 468, LR 0.002418 Loss 3.495273, Accuracy 93.199%\n",
      "Epoch 39, Batch 469, LR 0.002410 Loss 3.494521, Accuracy 93.199%\n",
      "Epoch 39, Batch 470, LR 0.002401 Loss 3.493840, Accuracy 93.198%\n",
      "Epoch 39, Batch 471, LR 0.002393 Loss 3.493420, Accuracy 93.193%\n",
      "Epoch 39, Batch 472, LR 0.002385 Loss 3.494249, Accuracy 93.189%\n",
      "Epoch 39, Batch 473, LR 0.002377 Loss 3.495422, Accuracy 93.183%\n",
      "Epoch 39, Batch 474, LR 0.002368 Loss 3.494406, Accuracy 93.186%\n",
      "Epoch 39, Batch 475, LR 0.002360 Loss 3.493293, Accuracy 93.192%\n",
      "Epoch 39, Batch 476, LR 0.002352 Loss 3.492192, Accuracy 93.200%\n",
      "Epoch 39, Batch 477, LR 0.002344 Loss 3.493329, Accuracy 93.187%\n",
      "Epoch 39, Batch 478, LR 0.002336 Loss 3.492537, Accuracy 93.193%\n",
      "Epoch 39, Batch 479, LR 0.002327 Loss 3.492454, Accuracy 93.194%\n",
      "Epoch 39, Batch 480, LR 0.002319 Loss 3.492798, Accuracy 93.190%\n",
      "Epoch 39, Batch 481, LR 0.002311 Loss 3.492310, Accuracy 93.190%\n",
      "Epoch 39, Batch 482, LR 0.002303 Loss 3.493419, Accuracy 93.189%\n",
      "Epoch 39, Batch 483, LR 0.002295 Loss 3.492715, Accuracy 93.195%\n",
      "Epoch 39, Batch 484, LR 0.002287 Loss 3.491599, Accuracy 93.198%\n",
      "Epoch 39, Batch 485, LR 0.002279 Loss 3.490318, Accuracy 93.206%\n",
      "Epoch 39, Batch 486, LR 0.002271 Loss 3.491413, Accuracy 93.208%\n",
      "Epoch 39, Batch 487, LR 0.002263 Loss 3.491920, Accuracy 93.206%\n",
      "Epoch 39, Batch 488, LR 0.002255 Loss 3.491977, Accuracy 93.206%\n",
      "Epoch 39, Batch 489, LR 0.002247 Loss 3.490672, Accuracy 93.210%\n",
      "Epoch 39, Batch 490, LR 0.002239 Loss 3.489737, Accuracy 93.208%\n",
      "Epoch 39, Batch 491, LR 0.002231 Loss 3.488599, Accuracy 93.212%\n",
      "Epoch 39, Batch 492, LR 0.002223 Loss 3.488463, Accuracy 93.213%\n",
      "Epoch 39, Batch 493, LR 0.002215 Loss 3.489751, Accuracy 93.203%\n",
      "Epoch 39, Batch 494, LR 0.002207 Loss 3.490324, Accuracy 93.201%\n",
      "Epoch 39, Batch 495, LR 0.002199 Loss 3.491491, Accuracy 93.202%\n",
      "Epoch 39, Batch 496, LR 0.002191 Loss 3.490961, Accuracy 93.207%\n",
      "Epoch 39, Batch 497, LR 0.002183 Loss 3.489042, Accuracy 93.216%\n",
      "Epoch 39, Batch 498, LR 0.002175 Loss 3.488378, Accuracy 93.218%\n",
      "Epoch 39, Batch 499, LR 0.002167 Loss 3.487482, Accuracy 93.222%\n",
      "Epoch 39, Batch 500, LR 0.002159 Loss 3.487031, Accuracy 93.228%\n",
      "Epoch 39, Batch 501, LR 0.002152 Loss 3.487351, Accuracy 93.231%\n",
      "Epoch 39, Batch 502, LR 0.002144 Loss 3.486387, Accuracy 93.238%\n",
      "Epoch 39, Batch 503, LR 0.002136 Loss 3.485795, Accuracy 93.244%\n",
      "Epoch 39, Batch 504, LR 0.002128 Loss 3.485880, Accuracy 93.245%\n",
      "Epoch 39, Batch 505, LR 0.002120 Loss 3.486061, Accuracy 93.239%\n",
      "Epoch 39, Batch 506, LR 0.002113 Loss 3.486698, Accuracy 93.240%\n",
      "Epoch 39, Batch 507, LR 0.002105 Loss 3.485764, Accuracy 93.243%\n",
      "Epoch 39, Batch 508, LR 0.002097 Loss 3.484803, Accuracy 93.247%\n",
      "Epoch 39, Batch 509, LR 0.002089 Loss 3.484926, Accuracy 93.245%\n",
      "Epoch 39, Batch 510, LR 0.002082 Loss 3.484539, Accuracy 93.251%\n",
      "Epoch 39, Batch 511, LR 0.002074 Loss 3.485782, Accuracy 93.249%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 512, LR 0.002066 Loss 3.485889, Accuracy 93.250%\n",
      "Epoch 39, Batch 513, LR 0.002059 Loss 3.485375, Accuracy 93.254%\n",
      "Epoch 39, Batch 514, LR 0.002051 Loss 3.484422, Accuracy 93.258%\n",
      "Epoch 39, Batch 515, LR 0.002043 Loss 3.483252, Accuracy 93.263%\n",
      "Epoch 39, Batch 516, LR 0.002036 Loss 3.485554, Accuracy 93.253%\n",
      "Epoch 39, Batch 517, LR 0.002028 Loss 3.486145, Accuracy 93.253%\n",
      "Epoch 39, Batch 518, LR 0.002020 Loss 3.484533, Accuracy 93.258%\n",
      "Epoch 39, Batch 519, LR 0.002013 Loss 3.483436, Accuracy 93.262%\n",
      "Epoch 39, Batch 520, LR 0.002005 Loss 3.483801, Accuracy 93.263%\n",
      "Epoch 39, Batch 521, LR 0.001998 Loss 3.484275, Accuracy 93.261%\n",
      "Epoch 39, Batch 522, LR 0.001990 Loss 3.483397, Accuracy 93.261%\n",
      "Epoch 39, Batch 523, LR 0.001983 Loss 3.483128, Accuracy 93.259%\n",
      "Epoch 39, Batch 524, LR 0.001975 Loss 3.484258, Accuracy 93.258%\n",
      "Epoch 39, Batch 525, LR 0.001968 Loss 3.484934, Accuracy 93.253%\n",
      "Epoch 39, Batch 526, LR 0.001960 Loss 3.484670, Accuracy 93.254%\n",
      "Epoch 39, Batch 527, LR 0.001953 Loss 3.483553, Accuracy 93.256%\n",
      "Epoch 39, Batch 528, LR 0.001945 Loss 3.483095, Accuracy 93.260%\n",
      "Epoch 39, Batch 529, LR 0.001938 Loss 3.483337, Accuracy 93.258%\n",
      "Epoch 39, Batch 530, LR 0.001930 Loss 3.484270, Accuracy 93.255%\n",
      "Epoch 39, Batch 531, LR 0.001923 Loss 3.483631, Accuracy 93.259%\n",
      "Epoch 39, Batch 532, LR 0.001916 Loss 3.483397, Accuracy 93.257%\n",
      "Epoch 39, Batch 533, LR 0.001908 Loss 3.484214, Accuracy 93.250%\n",
      "Epoch 39, Batch 534, LR 0.001901 Loss 3.484255, Accuracy 93.245%\n",
      "Epoch 39, Batch 535, LR 0.001894 Loss 3.484069, Accuracy 93.245%\n",
      "Epoch 39, Batch 536, LR 0.001886 Loss 3.484123, Accuracy 93.247%\n",
      "Epoch 39, Batch 537, LR 0.001879 Loss 3.483788, Accuracy 93.247%\n",
      "Epoch 39, Batch 538, LR 0.001872 Loss 3.483677, Accuracy 93.246%\n",
      "Epoch 39, Batch 539, LR 0.001864 Loss 3.483127, Accuracy 93.253%\n",
      "Epoch 39, Batch 540, LR 0.001857 Loss 3.483105, Accuracy 93.252%\n",
      "Epoch 39, Batch 541, LR 0.001850 Loss 3.483073, Accuracy 93.247%\n",
      "Epoch 39, Batch 542, LR 0.001842 Loss 3.482447, Accuracy 93.247%\n",
      "Epoch 39, Batch 543, LR 0.001835 Loss 3.483067, Accuracy 93.244%\n",
      "Epoch 39, Batch 544, LR 0.001828 Loss 3.483586, Accuracy 93.243%\n",
      "Epoch 39, Batch 545, LR 0.001821 Loss 3.483108, Accuracy 93.241%\n",
      "Epoch 39, Batch 546, LR 0.001814 Loss 3.483426, Accuracy 93.235%\n",
      "Epoch 39, Batch 547, LR 0.001806 Loss 3.483260, Accuracy 93.236%\n",
      "Epoch 39, Batch 548, LR 0.001799 Loss 3.484337, Accuracy 93.231%\n",
      "Epoch 39, Batch 549, LR 0.001792 Loss 3.485564, Accuracy 93.229%\n",
      "Epoch 39, Batch 550, LR 0.001785 Loss 3.485988, Accuracy 93.230%\n",
      "Epoch 39, Batch 551, LR 0.001778 Loss 3.486696, Accuracy 93.227%\n",
      "Epoch 39, Batch 552, LR 0.001771 Loss 3.486725, Accuracy 93.231%\n",
      "Epoch 39, Batch 553, LR 0.001764 Loss 3.487552, Accuracy 93.227%\n",
      "Epoch 39, Batch 554, LR 0.001757 Loss 3.486612, Accuracy 93.228%\n",
      "Epoch 39, Batch 555, LR 0.001749 Loss 3.486186, Accuracy 93.232%\n",
      "Epoch 39, Batch 556, LR 0.001742 Loss 3.485870, Accuracy 93.233%\n",
      "Epoch 39, Batch 557, LR 0.001735 Loss 3.484837, Accuracy 93.235%\n",
      "Epoch 39, Batch 558, LR 0.001728 Loss 3.486202, Accuracy 93.229%\n",
      "Epoch 39, Batch 559, LR 0.001721 Loss 3.485974, Accuracy 93.233%\n",
      "Epoch 39, Batch 560, LR 0.001714 Loss 3.485710, Accuracy 93.235%\n",
      "Epoch 39, Batch 561, LR 0.001707 Loss 3.485720, Accuracy 93.233%\n",
      "Epoch 39, Batch 562, LR 0.001700 Loss 3.485033, Accuracy 93.236%\n",
      "Epoch 39, Batch 563, LR 0.001693 Loss 3.485028, Accuracy 93.234%\n",
      "Epoch 39, Batch 564, LR 0.001687 Loss 3.486123, Accuracy 93.232%\n",
      "Epoch 39, Batch 565, LR 0.001680 Loss 3.485452, Accuracy 93.238%\n",
      "Epoch 39, Batch 566, LR 0.001673 Loss 3.485695, Accuracy 93.242%\n",
      "Epoch 39, Batch 567, LR 0.001666 Loss 3.485143, Accuracy 93.247%\n",
      "Epoch 39, Batch 568, LR 0.001659 Loss 3.486139, Accuracy 93.242%\n",
      "Epoch 39, Batch 569, LR 0.001652 Loss 3.486258, Accuracy 93.245%\n",
      "Epoch 39, Batch 570, LR 0.001645 Loss 3.485594, Accuracy 93.247%\n",
      "Epoch 39, Batch 571, LR 0.001638 Loss 3.486047, Accuracy 93.252%\n",
      "Epoch 39, Batch 572, LR 0.001632 Loss 3.485223, Accuracy 93.256%\n",
      "Epoch 39, Batch 573, LR 0.001625 Loss 3.484270, Accuracy 93.258%\n",
      "Epoch 39, Batch 574, LR 0.001618 Loss 3.485415, Accuracy 93.249%\n",
      "Epoch 39, Batch 575, LR 0.001611 Loss 3.485769, Accuracy 93.253%\n",
      "Epoch 39, Batch 576, LR 0.001604 Loss 3.486265, Accuracy 93.248%\n",
      "Epoch 39, Batch 577, LR 0.001598 Loss 3.485678, Accuracy 93.253%\n",
      "Epoch 39, Batch 578, LR 0.001591 Loss 3.486418, Accuracy 93.249%\n",
      "Epoch 39, Batch 579, LR 0.001584 Loss 3.486871, Accuracy 93.244%\n",
      "Epoch 39, Batch 580, LR 0.001577 Loss 3.486917, Accuracy 93.242%\n",
      "Epoch 39, Batch 581, LR 0.001571 Loss 3.486895, Accuracy 93.248%\n",
      "Epoch 39, Batch 582, LR 0.001564 Loss 3.485645, Accuracy 93.257%\n",
      "Epoch 39, Batch 583, LR 0.001557 Loss 3.484443, Accuracy 93.254%\n",
      "Epoch 39, Batch 584, LR 0.001551 Loss 3.483766, Accuracy 93.256%\n",
      "Epoch 39, Batch 585, LR 0.001544 Loss 3.483355, Accuracy 93.252%\n",
      "Epoch 39, Batch 586, LR 0.001538 Loss 3.482571, Accuracy 93.258%\n",
      "Epoch 39, Batch 587, LR 0.001531 Loss 3.482800, Accuracy 93.254%\n",
      "Epoch 39, Batch 588, LR 0.001524 Loss 3.483843, Accuracy 93.250%\n",
      "Epoch 39, Batch 589, LR 0.001518 Loss 3.484498, Accuracy 93.249%\n",
      "Epoch 39, Batch 590, LR 0.001511 Loss 3.484993, Accuracy 93.247%\n",
      "Epoch 39, Batch 591, LR 0.001505 Loss 3.486197, Accuracy 93.244%\n",
      "Epoch 39, Batch 592, LR 0.001498 Loss 3.487174, Accuracy 93.243%\n",
      "Epoch 39, Batch 593, LR 0.001492 Loss 3.487268, Accuracy 93.245%\n",
      "Epoch 39, Batch 594, LR 0.001485 Loss 3.488574, Accuracy 93.241%\n",
      "Epoch 39, Batch 595, LR 0.001479 Loss 3.487785, Accuracy 93.244%\n",
      "Epoch 39, Batch 596, LR 0.001472 Loss 3.487535, Accuracy 93.240%\n",
      "Epoch 39, Batch 597, LR 0.001466 Loss 3.487653, Accuracy 93.237%\n",
      "Epoch 39, Batch 598, LR 0.001459 Loss 3.486707, Accuracy 93.237%\n",
      "Epoch 39, Batch 599, LR 0.001453 Loss 3.486476, Accuracy 93.239%\n",
      "Epoch 39, Batch 600, LR 0.001446 Loss 3.487884, Accuracy 93.233%\n",
      "Epoch 39, Batch 601, LR 0.001440 Loss 3.488313, Accuracy 93.234%\n",
      "Epoch 39, Batch 602, LR 0.001434 Loss 3.488193, Accuracy 93.233%\n",
      "Epoch 39, Batch 603, LR 0.001427 Loss 3.488052, Accuracy 93.233%\n",
      "Epoch 39, Batch 604, LR 0.001421 Loss 3.487975, Accuracy 93.233%\n",
      "Epoch 39, Batch 605, LR 0.001414 Loss 3.488193, Accuracy 93.236%\n",
      "Epoch 39, Batch 606, LR 0.001408 Loss 3.488500, Accuracy 93.234%\n",
      "Epoch 39, Batch 607, LR 0.001402 Loss 3.489913, Accuracy 93.230%\n",
      "Epoch 39, Batch 608, LR 0.001395 Loss 3.489443, Accuracy 93.231%\n",
      "Epoch 39, Batch 609, LR 0.001389 Loss 3.489350, Accuracy 93.229%\n",
      "Epoch 39, Batch 610, LR 0.001383 Loss 3.489794, Accuracy 93.226%\n",
      "Epoch 39, Batch 611, LR 0.001377 Loss 3.489471, Accuracy 93.228%\n",
      "Epoch 39, Batch 612, LR 0.001370 Loss 3.489975, Accuracy 93.227%\n",
      "Epoch 39, Batch 613, LR 0.001364 Loss 3.490570, Accuracy 93.224%\n",
      "Epoch 39, Batch 614, LR 0.001358 Loss 3.491031, Accuracy 93.221%\n",
      "Epoch 39, Batch 615, LR 0.001352 Loss 3.490855, Accuracy 93.219%\n",
      "Epoch 39, Batch 616, LR 0.001346 Loss 3.489867, Accuracy 93.224%\n",
      "Epoch 39, Batch 617, LR 0.001339 Loss 3.488781, Accuracy 93.228%\n",
      "Epoch 39, Batch 618, LR 0.001333 Loss 3.487812, Accuracy 93.233%\n",
      "Epoch 39, Batch 619, LR 0.001327 Loss 3.487127, Accuracy 93.238%\n",
      "Epoch 39, Batch 620, LR 0.001321 Loss 3.487378, Accuracy 93.235%\n",
      "Epoch 39, Batch 621, LR 0.001315 Loss 3.487351, Accuracy 93.240%\n",
      "Epoch 39, Batch 622, LR 0.001309 Loss 3.486374, Accuracy 93.245%\n",
      "Epoch 39, Batch 623, LR 0.001303 Loss 3.485718, Accuracy 93.246%\n",
      "Epoch 39, Batch 624, LR 0.001297 Loss 3.486433, Accuracy 93.240%\n",
      "Epoch 39, Batch 625, LR 0.001290 Loss 3.486745, Accuracy 93.239%\n",
      "Epoch 39, Batch 626, LR 0.001284 Loss 3.486320, Accuracy 93.245%\n",
      "Epoch 39, Batch 627, LR 0.001278 Loss 3.486472, Accuracy 93.243%\n",
      "Epoch 39, Batch 628, LR 0.001272 Loss 3.486738, Accuracy 93.246%\n",
      "Epoch 39, Batch 629, LR 0.001266 Loss 3.486030, Accuracy 93.249%\n",
      "Epoch 39, Batch 630, LR 0.001260 Loss 3.485422, Accuracy 93.254%\n",
      "Epoch 39, Batch 631, LR 0.001254 Loss 3.485965, Accuracy 93.246%\n",
      "Epoch 39, Batch 632, LR 0.001248 Loss 3.485125, Accuracy 93.249%\n",
      "Epoch 39, Batch 633, LR 0.001242 Loss 3.485243, Accuracy 93.253%\n",
      "Epoch 39, Batch 634, LR 0.001237 Loss 3.484943, Accuracy 93.256%\n",
      "Epoch 39, Batch 635, LR 0.001231 Loss 3.484781, Accuracy 93.262%\n",
      "Epoch 39, Batch 636, LR 0.001225 Loss 3.485392, Accuracy 93.259%\n",
      "Epoch 39, Batch 637, LR 0.001219 Loss 3.484776, Accuracy 93.262%\n",
      "Epoch 39, Batch 638, LR 0.001213 Loss 3.485097, Accuracy 93.258%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 639, LR 0.001207 Loss 3.484197, Accuracy 93.259%\n",
      "Epoch 39, Batch 640, LR 0.001201 Loss 3.484368, Accuracy 93.257%\n",
      "Epoch 39, Batch 641, LR 0.001195 Loss 3.484394, Accuracy 93.259%\n",
      "Epoch 39, Batch 642, LR 0.001190 Loss 3.484655, Accuracy 93.261%\n",
      "Epoch 39, Batch 643, LR 0.001184 Loss 3.484938, Accuracy 93.255%\n",
      "Epoch 39, Batch 644, LR 0.001178 Loss 3.486040, Accuracy 93.251%\n",
      "Epoch 39, Batch 645, LR 0.001172 Loss 3.486974, Accuracy 93.250%\n",
      "Epoch 39, Batch 646, LR 0.001166 Loss 3.487304, Accuracy 93.252%\n",
      "Epoch 39, Batch 647, LR 0.001161 Loss 3.487887, Accuracy 93.243%\n",
      "Epoch 39, Batch 648, LR 0.001155 Loss 3.487557, Accuracy 93.246%\n",
      "Epoch 39, Batch 649, LR 0.001149 Loss 3.487270, Accuracy 93.247%\n",
      "Epoch 39, Batch 650, LR 0.001144 Loss 3.487035, Accuracy 93.250%\n",
      "Epoch 39, Batch 651, LR 0.001138 Loss 3.486590, Accuracy 93.248%\n",
      "Epoch 39, Batch 652, LR 0.001132 Loss 3.486774, Accuracy 93.249%\n",
      "Epoch 39, Batch 653, LR 0.001127 Loss 3.486259, Accuracy 93.250%\n",
      "Epoch 39, Batch 654, LR 0.001121 Loss 3.487378, Accuracy 93.245%\n",
      "Epoch 39, Batch 655, LR 0.001115 Loss 3.487550, Accuracy 93.241%\n",
      "Epoch 39, Batch 656, LR 0.001110 Loss 3.487344, Accuracy 93.244%\n",
      "Epoch 39, Batch 657, LR 0.001104 Loss 3.486344, Accuracy 93.245%\n",
      "Epoch 39, Batch 658, LR 0.001098 Loss 3.485929, Accuracy 93.248%\n",
      "Epoch 39, Batch 659, LR 0.001093 Loss 3.484975, Accuracy 93.253%\n",
      "Epoch 39, Batch 660, LR 0.001087 Loss 3.484609, Accuracy 93.256%\n",
      "Epoch 39, Batch 661, LR 0.001082 Loss 3.484657, Accuracy 93.258%\n",
      "Epoch 39, Batch 662, LR 0.001076 Loss 3.484058, Accuracy 93.257%\n",
      "Epoch 39, Batch 663, LR 0.001071 Loss 3.483887, Accuracy 93.256%\n",
      "Epoch 39, Batch 664, LR 0.001065 Loss 3.483589, Accuracy 93.255%\n",
      "Epoch 39, Batch 665, LR 0.001060 Loss 3.483331, Accuracy 93.255%\n",
      "Epoch 39, Batch 666, LR 0.001054 Loss 3.483066, Accuracy 93.260%\n",
      "Epoch 39, Batch 667, LR 0.001049 Loss 3.483010, Accuracy 93.259%\n",
      "Epoch 39, Batch 668, LR 0.001043 Loss 3.483181, Accuracy 93.256%\n",
      "Epoch 39, Batch 669, LR 0.001038 Loss 3.482870, Accuracy 93.257%\n",
      "Epoch 39, Batch 670, LR 0.001032 Loss 3.482825, Accuracy 93.256%\n",
      "Epoch 39, Batch 671, LR 0.001027 Loss 3.483188, Accuracy 93.251%\n",
      "Epoch 39, Batch 672, LR 0.001022 Loss 3.483448, Accuracy 93.252%\n",
      "Epoch 39, Batch 673, LR 0.001016 Loss 3.483706, Accuracy 93.254%\n",
      "Epoch 39, Batch 674, LR 0.001011 Loss 3.482680, Accuracy 93.260%\n",
      "Epoch 39, Batch 675, LR 0.001006 Loss 3.484545, Accuracy 93.255%\n",
      "Epoch 39, Batch 676, LR 0.001000 Loss 3.484165, Accuracy 93.258%\n",
      "Epoch 39, Batch 677, LR 0.000995 Loss 3.484548, Accuracy 93.260%\n",
      "Epoch 39, Batch 678, LR 0.000990 Loss 3.484745, Accuracy 93.256%\n",
      "Epoch 39, Batch 679, LR 0.000984 Loss 3.484637, Accuracy 93.255%\n",
      "Epoch 39, Batch 680, LR 0.000979 Loss 3.485004, Accuracy 93.251%\n",
      "Epoch 39, Batch 681, LR 0.000974 Loss 3.484776, Accuracy 93.249%\n",
      "Epoch 39, Batch 682, LR 0.000969 Loss 3.485444, Accuracy 93.245%\n",
      "Epoch 39, Batch 683, LR 0.000963 Loss 3.484608, Accuracy 93.249%\n",
      "Epoch 39, Batch 684, LR 0.000958 Loss 3.483930, Accuracy 93.252%\n",
      "Epoch 39, Batch 685, LR 0.000953 Loss 3.484028, Accuracy 93.253%\n",
      "Epoch 39, Batch 686, LR 0.000948 Loss 3.484620, Accuracy 93.251%\n",
      "Epoch 39, Batch 687, LR 0.000943 Loss 3.485122, Accuracy 93.248%\n",
      "Epoch 39, Batch 688, LR 0.000937 Loss 3.484581, Accuracy 93.249%\n",
      "Epoch 39, Batch 689, LR 0.000932 Loss 3.485574, Accuracy 93.247%\n",
      "Epoch 39, Batch 690, LR 0.000927 Loss 3.484999, Accuracy 93.248%\n",
      "Epoch 39, Batch 691, LR 0.000922 Loss 3.484956, Accuracy 93.246%\n",
      "Epoch 39, Batch 692, LR 0.000917 Loss 3.484785, Accuracy 93.245%\n",
      "Epoch 39, Batch 693, LR 0.000912 Loss 3.484794, Accuracy 93.242%\n",
      "Epoch 39, Batch 694, LR 0.000907 Loss 3.485132, Accuracy 93.243%\n",
      "Epoch 39, Batch 695, LR 0.000902 Loss 3.485601, Accuracy 93.241%\n",
      "Epoch 39, Batch 696, LR 0.000897 Loss 3.486252, Accuracy 93.238%\n",
      "Epoch 39, Batch 697, LR 0.000892 Loss 3.486067, Accuracy 93.238%\n",
      "Epoch 39, Batch 698, LR 0.000887 Loss 3.486603, Accuracy 93.234%\n",
      "Epoch 39, Batch 699, LR 0.000882 Loss 3.486469, Accuracy 93.234%\n",
      "Epoch 39, Batch 700, LR 0.000877 Loss 3.487000, Accuracy 93.232%\n",
      "Epoch 39, Batch 701, LR 0.000872 Loss 3.487936, Accuracy 93.228%\n",
      "Epoch 39, Batch 702, LR 0.000867 Loss 3.488020, Accuracy 93.224%\n",
      "Epoch 39, Batch 703, LR 0.000862 Loss 3.488025, Accuracy 93.222%\n",
      "Epoch 39, Batch 704, LR 0.000857 Loss 3.487539, Accuracy 93.224%\n",
      "Epoch 39, Batch 705, LR 0.000852 Loss 3.488001, Accuracy 93.219%\n",
      "Epoch 39, Batch 706, LR 0.000847 Loss 3.488390, Accuracy 93.218%\n",
      "Epoch 39, Batch 707, LR 0.000842 Loss 3.487661, Accuracy 93.222%\n",
      "Epoch 39, Batch 708, LR 0.000837 Loss 3.486922, Accuracy 93.223%\n",
      "Epoch 39, Batch 709, LR 0.000832 Loss 3.486708, Accuracy 93.224%\n",
      "Epoch 39, Batch 710, LR 0.000828 Loss 3.486958, Accuracy 93.227%\n",
      "Epoch 39, Batch 711, LR 0.000823 Loss 3.487827, Accuracy 93.219%\n",
      "Epoch 39, Batch 712, LR 0.000818 Loss 3.487760, Accuracy 93.218%\n",
      "Epoch 39, Batch 713, LR 0.000813 Loss 3.488384, Accuracy 93.214%\n",
      "Epoch 39, Batch 714, LR 0.000808 Loss 3.488264, Accuracy 93.217%\n",
      "Epoch 39, Batch 715, LR 0.000804 Loss 3.488018, Accuracy 93.219%\n",
      "Epoch 39, Batch 716, LR 0.000799 Loss 3.488101, Accuracy 93.220%\n",
      "Epoch 39, Batch 717, LR 0.000794 Loss 3.488407, Accuracy 93.217%\n",
      "Epoch 39, Batch 718, LR 0.000789 Loss 3.487631, Accuracy 93.220%\n",
      "Epoch 39, Batch 719, LR 0.000785 Loss 3.488137, Accuracy 93.218%\n",
      "Epoch 39, Batch 720, LR 0.000780 Loss 3.488426, Accuracy 93.219%\n",
      "Epoch 39, Batch 721, LR 0.000775 Loss 3.488401, Accuracy 93.219%\n",
      "Epoch 39, Batch 722, LR 0.000771 Loss 3.488271, Accuracy 93.218%\n",
      "Epoch 39, Batch 723, LR 0.000766 Loss 3.488799, Accuracy 93.218%\n",
      "Epoch 39, Batch 724, LR 0.000761 Loss 3.488770, Accuracy 93.217%\n",
      "Epoch 39, Batch 725, LR 0.000757 Loss 3.488416, Accuracy 93.218%\n",
      "Epoch 39, Batch 726, LR 0.000752 Loss 3.488448, Accuracy 93.216%\n",
      "Epoch 39, Batch 727, LR 0.000747 Loss 3.488516, Accuracy 93.216%\n",
      "Epoch 39, Batch 728, LR 0.000743 Loss 3.489070, Accuracy 93.212%\n",
      "Epoch 39, Batch 729, LR 0.000738 Loss 3.488900, Accuracy 93.213%\n",
      "Epoch 39, Batch 730, LR 0.000734 Loss 3.488743, Accuracy 93.213%\n",
      "Epoch 39, Batch 731, LR 0.000729 Loss 3.488854, Accuracy 93.210%\n",
      "Epoch 39, Batch 732, LR 0.000725 Loss 3.488074, Accuracy 93.212%\n",
      "Epoch 39, Batch 733, LR 0.000720 Loss 3.487626, Accuracy 93.211%\n",
      "Epoch 39, Batch 734, LR 0.000716 Loss 3.487491, Accuracy 93.206%\n",
      "Epoch 39, Batch 735, LR 0.000711 Loss 3.487544, Accuracy 93.207%\n",
      "Epoch 39, Batch 736, LR 0.000707 Loss 3.488017, Accuracy 93.208%\n",
      "Epoch 39, Batch 737, LR 0.000702 Loss 3.487913, Accuracy 93.206%\n",
      "Epoch 39, Batch 738, LR 0.000698 Loss 3.488172, Accuracy 93.206%\n",
      "Epoch 39, Batch 739, LR 0.000693 Loss 3.487332, Accuracy 93.209%\n",
      "Epoch 39, Batch 740, LR 0.000689 Loss 3.487052, Accuracy 93.211%\n",
      "Epoch 39, Batch 741, LR 0.000685 Loss 3.487132, Accuracy 93.211%\n",
      "Epoch 39, Batch 742, LR 0.000680 Loss 3.487590, Accuracy 93.211%\n",
      "Epoch 39, Batch 743, LR 0.000676 Loss 3.486298, Accuracy 93.216%\n",
      "Epoch 39, Batch 744, LR 0.000671 Loss 3.485706, Accuracy 93.219%\n",
      "Epoch 39, Batch 745, LR 0.000667 Loss 3.484755, Accuracy 93.223%\n",
      "Epoch 39, Batch 746, LR 0.000663 Loss 3.484748, Accuracy 93.219%\n",
      "Epoch 39, Batch 747, LR 0.000658 Loss 3.485175, Accuracy 93.216%\n",
      "Epoch 39, Batch 748, LR 0.000654 Loss 3.484945, Accuracy 93.217%\n",
      "Epoch 39, Batch 749, LR 0.000650 Loss 3.484911, Accuracy 93.218%\n",
      "Epoch 39, Batch 750, LR 0.000646 Loss 3.485044, Accuracy 93.218%\n",
      "Epoch 39, Batch 751, LR 0.000641 Loss 3.484334, Accuracy 93.220%\n",
      "Epoch 39, Batch 752, LR 0.000637 Loss 3.484514, Accuracy 93.223%\n",
      "Epoch 39, Batch 753, LR 0.000633 Loss 3.484187, Accuracy 93.223%\n",
      "Epoch 39, Batch 754, LR 0.000629 Loss 3.483997, Accuracy 93.225%\n",
      "Epoch 39, Batch 755, LR 0.000624 Loss 3.484897, Accuracy 93.222%\n",
      "Epoch 39, Batch 756, LR 0.000620 Loss 3.484777, Accuracy 93.225%\n",
      "Epoch 39, Batch 757, LR 0.000616 Loss 3.483912, Accuracy 93.231%\n",
      "Epoch 39, Batch 758, LR 0.000612 Loss 3.484541, Accuracy 93.230%\n",
      "Epoch 39, Batch 759, LR 0.000608 Loss 3.483349, Accuracy 93.234%\n",
      "Epoch 39, Batch 760, LR 0.000604 Loss 3.484129, Accuracy 93.229%\n",
      "Epoch 39, Batch 761, LR 0.000600 Loss 3.484107, Accuracy 93.228%\n",
      "Epoch 39, Batch 762, LR 0.000595 Loss 3.484254, Accuracy 93.227%\n",
      "Epoch 39, Batch 763, LR 0.000591 Loss 3.483608, Accuracy 93.230%\n",
      "Epoch 39, Batch 764, LR 0.000587 Loss 3.483305, Accuracy 93.232%\n",
      "Epoch 39, Batch 765, LR 0.000583 Loss 3.484234, Accuracy 93.227%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 766, LR 0.000579 Loss 3.484496, Accuracy 93.223%\n",
      "Epoch 39, Batch 767, LR 0.000575 Loss 3.484564, Accuracy 93.221%\n",
      "Epoch 39, Batch 768, LR 0.000571 Loss 3.484191, Accuracy 93.223%\n",
      "Epoch 39, Batch 769, LR 0.000567 Loss 3.484475, Accuracy 93.221%\n",
      "Epoch 39, Batch 770, LR 0.000563 Loss 3.483685, Accuracy 93.224%\n",
      "Epoch 39, Batch 771, LR 0.000559 Loss 3.483191, Accuracy 93.225%\n",
      "Epoch 39, Batch 772, LR 0.000555 Loss 3.484055, Accuracy 93.223%\n",
      "Epoch 39, Batch 773, LR 0.000551 Loss 3.484082, Accuracy 93.224%\n",
      "Epoch 39, Batch 774, LR 0.000547 Loss 3.483663, Accuracy 93.225%\n",
      "Epoch 39, Batch 775, LR 0.000543 Loss 3.482991, Accuracy 93.228%\n",
      "Epoch 39, Batch 776, LR 0.000540 Loss 3.483530, Accuracy 93.221%\n",
      "Epoch 39, Batch 777, LR 0.000536 Loss 3.484323, Accuracy 93.215%\n",
      "Epoch 39, Batch 778, LR 0.000532 Loss 3.484347, Accuracy 93.216%\n",
      "Epoch 39, Batch 779, LR 0.000528 Loss 3.485362, Accuracy 93.215%\n",
      "Epoch 39, Batch 780, LR 0.000524 Loss 3.485229, Accuracy 93.216%\n",
      "Epoch 39, Batch 781, LR 0.000520 Loss 3.486125, Accuracy 93.209%\n",
      "Epoch 39, Batch 782, LR 0.000517 Loss 3.486471, Accuracy 93.210%\n",
      "Epoch 39, Batch 783, LR 0.000513 Loss 3.486128, Accuracy 93.213%\n",
      "Epoch 39, Batch 784, LR 0.000509 Loss 3.485718, Accuracy 93.218%\n",
      "Epoch 39, Batch 785, LR 0.000505 Loss 3.485694, Accuracy 93.218%\n",
      "Epoch 39, Batch 786, LR 0.000501 Loss 3.486497, Accuracy 93.215%\n",
      "Epoch 39, Batch 787, LR 0.000498 Loss 3.486916, Accuracy 93.216%\n",
      "Epoch 39, Batch 788, LR 0.000494 Loss 3.486260, Accuracy 93.220%\n",
      "Epoch 39, Batch 789, LR 0.000490 Loss 3.486311, Accuracy 93.217%\n",
      "Epoch 39, Batch 790, LR 0.000487 Loss 3.486544, Accuracy 93.216%\n",
      "Epoch 39, Batch 791, LR 0.000483 Loss 3.487013, Accuracy 93.217%\n",
      "Epoch 39, Batch 792, LR 0.000479 Loss 3.486684, Accuracy 93.222%\n",
      "Epoch 39, Batch 793, LR 0.000476 Loss 3.486747, Accuracy 93.221%\n",
      "Epoch 39, Batch 794, LR 0.000472 Loss 3.486890, Accuracy 93.224%\n",
      "Epoch 39, Batch 795, LR 0.000468 Loss 3.486607, Accuracy 93.224%\n",
      "Epoch 39, Batch 796, LR 0.000465 Loss 3.486683, Accuracy 93.226%\n",
      "Epoch 39, Batch 797, LR 0.000461 Loss 3.486705, Accuracy 93.228%\n",
      "Epoch 39, Batch 798, LR 0.000457 Loss 3.486760, Accuracy 93.228%\n",
      "Epoch 39, Batch 799, LR 0.000454 Loss 3.486825, Accuracy 93.228%\n",
      "Epoch 39, Batch 800, LR 0.000450 Loss 3.487711, Accuracy 93.226%\n",
      "Epoch 39, Batch 801, LR 0.000447 Loss 3.487589, Accuracy 93.227%\n",
      "Epoch 39, Batch 802, LR 0.000443 Loss 3.487062, Accuracy 93.232%\n",
      "Epoch 39, Batch 803, LR 0.000440 Loss 3.487444, Accuracy 93.229%\n",
      "Epoch 39, Batch 804, LR 0.000436 Loss 3.486420, Accuracy 93.232%\n",
      "Epoch 39, Batch 805, LR 0.000433 Loss 3.486138, Accuracy 93.235%\n",
      "Epoch 39, Batch 806, LR 0.000429 Loss 3.486917, Accuracy 93.231%\n",
      "Epoch 39, Batch 807, LR 0.000426 Loss 3.486420, Accuracy 93.233%\n",
      "Epoch 39, Batch 808, LR 0.000422 Loss 3.486062, Accuracy 93.238%\n",
      "Epoch 39, Batch 809, LR 0.000419 Loss 3.485692, Accuracy 93.238%\n",
      "Epoch 39, Batch 810, LR 0.000416 Loss 3.485231, Accuracy 93.239%\n",
      "Epoch 39, Batch 811, LR 0.000412 Loss 3.485495, Accuracy 93.238%\n",
      "Epoch 39, Batch 812, LR 0.000409 Loss 3.485441, Accuracy 93.243%\n",
      "Epoch 39, Batch 813, LR 0.000405 Loss 3.485865, Accuracy 93.244%\n",
      "Epoch 39, Batch 814, LR 0.000402 Loss 3.484616, Accuracy 93.251%\n",
      "Epoch 39, Batch 815, LR 0.000399 Loss 3.484422, Accuracy 93.250%\n",
      "Epoch 39, Batch 816, LR 0.000395 Loss 3.484174, Accuracy 93.248%\n",
      "Epoch 39, Batch 817, LR 0.000392 Loss 3.484174, Accuracy 93.249%\n",
      "Epoch 39, Batch 818, LR 0.000389 Loss 3.484940, Accuracy 93.247%\n",
      "Epoch 39, Batch 819, LR 0.000386 Loss 3.484926, Accuracy 93.248%\n",
      "Epoch 39, Batch 820, LR 0.000382 Loss 3.484592, Accuracy 93.246%\n",
      "Epoch 39, Batch 821, LR 0.000379 Loss 3.483959, Accuracy 93.249%\n",
      "Epoch 39, Batch 822, LR 0.000376 Loss 3.483941, Accuracy 93.249%\n",
      "Epoch 39, Batch 823, LR 0.000373 Loss 3.484782, Accuracy 93.245%\n",
      "Epoch 39, Batch 824, LR 0.000369 Loss 3.484325, Accuracy 93.247%\n",
      "Epoch 39, Batch 825, LR 0.000366 Loss 3.484885, Accuracy 93.243%\n",
      "Epoch 39, Batch 826, LR 0.000363 Loss 3.484694, Accuracy 93.246%\n",
      "Epoch 39, Batch 827, LR 0.000360 Loss 3.484411, Accuracy 93.246%\n",
      "Epoch 39, Batch 828, LR 0.000357 Loss 3.484933, Accuracy 93.244%\n",
      "Epoch 39, Batch 829, LR 0.000354 Loss 3.486277, Accuracy 93.238%\n",
      "Epoch 39, Batch 830, LR 0.000350 Loss 3.485886, Accuracy 93.238%\n",
      "Epoch 39, Batch 831, LR 0.000347 Loss 3.485282, Accuracy 93.240%\n",
      "Epoch 39, Batch 832, LR 0.000344 Loss 3.484559, Accuracy 93.240%\n",
      "Epoch 39, Batch 833, LR 0.000341 Loss 3.483829, Accuracy 93.242%\n",
      "Epoch 39, Batch 834, LR 0.000338 Loss 3.483326, Accuracy 93.245%\n",
      "Epoch 39, Batch 835, LR 0.000335 Loss 3.484345, Accuracy 93.239%\n",
      "Epoch 39, Batch 836, LR 0.000332 Loss 3.485310, Accuracy 93.234%\n",
      "Epoch 39, Batch 837, LR 0.000329 Loss 3.485491, Accuracy 93.235%\n",
      "Epoch 39, Batch 838, LR 0.000326 Loss 3.485858, Accuracy 93.233%\n",
      "Epoch 39, Batch 839, LR 0.000323 Loss 3.485866, Accuracy 93.234%\n",
      "Epoch 39, Batch 840, LR 0.000320 Loss 3.485450, Accuracy 93.237%\n",
      "Epoch 39, Batch 841, LR 0.000317 Loss 3.486006, Accuracy 93.237%\n",
      "Epoch 39, Batch 842, LR 0.000314 Loss 3.484697, Accuracy 93.242%\n",
      "Epoch 39, Batch 843, LR 0.000311 Loss 3.484406, Accuracy 93.243%\n",
      "Epoch 39, Batch 844, LR 0.000308 Loss 3.484481, Accuracy 93.242%\n",
      "Epoch 39, Batch 845, LR 0.000305 Loss 3.484648, Accuracy 93.241%\n",
      "Epoch 39, Batch 846, LR 0.000302 Loss 3.485365, Accuracy 93.239%\n",
      "Epoch 39, Batch 847, LR 0.000300 Loss 3.485429, Accuracy 93.240%\n",
      "Epoch 39, Batch 848, LR 0.000297 Loss 3.485062, Accuracy 93.241%\n",
      "Epoch 39, Batch 849, LR 0.000294 Loss 3.484835, Accuracy 93.241%\n",
      "Epoch 39, Batch 850, LR 0.000291 Loss 3.485262, Accuracy 93.237%\n",
      "Epoch 39, Batch 851, LR 0.000288 Loss 3.485555, Accuracy 93.238%\n",
      "Epoch 39, Batch 852, LR 0.000285 Loss 3.484758, Accuracy 93.241%\n",
      "Epoch 39, Batch 853, LR 0.000283 Loss 3.485346, Accuracy 93.236%\n",
      "Epoch 39, Batch 854, LR 0.000280 Loss 3.485438, Accuracy 93.235%\n",
      "Epoch 39, Batch 855, LR 0.000277 Loss 3.486105, Accuracy 93.235%\n",
      "Epoch 39, Batch 856, LR 0.000274 Loss 3.486128, Accuracy 93.236%\n",
      "Epoch 39, Batch 857, LR 0.000272 Loss 3.486108, Accuracy 93.233%\n",
      "Epoch 39, Batch 858, LR 0.000269 Loss 3.486059, Accuracy 93.236%\n",
      "Epoch 39, Batch 859, LR 0.000266 Loss 3.486655, Accuracy 93.236%\n",
      "Epoch 39, Batch 860, LR 0.000263 Loss 3.486908, Accuracy 93.234%\n",
      "Epoch 39, Batch 861, LR 0.000261 Loss 3.486137, Accuracy 93.237%\n",
      "Epoch 39, Batch 862, LR 0.000258 Loss 3.486295, Accuracy 93.238%\n",
      "Epoch 39, Batch 863, LR 0.000255 Loss 3.486179, Accuracy 93.239%\n",
      "Epoch 39, Batch 864, LR 0.000253 Loss 3.485375, Accuracy 93.243%\n",
      "Epoch 39, Batch 865, LR 0.000250 Loss 3.485396, Accuracy 93.245%\n",
      "Epoch 39, Batch 866, LR 0.000248 Loss 3.485575, Accuracy 93.244%\n",
      "Epoch 39, Batch 867, LR 0.000245 Loss 3.485930, Accuracy 93.240%\n",
      "Epoch 39, Batch 868, LR 0.000242 Loss 3.485517, Accuracy 93.241%\n",
      "Epoch 39, Batch 869, LR 0.000240 Loss 3.484942, Accuracy 93.242%\n",
      "Epoch 39, Batch 870, LR 0.000237 Loss 3.484800, Accuracy 93.244%\n",
      "Epoch 39, Batch 871, LR 0.000235 Loss 3.484690, Accuracy 93.241%\n",
      "Epoch 39, Batch 872, LR 0.000232 Loss 3.485097, Accuracy 93.234%\n",
      "Epoch 39, Batch 873, LR 0.000230 Loss 3.485127, Accuracy 93.235%\n",
      "Epoch 39, Batch 874, LR 0.000227 Loss 3.484870, Accuracy 93.235%\n",
      "Epoch 39, Batch 875, LR 0.000225 Loss 3.484698, Accuracy 93.236%\n",
      "Epoch 39, Batch 876, LR 0.000222 Loss 3.484581, Accuracy 93.236%\n",
      "Epoch 39, Batch 877, LR 0.000220 Loss 3.485087, Accuracy 93.235%\n",
      "Epoch 39, Batch 878, LR 0.000217 Loss 3.485204, Accuracy 93.236%\n",
      "Epoch 39, Batch 879, LR 0.000215 Loss 3.485678, Accuracy 93.235%\n",
      "Epoch 39, Batch 880, LR 0.000213 Loss 3.485387, Accuracy 93.238%\n",
      "Epoch 39, Batch 881, LR 0.000210 Loss 3.485078, Accuracy 93.237%\n",
      "Epoch 39, Batch 882, LR 0.000208 Loss 3.484569, Accuracy 93.240%\n",
      "Epoch 39, Batch 883, LR 0.000206 Loss 3.484340, Accuracy 93.242%\n",
      "Epoch 39, Batch 884, LR 0.000203 Loss 3.484391, Accuracy 93.239%\n",
      "Epoch 39, Batch 885, LR 0.000201 Loss 3.484554, Accuracy 93.241%\n",
      "Epoch 39, Batch 886, LR 0.000199 Loss 3.484636, Accuracy 93.242%\n",
      "Epoch 39, Batch 887, LR 0.000196 Loss 3.484234, Accuracy 93.243%\n",
      "Epoch 39, Batch 888, LR 0.000194 Loss 3.484810, Accuracy 93.241%\n",
      "Epoch 39, Batch 889, LR 0.000192 Loss 3.485480, Accuracy 93.236%\n",
      "Epoch 39, Batch 890, LR 0.000189 Loss 3.485253, Accuracy 93.241%\n",
      "Epoch 39, Batch 891, LR 0.000187 Loss 3.484542, Accuracy 93.242%\n",
      "Epoch 39, Batch 892, LR 0.000185 Loss 3.484300, Accuracy 93.246%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 893, LR 0.000183 Loss 3.484948, Accuracy 93.243%\n",
      "Epoch 39, Batch 894, LR 0.000181 Loss 3.485626, Accuracy 93.241%\n",
      "Epoch 39, Batch 895, LR 0.000178 Loss 3.485830, Accuracy 93.236%\n",
      "Epoch 39, Batch 896, LR 0.000176 Loss 3.485523, Accuracy 93.237%\n",
      "Epoch 39, Batch 897, LR 0.000174 Loss 3.485455, Accuracy 93.238%\n",
      "Epoch 39, Batch 898, LR 0.000172 Loss 3.485839, Accuracy 93.234%\n",
      "Epoch 39, Batch 899, LR 0.000170 Loss 3.486088, Accuracy 93.233%\n",
      "Epoch 39, Batch 900, LR 0.000168 Loss 3.486542, Accuracy 93.232%\n",
      "Epoch 39, Batch 901, LR 0.000165 Loss 3.486507, Accuracy 93.230%\n",
      "Epoch 39, Batch 902, LR 0.000163 Loss 3.485734, Accuracy 93.232%\n",
      "Epoch 39, Batch 903, LR 0.000161 Loss 3.485688, Accuracy 93.231%\n",
      "Epoch 39, Batch 904, LR 0.000159 Loss 3.486073, Accuracy 93.230%\n",
      "Epoch 39, Batch 905, LR 0.000157 Loss 3.487370, Accuracy 93.217%\n",
      "Epoch 39, Batch 906, LR 0.000155 Loss 3.486277, Accuracy 93.219%\n",
      "Epoch 39, Batch 907, LR 0.000153 Loss 3.486251, Accuracy 93.222%\n",
      "Epoch 39, Batch 908, LR 0.000151 Loss 3.485746, Accuracy 93.225%\n",
      "Epoch 39, Batch 909, LR 0.000149 Loss 3.485524, Accuracy 93.226%\n",
      "Epoch 39, Batch 910, LR 0.000147 Loss 3.485377, Accuracy 93.223%\n",
      "Epoch 39, Batch 911, LR 0.000145 Loss 3.485381, Accuracy 93.221%\n",
      "Epoch 39, Batch 912, LR 0.000143 Loss 3.484435, Accuracy 93.226%\n",
      "Epoch 39, Batch 913, LR 0.000141 Loss 3.484456, Accuracy 93.226%\n",
      "Epoch 39, Batch 914, LR 0.000139 Loss 3.485210, Accuracy 93.223%\n",
      "Epoch 39, Batch 915, LR 0.000138 Loss 3.485302, Accuracy 93.224%\n",
      "Epoch 39, Batch 916, LR 0.000136 Loss 3.485341, Accuracy 93.220%\n",
      "Epoch 39, Batch 917, LR 0.000134 Loss 3.485085, Accuracy 93.223%\n",
      "Epoch 39, Batch 918, LR 0.000132 Loss 3.485702, Accuracy 93.220%\n",
      "Epoch 39, Batch 919, LR 0.000130 Loss 3.485760, Accuracy 93.215%\n",
      "Epoch 39, Batch 920, LR 0.000128 Loss 3.485674, Accuracy 93.218%\n",
      "Epoch 39, Batch 921, LR 0.000126 Loss 3.485302, Accuracy 93.220%\n",
      "Epoch 39, Batch 922, LR 0.000125 Loss 3.485329, Accuracy 93.220%\n",
      "Epoch 39, Batch 923, LR 0.000123 Loss 3.484659, Accuracy 93.223%\n",
      "Epoch 39, Batch 924, LR 0.000121 Loss 3.484954, Accuracy 93.217%\n",
      "Epoch 39, Batch 925, LR 0.000119 Loss 3.485412, Accuracy 93.214%\n",
      "Epoch 39, Batch 926, LR 0.000118 Loss 3.485003, Accuracy 93.216%\n",
      "Epoch 39, Batch 927, LR 0.000116 Loss 3.485262, Accuracy 93.215%\n",
      "Epoch 39, Batch 928, LR 0.000114 Loss 3.484773, Accuracy 93.216%\n",
      "Epoch 39, Batch 929, LR 0.000112 Loss 3.484339, Accuracy 93.220%\n",
      "Epoch 39, Batch 930, LR 0.000111 Loss 3.484437, Accuracy 93.221%\n",
      "Epoch 39, Batch 931, LR 0.000109 Loss 3.484038, Accuracy 93.226%\n",
      "Epoch 39, Batch 932, LR 0.000107 Loss 3.484339, Accuracy 93.224%\n",
      "Epoch 39, Batch 933, LR 0.000106 Loss 3.484580, Accuracy 93.222%\n",
      "Epoch 39, Batch 934, LR 0.000104 Loss 3.484328, Accuracy 93.225%\n",
      "Epoch 39, Batch 935, LR 0.000103 Loss 3.484005, Accuracy 93.227%\n",
      "Epoch 39, Batch 936, LR 0.000101 Loss 3.484064, Accuracy 93.224%\n",
      "Epoch 39, Batch 937, LR 0.000099 Loss 3.484519, Accuracy 93.221%\n",
      "Epoch 39, Batch 938, LR 0.000098 Loss 3.484048, Accuracy 93.225%\n",
      "Epoch 39, Batch 939, LR 0.000096 Loss 3.484068, Accuracy 93.227%\n",
      "Epoch 39, Batch 940, LR 0.000095 Loss 3.483272, Accuracy 93.230%\n",
      "Epoch 39, Batch 941, LR 0.000093 Loss 3.483146, Accuracy 93.232%\n",
      "Epoch 39, Batch 942, LR 0.000092 Loss 3.482971, Accuracy 93.232%\n",
      "Epoch 39, Batch 943, LR 0.000090 Loss 3.482727, Accuracy 93.236%\n",
      "Epoch 39, Batch 944, LR 0.000089 Loss 3.482328, Accuracy 93.241%\n",
      "Epoch 39, Batch 945, LR 0.000087 Loss 3.482137, Accuracy 93.243%\n",
      "Epoch 39, Batch 946, LR 0.000086 Loss 3.482340, Accuracy 93.244%\n",
      "Epoch 39, Batch 947, LR 0.000084 Loss 3.482658, Accuracy 93.240%\n",
      "Epoch 39, Batch 948, LR 0.000083 Loss 3.482268, Accuracy 93.242%\n",
      "Epoch 39, Batch 949, LR 0.000081 Loss 3.481827, Accuracy 93.247%\n",
      "Epoch 39, Batch 950, LR 0.000080 Loss 3.481828, Accuracy 93.251%\n",
      "Epoch 39, Batch 951, LR 0.000079 Loss 3.481515, Accuracy 93.254%\n",
      "Epoch 39, Batch 952, LR 0.000077 Loss 3.481606, Accuracy 93.253%\n",
      "Epoch 39, Batch 953, LR 0.000076 Loss 3.481911, Accuracy 93.252%\n",
      "Epoch 39, Batch 954, LR 0.000075 Loss 3.481466, Accuracy 93.255%\n",
      "Epoch 39, Batch 955, LR 0.000073 Loss 3.481427, Accuracy 93.255%\n",
      "Epoch 39, Batch 956, LR 0.000072 Loss 3.481067, Accuracy 93.256%\n",
      "Epoch 39, Batch 957, LR 0.000071 Loss 3.480497, Accuracy 93.261%\n",
      "Epoch 39, Batch 958, LR 0.000069 Loss 3.481177, Accuracy 93.257%\n",
      "Epoch 39, Batch 959, LR 0.000068 Loss 3.480745, Accuracy 93.257%\n",
      "Epoch 39, Batch 960, LR 0.000067 Loss 3.480769, Accuracy 93.257%\n",
      "Epoch 39, Batch 961, LR 0.000066 Loss 3.480637, Accuracy 93.257%\n",
      "Epoch 39, Batch 962, LR 0.000064 Loss 3.480729, Accuracy 93.256%\n",
      "Epoch 39, Batch 963, LR 0.000063 Loss 3.480620, Accuracy 93.255%\n",
      "Epoch 39, Batch 964, LR 0.000062 Loss 3.480325, Accuracy 93.255%\n",
      "Epoch 39, Batch 965, LR 0.000061 Loss 3.480577, Accuracy 93.255%\n",
      "Epoch 39, Batch 966, LR 0.000060 Loss 3.480563, Accuracy 93.253%\n",
      "Epoch 39, Batch 967, LR 0.000058 Loss 3.481133, Accuracy 93.247%\n",
      "Epoch 39, Batch 968, LR 0.000057 Loss 3.481321, Accuracy 93.248%\n",
      "Epoch 39, Batch 969, LR 0.000056 Loss 3.481970, Accuracy 93.245%\n",
      "Epoch 39, Batch 970, LR 0.000055 Loss 3.481706, Accuracy 93.246%\n",
      "Epoch 39, Batch 971, LR 0.000054 Loss 3.482170, Accuracy 93.247%\n",
      "Epoch 39, Batch 972, LR 0.000053 Loss 3.481893, Accuracy 93.249%\n",
      "Epoch 39, Batch 973, LR 0.000052 Loss 3.482365, Accuracy 93.249%\n",
      "Epoch 39, Batch 974, LR 0.000051 Loss 3.482135, Accuracy 93.249%\n",
      "Epoch 39, Batch 975, LR 0.000050 Loss 3.482629, Accuracy 93.248%\n",
      "Epoch 39, Batch 976, LR 0.000049 Loss 3.482797, Accuracy 93.249%\n",
      "Epoch 39, Batch 977, LR 0.000048 Loss 3.482695, Accuracy 93.247%\n",
      "Epoch 39, Batch 978, LR 0.000047 Loss 3.482133, Accuracy 93.252%\n",
      "Epoch 39, Batch 979, LR 0.000046 Loss 3.481622, Accuracy 93.253%\n",
      "Epoch 39, Batch 980, LR 0.000045 Loss 3.481681, Accuracy 93.253%\n",
      "Epoch 39, Batch 981, LR 0.000044 Loss 3.481446, Accuracy 93.253%\n",
      "Epoch 39, Batch 982, LR 0.000043 Loss 3.480822, Accuracy 93.257%\n",
      "Epoch 39, Batch 983, LR 0.000042 Loss 3.481255, Accuracy 93.253%\n",
      "Epoch 39, Batch 984, LR 0.000041 Loss 3.481433, Accuracy 93.252%\n",
      "Epoch 39, Batch 985, LR 0.000040 Loss 3.481012, Accuracy 93.252%\n",
      "Epoch 39, Batch 986, LR 0.000039 Loss 3.480839, Accuracy 93.254%\n",
      "Epoch 39, Batch 987, LR 0.000038 Loss 3.480694, Accuracy 93.255%\n",
      "Epoch 39, Batch 988, LR 0.000037 Loss 3.480210, Accuracy 93.258%\n",
      "Epoch 39, Batch 989, LR 0.000037 Loss 3.479045, Accuracy 93.263%\n",
      "Epoch 39, Batch 990, LR 0.000036 Loss 3.479306, Accuracy 93.262%\n",
      "Epoch 39, Batch 991, LR 0.000035 Loss 3.479115, Accuracy 93.261%\n",
      "Epoch 39, Batch 992, LR 0.000034 Loss 3.479098, Accuracy 93.259%\n",
      "Epoch 39, Batch 993, LR 0.000033 Loss 3.478609, Accuracy 93.260%\n",
      "Epoch 39, Batch 994, LR 0.000033 Loss 3.479010, Accuracy 93.260%\n",
      "Epoch 39, Batch 995, LR 0.000032 Loss 3.479006, Accuracy 93.261%\n",
      "Epoch 39, Batch 996, LR 0.000031 Loss 3.478369, Accuracy 93.263%\n",
      "Epoch 39, Batch 997, LR 0.000030 Loss 3.478283, Accuracy 93.267%\n",
      "Epoch 39, Batch 998, LR 0.000030 Loss 3.478204, Accuracy 93.267%\n",
      "Epoch 39, Batch 999, LR 0.000029 Loss 3.478392, Accuracy 93.267%\n",
      "Epoch 39, Batch 1000, LR 0.000028 Loss 3.478794, Accuracy 93.267%\n",
      "Epoch 39, Batch 1001, LR 0.000028 Loss 3.479225, Accuracy 93.270%\n",
      "Epoch 39, Batch 1002, LR 0.000027 Loss 3.479592, Accuracy 93.269%\n",
      "Epoch 39, Batch 1003, LR 0.000026 Loss 3.479312, Accuracy 93.270%\n",
      "Epoch 39, Batch 1004, LR 0.000026 Loss 3.479120, Accuracy 93.272%\n",
      "Epoch 39, Batch 1005, LR 0.000025 Loss 3.479607, Accuracy 93.270%\n",
      "Epoch 39, Batch 1006, LR 0.000025 Loss 3.479619, Accuracy 93.269%\n",
      "Epoch 39, Batch 1007, LR 0.000024 Loss 3.479709, Accuracy 93.270%\n",
      "Epoch 39, Batch 1008, LR 0.000023 Loss 3.479440, Accuracy 93.270%\n",
      "Epoch 39, Batch 1009, LR 0.000023 Loss 3.478506, Accuracy 93.275%\n",
      "Epoch 39, Batch 1010, LR 0.000022 Loss 3.478313, Accuracy 93.277%\n",
      "Epoch 39, Batch 1011, LR 0.000022 Loss 3.478293, Accuracy 93.276%\n",
      "Epoch 39, Batch 1012, LR 0.000021 Loss 3.478361, Accuracy 93.274%\n",
      "Epoch 39, Batch 1013, LR 0.000021 Loss 3.478364, Accuracy 93.276%\n",
      "Epoch 39, Batch 1014, LR 0.000020 Loss 3.478441, Accuracy 93.275%\n",
      "Epoch 39, Batch 1015, LR 0.000020 Loss 3.478541, Accuracy 93.277%\n",
      "Epoch 39, Batch 1016, LR 0.000019 Loss 3.478471, Accuracy 93.279%\n",
      "Epoch 39, Batch 1017, LR 0.000019 Loss 3.478483, Accuracy 93.281%\n",
      "Epoch 39, Batch 1018, LR 0.000019 Loss 3.478624, Accuracy 93.279%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 1019, LR 0.000018 Loss 3.478254, Accuracy 93.278%\n",
      "Epoch 39, Batch 1020, LR 0.000018 Loss 3.478123, Accuracy 93.280%\n",
      "Epoch 39, Batch 1021, LR 0.000017 Loss 3.478062, Accuracy 93.278%\n",
      "Epoch 39, Batch 1022, LR 0.000017 Loss 3.477604, Accuracy 93.278%\n",
      "Epoch 39, Batch 1023, LR 0.000017 Loss 3.477715, Accuracy 93.279%\n",
      "Epoch 39, Batch 1024, LR 0.000016 Loss 3.478340, Accuracy 93.276%\n",
      "Epoch 39, Batch 1025, LR 0.000016 Loss 3.478309, Accuracy 93.277%\n",
      "Epoch 39, Batch 1026, LR 0.000016 Loss 3.478136, Accuracy 93.278%\n",
      "Epoch 39, Batch 1027, LR 0.000015 Loss 3.478422, Accuracy 93.279%\n",
      "Epoch 39, Batch 1028, LR 0.000015 Loss 3.478411, Accuracy 93.276%\n",
      "Epoch 39, Batch 1029, LR 0.000015 Loss 3.478405, Accuracy 93.275%\n",
      "Epoch 39, Batch 1030, LR 0.000015 Loss 3.478196, Accuracy 93.277%\n",
      "Epoch 39, Batch 1031, LR 0.000014 Loss 3.477877, Accuracy 93.277%\n",
      "Epoch 39, Batch 1032, LR 0.000014 Loss 3.477039, Accuracy 93.281%\n",
      "Epoch 39, Batch 1033, LR 0.000014 Loss 3.476900, Accuracy 93.283%\n",
      "Epoch 39, Batch 1034, LR 0.000014 Loss 3.477554, Accuracy 93.281%\n",
      "Epoch 39, Batch 1035, LR 0.000014 Loss 3.477451, Accuracy 93.280%\n",
      "Epoch 39, Batch 1036, LR 0.000013 Loss 3.477606, Accuracy 93.280%\n",
      "Epoch 39, Batch 1037, LR 0.000013 Loss 3.477706, Accuracy 93.282%\n",
      "Epoch 39, Batch 1038, LR 0.000013 Loss 3.477538, Accuracy 93.285%\n",
      "Epoch 39, Batch 1039, LR 0.000013 Loss 3.477458, Accuracy 93.287%\n",
      "Epoch 39, Batch 1040, LR 0.000013 Loss 3.477565, Accuracy 93.287%\n",
      "Epoch 39, Batch 1041, LR 0.000013 Loss 3.477322, Accuracy 93.285%\n",
      "Epoch 39, Batch 1042, LR 0.000013 Loss 3.477119, Accuracy 93.285%\n",
      "Epoch 39, Batch 1043, LR 0.000013 Loss 3.476618, Accuracy 93.286%\n",
      "Epoch 39, Batch 1044, LR 0.000013 Loss 3.476219, Accuracy 93.288%\n",
      "Epoch 39, Batch 1045, LR 0.000013 Loss 3.476481, Accuracy 93.286%\n",
      "Epoch 39, Batch 1046, LR 0.000013 Loss 3.476706, Accuracy 93.283%\n",
      "Epoch 39, Batch 1047, LR 0.000013 Loss 3.476909, Accuracy 93.284%\n",
      "Epoch 39, Loss (train set) 3.476909, Accuracy (train set) 93.284%\n",
      "Epoch 39, Accuracy (validation set) 91.406%\n"
     ]
    }
   ],
   "source": [
    "path_to_save = './runs/ResNet_with_augmentation'\n",
    "writer = SummaryWriter(path_to_save)\n",
    "writer.add_graph(main_model, next(iter(train_loader))[0].cuda())\n",
    "\n",
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "   \n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "\n",
    "    writer.add_scalar(\"Top1/train\", train_top1, num_epoch)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, num_epoch)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        writer.add_scalar(\"Top1/test\", val_top1, num_epoch/val_interval)\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models_aug')\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f235a",
   "metadata": {},
   "source": [
    "**4. Тестирование блока построения дикторских моделей**\n",
    "\n",
    "Из литературы известно, что применение алгоритмов машинного обучения на практике требует использования трёх наборов данных: *тренировочное множество* (используется для обучения параметров модели), *валидационное множество* (используется для настройки гиперпараметров), *тестовое множество* (используется для итогового тестирования).\n",
    "\n",
    "В рамках настоящего пункта предлагается выполнить итоговое тестирования блоков генерации дикторских моделей, обученных без аугментации и с аугментацией тренировочных данных, и сравнить полученные результаты. При проведении процедуры тестирования рекомендуется выбрать различное количество фреймов для тестовых звукозаписей, чтобы грубо понять то, как длительность фонограммы влияет на качество распознавания диктора.\n",
    "\n",
    "В качестве целевой метрики предлагается использовать *долю правильных ответов*, то есть количество верно классифицированных объектов по отношению к общему количеству объектов тестового множества. Как и при проведении процедуры обучения и валидации, рассматриваемая процедура тестирования предполагает решение задачи идентификации диктора на закрытом множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7080f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test dataloader\n",
    "test_dataset = test_dataset_loader(test_list=test_list, max_frames=max_frames_test, test_path=test_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, num_workers=num_workers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "316a9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Accuracy (test set) 95.368%\n"
     ]
    }
   ],
   "source": [
    "# Load model without augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "752082b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Accuracy (test set) 96.025%\n"
     ]
    }
   ],
   "source": [
    "# Load model with augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23048922-86df-4f45-8253-b1b8697f430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "image/jpeg": {
       "height": 250,
       "width": 250
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='1.jpg', width=250, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a914cade-521f-4b1b-b748-affa11098a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "image/jpeg": {
       "height": 250,
       "width": 250
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='2.jpg', width=250, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d5dacde-ce8f-458d-ac6c-976c888aef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "image/jpeg": {
       "height": 250,
       "width": 250
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='3.jpg', width=250, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4189778",
   "metadata": {},
   "source": [
    "**5. Контрольные вопросы**\n",
    "\n",
    "1. Что такое верификация и идентицикация диктора?\n",
    "\n",
    "2. Что такое распознавание диктора на закрытом и открытом множестве?\n",
    "\n",
    "3. Что такое текстозависимое и текстонезависимое распознавание диктора?\n",
    "\n",
    "4. Описать схему обучения блока генерации дикторских моделей на основе нейронных сетей.\n",
    "\n",
    "5. Описать основные компоненты, из которых состоит нейросетевой блок генерации дикторских моделей (фреймовый уровень, слой статистического пулинга, сегментный уровень, выходной слой).\n",
    "\n",
    "6. Как устроены нейросетевые архитектуры на основе ResNet-блоков?\n",
    "\n",
    "7. Что такое полносвязная нейронная сеть прямого распространения?\n",
    "\n",
    "8. Как устроена стоимостная функция для обучения нейросетевого блока генерации дикторских моделей?\n",
    "\n",
    "9. Что такое аугментация данных?\n",
    "\n",
    "10. Что такое дикторский эмбеддинг и на каком уровне блока построения дикторских моделей он генерируется?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a44bb-878c-4002-a22a-815c4ceac4b8",
   "metadata": {},
   "source": [
    "**6. Список литературы**\n",
    "\n",
    "1. Bai Z., Zhang X.-L., Chen J. Speaker recognition based on deep learning: an overview // \tarXiv:2012.00931 [eess.AS] ([ссылка](https://arxiv.org/pdf/2012.00931.pdf)).\n",
    "\n",
    "2. Hansen J.H.L., Hasan T. Speaker recognition by machines and humans: a tutorial review // IEEE Signal Processing Magazine, 2015. V. 32. № 6. P. 74–99 ([ссылка](https://www.researchgate.net/publication/282940395_Speaker_Recognition_by_Machines_and_Humans_A_tutorial_review))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
