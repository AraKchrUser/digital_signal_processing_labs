{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a45842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:49:45.770353Z",
     "start_time": "2023-05-29T08:49:43.232362Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch.utils.data as data_utils\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f64a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:49:45.818491Z",
     "start_time": "2023-05-29T08:49:45.773366Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev)\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605da58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:49:45.833674Z",
     "start_time": "2023-05-29T08:49:45.820499Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_file(path, arr):\n",
    "    with open(path, 'wb') as f:\n",
    "        np.save(f, arr)\n",
    "    return\n",
    "\n",
    "def load_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        arr = np.load(f)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa3ec3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf2e91e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:45:38.662573Z",
     "start_time": "2023-05-29T08:45:38.656578Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    from string import punctuation\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    return [w.lower() for w in tokens if w not in punctuation]\n",
    "\n",
    "def morph_analyze(w, morph):\n",
    "    return morph.parse(w)[0].normal_form\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    freqdist = nltk.FreqDist()\n",
    "    for s in data:\n",
    "        for w in s:\n",
    "            freqdist[w] += 1\n",
    "    cwords = freqdist.most_common(10_000)\n",
    "    for i, w in enumerate(cwords):\n",
    "        vocab[w[0]] = i + 1\n",
    "    return vocab\n",
    "\n",
    "def w2i(data, vocab):\n",
    "    token_is = []\n",
    "    for s in data:\n",
    "        temp = []\n",
    "        for w in s:\n",
    "            if w in vocab.keys():\n",
    "                temp.append(vocab[w])\n",
    "        token_is.append(temp)\n",
    "    return token_is\n",
    "\n",
    "def pad(tokens, max_len):\n",
    "    pad_i = 0\n",
    "    x_pad = []\n",
    "    for s in tokens:\n",
    "        if len(s) < max_len:\n",
    "            while len(s) < max_len:\n",
    "                s.insert(len(s), pad_i)\n",
    "            x_pad.append(s)\n",
    "        else:\n",
    "            x_pad.append(s[:max_len])\n",
    "    return x_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "460ea239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:45:44.210578Z",
     "start_time": "2023-05-29T08:45:38.924940Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel('../2/X_y_train.xlsx')\n",
    "test = pd.read_excel('../2/X_y_test.xlsx')\n",
    "X_train, y_train, X_test, y_test = (train.drop(['Class'], axis=1).Text, train.Class, \n",
    "                                    test.drop(['Class'], axis=1).Text, test.Class)\n",
    "y_train = y_train.apply(lambda x: x if x == 1 else 0).values\n",
    "y_test = y_test.apply(lambda x: x if x == 1 else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1dbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.319603Z",
     "start_time": "2023-05-29T08:45:44.213587Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokens = [tokenize(s) for s in X_train.values]\n",
    "X_test_tokens = [tokenize(s) for s in X_test.values]\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "X_train_tokens = [[morph_analyze(w, morph) for w in s] for s in X_train_tokens]\n",
    "X_test_tokens = [[morph_analyze(w, morph) for w in s] for s in X_test_tokens]\n",
    "vocab_ = build_vocab(X_train_tokens + X_test_tokens)\n",
    "X_train_wi = w2i(X_train_tokens, vocab_)\n",
    "X_test_wi = w2i(X_test_tokens, vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a7774f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.351398Z",
     "start_time": "2023-05-29T08:46:28.320603Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "X_train_wi = pad(X_train_wi, max_len=max_len)\n",
    "X_test_wi = pad(X_test_wi, max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e156ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T20:49:07.454033Z",
     "start_time": "2023-04-16T20:49:07.440032Z"
    }
   },
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1549baf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.367406Z",
     "start_time": "2023-05-29T08:46:28.353399Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size=len(vocab_), \n",
    "                 embed_dim=100, \n",
    "                 conv_layer_count=2, \n",
    "                 stride=1,\n",
    "                 kernel_size=3,\n",
    "                 seq_len=max_len, \n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1,\n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "        self.conv1d_layers = nn.ModuleList([])\n",
    "        out_shape = self.seq_len\n",
    "        padding = kernel_size // 2\n",
    "        for i in range(conv_layer_count):\n",
    "            self.conv1d_layers.append(nn.Conv1d(in_channels=embed_dim, \n",
    "                                                out_channels=embed_dim,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                stride=stride, padding=padding))\n",
    "            out_shape = 1 + (out_shape + 2 * padding - kernel_size) // stride\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(out_shape * embed_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.reshape(len(x), self.embed_dim, self.seq_len)\n",
    "        for conv_layer in self.conv1d_layers:\n",
    "            x = F.relu(conv_layer(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        logits = self.fc(self.dropout(x))\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884a69ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.398423Z",
     "start_time": "2023-05-29T08:46:28.369408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7398, -0.6486],\n",
       "        [-0.6804, -0.7060],\n",
       "        [-0.7317, -0.6560],\n",
       "        [-0.7459, -0.6431],\n",
       "        [-0.7014, -0.6849]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net(torch.tensor(X_train_wi[:5], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b71695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T14:52:37.440131Z",
     "start_time": "2023-05-06T14:52:36.979180Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb3eaa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.414417Z",
     "start_time": "2023-05-29T08:46:28.401415Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(net, num_epoch, trainset, optimizer, lr, scheduler, log=False):\n",
    "#     loss_f = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for data in trainset:\n",
    "            X, y = data\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = F.cross_entropy(output, y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if scheduler is not None: scheduler.step()\n",
    "        \n",
    "        if log:  print('loss ====> ', loss.item())\n",
    "    return net\n",
    "\n",
    "def predict(net, testset):\n",
    "    ans = []\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                ans.append(i.cpu().data.numpy().argmax().item())\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21983f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:46:28.430421Z",
     "start_time": "2023-05-29T08:46:28.416416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"lr\": [3e-4],\n",
    "    \"epochs\": [5, 10],\n",
    "    \"optimizer\": [optim.SGD, optim.AdamW],\n",
    "    \"batch_size\": [batch_size],\n",
    "    \"layers_count\": [3, 5],\n",
    "    \"kernel_size\": [3, 5],\n",
    "    \"stride\": [1, 3]\n",
    "}\n",
    "params_list = ParameterGrid(param_grid)\n",
    "len(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f945c9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:49:09.104908Z",
     "start_time": "2023-05-29T08:46:32.390290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:36<00:00,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "configs = []\n",
    "\n",
    "inputs_train = torch.tensor(X_train_wi, dtype=torch.int32).to(device)\n",
    "targets_train = torch.tensor(y_train, dtype=torch.int32).to(device)\n",
    "\n",
    "inputs_test = torch.tensor(X_test_wi, dtype=torch.int32).to(device)\n",
    "targets_test = torch.tensor(y_test, dtype=torch.int32).to(device)\n",
    "\n",
    "train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# grid search\n",
    "for params in tqdm(params_list):\n",
    "    \n",
    "    # get param for pass to network\n",
    "    lr = params['lr']\n",
    "    epochs = params['epochs']\n",
    "    optimizer = params['optimizer']\n",
    "    batch_size = params['batch_size']\n",
    "    layers_count = params['layers_count']\n",
    "    kernel_size = params['kernel_size']\n",
    "    stride = params['stride']\n",
    "    \n",
    "    # net build\n",
    "    net = Net(vocab_size=len(vocab_), \n",
    "              embed_dim=100, \n",
    "              conv_layer_count=layers_count, \n",
    "              stride=stride, \n",
    "              kernel_size=kernel_size, \n",
    "              seq_len=max_len)\n",
    "    net.to(device)\n",
    "\n",
    "    # fit\n",
    "    net = fit(net, epochs, trainset, optimizer, lr, None, False)\n",
    "    \n",
    "    # predict\n",
    "    ans = predict(net, testset)\n",
    "    \n",
    "    # add param in config\n",
    "    config = [epochs, optimizer.__name__, batch_size, layers_count, \n",
    "              kernel_size, stride, f'{f1_score(y_test, ans, average=\"weighted\"):.5f}']\n",
    "    configs.append(config)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8233ddc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:49:09.136909Z",
     "start_time": "2023-05-29T08:49:09.107906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layers_count</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>stride</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.81964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.74031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.70036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.68000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.63123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.37870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs optimizer  batch_size  layers_count  kernel_size  stride f1_score\n",
       "0        5     AdamW         512             5            5       3  0.81964\n",
       "1        5     AdamW         512             5            5       1  0.79904\n",
       "2       10     AdamW         512             5            5       3  0.77956\n",
       "3        5     AdamW         512             3            3       3  0.77591\n",
       "4        5     AdamW         512             5            3       3  0.74031\n",
       "5       10     AdamW         512             3            5       1  0.72045\n",
       "6        5     AdamW         512             5            3       1  0.72000\n",
       "7        5     AdamW         512             3            3       1  0.70036\n",
       "8       10     AdamW         512             5            3       3  0.70036\n",
       "9       10     AdamW         512             5            5       1  0.68000\n",
       "10      10     AdamW         512             3            5       3  0.68000\n",
       "11       5     AdamW         512             3            5       1  0.68000\n",
       "12      10     AdamW         512             5            3       1  0.65709\n",
       "13      10     AdamW         512             3            3       1  0.64000\n",
       "14      10     AdamW         512             3            3       3  0.63123\n",
       "15       5     AdamW         512             3            5       3  0.60000\n",
       "16       5       SGD         512             3            5       1  0.51691\n",
       "17       5       SGD         512             3            3       1  0.50904\n",
       "18      10       SGD         512             3            3       1  0.49900\n",
       "19      10       SGD         512             3            5       3  0.45537\n",
       "20      10       SGD         512             3            5       1  0.45155\n",
       "21       5       SGD         512             3            3       3  0.37870\n",
       "22      10       SGD         512             5            3       1  0.37870\n",
       "23       5       SGD         512             5            5       1  0.37870\n",
       "24       5       SGD         512             5            3       3  0.37870\n",
       "25      10       SGD         512             5            5       1  0.37870\n",
       "26      10       SGD         512             5            5       3  0.37870\n",
       "27       5       SGD         512             5            3       1  0.37870\n",
       "28      10       SGD         512             3            3       3  0.28986\n",
       "29      10       SGD         512             5            3       3  0.28986\n",
       "30       5       SGD         512             5            5       3  0.28986\n",
       "31       5       SGD         512             3            5       3  0.28986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(configs)\n",
    "df.columns = ['epochs', 'optimizer', 'batch_size', 'layers_count', 'kernel_size', 'stride', 'f1_score']\n",
    "df.sort_values(by='f1_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815cc33",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce6d9dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:50:30.644670Z",
     "start_time": "2023-05-29T08:50:25.339821Z"
    }
   },
   "outputs": [],
   "source": [
    "path_ = '../2/'\n",
    "train = pd.read_excel(path_ + 'X_y_train.xlsx')\n",
    "test = pd.read_excel(path_ + 'X_y_test.xlsx')\n",
    "X_train, y_train, X_test, y_test = train.drop(['Class'], axis=1), train.Class, test.drop(['Class'], axis=1), test.Class\n",
    "assert y_train.shape == (X_train.shape[0],) and y_test.shape == (X_test.shape[0], )\n",
    "y_train = y_train.apply(lambda x: x if x == 1 else 0)\n",
    "y_test = y_test.apply(lambda x: x if x == 1 else 0)\n",
    "\n",
    "\n",
    "path_ = '../2/saved/'\n",
    "__train_w2v_pretrain = load_file(path_ + '__train_w2v_pretrain.npy')\n",
    "__test_w2v_pretrain = load_file(path_ + '__test_w2v_pretrain.npy')\n",
    "\n",
    "__train_w2v = load_file(path_ + '__train_w2v.npy')\n",
    "__test_w2v = load_file(path_ + '__test_w2v.npy')\n",
    "\n",
    "__train_fasttext_500 = load_file(path_ + '__train_fasttext_500_10.npy')\n",
    "__test_fasttext_500 = load_file(path_ + '__test_fasttext_500_10.npy')\n",
    "\n",
    "__train_fasttext_pretrain = load_file(path_ + '__train_fasttext_pretrain.npy')\n",
    "__test_fasttext_pretrain = load_file(path_ + '__test_fasttext_pretrain.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efeb0339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:50:30.660672Z",
     "start_time": "2023-05-29T08:50:30.647672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22348, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__train_fasttext_pretrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a105b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:31:43.727834Z",
     "start_time": "2023-05-29T08:31:43.713826Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim=300, \n",
    "                 conv_layer_count=2, \n",
    "                 stride=1,\n",
    "                 kernel_size=3,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv1d_layers = nn.ModuleList([])\n",
    "        out_shape = 1\n",
    "        padding = kernel_size // 2\n",
    "        for i in range(conv_layer_count):\n",
    "            self.conv1d_layers.append(nn.Conv1d(in_channels=embed_dim, \n",
    "                                                out_channels=embed_dim,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                stride=stride, padding=padding))\n",
    "            out_shape = 1 + (out_shape + 2 * padding - kernel_size) // stride\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(out_shape *  embed_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None]\n",
    "        for conv_layer in self.conv1d_layers:\n",
    "            x = F.relu(conv_layer(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        logits = self.fc(self.dropout(x))\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c322043f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:31:43.743863Z",
     "start_time": "2023-05-29T08:31:43.730845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"embeddings\": [(__train_w2v_pretrain, __test_w2v_pretrain), \n",
    "                   (__train_w2v, __test_w2v), \n",
    "                   (__train_fasttext_500, __test_fasttext_500),\n",
    "                   (__train_fasttext_pretrain, __test_fasttext_pretrain)],\n",
    "    \"lr\": [3e-4],\n",
    "    \"epochs\": [5, 10],\n",
    "    \"optimizer\": [optim.SGD, optim.AdamW],\n",
    "    \"batch_size\": [512],\n",
    "    \"layers_count\": [3, 5],\n",
    "    \"kernel_size\": [3, 5],\n",
    "    \"stride\": [1, 3]\n",
    "}\n",
    "params_list = ParameterGrid(param_grid)\n",
    "len(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac7c6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:31:43.759866Z",
     "start_time": "2023-05-29T08:31:43.746848Z"
    }
   },
   "outputs": [],
   "source": [
    "# f1_score(y_test, ans, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b1672c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:39:47.348015Z",
     "start_time": "2023-05-29T08:32:11.630817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [07:35<00:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "configs = []\n",
    "\n",
    "# grid search\n",
    "for params in tqdm(params_list):\n",
    "    \n",
    "    # get param for pass to network\n",
    "    X_train, X_test = params['embeddings']\n",
    "    lr = params['lr']\n",
    "    epochs = params['epochs']\n",
    "    optimizer = params['optimizer']\n",
    "    batch_size = params['batch_size']\n",
    "    layers_count = params['layers_count']\n",
    "    kernel_size = params['kernel_size']\n",
    "    stride = params['stride']\n",
    "    \n",
    "    inputs_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    targets_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    inputs_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    targets_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "    test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "    trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "    testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    # net build\n",
    "    net = Net(embed_dim=300, \n",
    "              conv_layer_count=layers_count, \n",
    "              stride=stride, \n",
    "              kernel_size=kernel_size)\n",
    "    net.to(device)\n",
    "\n",
    "    # fit\n",
    "    net = fit(net, epochs, trainset, optimizer, lr, None, False)\n",
    "    \n",
    "    # predict\n",
    "    ans = predict(net, testset)\n",
    "    \n",
    "    # add param in config\n",
    "    config = [epochs, optimizer.__name__, batch_size, layers_count, \n",
    "              kernel_size, stride, f'{f1_score(y_test, ans, average=\"weighted\"):.5f}']\n",
    "    configs.append(config)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7239f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:39:47.364011Z",
     "start_time": "2023-05-29T08:39:47.350008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layers_count</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>stride</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.85880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.85880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.84026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>10</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs optimizer  batch_size  layers_count  kernel_size  stride f1_score\n",
       "0        10     AdamW         512             3            5       3  0.85880\n",
       "1        10     AdamW         512             3            3       3  0.85880\n",
       "2        10     AdamW         512             3            3       1  0.85880\n",
       "3        10     AdamW         512             5            5       3  0.84026\n",
       "4        10     AdamW         512             5            5       1  0.84000\n",
       "..      ...       ...         ...           ...          ...     ...      ...\n",
       "123       5       SGD         512             5            5       1  0.28986\n",
       "124      10       SGD         512             3            3       1  0.28986\n",
       "125      10       SGD         512             3            3       3  0.28986\n",
       "126      10       SGD         512             5            5       3  0.28986\n",
       "127       5       SGD         512             3            3       1  0.28986\n",
       "\n",
       "[128 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(configs)\n",
    "df.columns = ['epochs', 'optimizer', 'batch_size', 'layers_count', 'kernel_size', 'stride', 'f1_score']\n",
    "df.sort_values(by='f1_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac537db8",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a99523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:50:10.423505Z",
     "start_time": "2023-05-29T08:50:10.406490Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dim=300, \n",
    "                 conv_layer_count=2, \n",
    "                 stride=1,\n",
    "                 kernel_size=3,\n",
    "                 dropout=0, \n",
    "                 init=None, \n",
    "                 reg=None,\n",
    "                 norm=None, \n",
    "                 bs=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv1d_layers = nn.ModuleList([])\n",
    "        out_shape = 1\n",
    "        padding = kernel_size // 2\n",
    "        for i in range(conv_layer_count):\n",
    "            self.conv1d_layers.append(nn.Conv1d(in_channels=embed_dim, \n",
    "                                                out_channels=embed_dim,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                stride=stride, \n",
    "                                                padding=padding))\n",
    "            out_shape = 1 + (out_shape + 2 * padding - kernel_size) // stride\n",
    "        self.fc = nn.Linear(out_shape *  embed_dim, 2)\n",
    "        self.dropout = nn.Dropout1d(p=0.3) if reg == 'dropout' else nn.Dropout1d(p=.0)\n",
    "        if norm.__name__ == 'LayerNorm':\n",
    "            self.norm = norm((self.embed_dim, 1), device=device)\n",
    "        else:\n",
    "            self.norm = norm(self.embed_dim, device=device)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv1d):\n",
    "                init(module.weight)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None]\n",
    "        for conv_layer in self.conv1d_layers:\n",
    "            x = F.relu(conv_layer(x))\n",
    "            x = self.dropout(self.norm(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        logits = self.fc(x)\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab06b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:50:10.565052Z",
     "start_time": "2023-05-29T08:50:10.555051Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(net, num_epoch, trainset, optimizer, lr, scheduler, weight_decay, log=False):\n",
    "#     loss_f = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if scheduler.__name__ == 'ExponentialLR':\n",
    "        scheduler = scheduler(optimizer, gamma=0.9)\n",
    "    else:\n",
    "        scheduler = scheduler(optimizer, milestones=list(range(0, epochs)), gamma=0.9)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for data in trainset:\n",
    "            X, y = data\n",
    "            net.zero_grad()\n",
    "            output = net(X)\n",
    "            loss = F.cross_entropy(output, y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if scheduler is not None: scheduler.step()\n",
    "        \n",
    "        if log:  print('loss ====> ', loss.item())\n",
    "    return net\n",
    "\n",
    "def predict(net, testset):\n",
    "    ans = []\n",
    "    with torch.no_grad():\n",
    "        for data in testset:\n",
    "            X, y = data\n",
    "            output = net(X)\n",
    "            for idx, i in enumerate(output):\n",
    "                ans.append(i.cpu().data.numpy().argmax().item())\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a2e85f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:50:34.450036Z",
     "start_time": "2023-05-29T08:50:34.443034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"embeddings\": [(__train_w2v, __test_w2v), \n",
    "                   (__train_fasttext_500, __test_fasttext_500)],\n",
    "    \"lr\": [3e-4],\n",
    "    \"epochs\": [10, 20],\n",
    "    \"optimizer\": [optim.AdamW],\n",
    "    \"batch_size\": [512],\n",
    "    \"layers_count\": [3],\n",
    "    \"kernel_size\": [3],\n",
    "    \"stride\": [1],\n",
    "    \"init\": [torch.nn.init.kaiming_uniform_, torch.nn.init.xavier_uniform_],\n",
    "    \"reg\": ['dropout', 'l2_reg'],\n",
    "    \"norm\": [torch.nn.LayerNorm, torch.nn.BatchNorm1d],\n",
    "    \"scheduler\": [torch.optim.lr_scheduler.ExponentialLR, torch.optim.lr_scheduler.MultiStepLR]\n",
    "}\n",
    "params_list = ParameterGrid(param_grid)\n",
    "len(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38deeecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1c5a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:56:01.490974Z",
     "start_time": "2023-05-29T08:50:34.762421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [05:26<00:00,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "configs = []\n",
    "\n",
    "# grid search\n",
    "for params in tqdm(params_list):\n",
    "    \n",
    "    # get param for pass to network\n",
    "    X_train, X_test = params['embeddings']\n",
    "    lr = params['lr']\n",
    "    epochs = params['epochs']\n",
    "    optimizer = params['optimizer']\n",
    "    batch_size = params['batch_size']\n",
    "    layers_count = params['layers_count']\n",
    "    kernel_size = params['kernel_size']\n",
    "    stride = params['stride']\n",
    "    \n",
    "    init = params['init']\n",
    "    reg = params['reg']\n",
    "    norm = params['norm']\n",
    "    scheduler = params['scheduler']\n",
    "    \n",
    "    inputs_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    targets_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    inputs_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    targets_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    train = data_utils.TensorDataset(inputs_train, targets_train)\n",
    "    test = data_utils.TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "    trainset = torch.utils.data.DataLoader(train, shuffle=True, batch_size=batch_size)\n",
    "    testset = torch.utils.data.DataLoader(test, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    # net build\n",
    "    net = Net(embed_dim=300, \n",
    "              conv_layer_count=layers_count, \n",
    "              stride=stride, \n",
    "              kernel_size=kernel_size,\n",
    "              init=init, \n",
    "              reg=reg,\n",
    "              norm=norm,\n",
    "              bs=batch_size)\n",
    "    net.to(device)\n",
    "    \n",
    "    # fit\n",
    "    net = fit(net, epochs, trainset, optimizer, lr, scheduler, 0.01 if reg == 'l2_reg' else 0, False)\n",
    "    \n",
    "    # predict\n",
    "    ans = predict(net, testset)\n",
    "    \n",
    "    # add param in config\n",
    "    config = [epochs, optimizer.__name__, batch_size, \n",
    "              layers_count, kernel_size, stride, \n",
    "              init.__name__, reg, norm.__name__, scheduler.__name__, f'{f1_score(y_test, ans, average=\"weighted\"):.5f}']\n",
    "    configs.append(config)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d77f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T08:56:01.521968Z",
     "start_time": "2023-05-29T08:56:01.493964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layers_count</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>stride</th>\n",
       "      <th>init</th>\n",
       "      <th>reg</th>\n",
       "      <th>norm</th>\n",
       "      <th>scheduler</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kaiming_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>ExponentialLR</td>\n",
       "      <td>0.88000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kaiming_uniform_</td>\n",
       "      <td>l2_reg</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.88000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>xavier_uniform_</td>\n",
       "      <td>l2_reg</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>ExponentialLR</td>\n",
       "      <td>0.82022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>xavier_uniform_</td>\n",
       "      <td>l2_reg</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.80032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>xavier_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kaiming_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.64058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>20</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kaiming_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>ExponentialLR</td>\n",
       "      <td>0.64000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>xavier_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.63825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>xavier_uniform_</td>\n",
       "      <td>dropout</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.62046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>20</td>\n",
       "      <td>AdamW</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>kaiming_uniform_</td>\n",
       "      <td>l2_reg</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>MultiStepLR</td>\n",
       "      <td>0.60000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs optimizer  batch_size  layers_count  kernel_size  stride  \\\n",
       "0       10     AdamW         512             3            3       1   \n",
       "1       10     AdamW         512             3            3       1   \n",
       "2       20     AdamW         512             3            3       1   \n",
       "3       10     AdamW         512             3            3       1   \n",
       "4       10     AdamW         512             3            3       1   \n",
       "..     ...       ...         ...           ...          ...     ...   \n",
       "59      20     AdamW         512             3            3       1   \n",
       "60      20     AdamW         512             3            3       1   \n",
       "61      20     AdamW         512             3            3       1   \n",
       "62      10     AdamW         512             3            3       1   \n",
       "63      20     AdamW         512             3            3       1   \n",
       "\n",
       "                init      reg         norm      scheduler f1_score  \n",
       "0   kaiming_uniform_  dropout    LayerNorm  ExponentialLR  0.88000  \n",
       "1   kaiming_uniform_   l2_reg    LayerNorm    MultiStepLR  0.88000  \n",
       "2    xavier_uniform_   l2_reg  BatchNorm1d  ExponentialLR  0.82022  \n",
       "3    xavier_uniform_   l2_reg    LayerNorm    MultiStepLR  0.80032  \n",
       "4    xavier_uniform_  dropout  BatchNorm1d    MultiStepLR  0.80000  \n",
       "..               ...      ...          ...            ...      ...  \n",
       "59  kaiming_uniform_  dropout  BatchNorm1d    MultiStepLR  0.64058  \n",
       "60  kaiming_uniform_  dropout  BatchNorm1d  ExponentialLR  0.64000  \n",
       "61   xavier_uniform_  dropout  BatchNorm1d    MultiStepLR  0.63825  \n",
       "62   xavier_uniform_  dropout  BatchNorm1d    MultiStepLR  0.62046  \n",
       "63  kaiming_uniform_   l2_reg  BatchNorm1d    MultiStepLR  0.60000  \n",
       "\n",
       "[64 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(configs)\n",
    "df.columns = ['epochs', 'optimizer', 'batch_size', 'layers_count', \n",
    "              'kernel_size', 'stride', 'init', 'reg', 'norm', 'scheduler', 'f1_score']\n",
    "df.sort_values(by='f1_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825772e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0d470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
